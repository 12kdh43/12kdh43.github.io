<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Search Result</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
	
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="" />
    <link rel="shortcut icon" href="http://0.0.0.0:4000/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="http://0.0.0.0:4000/search" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Darron's Devlog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Search Result" />
    <meta property="og:description" content="" />
    <meta property="og:url" content="http://0.0.0.0:4000/search" />
    <meta property="og:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Search Result" />
    <meta name="twitter:description" content="" />
    <meta name="twitter:url" content="http://0.0.0.0:4000/" />
    <meta name="twitter:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Darron's Devlog" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="666" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Darron's Devlog",
        "logo": "http://0.0.0.0:4000/"
    },
    "url": "http://0.0.0.0:4000/search",
    "image": {
        "@type": "ImageObject",
        "url": "http://0.0.0.0:4000/assets/built/images/blog-cover1.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://0.0.0.0:4000/search"
    },
    "description": ""
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Search Result" href="/feed.xml" />


</head>
<body class="page-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="/">Darron's Devlog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/jekyll/">Jekyll</a></li>
    <li class="nav-projects" role="menuitem"><a href="/tag/projects/">Projects</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/studies/">Studies</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post page no-image">

            <header class="post-full-header">
                <h1 class="post-full-title">Search Result</h1>
            </header>

            

            <section class="post-full-content">
                <form action="/search" method="get" hidden="hidden">
    <label for="search-box"></label>
    <input type="text" id="search-box" name="query">
</form>

<ul class="mylist" id="search-results"></ul>

<script>
    window.store = {
    
    "helloworld": {
        "title": "Hello, World!",
        "author": "Darron Kwon",
        "category": "",
        "content": "My First Notebook from Docker-workspace.import tensorflow as tftf.__version__'2.4.1'",
        "url": "/helloworld"
    }
    ,
    
    "appml-assginment1": {
        "title": "AppML - Assignment1",
        "author": "Darron Kwon",
        "category": "",
        "content": "2021-1. Applied Machine Learning# This Python 3 environment comes with many helpful analytics libraries installed# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python# For example, here's several helpful packages to loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files are available in the read-only \"../input/\" directory# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport osfor dirname, _, filenames in os.walk('/kaggle/input'):    for filename in filenames:        print(os.path.join(dirname, filename))# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session/kaggle/input/dsc3011/sample_submission.csv/kaggle/input/dsc3011/X_test.csv/kaggle/input/dsc3011/y_train.csv/kaggle/input/dsc3011/X_train.csv1. Load DataX_train = pd.read_csv(\"/kaggle/input/dsc3011/X_train.csv\")y_train = pd.read_csv(\"/kaggle/input/dsc3011/y_train.csv\")X_test = pd.read_csv(\"/kaggle/input/dsc3011/X_test.csv\")sample = pd.read_csv(\"/kaggle/input/dsc3011/sample_submission.csv\")X_train.head(5)                   id      age      sex      cp      trestbps      chol      fbs      restecg      thalach      exang      oldpeak      slope      ca      thal                  0      1      63      1      3      145      233      1      0      150      0      2.3      0      0      1              1      2      37      1      2      130      250      0      1      187      0      3.5      0      0      2              2      3      41      0      1      130      204      0      0      172      0      1.4      2      0      2              3      4      56      1      1      120      236      0      1      178      0      0.8      2      0      2              4      5      57      0      0      120      354      0      1      163      1      0.6      2      0      2      y_train.head()                  id      target                  0      1      1              1      2      1              2      3      1              3      4      1              4      5      1      2. Statistics &amp; Visualization# data typeslen(X_train), X_train.dtypes(233, id            int64 age           int64 sex           int64 cp            int64 trestbps      int64 chol          int64 fbs           int64 restecg       int64 thalach       int64 exang         int64 oldpeak     float64 slope         int64 ca            int64 thal          int64 dtype: object)# no missing valuesX_train.isnull().sum(axis=0)id          0age         0sex         0cp          0trestbps    0chol        0fbs         0restecg     0thalach     0exang       0oldpeak     0slope       0ca          0thal        0dtype: int64y_train['target'].value_counts()1    1330    100Name: target, dtype: int64# count uniques for categorical variablescats = ['sex','cp','fbs','restecg','exang','slope','ca','thal']for c in cats:    print(X_train[c].value_counts())1    1580     75Name: sex, dtype: int640    1072     721     363     18Name: cp, dtype: int640    1971     36Name: fbs, dtype: int640    1251    108Name: restecg, dtype: int640    1541     79Name: exang, dtype: int642    1101    1060     17Name: slope, dtype: int640    1371     492     273     184      2Name: ca, dtype: int642    1273     951     100      1Name: thal, dtype: int64import matplotlib.pyplot as pltfig, axes = plt.subplots(2, 4, figsize=(15, 10))i=0for c in cats:    axes[i//4,i%4].hist(X_train[c])    i+=1# statistics for numerical variablesnums = ['age','trestbps','chol','thalach', 'oldpeak']fig, axes = plt.subplots(2, 3, figsize=(15, 10))i=0for c in nums:    axes[i//3,i%3].hist(X_train.loc[:,c])    i+=1pd.concat([X_train[nums],y_train['target']],axis=1).corr()                  age      trestbps      chol      thalach      oldpeak      target                  age      1.000000      0.293608      0.309546      -0.386512      0.200013      -0.246499              trestbps      0.293608      1.000000      0.154624      -0.055758      0.203551      -0.157073              chol      0.309546      0.154624      1.000000      -0.054981      0.029328      -0.115616              thalach      -0.386512      -0.055758      -0.054981      1.000000      -0.346992      0.435763              oldpeak      0.200013      0.203551      0.029328      -0.346992      1.000000      -0.420039              target      -0.246499      -0.157073      -0.115616      0.435763      -0.420039      1.000000      3. Preprocessing &amp; Classification Accuracy3.1. Naive Bayes w/o preprocessing as a baselinefrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.naive_bayes import GaussianNBX_train = pd.read_csv(\"/kaggle/input/dsc3011/X_train.csv\")y_train = pd.read_csv(\"/kaggle/input/dsc3011/y_train.csv\")X_test = pd.read_csv(\"/kaggle/input/dsc3011/X_test.csv\")X_tr = X_train.drop(columns=[\"id\"])X_te = X_test.drop(columns=[\"id\"])y_tr = y_train['target'].valuesX_t,X_v,y_t,y_v = train_test_split(X_tr,y_tr, test_size=0.2, random_state=2021 )gnb = GaussianNB()gnb.fit(X_t, y_t)y_pred = gnb.predict(X_v)print(accuracy_score(y_v,y_pred))0.851063829787234gnb=GaussianNB()gnb.fit(X_tr,y_tr)y_pred = gnb.predict(X_te)submission0 = pd.DataFrame(X_test['id'])submission0['target'] = y_predsubmission0.to_csv(\"submission0.csv\",sep=',',index=False)3.2. One-hot encodingX_train = pd.read_csv(\"/kaggle/input/dsc3011/X_train.csv\",dtype={c:str for c in cats})y_train = pd.read_csv(\"/kaggle/input/dsc3011/y_train.csv\")X_test = pd.read_csv(\"/kaggle/input/dsc3011/X_test.csv\",dtype={c:str for c in cats})X_train = pd.concat([X_train,pd.get_dummies(X_train[cats],drop_first=True)],axis=1).drop(columns=cats)X_test = pd.concat([X_test,pd.get_dummies(X_test[cats],drop_first=True)],axis=1).drop(columns=cats)print(X_train.columns, X_test.columns)X_train['restecg_2']=0X_train.shape, X_test.shapeIndex(['id', 'age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'sex_1', 'cp_1',       'cp_2', 'cp_3', 'fbs_1', 'restecg_1', 'exang_1', 'slope_1', 'slope_2',       'ca_1', 'ca_2', 'ca_3', 'ca_4', 'thal_1', 'thal_2', 'thal_3'],      dtype='object') Index(['id', 'age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'sex_1', 'cp_1',       'cp_2', 'cp_3', 'fbs_1', 'restecg_1', 'restecg_2', 'exang_1', 'slope_1',       'slope_2', 'ca_1', 'ca_2', 'ca_3', 'ca_4', 'thal_1', 'thal_2',       'thal_3'],      dtype='object')((233, 23), (70, 23))X_tr = X_train.drop(columns=[\"id\"])X_te = X_test.drop(columns=[\"id\"])y_tr = y_train['target'].valuesX_t, X_v, y_t, y_v = train_test_split(X_tr,y_tr,test_size = 0.2,random_state=2021)gnb = GaussianNB()gnb.fit(X_t, y_t)y_pred = gnb.predict(X_v)print(accuracy_score(y_v,y_pred))0.8085106382978723gnb=GaussianNB()gnb.fit(X_tr,y_tr)y_pred = gnb.predict(X_te)submission1 = pd.DataFrame(X_test['id'])submission1['target'] = y_predsubmission1.to_csv(\"submission1.csv\",sep=',',index=False)3.2.1. One-Hot + StandardScalerfrom sklearn.preprocessing import StandardScalersc = StandardScaler()X_tr_pp = X_tr.copy()X_te_pp = X_te.copy()X_tr_pp[nums] = sc.fit_transform(X_tr_pp[nums])X_te_pp[nums] = sc.fit_transform(X_te_pp[nums])X_tr_pp[nums].describe()                  age      trestbps      chol      thalach      oldpeak                  count      2.330000e+02      2.330000e+02      2.330000e+02      2.330000e+02      2.330000e+02              mean      -9.309434e-17      -7.881154e-16      -1.405647e-16      1.038749e-16      1.777310e-16              std      1.002153e+00      1.002153e+00      1.002153e+00      1.002153e+00      1.002153e+00              min      -2.812990e+00      -2.108738e+00      -2.400765e+00      -2.841943e+00      -9.056098e-01              25%      -6.969140e-01      -6.559080e-01      -7.090580e-01      -5.598232e-01      -9.056098e-01              50%      -2.867959e-02      -9.712714e-02      -9.389202e-02      2.008834e-01      -2.149156e-01              75%      7.509272e-01      4.616537e-01      5.020500e-01      7.378527e-01      4.757786e-01              max      2.532886e+00      3.814339e+00      6.019320e+00      2.259266e+00      4.447270e+00      X_t, X_v, y_t, y_v = train_test_split(X_tr_pp,y_tr,test_size = 0.2,random_state=2021)gnb = GaussianNB()gnb.fit(X_t, y_t)y_pred = gnb.predict(X_v)print(accuracy_score(y_v,y_pred))0.8085106382978723gnb=GaussianNB()gnb.fit(X_tr_pp,y_tr)y_pred = gnb.predict(X_te_pp)submission2 = pd.DataFrame(X_test['id'])submission2['target'] = y_predsubmission2.to_csv(\"submission2.csv\",sep=',',index=False)3.2.2 One-hot + Log-transformX_tr_log = X_tr.copy()X_te_log = X_te.copy()for c in nums:    X_tr_log[c] = np.log(X_tr_log[c]+1)    X_te_log[c] = np.log(X_te_log[c]+1)X_tr_log[nums].describe()                  age      trestbps      chol      thalach      oldpeak                  count      233.000000      233.000000      233.000000      233.000000      233.000000              mean      3.998016      4.879579      5.509133      5.015565      0.575909              std      0.170517      0.132206      0.197961      0.156684      0.522850              min      3.401197      4.553877      4.844187      4.488636      0.000000              25%      3.891820      4.795791      5.370638      4.941642      0.000000              50%      4.007333      4.875197      5.509388      5.056246      0.587787              75%      4.127134      4.948760      5.627621      5.129899      0.955511              max      4.356709      5.303305      6.336826      5.313206      1.974081      X_t, X_v, y_t, y_v = train_test_split(X_tr_log,y_tr,test_size = 0.2,random_state=2021)gnb = GaussianNB()gnb.fit(X_t, y_t)y_pred = gnb.predict(X_v)print(accuracy_score(y_v,y_pred))0.8085106382978723gnb=GaussianNB()gnb.fit(X_tr_log,y_tr)y_pred = gnb.predict(X_te_log)submission3 = pd.DataFrame(X_test['id'])submission3['target'] = y_predsubmission3.to_csv(\"submission3.csv\",sep=',',index=False)3.4. Varrying Classification Methodsfrom sklearn.model_selection import cross_val_scorefrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCVset1= X_tr,X_teset2= X_tr_pp, X_te_ppset3= X_tr_log, X_te_logdef grid_selection(set):    knn_grid =[        {'n_neighbors': range(3,15,1), 'p': [1,2]}]    knn = KNeighborsClassifier()    grid_knn = GridSearchCV(knn, knn_grid, cv=5,                               scoring='neg_mean_squared_error',                               return_train_score=True)    grid_knn.fit(X_tr,y_tr)        rf_grid =[        {'n_estimators': [5, 10, 20, 50, 75, 100], 'max_features': [2, 4, 6, 8]}]    rf = RandomForestClassifier()    grid_rf = GridSearchCV(rf, rf_grid, cv=5,                               scoring='neg_mean_squared_error',                               return_train_score=True)    grid_rf.fit(set[0],y_tr)    return grid_knn.best_estimator_, grid_rf.best_estimator_def rmse_scores(set):    gnb = GaussianNB()    svc = SVC()    dt = DecisionTreeClassifier()    knn, rf = grid_selection(set)    models = [gnb, svc, dt, knn, rf]    results = {}    for model in models:        scores = cross_val_score(model, set[0],y_tr,                                scoring=\"neg_mean_squared_error\",cv=10)        results[model] = np.sqrt(-scores).mean()    return resultsset1_result = rmse_scores(set1)set2_result = rmse_scores(set2)set3_result = rmse_scores(set3)set1_result, set2_result, set3_result({GaussianNB(): 0.4178876039491642,  SVC(): 0.5823397149422178,  DecisionTreeClassifier(): 0.4729488489553738,  KNeighborsClassifier(n_neighbors=13, p=1): 0.5371866622228357,  RandomForestClassifier(max_features=2, n_estimators=20): 0.38835845413636216}, {GaussianNB(): 0.4415320745933398,  SVC(): 0.4067235886003079,  DecisionTreeClassifier(): 0.4586168121445854,  KNeighborsClassifier(n_neighbors=13, p=1): 0.413359640076294,  RandomForestClassifier(max_features=2, n_estimators=10): 0.483896282692258}, {GaussianNB(): 0.4415320745933398,  SVC(): 0.3752652644845328,  DecisionTreeClassifier(): 0.46699040892811083,  KNeighborsClassifier(n_neighbors=13, p=1): 0.38902561032241134,  RandomForestClassifier(max_features=4): 0.43207507210776175})print(min(set1_result.items(), key = lambda x: x[1]))print(min(set2_result.items(), key = lambda x: x[1]))print(min(set3_result.items(), key = lambda x: x[1]))(RandomForestClassifier(max_features=2, n_estimators=20), 0.38835845413636216)(SVC(), 0.4067235886003079)(SVC(), 0.3752652644845328)4. Predictions from the best classifiersrf = RandomForestClassifier(max_features=2, n_estimators=50)rf.fit(X_tr_pp,y_tr)y_pred = rf.predict(X_te_pp)submission4 = pd.DataFrame(X_test['id'])submission4['target'] = y_predsubmission4.to_csv(\"submission4.csv\",sep=',',index=False)# grid to SVC# C: float, default=1.0# kernel: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’# gamma: {‘scale’, ‘auto’} or float, default=’scale’X_t, X_v, y_t, y_v = train_test_split(X_tr_log,y_tr,test_size = 0.2,random_state=2021)svc = SVC(C=0.3)svc.fit(X_t, y_t)y_pred = svc.predict(X_v)print(accuracy_score(y_v,y_pred))0.8723404255319149svc = SVC()svc.fit(X_tr_log,y_tr)y_pred = svc.predict(X_te_log)submission5 = pd.DataFrame(X_test['id'])submission5['target'] = y_predsubmission5.to_csv(\"submission5.csv\",sep=',',index=False)5. Other Algorithms: Logistic RegressionX_t, X_v, y_t, y_v = train_test_split(X_tr_pp,y_tr,test_size = 0.2,random_state=2021)lr_clf = LogisticRegression()lr_clf.fit(X_t, y_t)y_pred = lr_clf.predict(X_v)print(accuracy_score(y_v,y_pred))0.8723404255319149lr_clf=LogisticRegression()lr_scores = cross_val_score(lr_clf, X_tr_pp,y_tr,                        scoring=\"neg_mean_squared_error\",cv=10)np.sqrt(-lr_scores).mean()0.3575812829170383lr_clf=LogisticRegression()lr_clf.fit(X_tr_pp, y_tr)y_pred = lr_clf.predict(X_te_pp)submission6 = pd.DataFrame(X_test['id'])submission6['target'] = y_predsubmission6.to_csv(\"submission6.csv\",sep=',',index=False)",
        "url": "/AppML_Assginment1"
    }
    ,
    
    "ds-r-final": {
        "title": "Data Science and R - Final",
        "author": "Darron Kwon",
        "category": "",
        "content": "  1. About  2. Load Data and Preprocess          2.1. Set Attributes      2.2. Handling NA values        3. EDA          3.1. 통합대기환경지수(Comprehensive air-quality index), CAI 계산      3.2. Timestep-wise Visualization                  3.2.1. Hourly          3.2.2. Daily          3.2.3. Montly          3.2.4. Quarterly          3.2.5. Half-yearly                      4. Summary1. About  2020-2, Data Science and R, Final Proejct: 대기환경오염도 데이터 분석2. Load Data and Preprocess  Collected data from Air Korea (https://www.airkorea.or.kr)library(dplyr)library(tidyverse)library(grid)library(vcd)library(ggplot2)library(readxl)data= read_xlsx(\"./final_data/data.xlsx\");head(data,5)nrow(data); names(data)날짜시도측정소명측정소코드아황산가스일산화탄소오존이산화질소PM10PM2.5\t2017-07-01 01인천 옹진군  백령도       831492       .0016        .3           .052         .0016        44           35           \t2017-07-01 02인천 옹진군  백령도       831492       .0016        .3           .052         .0017        21           NA           \t2017-07-01 03인천 옹진군  백령도       831492       .0017        .3           .051         .0017        32           18           \t2017-07-01 04인천 옹진군  백령도       831492       .0017        .3           .05          .002         10           NA           \t2017-07-01 05인천 옹진군  백령도       831492       .0016        .3           .048         .0018        24           22           26304&lt;ol class=list-inline&gt;\t&lt;li&gt;‘날짜’&lt;/li&gt;\t&lt;li&gt;‘시도’&lt;/li&gt;\t&lt;li&gt;‘측정소명’&lt;/li&gt;\t&lt;li&gt;‘측정소코드’&lt;/li&gt;\t&lt;li&gt;‘아황산가스’&lt;/li&gt;\t&lt;li&gt;‘일산화탄소’&lt;/li&gt;\t&lt;li&gt;‘오존’&lt;/li&gt;\t&lt;li&gt;‘이산화질소’&lt;/li&gt;\t&lt;li&gt;‘PM10’&lt;/li&gt;\t&lt;li&gt;‘PM2.5’&lt;/li&gt;&lt;/ol&gt;2.1. Set Attributes#   시도, 측정소명, 측정소코드는 모두 같으므로 제외#   날짜는 시구간별 분석을 위해 연, 월, 일, 시로 분리: int타입으로 반환됨#   측정 대상 오염 물질의 자료형 변환data &lt;- data %&gt;%  select(-c(시도,측정소명,측정소코드)) %&gt;%  separate(col=날짜,           into=c(\"year\",\"month\",\"day\",\"hour\"),           convert=TRUE) %&gt;%  mutate_if(is.character,as.numeric) %&gt;%  rename(SO2=아황산가스,CO=일산화탄소,O3=오존,NO2=이산화질소)head(data,5)yearmonthdayhourSO2COO3NO2PM10PM2.5\t2017  7     1     1     0.00160.3   0.052 0.001644    35    \t2017  7     1     2     0.00160.3   0.052 0.001721    NA    \t2017  7     1     3     0.00170.3   0.051 0.001732    18    \t2017  7     1     4     0.00170.3   0.050 0.002010    NA    \t2017  7     1     5     0.00160.3   0.048 0.001824    22    2.2. Handling NA values# 일평균 값을 확인daily &lt;- data %&gt;%  group_by(year,month,day) %&gt;%  select_if(is.double) %&gt;%  summarise_all(mean,na.rm=TRUE) %&gt;%  ungroup()summary(daily)      year          month             day             SO2            Min.   :2017   Min.   : 1.000   Min.   : 1.00   Min.   :0.0001765   1st Qu.:2018   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.:0.0012553   Median :2018   Median : 7.000   Median :16.00   Median :0.0017042   Mean   :2018   Mean   : 6.522   Mean   :15.73   Mean   :0.0018339   3rd Qu.:2019   3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.:0.0022417   Max.   :2020   Max.   :12.000   Max.   :31.00   Max.   :0.0078375                                                   NA's   :1                 CO                O3               NO2                PM10        Min.   :0.04583   Min.   :0.01188   Min.   :0.000850   Min.   :  2.00   1st Qu.:0.23750   1st Qu.:0.03453   1st Qu.:0.002373   1st Qu.: 20.88   Median :0.32917   Median :0.04290   Median :0.003296   Median : 29.25   Mean   :0.37287   Mean   :0.04405   Mean   :0.003853   Mean   : 35.58   3rd Qu.:0.46667   3rd Qu.:0.05192   3rd Qu.:0.004764   3rd Qu.: 42.54   Max.   :1.39167   Max.   :0.09971   Max.   :0.020209   Max.   :217.04   NA's   :2                                              NA's   :5            PM2.5         Min.   :  1.826   1st Qu.: 10.042   Median : 14.625   Mean   : 18.736   3rd Qu.: 22.375   Max.   :126.375   NA's   :23       # NA: 하루 전체 관측치가 NA인 경우에 발생# 월평균 값을 확인monthly &lt;- data %&gt;%  group_by(year,month) %&gt;%  select_if(is.double) %&gt;%  summarise_all(mean,na.rm=TRUE) %&gt;%  ungroup()summary(monthly)      year          month            SO2                 CO         Min.   :2017   Min.   : 1.00   Min.   :0.001099   Min.   :0.1911   1st Qu.:2018   1st Qu.: 3.75   1st Qu.:0.001394   1st Qu.:0.2646   Median :2018   Median : 6.50   Median :0.001789   Median :0.3562   Mean   :2018   Mean   : 6.50   Mean   :0.001832   Mean   :0.3730   3rd Qu.:2019   3rd Qu.: 9.25   3rd Qu.:0.002084   3rd Qu.:0.4491   Max.   :2020   Max.   :12.00   Max.   :0.003140   Max.   :0.7588         O3               NO2                PM10           PM2.5       Min.   :0.02447   Min.   :0.001857   Min.   :20.24   Min.   :10.62   1st Qu.:0.03821   1st Qu.:0.002876   1st Qu.:27.31   1st Qu.:13.94   Median :0.04175   Median :0.004066   Median :35.42   Median :18.41   Mean   :0.04410   Mean   :0.003850   Mean   :35.69   Mean   :18.92   3rd Qu.:0.05154   3rd Qu.:0.004658   3rd Qu.:42.46   3rd Qu.:22.84   Max.   :0.06422   Max.   :0.005683   Max.   :58.00   Max.   :36.51  # NA값들을 일평균 또는 월평균 값으로 대체for (i in (1:nrow(data))){  for (j in c(\"SO2\",\"CO\",\"O3\",\"NO2\",\"PM10\",\"PM2.5\")){    if (is.na(data[i,][[j]])){      tmp&lt;-data[i,]      y&lt;-tmp$year      m&lt;-tmp$month      d&lt;-tmp$day      if (!is.nan(daily[daily$year==y &amp; daily$month==m &amp; daily$day==d,][[j]])){        data[i,][[j]]&lt;-daily[daily$year==y &amp; daily$month==m &amp; daily$day==d,][[j]]      }else{        data[i,][[j]]&lt;-monthly[monthly$year==y &amp; monthly$month==m,][[j]]      }    }  }}summary(data)      year          month             day             hour       Min.   :2017   Min.   : 1.000   Min.   : 1.00   Min.   : 1.00   1st Qu.:2018   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.: 6.75   Median :2018   Median : 7.000   Median :16.00   Median :12.50   Mean   :2018   Mean   : 6.522   Mean   :15.73   Mean   :12.50   3rd Qu.:2019   3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.:18.25   Max.   :2020   Max.   :12.000   Max.   :31.00   Max.   :24.00        SO2                 CO               O3               NO2           Min.   :0.000000   Min.   :0.0000   Min.   :0.00100   Min.   :0.000300   1st Qu.:0.001200   1st Qu.:0.2000   1st Qu.:0.03400   1st Qu.:0.002100   Median :0.001700   Median :0.3000   Median :0.04300   Median :0.003000   Mean   :0.001834   Mean   :0.3726   Mean   :0.04405   Mean   :0.003853   3rd Qu.:0.002300   3rd Qu.:0.5000   3rd Qu.:0.05200   3rd Qu.:0.004600   Max.   :0.034700   Max.   :2.5000   Max.   :0.13200   Max.   :0.045900        PM10            PM2.5        Min.   :  1.00   Min.   :  0.00   1st Qu.: 19.00   1st Qu.:  9.00   Median : 28.00   Median : 14.00   Mean   : 35.57   Mean   : 18.78   3rd Qu.: 43.00   3rd Qu.: 22.77   Max.   :461.00   Max.   :219.00  3. EDA3.1. 통합대기환경지수(Comprehensive air-quality index), CAI 계산### 통합대기환경지수CAI 지수를 계산# 항목별 계산 함수를 정의SO2_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,0.02,0.05,0.15,1,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0.02,0.02,0.05,0.15,1,1))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,0.021,0.051,0.151,0.151))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (((I_hi-I_lo)/(BP_hi-BP_lo)) * (C_p - BP_lo)) + I_lo  return(I_p)}CO_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,2,9,15,50,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(2,2,9,15,50,50))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,2.01,9.01,15.01,15.01))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}O3_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,0.03,0.09,0.15,0.6,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0.03,0.03,0.09,0.15,0.6,0.6))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,0.031,0.091,0.151,0.151))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}NO2_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,0.03,0.06,0.2,2,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0.03,0.03,0.06,0.2,2,2))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,0.031,0.061,0.201,0.201))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}# PM10, PM2.5의 이동평균을 구하기 위한 함수 정의C12_cal &lt;- function(PM){  C12 &lt;- c()  for (i in (1:length(PM))){    if (i&lt;12){      tmp &lt;- sum(PM[1:i])/i    }else{      tmp &lt;- sum(PM[(i-11):i])/12    }    C12[i] &lt;- tmp  }  return (C12)}C_ai_cal &lt;- function(C_i,M,C12){  if (C_i &lt; M){    C_ai &lt;- C_i  }else if(0.9&lt;= (C_i/C12) &amp;&amp; (C_i/C12) &lt;=1.7){    C_ai &lt;- 0.75*C_i    }else {    C_ai &lt;- C_i  }  return (C_ai)}PM_cal &lt;- function(PM,m,PM_C12){  M=m  C24E=c()  for (i in (1:length(PM))){    if (i&lt;4){      C4 &lt;-0      for (j in (i:1)){        C4 &lt;- C4+ C_ai_cal(PM[j],M,PM_C12[j])      }      C4/i    } else{      C4 &lt;- sum(C_ai_cal(PM[i],M,PM_C12[i]),C_ai_cal(PM[i-1],M,PM_C12[i-1]),              C_ai_cal(PM[i-2],M,PM_C12[i-2]),C_ai_cal(PM[i-3],M,PM_C12[i-3]))/4    }    C12&lt;-PM_C12[i]    C24E[i] = (C12*12+C4*12)/24  }  return (C24E)}PM10_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,30,80,150,600,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE,                                       labels=c(30,30,80,150,600,600))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE,                                       labels=c(0,0,31,81,151,151))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE,                                      labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE,                                      labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}PM2.5_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,15,35,75,500,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(15,15,35,75,500,500))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,16,36,76,76))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}PM10_C12&lt;-C12_cal(data$PM10)PM10_C_p &lt;- PM_cal(data$PM10,70,PM10_C12)PM2.5_C12 &lt;- C12_cal(data$PM2.5)PM2.5_C_p &lt;- PM_cal(data$PM2.5,30,PM2.5_C12)data &lt;- data %&gt;%  bind_cols(\"SO2_I_p\" = SO2_I_p(data$SO2)) %&gt;%  bind_cols(\"CO_I_p\" = CO_I_p(data$CO)) %&gt;%  bind_cols(\"O3_I_p\" = O3_I_p(data$O3)) %&gt;%  bind_cols(\"NO2_I_p\" = NO2_I_p(data$NO2)) %&gt;%  bind_cols(\"PM10_I_p\" = PM10_I_p(PM10_C_p)) %&gt;%  bind_cols(\"PM2.5_I_p\" = PM2.5_I_p(PM2.5_C_p))CAI_table &lt;- data %&gt;%  select(ends_with(\"I_p\")) %&gt;%  mutate(BAD = rowSums(. &gt;100)) %&gt;%  mutate(CAI = apply(., 1, max) + case_when(BAD&gt;=3 ~ 75, BAD&gt;=2 ~ 50,TRUE ~ 0)) %&gt;%  mutate(CAI_index = case_when(CAI&gt;250 ~ 3, CAI&gt;100 ~ 2,                               CAI&gt;50 ~ 1, TRUE ~ 0)) %&gt;%  select(starts_with(\"CAI\"))data &lt;- data %&gt;%  bind_cols(CAI_table)head(data[c(11:18)],5)SO2_I_pCO_I_pO3_I_pNO2_I_pPM10_I_pPM2.5_I_pCAICAI_index\t4.00     7.5      68.44068 2.666667 64.00000  88.71711 88.717111        \t4.00     7.5      68.44068 2.833333 68.75000 103.38782103.387822        \t4.25     7.5      67.61017 2.833333 84.66667 131.82942131.829422        \t4.25     7.5      66.77966 3.333333 44.58333  66.44682 66.779661        \t4.00     7.5      65.11864 3.000000 39.95833  64.79737 65.118641        3.2. Timestep-wise Visualization  시구간별 시각화 및 분석3.2.1. Hourly# 시간대(주간: 07~18, 야간: 19~06)에 따른 CAI 지수 분포CAI_hourly &lt;- data %&gt;%  mutate(time_tmp = case_when(    06&lt;hour &amp; hour&lt;19 ~ \"daytime\", TRUE ~ \"nighttime\")) %&gt;%  group_by(year,month,day,time_tmp) %&gt;%  summarise(mean_CAI=mean(CAI)) %&gt;%  mutate(CAI_idx = case_when(mean_CAI&gt;250 ~ 3, mean_CAI&gt;100 ~ 2,                             mean_CAI&gt;50 ~ 1, TRUE ~ 0)) %&gt;% # 해당 시간대의 평균 오염도  ungroup()# 시간에 따른 오염도의 분포는 거의 차이가 없음을 확인CAI_hourly %&gt;%  group_by(time_tmp,CAI_idx)%&gt;%  tally()`summarise()` has grouped output by 'year', 'month', 'day'. You can override using the `.groups` argument.time_tmpCAI_idxn\tdaytime  0        100      \tdaytime  1        886      \tdaytime  2         93      \tdaytime  3         17      \tnighttime0         73      \tnighttime1        895      \tnighttime2        115      \tnighttime3         13      3.2.2. Daily### 일단위: 일평균 CAI 지수의 분포CAI_daily &lt;- data %&gt;%  group_by(year,month,day) %&gt;%  summarise_at(.vars=\"CAI\", .funs=mean) %&gt;%  ungroup() %&gt;%  mutate(CAI_idx = case_when(CAI&gt;250 ~ 3, CAI&gt;100 ~ 2,                             CAI&gt;50 ~ 1, TRUE ~ 0))summary(factor(CAI_daily$CAI_idx))# 평균 수준보다 오염도가 높은 날들은 연중 특정 시기에 집중되어있음CAI_daily %&gt;%  unite(\"date\",1:3,sep=\"/\") %&gt;%  mutate_at(\"date\",as.Date) %&gt;%  ggplot() +  geom_point(mapping=aes(x=date,y=CAI))&lt;dl class=dl-horizontal&gt;\t&lt;dt&gt;0&lt;/dt&gt;\t\t&lt;dd&gt;84&lt;/dd&gt;\t&lt;dt&gt;1&lt;/dt&gt;\t\t&lt;dd&gt;885&lt;/dd&gt;\t&lt;dt&gt;2&lt;/dt&gt;\t\t&lt;dd&gt;113&lt;/dd&gt;\t&lt;dt&gt;3&lt;/dt&gt;\t\t&lt;dd&gt;14&lt;/dd&gt;&lt;/dl&gt;3.2.3. Montly### 월단위: 오염 수준 나쁨 이상인 \"일수\" 확인CAI_monthly &lt;- CAI_daily %&gt;%  group_by(year,month) %&gt;%  summarise(mean_CAI = mean(CAI),sum_CAI_idx = sum(CAI_idx&gt;1)) %&gt;%  mutate(m_tmp = case_when(month&lt;10 ~ paste(\"0\",as.character(month),sep=\"\"),                           TRUE ~ paste(as.character(month))))%&gt;%  mutate(y_tmp=as.character(year)) %&gt;%  unite(date,y_tmp,m_tmp,sep=\"\")# 6월~9월은 장마 영향으로 추정. 겨울~봄에 집중적.CAI_monthly %&gt;%  ggplot() +  geom_col(mapping=aes(x=date,y=sum_CAI_idx))`summarise()` has grouped output by 'year'. You can override using the `.groups` argument.3.2.4. Quarterly### 분기 단위CAI_qtr &lt;- CAI_monthly %&gt;%  mutate(qtr_tmp = case_when(    month &lt; 4 ~ \"Q1\", month &lt; 7 ~ \"Q2\", month &lt; 10 ~ \"Q3\", TRUE ~ \"Q4\")) %&gt;%  mutate(y_tmp=as.character(year)) %&gt;%  unite(Qtr, y_tmp, qtr_tmp, sep=\"-\") %&gt;%  group_by(Qtr) %&gt;%  summarise(mean_CAI = mean(mean_CAI), sum_CAI_idx = sum(sum_CAI_idx))CAI_qtr %&gt;%  ggplot() +  geom_col(aes(x=Qtr,y=sum_CAI_idx))3.2.5. Half-yearly### 반기 단위: 2019년 상반기가 매우 심한 것이었음을 알 수 있음CAI_half &lt;- CAI_monthly %&gt;%  transform(date=as.integer(date)) %&gt;%  mutate(year = case_when(    date &lt; 201801 ~ \"2017_H2\", date &lt; 201807 ~ \"2018-H1\",    date &lt; 201901 ~ \"2018_H2\", date &lt; 201907 ~ \"2019-H1\",    date &lt; 202001 ~ \"2019-H2\", TRUE ~ \"2020-H1\")) %&gt;%  group_by(year) %&gt;%  summarise(mean_CAI = mean(mean_CAI), sum_CAI_idx = sum(sum_CAI_idx))CAI_half %&gt;%  ggplot()+  geom_col(aes(x=year,y=sum_CAI_idx))4. Summary대기오염도는 대체적으로 여름보다 겨울에 높았고, 2019년이 특히 심했다는 것을 알 수 있다. 2020년을 기준으로 비교했을 때, 코로나의 영향은 거의 없었던 것으로 추정된다.",
        "url": "/ds_R_final"
    }
    ,
    
    "nlp-final": {
        "title": "NLP - Final",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-1. NLP# 한국어 자료import sysimport osimport numpy as npimport nltkimport konlpyimport pandas as pdimport reimport randomimport itertoolsimport warningswarnings.filterwarnings(action='ignore')from sklearn.metrics import classification_report,f1_score,precision_score,recall_scorerandom.seed(1)# 데이터 로드train_data=pd.read_csv('final_train_data.csv')test_data=pd.read_csv('final_test_data.csv')# 384 duplicates in content, 240 in titleprint(train_data.describe())       category                                            content  titlecount     10686                                              10686  10686unique        7                                              10302  10124top        정치개혁  개인회생 36개월 단축소급 전국 적용을 위해 춘천지방법원의 법원에 바란다에 글을 올...   경남제약freq       3094                                                 16     21# preprocess: duplicatetrain_data = train_data.drop_duplicates(['content'],keep='first')# all duplicates removedtrain_data.duplicated().sum()0train_data['document']=train_data.iloc[:,1]+train_data.iloc[:,2]train_data['document']test_data['document']=test_data.iloc[:,1]+test_data.iloc[:,2]train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행tokenizing process with konlpy-Okt# tokenizefrom konlpy.tag import Oktokt = Okt() #형태소 분석기tokenized_data = []stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']for sentence in train_data['document']:    temp_X = okt.morphs(sentence, norm=True, stem=True) # 형태소 추출    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거    tokenized_data.append(temp_X)x_train=tokenized_dataokt = Okt() #형태소 분석기tokenized_data = []stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']for sentence in test_data['document']:    temp_X = okt.morphs(sentence, norm=True, stem=True) # 형태소 추출 - 토큰화 #norm=True : 근사어    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거 #https://www.ranks.nl/stopwords/korean    tokenized_data.append(temp_X)x_test=tokenized_datafor this tokenizing process takes a long time, save and import preprocessed data.import picklewith open('nlp_final_x_tr.data', 'wb') as f:    pickle.dump(x_train, f)with open('nlp_final_x_te.data', 'wb') as f:    pickle.dump(x_test, f)import picklewith open('nlp_final_x_tr.data', 'rb') as f:    x_train = pickle.load(f)with open('nlp_final_x_te.data', 'rb') as f:    x_test = pickle.load(f)from tensorflow.keras.preprocessing.text import Tokenizertokenizer = Tokenizer()tokenizer.fit_on_texts(x_train)threshold = 3total_cnt = len(tokenizer.word_index) # 단어의 수rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.for key, value in tokenizer.word_counts.items():    total_freq = total_freq + value    # 단어의 등장 빈도수가 threshold보다 작으면    if(value &lt; threshold):        rare_cnt = rare_cnt + 1        rare_freq = rare_freq + valueprint('단어 집합(vocabulary)의 크기 :',total_cnt)print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)단어 집합(vocabulary)의 크기 : 34537등장 빈도가 2번 이하인 희귀 단어의 수: 15483단어 집합에서 희귀 단어의 비율: 44.830182123519705전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.1132793250941202vocab_size = total_cnt - rare_cnt + 1 # 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거. 0번 패딩 토큰을 고려하여 +1print('단어 집합의 크기 :',vocab_size)단어 집합의 크기 : 19055tokenizer = Tokenizer(vocab_size) tokenizer.fit_on_texts(x_train)X_train = tokenizer.texts_to_sequences(x_train)X_test = tokenizer.texts_to_sequences(x_test)y_train=np.array(train_data.category)y_test=np.array(test_data.category)drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) &lt; 1]drop_test = [index for index, sentence in enumerate(X_test) if len(sentence) &lt; 1]X_train = np.delete(X_train, drop_train, axis=0)y_train = np.delete(y_train, drop_train, axis=0)print(len(X_train))print(len(y_train))1030110301X_test = np.delete(X_test, drop_test, axis=0)y_test = np.delete(y_test, drop_test, axis=0)print(len(X_train))print(len(X_test))print(len(y_train))print(len(y_test))103011158103011158import matplotlib.pyplot as pltprint('리뷰의 최대 길이 :',max(len(l) for l in X_train))print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))plt.hist([len(s) for s in X_train], bins=50)plt.xlabel('length of samples')plt.ylabel('number of samples')plt.show()리뷰의 최대 길이 : 9032리뷰의 평균 길이 : 170.45791670711583def below_threshold_len(max_len, nested_list):  cnt = 0  for s in nested_list:    if(len(s) &lt;= max_len):        cnt = cnt + 1  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))max_len = 600below_threshold_len(max_len, X_train)전체 샘플 중 길이가 600 이하인 샘플의 비율: 96.02951169789341from tensorflow.keras.preprocessing.sequence import pad_sequencesX_train = pad_sequences(X_train, maxlen = max_len)X_test = pad_sequences(X_test, maxlen = max_len)y_train=np.array(train_data.category)y_test=np.array(test_data.category)y_train = np.delete(y_train, drop_train, axis=0)y_test = np.delete(y_test, drop_test, axis=0)print(len(y_train),len(y_test))10301 1158t={'경제민주화': 1, '교통/건축/국토': 2, '보건복지': 3, '육아/교육': 4, '인권/성평등': 5, '일자리': 6, '정치개혁': 7}print(t['보건복지'])index1=np.zeros([10301,7])for i in range(len(y_train)):  index1[i][t[y_train[i]]-1]=1y_train=index1index1=np.zeros([1158,7])for i in range(len(y_test)):  index1[i][t[y_test[i]]-1]=1y_test=index1print(y_train.shape,y_test.shape)3(10301, 7) (1158, 7)X_train=np.array(X_train)X_test=np.array(X_test)y_train=np.array(y_train)y_test=np.array(y_test)print(X_train.shape)print(X_test.shape)print(y_train.shape)print(y_test.shape)print(vocab_size)(10301, 600)(1158, 600)(10301, 7)(1158, 7)19055from tensorflow.keras.layers import Embedding, Dense, LSTMfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.models import load_modelfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpointimport tensorflow_addons as tfaf1 = tfa.metrics.F1Score(num_classes=7,threshold=0.5)from tensorflow import keraskeras.__version__'2.6.0'model = Sequential()model.add(Embedding(vocab_size, 128))model.add(LSTM(128))model.add(Dense(7, activation='sigmoid'))es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=4)mc = ModelCheckpoint('best_model.h5', monitor=f1, mode='max', verbose=2, save_best_only=True)model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc',f1])model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=512, validation_split=0.2)Epoch 1/1517/17 [==============================] - 39s 2s/step - loss: 1.8842 - acc: 0.2830 - f1_score: 0.2583 - val_loss: 1.7326 - val_acc: 0.3081 - val_f1_score: 0.2726WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 2/1517/17 [==============================] - 38s 2s/step - loss: 1.6177 - acc: 0.4615 - f1_score: 0.3981 - val_loss: 1.5555 - val_acc: 0.4639 - val_f1_score: 0.3776WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 3/1517/17 [==============================] - 38s 2s/step - loss: 1.3590 - acc: 0.5495 - f1_score: 0.4018 - val_loss: 1.3879 - val_acc: 0.5075 - val_f1_score: 0.4232WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 4/1517/17 [==============================] - 38s 2s/step - loss: 1.1364 - acc: 0.6211 - f1_score: 0.4351 - val_loss: 1.3140 - val_acc: 0.5303 - val_f1_score: 0.4173WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 5/1517/17 [==============================] - 38s 2s/step - loss: 0.9084 - acc: 0.7212 - f1_score: 0.4893 - val_loss: 1.1852 - val_acc: 0.6487 - val_f1_score: 0.4386WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 6/1517/17 [==============================] - 38s 2s/step - loss: 0.7336 - acc: 0.7920 - f1_score: 0.5183 - val_loss: 1.1071 - val_acc: 0.6473 - val_f1_score: 0.4536WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 7/1517/17 [==============================] - 39s 2s/step - loss: 0.5796 - acc: 0.8381 - f1_score: 0.5395 - val_loss: 1.0900 - val_acc: 0.6458 - val_f1_score: 0.4837WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 8/1517/17 [==============================] - 38s 2s/step - loss: 0.4641 - acc: 0.8757 - f1_score: 0.5575 - val_loss: 0.9854 - val_acc: 0.6982 - val_f1_score: 0.4781WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 9/1517/17 [==============================] - 38s 2s/step - loss: 0.3588 - acc: 0.9039 - f1_score: 0.5759 - val_loss: 1.2205 - val_acc: 0.6458 - val_f1_score: 0.4993WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 10/1517/17 [==============================] - 38s 2s/step - loss: 0.3082 - acc: 0.9181 - f1_score: 0.5908 - val_loss: 1.0990 - val_acc: 0.6870 - val_f1_score: 0.5031WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 11/1517/17 [==============================] - 39s 2s/step - loss: 0.2331 - acc: 0.9408 - f1_score: 0.6076 - val_loss: 1.8883 - val_acc: 0.5793 - val_f1_score: 0.4935WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 12/1517/17 [==============================] - 39s 2s/step - loss: 0.2625 - acc: 0.9380 - f1_score: 0.6111 - val_loss: 1.3714 - val_acc: 0.6642 - val_f1_score: 0.5215WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 00012: early stopping&lt;keras.callbacks.History at 0x7f1bf1402a60&gt;# evaluating function: report f1_macrodef evaluate(test_x,test_y,model):    predictions=model.predict(test_x)    y_pred=max(predictions)    print(classification_report(test_y,y_pred))model.evaluate(X_test, y_test)37/37 [==============================] - 6s 149ms/step - loss: 1.1846 - acc: 0.6986 - f1_score: 0.5419[1.1846439838409424, 0.6986182928085327, array([0.5124555 , 0.59907836, 0.48712873, 0.45633796, 0.5854922 ,        0.4855967 , 0.6674057 ], dtype=float32)]# from saved best modelloaded_model = load_model('best_model.h5')loaded_model.evaluate(X_test, y_test)",
        "url": "/NLP_final"
    }
    ,
    
    "nlp-midterm": {
        "title": "NLP - Midterm",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-1. NLP  1. Data Info  2. Preprocessing          2.1 duplicated data found in train_data        3. Comparing classification models          3.1 With Tf-idf vectorizer                  3.1.1 MultinomialNB          3.1.2 LogisticRegression                    3.2 With CountVectorizer                  3.2.1 MultinomialNB                      4. Balanced sampling approach - imblearn  5. Result: Best Model  commentimport sysimport pandas as pdimport osimport numpy as npimport reimport randomimport itertoolsimport warningswarnings.filterwarnings(action='ignore')from sklearn.metrics import classification_reportfrom sklearn.metrics import f1_scorerandom.seed(1)train_data=pd.read_csv('midterm_train.csv')test_data=pd.read_csv('midterm_test.csv')# evaluating functiondef evaluate(test_x,test_y,model):    predictions=model.predict(test_x)    print(classification_report(test_y,predictions))1. Data Infotrain_data.head(3)                  text      senti                  0      J brand is by far the best premium denim line ...      pos              1      I loved this dress. i kept putting it on tryin...      pos              2      I found this at my local store and ended up bu...      pos      print(train_data.groupby('senti').count())print(test_data.groupby('senti').count())print(train_data.describe())print(test_data.describe())        textsenti       neg     2139pos    11279       textsenti      neg     231pos    1260                                                     text  senticount                                               13418  13418unique                                              13414      2top     Perfect fit and i've gotten so many compliment...    posfreq                                                    2  11279                                                     text senticount                                                1491  1491unique                                               1491     2top     Have to disagree with previous posters. i foun...   posfreq                                                    1  12602. Preprocessing2.1 duplicated data found in train_data# remove duplicated dataprint(train_data.text.duplicated().sum())train_data = train_data.drop_duplicates(['text'],keep='first')train_data.duplicated().sum()40print(train_data.describe())                                                     text  senticount                                               13414  13414unique                                              13414      2top     J brand is by far the best premium denim line ...    posfreq                                                    1  11276# train-test split unduplicated datax_train=np.array(train_data.text)x_test=np.array(test_data.text)y_train=np.array(train_data.senti)y_test=np.array(test_data.senti)x_train[0]'J brand is by far the best premium denim line retailer sells! the fit on these jeans is amazing..worth every penny..also, considering it is a crop jean - warm weather wear - the denim weight is light and not too thick...the color is different from ordinary regular denim blue..lighter wash for spring/summer!'# preprocessing: remove non-alphabet charactersx_train_clean=np.array([re.sub('[^a-zA-Z]',' ',text) for text in x_train])x_test_clean=np.array([re.sub('[^a-zA-Z]',' ',text) for text in x_test])x_train_clean[0]'J brand is by far the best premium denim line retailer sells  the fit on these jeans is amazing  worth every penny  also  considering it is a crop jean   warm weather wear   the denim weight is light and not too thick   the color is different from ordinary regular denim blue  lighter wash for spring summer '3. Comparing classification models# tuning parameter setsngram_range= [(1, 1), (1, 2),(2,2)]stop_words=[None,'english']clf__alpha=[0.005,0.01,0.05,0.1]params = dict(ngram_range=ngram_range,              stop_words=stop_words,              clf__alpha=clf__alpha)keys=params.keys()values = (params[key] for key in keys)combinations = [dict(zip(keys, combination)) for combination in itertools.product(*values)]# tuning functiondef tuning_model(params,vectorizer,classifier,x_train,x_test,y_train,y_test):    ngram_range=params['ngram_range']    stop_words=params['stop_words']    vec = vectorizer(ngram_range=ngram_range,stop_words=stop_words)    vec_train = vec.fit_transform(x_train)    vec_test = vec.transform(x_test)    if classifier==MultinomialNB:        alpha=params['clf__alpha']        clf=classifier(alpha)    else:        clf=classifier(random_state=1,max_iter=500)    clf.fit(vec_train, y_train)    pred=clf.predict(vec_test)    return f1_score(y_test,pred,average='macro'),params# get best score &amp; parametersdef get_result(combinations,vectorizer,classifier,x_train,x_test,y_train,y_test):    results=[]    for params in combinations:        results.append(tuning_model(params,vectorizer,classifier,x_train,x_test,y_train,y_test))    return max(results,key=lambda item: item[0])3.1 With Tf-idf vectorizer3.1.1 MultinomialNBpreprocessing seems to have no effect in ngram_range: (1, 2) and also stop words are not important featuresf1-score macro avg: 0.85from sklearn.naive_bayes import MultinomialNBfrom sklearn.feature_extraction.text import TfidfVectorizerget_result(combinations,TfidfVectorizer,MultinomialNB,x_train,x_test,y_train,y_test)(0.8528815948449455, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.01})get_result(combinations,TfidfVectorizer,MultinomialNB,x_train_clean,x_test_clean,y_train,y_test)(0.8528815948449455, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.005})3.1.2 LogisticRegressionfrom sklearn.linear_model import LogisticRegressionget_result(combinations,TfidfVectorizer,LogisticRegression,x_train,x_test,y_train,y_test)(0.8811180515581001, {'ngram_range': (1, 1), 'stop_words': None, 'clf__alpha': 0.005})get_result(combinations,TfidfVectorizer,LogisticRegression,x_train_clean,x_test_clean,y_train,y_test)(0.8823175752378818, {'ngram_range': (1, 1), 'stop_words': None, 'clf__alpha': 0.005})3.2 With CountVectorizer3.2.1 MultinomialNBfrom sklearn.feature_extraction.text import CountVectorizerget_result(combinations,CountVectorizer,MultinomialNB,x_train,x_test,y_train,y_test)(0.9366865264206945, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.1})get_result(combinations,CountVectorizer,MultinomialNB,x_train_clean,x_test_clean,y_train,y_test)(0.935509934324805, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.1})4. Balanced sampling approach - imblearnfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.linear_model import LogisticRegression# tuning parameter sets for all casesvectorizer=[CountVectorizer,TfidfVectorizer]classifier=[MultinomialNB,LogisticRegression]ngram_range= [(1, 1), (1, 2),(2,2)]stop_words=[None,'english']clf__alpha=[0.005,0.01,0.05,0.1]params = dict(ngram_range=ngram_range,              vectorizer=vectorizer,              classifier=classifier,              stop_words=stop_words,              clf__alpha=clf__alpha)keys=params.keys()values = (params[key] for key in keys)combinations = [dict(zip(keys, combination)) for combination in itertools.product(*values)]from imblearn.over_sampling import SMOTEdef smote_tuning(params,x_train,x_test,y_train,y_test):    ngram_range=params['ngram_range']    stop_words=params['stop_words']    classifier=params['classifier']    vec=params['vectorizer'](ngram_range=ngram_range,stop_words=stop_words)    x_train_imb=vec.fit_transform(x_train)    y_train_imb=np.array(train_data['senti']=='pos').astype('int')    y_test_imb=np.array(test_data['senti']=='pos').astype('int')    vec_train_over, y_train_over = SMOTE(random_state=1).fit_resample(x_train_imb,y_train_imb)    vec_test = vec.transform(x_test)    if classifier==MultinomialNB:        alpha=params['clf__alpha']        clf=classifier(alpha)    else:        clf=classifier(random_state=1,max_iter=500)    clf.fit(vec_train_over, y_train_over)    pred=clf.predict(vec_test)    return f1_score(y_test_imb,pred,average='macro'),params# get best score &amp; parametersdef get_smote_result(combinations,x_train,x_test,y_train,y_test):    results=[]    for params in combinations:        results.append(smote_tuning(params,x_train,x_test,y_train,y_test))    return max(results,key=lambda item: item[0])get_smote_result(combinations,x_train,x_test,y_train,y_test)(0.9285352359562871, {'ngram_range': (1, 2),  'vectorizer': sklearn.feature_extraction.text.TfidfVectorizer,  'classifier': sklearn.naive_bayes.MultinomialNB,  'stop_words': None,  'clf__alpha': 0.1})get_smote_result(combinations,x_train_clean,x_test_clean,y_train,y_test)(0.9276401547886131, {'ngram_range': (1, 2),  'vectorizer': sklearn.feature_extraction.text.TfidfVectorizer,  'classifier': sklearn.naive_bayes.MultinomialNB,  'stop_words': None,  'clf__alpha': 0.1})5. Result: Best Model# 'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.1}vectorizer=CountVectorizer(ngram_range=(1,2))vectors_train=vectorizer.fit_transform(x_train)vectors_test=vectorizer.transform(x_test)clf=MultinomialNB(alpha=0.1)clf.fit(vectors_train,y_train)evaluate(vectors_test,y_test,clf)              precision    recall  f1-score   support         neg       0.90      0.88      0.89       231         pos       0.98      0.98      0.98      1260    accuracy                           0.97      1491   macro avg       0.94      0.93      0.94      1491weighted avg       0.97      0.97      0.97      1491comment데이터사이언스 복수전공 후 첫 강의. 중간고사까지 배웠던 기본적인 전처리와 ML 함수들을 적용했다. 추가적으로 imblanced data case를 SMOTE 패키지를 사용해 간단히 처리해보았다. 당시 수업에서 가장 높은 accuracy를 기록했다.",
        "url": "/NLP_midterm"
    }
    ,
    
    "stat-hw2": {
        "title": "Statistical Mining - HW2",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-1. Statistical Mining#2set.seed(1)x1=runif(100)x2=0.5*x1 + rnorm(100)/10y= 2+ 2*x1 + 0.3*x2 + rnorm(100)fit = lm(y ~  x1 + x2,data=y)Error in eval(predvars, data, env): 길이가 1이 아닌 수치형 'envir' 인자입니다Traceback:1. lm(y ~ x1 + x2, data = y)2. eval(mf, parent.frame())3. eval(mf, parent.frame())4. stats::model.frame(formula = y ~ x1 + x2, data = y, drop.unused.levels = TRUE)5. model.frame.default(formula = y ~ x1 + x2, data = y, drop.unused.levels = TRUE)6. eval(predvars, data, env)cor(x1,x2)0.835121242463113plot(x1,x2)fit &lt;- lm(y~x1+x2)summary(fit)Call:lm(formula = y ~ x1 + x2)Residuals:    Min      1Q  Median      3Q     Max -2.8311 -0.7273 -0.0537  0.6338  2.3359 Coefficients:            Estimate Std. Error t value Pr(&gt;|t|)    (Intercept)   2.1305     0.2319   9.188 7.61e-15 ***x1            1.4396     0.7212   1.996   0.0487 *  x2            1.0097     1.1337   0.891   0.3754    ---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1Residual standard error: 1.056 on 97 degrees of freedomMultiple R-squared:  0.2088,\tAdjusted R-squared:  0.1925 F-statistic:  12.8 on 2 and 97 DF,  p-value: 1.164e-05fit2 &lt;- lm(y~x1)summary(fit2)Call:lm(formula = y ~ x1)Residuals:     Min       1Q   Median       3Q      Max -2.89495 -0.66874 -0.07785  0.59221  2.45560 Coefficients:            Estimate Std. Error t value Pr(&gt;|t|)    (Intercept)   2.1124     0.2307   9.155 8.27e-15 ***x1            1.9759     0.3963   4.986 2.66e-06 ***---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1Residual standard error: 1.055 on 98 degrees of freedomMultiple R-squared:  0.2024,\tAdjusted R-squared:  0.1942 F-statistic: 24.86 on 1 and 98 DF,  p-value: 2.661e-06fit3 &lt;- lm(y~x2)summary(fit3)Call:lm(formula = y ~ x2)Residuals:     Min       1Q   Median       3Q      Max -2.62687 -0.75156 -0.03598  0.72383  2.44890 Coefficients:            Estimate Std. Error t value Pr(&gt;|t|)    (Intercept)   2.3899     0.1949   12.26  &lt; 2e-16 ***x2            2.8996     0.6330    4.58 1.37e-05 ***---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1Residual standard error: 1.072 on 98 degrees of freedomMultiple R-squared:  0.1763,\tAdjusted R-squared:  0.1679 F-statistic: 20.98 on 1 and 98 DF,  p-value: 1.366e-05",
        "url": "/stat_HW2"
    }
    ,
    
    "stat-week2": {
        "title": "Statistical Mining - Week2",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-1. Statistical MiningCh 2. Supervised LearningFormat of Supervised Learning:  $ Y=f(X)+\\epsilon $$f$ : Fixed but UNKNOWN function of predictors X$\\rightarrow$ Supervised Learning: A set of methods estimating $f$.  Why do we estimate $f$?1) Prediction: Y2) Inference: Relationship between X &amp; Y1) Prediction:$\\rightarrow\\text{Prediction of Y} : \\hat{Y} = \\hat{f}(X)\\quad (\\hat{f} : \\text{Estimate of }f) $$\\rightarrow\\text{Accuracy of }\\hat{Y}\\text{ depends on reducible &amp; irreducible errors}$  $E[Y-\\hat{Y}]^2 = [f(X)-\\hat{f}(X)]^2 + Var(\\epsilon)$$\\quad (MSE)\\quad\\quad (reducible)\\quad (irreducible)$  Reducible error : controlable, can be reduced by selecting a better model  Irreducible error: uncontrolable, nature of data2) Inference:  Characteristics of liner &amp; nonlinear models:1) Linear : Relatively Simple &amp; interpretable inference but poor prediction (when true function $f$ is nonlinear)2) Nonlinear : More accurate but difficult to interpret$\\rightarrow$ Depends on your Goal of analysis3) Estimation F: By Using Training Data  Estimation Methods:1) Parametric: Assume specific function $f$ (linear&amp;non- both)2) Nonparametric: No Assumption; Data-driven method (nonlinear)4) Parametric Methods: Model-based approach  Step 1: Assume about the functional form of $f$$\\quad\\text{e.x) when } X_j = ( x_{1j}, \\ldots , x_{nj} ) ^T : n * 1 \\space \\text{obs. ; vector for the j th predictor} \\quad\\quad \\rightarrow linear: f(X)= \\beta_0 + \\beta_1X_1 + … + \\beta_pX_p$  Step 2: Estimate Parameters$\\quad\\quad\\quad Y \\approx \\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + … + \\hat{\\beta}_pX_p = \\hat{f}(X)$  Disadvantage: Specific form of $f$$\\quad\\quad\\quad\\rightarrow$ Not match with the true $f$: poor estimation &amp; poor prediction$\\quad\\quad\\quad\\rightarrow$ Flextible parametric model : model with more parameters$\\quad\\quad\\quad\\quad$ Flexible model $\\equiv$ Complex model$\\quad\\quad\\quad\\quad$ but overfitting problem still remains: model follows errors or noisesUnderfitting vs Overfitting  under: estimation has not enough accuracy, miss to capture the true structure  over: more complexity than true $f$5) Nonparametric Methods: Data-driven approach  No assumptions about functional form of $f \\Rightarrow $ wider range of possible shapes of $f$  Disadvantage:$\\quad\\quad\\quad\\rightarrow$ A vary large # of obs. is required to get an accurae $f$ (relatively to parametric method)$\\quad\\quad\\quad\\rightarrow$ Overfitting $\\Leftrightarrow$ Level of smoothness; Model complexity (how to determine it?)    Example of Nonparametric model: KNN Reg.    Idea: Similar inputs have Similar output  Step: when predicting $\\hat{Y}_0$ with input $X_0$1) Determine $K$ (level of smoothness) #if K==N: Y = Var(Y) / if K==1: Y = perfect fit; no training error2) find $K$ closest training obs. from the target input point($X_0$) using Euclidean distance\\(\\rightarrow \\hat{Y}_0 = \\frac{1}{K}\\sum_{x_i\\in \\mathcal N}^K y_i \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\)3) average of these #k Y values = $\\hat{Y}_0$  Effect of $K$ (tuning parameter/ model flexibility parameter)          decides the level of smoothness (under or overfitting)      As $K \\uparrow \\Rightarrow \\text{flexibility} \\downarrow$ : the fitted line is simple      As $K \\downarrow \\Rightarrow \\text{flexibility} \\uparrow$ : the fitted line is wiggly      6) Trade-off between Prediction Accuray &amp; Interpretability  Restrictive model(Simple model) vs. Flexible model  our goal: inference ($\\rightarrow$ restrictive model) or prediction ($\\rightarrow$ flexible model)7) Model Assessment &amp; Selection  Evaluate the performance of models on a given dataset  and Select the best model  Criterion: Better prediction of $Y$  #When Regression problem: Mean squared error (MSE)      training MSE(used for building model) is not our interest; cannot be a measure for selectione.g) $\\begin{align} R^2 = 1 - \\frac{RSS}{TSS} \\end {align}\\text{ (Residual Sum of Squares)  / (Total Sum of Square)} \\\\quad\\quad \\text{training RSS}= \\sum(y_i - \\hat{y_i})^2  \\quad \\text{&amp;} \\quad \\text{MSE} =\\frac{RSS}{N}$                  when more X variables added $\\rightarrow \\text{RSS&amp;MSE}\\downarrow , \\space R^2\\uparrow$          even X are not important variables        complex model has smaller training error (regardless of existence of test data)                            Test MSE $= \\begin{align}\\frac{1}{m}\\sum_{i=1}^m(y_i^0 - \\hat{f}(x_i^0))^2 \\end{align}$        with low test MSE $\\Rightarrow$ better model$\\rightarrow$ How to find optimal Flexibility?$\\rightarrow$ Test MSE(error rate) is always larger than Training MSE; We estimated model function $f$ to minimize its training error$\\rightarrow$ No test data: Sample re-use methods (e.g., bootstrap, cross-validation)              Classification problem: Misclassification rate8) Bias-Variance Trade-off 편향분산 교차  Expected test MSE(ideal measure): $E[y_0-\\hat{f}(x_0)]^2 \\quad ((x_0,y_0) \\text{ is a test obs})$      in population: #K training sets &amp; #K function $f_k$Estimation of expected test MSE:\\(\\hat{E}[y_0 - \\hat{f}(x_0)]^2 = \\frac{1}{K} \\sum_{k=1}^K[y_0-\\hat{f}_k(x_0)]^2\\)    For given $x_0$,  (when $\\epsilon$ is irreducible error)$\\begin{matrix}\\begin{align}\\hat{E}[y_0 - \\hat{f}(x_0)]^2 &amp;= Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon) \\               \\text{} \\quad&amp;= E [ \\{ y_0 -E(\\hat{f}(x_0)) \\} - \\{ \\hat{f}(x_0) - E(\\hat{f}(x_0)) \\} ]^2 \\\\                                             &amp;= \\quad E [  y_0 -E(\\hat{f}(x_0)) ] ^2 \\quad \\quad \\quad &amp; \\rightarrow (1) \\\\                                             &amp; \\quad \\; + E [\\hat{f}(x_0) - E(\\hat{f}(x_0)) ] ^2 &amp; \\rightarrow (2)\\\\                                             &amp; \\quad \\; - 2E[ \\{ y_0 -E(\\hat{f}(x_0)) \\} \\{ \\hat{f}(x_0) - E(\\hat{f}(x_0)) \\} ] &amp; \\rightarrow (3)\\end{align}\\end{matrix}$      $\\begin {matrix} \\begin{align}(1) &amp;= E [(f(x_0) + \\epsilon - E(\\hat{f}(x_0)) ] ^2    &amp;= E[ f(x_0)^2 +\\epsilon^2 + [E(\\hat{f}(x_0)) ]^2 + 2f(x_0)\\epsilon -2f(x_0)E(\\hat{f}(x_0)) - 2\\epsilon E(\\hat{f}(x_0)) ]           \\quad \\leftarrow \\text{when } E(\\epsilon) \\text{ goes to } 0     &amp;= E[E(\\hat{f}(x_0)) - f(x_0)] ^2 + E(\\epsilon^2)  \\quad  \\leftarrow \\text{ in E[ ] is Bias }     &amp;= [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon^2)\\end{align}\\end{matrix} $    $(2) = Var(\\hat{f}(x_0)) \\leftarrow \\text { model variance (different fit’s from randomness of training sets)} $  $(3) = 0$$\\therefore$ Expected test MSE is consisted of Reducible Error(Var+ Bias$^2$) and Irreducible Error(Var($\\epsilon$))2020-04-03 Week 3      $Var(\\hat{f(x_0})$ : model variance        $ [Bias(\\hat{f(x_0})]$ : Systematic model error ( caused by model assumptions; e.g. true f: nonlinear)        $ Var(\\epsilon)$ : Irreducible error  Model Flextibility $\\uparrow$ $\\Rightarrow$ model variance $\\uparrow$ &amp; model bias $\\downarrow$Model Flextibility $\\downarrow$ $\\Rightarrow$ model variance $\\downarrow$ &amp; model bias $\\uparrow$$\\rightarrow$ optimal model flextibility is different for each datasetsTrade-off in KNN Reg.  given) Y = f (X) + $\\epsilon$ with E($\\epsilon$) = 0 and Var ($\\epsilon$) = $\\sigma^2$      Expected test MSE at $x_0$:$E[ (Y- \\hat{f_k}(x_0) )^2 | X = x_0 ] = \\sigma^2 + Bias^2(\\hat{f_k} (x_0)) + Var (\\hat{f_k} (x_0)) \\rightarrow Bias(\\hat{f_k} (x_0)) =  f(x_0) - E(\\hat{f}(x_0))   \\text{ : given “f” is KNN func.}\\ \\quad= { f(x_0) - E[\\frac1 K \\sum_{i \\in \\mathcal N (x_0)} ^ K Y_i ] }  \\quad= { f(x_0)- \\frac 1 K \\sum E (f(x_i)+\\epsilon_i)}  \\quad= f(x_0) - \\frac 1 K \\sum E (f(x_i)  \\rightarrow Var(\\hat{f_k} (x_0)) = Var(\\frac 1 K \\sum_{i \\in \\mathcal N (x_0)} Yi )\\quad= \\frac 1 {K^2} \\sum  Var( Yi ) \\quad=  \\frac 1 {K^2} \\sum Var( f( x_i) + \\epsilon_i ) \\quad=  \\frac 1 {K^2} \\sum \\sigma^2 = \\frac 1 {K^2} * K * \\sigma^2 = \\frac {\\sigma^2} K $\\(\\therefore \\text{ Expected test MSE at } x_0 = \\sigma ^2 + [f(x_0) - \\frac 1 K \\sum_{x_i \\in \\mathcal N (x_0)}^k f(x_i) ]^2 + \\frac {\\sigma^2} K\\)    when we use large number of K :as K $\\uparrow\\rightarrow$ model complexity$\\downarrow$ model variance $\\downarrow \\rightarrow$ bias $\\uparrow$ :simpler modelin Classification: Using Misclassification rate\\(= \\frac 1 N \\sum_{i=1}^n I(y_i \\not = \\hat y_i )\\)  Bayes Classifier  KNN Classifier$min_\\hat Y E( I(Y \\not = \\hat Y) = min_\\hat Y E[ E(I(Y \\not = \\hat Y | X) ] = min_{\\hat f (x)} E[ \\sum_{g=1}^G I( Y= g\\not= \\hat f (x) ) P ( Y= g| X) ] \\equiv min_{\\hat f (x)} \\sum_{g=1}^G I ( Y=g \\not= \\hat f (x) ) P ( Y= g| X) \\Rightarrow \\hat f (x) = min_g \\sum_{g=1}^G I ( Y=g \\not= \\hat f (x) ) P ( Y= g| X) \\quad \\equiv min_g ( 1- P ( Y= g| X) )$E.g)in 3 groups -&gt; Y = 1, 2, 3P(Y=1 | X) = 0.3 , P(Y=2 | X) = 0.5, P(Y=3 | X) =0.2$\\hat f(x) = 1 \\Rightarrow$ EPE(expected prediction error; miss classification rate) = 0 * 0.3 + 1 * 0.5 + 1 * 0.2 = 0.7$\\hat f(x) = 2 \\Rightarrow$ EPE = 0.5 : min value$\\hat f(x) = 3 \\Rightarrow$ EPE = 0.8            $\\therefore min_g ( 1- P ( Y= g      X) ) \\              \\quad  \\equiv max_g P(Y = g      X)$                  Bayes error rate at X = $x_0$ : $1 - max_g P(Y=g      X=x_0)$              Overall Bayes error rate = $1 - E[ max_g P(Y=g      X= x) ]$      ",
        "url": "/stat_week2"
    }
    
    
    };
</script>
<script src="assets/js/lunr.js"></script>
<script src="assets/js/search.js"></script>
            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Darron's Devlog</a> &copy; 2021</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search Darron's Devlog</h1>
                <p class="subscribe-overlay-description">
				lunr.js를 이용한 posts 검색 </p>
                <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
