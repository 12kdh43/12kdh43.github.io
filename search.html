<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
	
	<!-- On Post front-matter YAML, set "use_math: true" to use LaTex -->
	

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Search Result</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
	
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="" />
    <link rel="shortcut icon" href="http://0.0.0.0:4000/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="http://0.0.0.0:4000/search" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Darron's Devlog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Search Result" />
    <meta property="og:description" content="" />
    <meta property="og:url" content="http://0.0.0.0:4000/search" />
    <meta property="og:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Search Result" />
    <meta name="twitter:description" content="" />
    <meta name="twitter:url" content="http://0.0.0.0:4000/" />
    <meta name="twitter:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Darron's Devlog" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="666" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Darron's Devlog",
        "logo": "http://0.0.0.0:4000/"
    },
    "url": "http://0.0.0.0:4000/search",
    "image": {
        "@type": "ImageObject",
        "url": "http://0.0.0.0:4000/assets/built/images/blog-cover1.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://0.0.0.0:4000/search"
    },
    "description": ""
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Search Result" href="/feed.xml" />


</head>
<body class="page-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="/">Darron's Devlog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/jekyll/">Jekyll</a></li>
    <li class="nav-projects" role="menuitem"><a href="/tag/projects/">Projects</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/studies/">Studies</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post page no-image">

            <header class="post-full-header">
                <h1 class="post-full-title">Search Result</h1>
            </header>

            

            <section class="post-full-content">
                <form action="/search" method="get" hidden="hidden">
    <label for="search-box"></label>
    <input type="text" id="search-box" name="query">
</form>

<ul class="mylist" id="search-results"></ul>

<script>
    window.store = {
    
    "helloworld": {
        "title": "Hello, World!",
        "author": "Darron Kwon",
        "category": "",
        "content": "My First Notebook from Docker-workspace.import tensorflow as tftf.__version__'2.4.1'",
        "url": "/helloworld"
    }
    ,
    
    "fashion-gnn": {
        "title": "Fashion GNN",
        "author": "Darron Kwon",
        "category": "",
        "content": "1. About1.1. Workspace  Windows 11 &gt; docker - ubuntu kernel &gt; CUDA on WSLimport randomrandom.seed(2021)import numpy as npnp.random.seed(2021)import tensorflow as tftf.random.set_seed(2021)tf.device('/device:GPU:0')import networkx as nximport stellargraph as sgimport argparsefrom graph_model import *import osdef str2bool(v):    \"\"\"    function: convert into bool type(True or False)    \"\"\"    if isinstance(v, bool):         return v     if v.lower() in ('yes', 'true', 't', 'y', '1'):         return True     elif v.lower() in ('no', 'false', 'f', 'n', '0'):         return False     else:         raise argparse.ArgumentTypeError('Boolean value expected.')# input optionsparser = argparse.ArgumentParser(description='AI Fashion Coordinator.')parser.add_argument('--mode', type=str,                     default='train',                     help='training or eval or test mode')parser.add_argument('--in_file_trn_dialog', type=str,                     default='./data/ddata.wst.txt',                     help='training dialog DB')parser.add_argument('--in_file_tst_dialog', type=str,                     default='./data/ac_eval_t1.wst.dev',                     help='test dialog DB')parser.add_argument('--in_file_fashion', type=str,                     default='./data/mdata.wst.txt',                     help='fashion item metadata')parser.add_argument('--in_file_img_feats', type=str,                     default='./data/extracted_feat.json',                     help='fashion item image features')parser.add_argument('--model_path', type=str,                     default='./gAIa_model',                     help='path to save/read model')parser.add_argument('--model_file', type=str,                     default=None,                     help='model file name')parser.add_argument('--eval_node', type=str,                     default='[6000,6000,6000,200][2000,2000]',                     help='nodes of evaluation network')parser.add_argument('--subWordEmb_path', type=str,                     default='./sstm_v0p5_deploy/sstm_v4p49_np_final_n36134_d128_r_eng_upper.dat',                     help='path of subword embedding')parser.add_argument('--learning_rate', type=float,                    default=0.0001,                     help='learning rate')parser.add_argument('--max_grad_norm', type=float,                    default=40.0,                     help='clip gradients to this norm')parser.add_argument('--zero_prob', type=float,                    default=0.0,                     help='dropout prob.')parser.add_argument('--corr_thres', type=float,                    default=0.7,                     help='correlation threshold')parser.add_argument('--batch_size', type=int,                    default=100,                       help='batch size for training')parser.add_argument('--epochs', type=int,                    default=10,                       help='epochs to training')parser.add_argument('--save_freq', type=int,                    default=2,                       help='evaluate and save results per # epochs')parser.add_argument('--hops', type=int,                    default=3,                       help='number of hops in the MemN2N')parser.add_argument('--mem_size', type=int,                    default=16,                       help='memory size for the MemN2N')parser.add_argument('--key_size', type=int,                    default=300,                       help='memory size for the MemN2N')parser.add_argument('--permutation_iteration', type=int,                    default=3,                       help='# of permutation iteration')parser.add_argument('--evaluation_iteration', type=int,                    default=10,                       help='# of test iteration')parser.add_argument('--num_augmentation', type=int,                    default=3,                       help='# of data augmentation')parser.add_argument('--use_batch_norm', type=str2bool,                     default=False,                     help='use batch normalization')parser.add_argument('--use_dropout', type=str2bool,                     default=False,                     help='use dropout')parser.add_argument('--use_multimodal', type=str2bool,                    default=True,                     help='use multimodal input')_StoreAction(option_strings=['--use_multimodal'], dest='use_multimodal', nargs=None, const=None, default=True, type=&lt;function str2bool at 0x7f85a894e378&gt;, choices=None, help='use multimodal input', metavar=None)args = parser.parse_args(args=[])if __name__ == '__predict__':        print('\\n')    print('-'*60)    print('\\t\\tAI Fashion Coordinator')    print('-'*60)    print('\\n')g_model = graph_model(args)&lt;Initialize subword embedding&gt;loading= ./sstm_v0p5_deploy/sstm_v4p49_np_final_n36134_d128_r_eng_upper.dat&lt;Make metadata&gt;loading fashion item metadatavectorizing data&lt;Make input &amp; output data&gt;loading dialog DB# of dialog: 7236 setsvectorizing datamemorizing data&lt;Make input &amp; output data&gt;loading dialog DB# of dialog: 200 setsvectorizing datamemorizing datag_model.train()link_regression: using 'concat' method to combine node embeddings into edge embeddings14/14 [==============================] - 3s 92ms/step - loss: 0.1245 - root_mean_square_error: 0.3521 - mean_absolute_error: 0.2666Untrained model's Test Evaluation:\tloss: 0.1245\troot_mean_square_error: 0.3521\tmean_absolute_error: 0.2666Epoch 1/1033/33 [==============================] - 8s 222ms/step - loss: 0.0947 - root_mean_square_error: 0.2792 - mean_absolute_error: 0.2281 - val_loss: 0.0451 - val_root_mean_square_error: 0.2123 - val_mean_absolute_error: 0.1991Epoch 2/1033/33 [==============================] - 7s 217ms/step - loss: 0.0441 - root_mean_square_error: 0.2093 - mean_absolute_error: 0.1761 - val_loss: 0.0427 - val_root_mean_square_error: 0.2062 - val_mean_absolute_error: 0.1558Epoch 3/1033/33 [==============================] - 8s 226ms/step - loss: 0.0426 - root_mean_square_error: 0.2062 - mean_absolute_error: 0.1684 - val_loss: 0.0423 - val_root_mean_square_error: 0.2053 - val_mean_absolute_error: 0.1613Epoch 4/1033/33 [==============================] - 8s 224ms/step - loss: 0.0425 - root_mean_square_error: 0.2053 - mean_absolute_error: 0.1683 - val_loss: 0.0422 - val_root_mean_square_error: 0.2050 - val_mean_absolute_error: 0.1620Epoch 5/1033/33 [==============================] - 8s 234ms/step - loss: 0.0423 - root_mean_square_error: 0.2053 - mean_absolute_error: 0.1679 - val_loss: 0.0425 - val_root_mean_square_error: 0.2058 - val_mean_absolute_error: 0.1805Epoch 6/1033/33 [==============================] - 8s 231ms/step - loss: 0.0423 - root_mean_square_error: 0.2057 - mean_absolute_error: 0.1682 - val_loss: 0.0420 - val_root_mean_square_error: 0.2047 - val_mean_absolute_error: 0.1751Epoch 7/1033/33 [==============================] - 8s 233ms/step - loss: 0.0421 - root_mean_square_error: 0.2046 - mean_absolute_error: 0.1701 - val_loss: 0.0417 - val_root_mean_square_error: 0.2039 - val_mean_absolute_error: 0.1685Epoch 8/1033/33 [==============================] - 8s 229ms/step - loss: 0.0421 - root_mean_square_error: 0.2050 - mean_absolute_error: 0.1662 - val_loss: 0.0415 - val_root_mean_square_error: 0.2033 - val_mean_absolute_error: 0.1686Epoch 9/1033/33 [==============================] - 8s 238ms/step - loss: 0.0415 - root_mean_square_error: 0.2036 - mean_absolute_error: 0.1679 - val_loss: 0.0411 - val_root_mean_square_error: 0.2025 - val_mean_absolute_error: 0.1695Epoch 10/1033/33 [==============================] - 8s 232ms/step - loss: 0.0411 - root_mean_square_error: 0.2023 - mean_absolute_error: 0.1668 - val_loss: 0.0409 - val_root_mean_square_error: 0.2017 - val_mean_absolute_error: 0.153514/14 [==============================] - 2s 121ms/step - loss: 0.0408 - root_mean_square_error: 0.2016 - mean_absolute_error: 0.1535Test Evaluation:\tloss: 0.0408\troot_mean_square_error: 0.2016\tmean_absolute_error: 0.1535Mean Baseline Test set metrics:\troot_mean_square_error =  0.2054230809528016\tmean_absolute_error =  0.1687945779971711Model Test set metrics:\troot_mean_square_error =  0.20211308444911932\tmean_absolute_error =  0.15357324988306925link_regression: using 'concat' method to combine node embeddings into edge embeddings14/14 [==============================] - 3s 126ms/step - loss: 0.0841 - root_mean_square_error: 0.2878 - mean_absolute_error: 0.1999Untrained model's Test Evaluation:\tloss: 0.0841\troot_mean_square_error: 0.2878\tmean_absolute_error: 0.1999Epoch 1/1032/32 [==============================] - 9s 238ms/step - loss: 0.0750 - root_mean_square_error: 0.2574 - mean_absolute_error: 0.2070 - val_loss: 0.0436 - val_root_mean_square_error: 0.2081 - val_mean_absolute_error: 0.1893Epoch 2/1032/32 [==============================] - 7s 225ms/step - loss: 0.0430 - root_mean_square_error: 0.2071 - mean_absolute_error: 0.1727 - val_loss: 0.0418 - val_root_mean_square_error: 0.2035 - val_mean_absolute_error: 0.1678Epoch 3/1032/32 [==============================] - 7s 224ms/step - loss: 0.0426 - root_mean_square_error: 0.2061 - mean_absolute_error: 0.1698 - val_loss: 0.0417 - val_root_mean_square_error: 0.2033 - val_mean_absolute_error: 0.1609Epoch 4/1032/32 [==============================] - 8s 233ms/step - loss: 0.0424 - root_mean_square_error: 0.2059 - mean_absolute_error: 0.1690 - val_loss: 0.0421 - val_root_mean_square_error: 0.2046 - val_mean_absolute_error: 0.1809Epoch 5/1032/32 [==============================] - 8s 233ms/step - loss: 0.0426 - root_mean_square_error: 0.2063 - mean_absolute_error: 0.1698 - val_loss: 0.0413 - val_root_mean_square_error: 0.2023 - val_mean_absolute_error: 0.1663Epoch 6/1032/32 [==============================] - 7s 223ms/step - loss: 0.0423 - root_mean_square_error: 0.2053 - mean_absolute_error: 0.1686 - val_loss: 0.0412 - val_root_mean_square_error: 0.2023 - val_mean_absolute_error: 0.1734Epoch 7/1032/32 [==============================] - 8s 227ms/step - loss: 0.0419 - root_mean_square_error: 0.2042 - mean_absolute_error: 0.1682 - val_loss: 0.0412 - val_root_mean_square_error: 0.2022 - val_mean_absolute_error: 0.1757Epoch 8/1032/32 [==============================] - 7s 224ms/step - loss: 0.0416 - root_mean_square_error: 0.2036 - mean_absolute_error: 0.1680 - val_loss: 0.0405 - val_root_mean_square_error: 0.2003 - val_mean_absolute_error: 0.1557Epoch 9/1032/32 [==============================] - 7s 225ms/step - loss: 0.0410 - root_mean_square_error: 0.2025 - mean_absolute_error: 0.1644 - val_loss: 0.0398 - val_root_mean_square_error: 0.1988 - val_mean_absolute_error: 0.1653Epoch 10/1032/32 [==============================] - 8s 238ms/step - loss: 0.0396 - root_mean_square_error: 0.1985 - mean_absolute_error: 0.1610 - val_loss: 0.0388 - val_root_mean_square_error: 0.1964 - val_mean_absolute_error: 0.161014/14 [==============================] - 2s 128ms/step - loss: 0.0387 - root_mean_square_error: 0.1962 - mean_absolute_error: 0.1607Test Evaluation:\tloss: 0.0387\troot_mean_square_error: 0.1962\tmean_absolute_error: 0.1607Mean Baseline Test set metrics:\troot_mean_square_error =  0.2037237830436417\tmean_absolute_error =  0.16601351653248084Model Test set metrics:\troot_mean_square_error =  0.19741210353956345\tmean_absolute_error =  0.1613392177040303link_regression: using 'concat' method to combine node embeddings into edge embeddings18/18 [==============================] - 3s 139ms/step - loss: 0.5363 - root_mean_square_error: 0.7332 - mean_absolute_error: 0.6958Untrained model's Test Evaluation:\tloss: 0.5363\troot_mean_square_error: 0.7332\tmean_absolute_error: 0.6958Epoch 1/1041/41 [==============================] - 11s 241ms/step - loss: 0.0752 - root_mean_square_error: 0.2546 - mean_absolute_error: 0.2105 - val_loss: 0.0452 - val_root_mean_square_error: 0.2128 - val_mean_absolute_error: 0.1887Epoch 2/1041/41 [==============================] - 9s 221ms/step - loss: 0.0441 - root_mean_square_error: 0.2101 - mean_absolute_error: 0.1735 - val_loss: 0.0448 - val_root_mean_square_error: 0.2118 - val_mean_absolute_error: 0.1801Epoch 3/1041/41 [==============================] - 10s 233ms/step - loss: 0.0440 - root_mean_square_error: 0.2094 - mean_absolute_error: 0.1756 - val_loss: 0.0447 - val_root_mean_square_error: 0.2117 - val_mean_absolute_error: 0.1763Epoch 4/1041/41 [==============================] - 9s 221ms/step - loss: 0.0438 - root_mean_square_error: 0.2093 - mean_absolute_error: 0.1745 - val_loss: 0.0448 - val_root_mean_square_error: 0.2117 - val_mean_absolute_error: 0.1754Epoch 5/1041/41 [==============================] - 9s 225ms/step - loss: 0.0439 - root_mean_square_error: 0.2096 - mean_absolute_error: 0.1754 - val_loss: 0.0447 - val_root_mean_square_error: 0.2115 - val_mean_absolute_error: 0.1823Epoch 6/1041/41 [==============================] - 10s 231ms/step - loss: 0.0437 - root_mean_square_error: 0.2091 - mean_absolute_error: 0.1748 - val_loss: 0.0446 - val_root_mean_square_error: 0.2114 - val_mean_absolute_error: 0.1794Epoch 7/1041/41 [==============================] - 10s 233ms/step - loss: 0.0437 - root_mean_square_error: 0.2086 - mean_absolute_error: 0.1746 - val_loss: 0.0446 - val_root_mean_square_error: 0.2113 - val_mean_absolute_error: 0.1751Epoch 8/1041/41 [==============================] - 10s 228ms/step - loss: 0.0438 - root_mean_square_error: 0.2086 - mean_absolute_error: 0.1751 - val_loss: 0.0452 - val_root_mean_square_error: 0.2128 - val_mean_absolute_error: 0.1634Epoch 9/1041/41 [==============================] - 10s 226ms/step - loss: 0.0442 - root_mean_square_error: 0.2099 - mean_absolute_error: 0.1727 - val_loss: 0.0445 - val_root_mean_square_error: 0.2112 - val_mean_absolute_error: 0.1838Epoch 10/1041/41 [==============================] - 9s 226ms/step - loss: 0.0436 - root_mean_square_error: 0.2088 - mean_absolute_error: 0.1749 - val_loss: 0.0446 - val_root_mean_square_error: 0.2113 - val_mean_absolute_error: 0.169618/18 [==============================] - 3s 132ms/step - loss: 0.0446 - root_mean_square_error: 0.2114 - mean_absolute_error: 0.1697Test Evaluation:\tloss: 0.0446\troot_mean_square_error: 0.2114\tmean_absolute_error: 0.1697Mean Baseline Test set metrics:\troot_mean_square_error =  0.21115701980168747\tmean_absolute_error =  0.1783491631950995Model Test set metrics:\troot_mean_square_error =  0.21133401478765093\tmean_absolute_error =  0.16970877193269276link_regression: using 'concat' method to combine node embeddings into edge embeddings15/15 [==============================] - 3s 143ms/step - loss: 1.1537 - root_mean_square_error: 1.0739 - mean_absolute_error: 1.0498Untrained model's Test Evaluation:\tloss: 1.1537\troot_mean_square_error: 1.0739\tmean_absolute_error: 1.0498Epoch 1/1035/35 [==============================] - 9s 234ms/step - loss: 0.0790 - root_mean_square_error: 0.2416 - mean_absolute_error: 0.2106 - val_loss: 0.0444 - val_root_mean_square_error: 0.2104 - val_mean_absolute_error: 0.1774Epoch 2/1035/35 [==============================] - 8s 227ms/step - loss: 0.0459 - root_mean_square_error: 0.2138 - mean_absolute_error: 0.1824 - val_loss: 0.0445 - val_root_mean_square_error: 0.2107 - val_mean_absolute_error: 0.1839Epoch 3/1035/35 [==============================] - 8s 231ms/step - loss: 0.0457 - root_mean_square_error: 0.2141 - mean_absolute_error: 0.1821 - val_loss: 0.0449 - val_root_mean_square_error: 0.2117 - val_mean_absolute_error: 0.1913Epoch 4/1035/35 [==============================] - 8s 230ms/step - loss: 0.0456 - root_mean_square_error: 0.2143 - mean_absolute_error: 0.1824 - val_loss: 0.0449 - val_root_mean_square_error: 0.2118 - val_mean_absolute_error: 0.1932Epoch 5/1035/35 [==============================] - 8s 229ms/step - loss: 0.0459 - root_mean_square_error: 0.2133 - mean_absolute_error: 0.1822 - val_loss: 0.0435 - val_root_mean_square_error: 0.2084 - val_mean_absolute_error: 0.1746Epoch 6/1035/35 [==============================] - 8s 232ms/step - loss: 0.0445 - root_mean_square_error: 0.2095 - mean_absolute_error: 0.1774 - val_loss: 0.0427 - val_root_mean_square_error: 0.2064 - val_mean_absolute_error: 0.1650Epoch 7/1035/35 [==============================] - 8s 220ms/step - loss: 0.0429 - root_mean_square_error: 0.2073 - mean_absolute_error: 0.1712 - val_loss: 0.0445 - val_root_mean_square_error: 0.2108 - val_mean_absolute_error: 0.1963Epoch 8/1035/35 [==============================] - 9s 238ms/step - loss: 0.0417 - root_mean_square_error: 0.2053 - mean_absolute_error: 0.1672 - val_loss: 0.0455 - val_root_mean_square_error: 0.2132 - val_mean_absolute_error: 0.2007Epoch 9/1035/35 [==============================] - 8s 234ms/step - loss: 0.0410 - root_mean_square_error: 0.2026 - mean_absolute_error: 0.1665 - val_loss: 0.0414 - val_root_mean_square_error: 0.2033 - val_mean_absolute_error: 0.1802Epoch 10/1035/35 [==============================] - 8s 229ms/step - loss: 0.0389 - root_mean_square_error: 0.1973 - mean_absolute_error: 0.1607 - val_loss: 0.0410 - val_root_mean_square_error: 0.2023 - val_mean_absolute_error: 0.176515/15 [==============================] - 2s 129ms/step - loss: 0.0411 - root_mean_square_error: 0.2025 - mean_absolute_error: 0.1768Test Evaluation:\tloss: 0.0411\troot_mean_square_error: 0.2025\tmean_absolute_error: 0.1768Mean Baseline Test set metrics:\troot_mean_square_error =  0.21047757725548227\tmean_absolute_error =  0.17720323507903052Model Test set metrics:\troot_mean_square_error =  0.20244772001149536\tmean_absolute_error =  0.17655691348749106Done trainingg_model.test()&lt;Evaluate&gt;WARNING:tensorflow:5 out of the last 19 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7f8539bf9d08&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.--------------------------------------------------Prediction Time: 379.64sec# of Test Examples: 200Average Weighted Kendalll Tau Corrleation over iterations: 0.0575Best Weighted Kendalll Tau Corrleation: 0.0914--------------------------------------------------import tensorflow as tftf.keras.backend.clear_session() # not workingfrom numba import cudadevice = cuda.get_current_device()device.reset()",
        "url": "/fashion_GNN"
    }
    ,
    
    "dm-final": {
        "title": "Data Mining - Final",
        "author": "Darron Kwon",
        "category": "",
        "content": "Final Project  1. Data Load and Preprocessing  2. Data Visualization          2.1. Stock Price Volatility      2.2. Rolling Average of Stock Price Correlation        3. Network Analysis          3.1. Build Graph with Correlation table      3.2. Setting threshold on weights      3.3. Community Detection      Louvain Algorithm      1. Data Load and Preprocessingimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltfrom matplotlib.collections import LineCollectionfrom sklearn import cluster, covariance, manifoldfrom community import community_louvain as louvainimport matplotlib.cm as cmimport networkx as nximport networkx.algorithms.community as nxcomfrom importlib import reloadimport csvimport osimport re%matplotlib inlinedf_prices = pd.read_csv(\"SP500_prices.csv\", index_col = 0)df_indices = pd.read_csv(\"indices.csv\")df_SP500 = pd.read_csv(\"SP500.csv\",index_col = 0)df_prices.describe()                  open      high      low      close      volume      adjusted                  count      282821.000000      282821.000000      282821.000000      282821.000000      2.828210e+05      282821.000000              mean      139.384227      141.184891      137.569435      139.425703      4.983235e+06      137.510842              std      249.635882      253.080583      246.310869      249.739199      1.180107e+07      249.704273              min      3.220000      3.290000      3.020000      3.120000      0.000000e+00      3.092542              25%      47.780000      48.419998      47.120000      47.790001      1.010000e+06      46.309925              50%      86.430000      87.480003      85.320000      86.420000      2.034800e+06      84.330002              75%      151.500000      153.380005      149.600006      151.570000      4.528900e+06      148.930000              max      4742.610000      4832.800000      4700.000000      4776.410000      4.286171e+08      4776.410000      df_prices.head()                  symbol      date      open      high      low      close      volume      adjusted                  1      AAPL      2019-01-02      38.722500      39.712502      38.557499      39.480000      148158800.0      38.505024              2      AAPL      2019-01-03      35.994999      36.430000      35.500000      35.547501      365248800.0      34.669640              3      AAPL      2019-01-04      36.132500      37.137501      35.950001      37.064999      234428400.0      36.149662              4      AAPL      2019-01-07      37.174999      37.207500      36.474998      36.982498      219111200.0      36.069202              5      AAPL      2019-01-08      37.389999      37.955002      37.130001      37.687500      164101200.0      36.756794      df_prices_adj = df_prices[['symbol','date', 'adjusted']]df_prices_adj.columns = ['symbol','date','price']df_prices_adj.tail()                  symbol      date      price                  282817      NWS      2021-03-24      23.69              282818      NWS      2021-03-25      24.42              282819      NWS      2021-03-26      24.01              282820      NWS      2021-03-29      23.39              282821      NWS      2021-03-30      23.83      df_indices.describe()                  Unnamed: 0      price                  count      16385.000000      16215.000000              mean      8193.000000      588.995521              std      4730.086416      3309.895206              min      1.000000      -36.980000              25%      4097.000000      1.284350              50%      8193.000000      4.227200              75%      12289.000000      107.426300              max      16385.000000      59221.230000      df_indices.head()                  Unnamed: 0      symbol      date      price                  0      1      DPROPANEMBTX      2019-01-02      0.641              1      2      DPROPANEMBTX      2019-01-03      0.630              2      3      DPROPANEMBTX      2019-01-04      0.635              3      4      DPROPANEMBTX      2019-01-07      0.623              4      5      DPROPANEMBTX      2019-01-08      0.628      df_indices = df_indices[[\"symbol\",\"date\",\"price\"]]df_indices.tail()                  symbol      date      price                  16380      THREEFY5      2021-03-24      0.8123              16381      THREEFY5      2021-03-25      0.8126              16382      THREEFY5      2021-03-26      0.8354              16383      THREEFY5      2021-03-29      0.8687              16384      THREEFY5      2021-03-30      0.8818      df_SP500.head()                  symbol      company      identifier      sedol      weight      sector      shares_held      local_currency      exchange                  1      AAPL      Apple Inc.      03783310      2046251      0.059338      Information Technology      161340980      USD      SP500              2      MSFT      Microsoft Corporation      59491810      2588173      0.055094      Information Technology      77110660      USD      SP500              3      AMZN      Amazon.com Inc.      02313510      2000019      0.040733      Consumer Discretionary      4376067      USD      SP500              4      FB      Facebook Inc. Class A      30303M10      B7TL820      0.021718      Communication Services      24592958      USD      SP500              5      GOOGL      Alphabet Inc. Class A      02079K30      BYVY8G0      0.019521      Communication Services      3074670      USD      SP500      df = pd.concat([df_prices_adj,df_indices])df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='ignore')df.set_index(['date','symbol'],inplace=True)df=df.unstack()['price']df.fillna(method='bfill',inplace=True)df            symbol      A      AAL      AAP      AAPL      ABBV      ABC      ABMD      ABT      ACN      ADBE      ...      XEL      XLNX      XOM      XRAY      XYL      YUM      ZBH      ZBRA      ZION      ZTS              date                                                                                                                                                2019-01-02      64.511734      31.96316      156.2589      38.505024      79.101799      71.46416      309.96      67.034943      136.179626      224.570007      ...      45.400452      84.360565      60.557911      37.21114      64.63606      87.819199      100.576180      156.24      38.71991      83.337715              2019-01-03      62.135132      29.58167      161.1371      34.669640      76.495514      70.42746      302.29      63.871284      131.530212      215.699997      ...      45.221561      81.184296      59.628124      37.23077      62.42029      85.610275      98.756989      146.88      38.50573      80.457184              2019-01-04      64.285828      31.53016      157.1396      36.149662      78.959961      71.24338      313.44      65.694260      136.644577      226.190002      ...      45.664082      84.943359      61.826595      38.31106      65.05392      87.838394      102.129860      152.97      39.68837      83.613907              2019-01-07      65.650917      32.42568      159.4450      36.069202      80.112411      71.75211      314.80      66.678070      137.119217      229.259995      ...      45.466366      87.187141      62.148109      38.99852      64.09184      87.742371      102.169182      155.29      39.84668      84.117012              2019-01-08      66.613335      31.90411      158.3368      36.756794      80.484734      72.52003      318.42      65.877518      140.586914      232.679993      ...      45.993618      85.526176      62.599968      38.73336      64.69435      87.569496      99.877983      156.33      40.20985      85.369850              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              2021-03-24      120.656403      21.81000      181.7300      120.089996      103.059998      115.37000      294.21      118.019997      267.549988      451.510010      ...      65.580002      119.959999      56.340000      60.25000      101.11000      107.080002      157.210648      463.81      53.01000      155.429993              2021-03-25      121.714798      22.77000      185.6500      120.589996      103.879997      117.34000      294.14      119.050003      268.609985      450.989990      ...      66.000000      120.029999      56.180000      60.55000      101.93000      107.379997      157.639999      461.26      54.74000      152.880005              2021-03-26      125.449112      22.93000      187.3200      121.209999      105.980003      118.73000      301.40      122.070000      280.769989      469.089996      ...      66.309998      123.139999      57.709999      61.28000      104.76000      108.059998      161.320007      476.96      55.85000      156.149994              2021-03-29      125.229446      22.91000      185.0600      121.389999      106.730003      119.05000      305.77      122.230003      279.540009      469.320007      ...      67.000000      122.230003      57.400002      62.06000      104.27000      109.209999      160.210007      467.07      53.89000      158.389999              2021-03-30      124.650322      24.12000      186.0700      119.900002      106.790001      119.06000      309.88      119.750000      278.549988      465.459991      ...      66.010002      120.300003      56.689999      63.54000      104.88000      109.769997      161.220001      474.83      55.91000      157.039993      565 rows × 531 columnsdf = (df-df.mean())/df.std()df.describe()            symbol      A      AAL      AAP      AAPL      ABBV      ABC      ABMD      ABT      ACN      ADBE      ...      XEL      XLNX      XOM      XRAY      XYL      YUM      ZBH      ZBRA      ZION      ZTS                  count      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      ...      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02              mean      -5.533430e-16      -1.156990e-15      -2.339132e-15      2.364284e-15      -3.420666e-15      -1.307902e-15      7.797106e-16      4.426744e-15      8.551665e-16      4.326136e-15      ...      -1.237476e-14      4.099769e-15      7.545587e-16      -1.760637e-16      -7.646194e-15      -1.936701e-15      -2.678683e-15      -1.810941e-15      -2.867323e-15      -3.068539e-15              std      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      ...      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00              min      -1.402956e+00      -1.578205e+00      -4.368070e+00      -1.422373e+00      -1.783790e+00      -1.920594e+00      -1.960001e+00      -1.883874e+00      -2.071674e+00      -1.646058e+00      ...      -2.597860e+00      -2.077320e+00      -1.906102e+00      -2.730556e+00      -2.111907e+00      -4.089893e+00      -3.106090e+00      -1.373928e+00      -2.269724e+00      -2.059833e+00              25%      -7.205020e-01      -1.081580e+00      -3.269484e-01      -9.160365e-01      -8.075055e-01      -6.871508e-01      -9.956515e-01      -7.258871e-01      -7.138452e-01      -9.190687e-01      ...      -5.309481e-01      -7.727544e-01      -9.708038e-01      -8.461749e-01      -5.681917e-01      -6.355256e-01      -7.359661e-01      -6.979548e-01      -1.004871e+00      -7.099711e-01              50%      -3.988059e-01      3.186508e-01      2.191719e-01      -2.702986e-01      -7.196453e-02      -2.771314e-01      2.134506e-01      -3.168558e-01      -2.189815e-01      -2.961142e-01      ...      6.798383e-02      -1.999376e-01      3.236947e-01      1.573233e-01      -1.819963e-01      3.273311e-02      1.208611e-01      -2.155373e-01      2.149392e-01      -1.179682e-01              75%      6.010351e-01      8.994444e-01      5.329562e-01      1.102648e+00      7.246279e-01      7.769495e-01      8.128134e-01      8.944433e-01      7.914156e-01      1.047349e+00      ...      6.942542e-01      6.930862e-01      8.373723e-01      8.655397e-01      3.657484e-01      7.911210e-01      7.965623e-01      2.228456e-01      6.929752e-01      1.075490e+00              max      2.370841e+00      1.631285e+00      2.129579e+00      2.053239e+00      2.192993e+00      2.759439e+00      1.856881e+00      2.525851e+00      2.386994e+00      1.970695e+00      ...      2.121844e+00      2.439377e+00      1.533337e+00      2.122636e+00      2.542459e+00      1.801157e+00      1.843039e+00      3.113057e+00      2.442046e+00      1.818189e+00      8 rows × 531 columns2. Data Visualization2.1. Stock Price Volatility%matplotlib inlinefig, ax1 = plt.subplots(figsize=(20, 15))df.iloc[:,:20].plot(ax=ax1, legend=False)plt.tight_layout()plt.show()df_delta = df.copy()for column in df_delta.columns.values.tolist():    df_delta[column] = df_delta[column]- df_delta[column].shift(1)df_delta.iloc[0]=0df_delta            symbol      A      AAL      AAP      AAPL      ABBV      ABC      ABMD      ABT      ACN      ADBE      ...      XEL      XLNX      XOM      XRAY      XYL      YUM      ZBH      ZBRA      ZION      ZTS              date                                                                                                                                                2019-01-02      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      ...      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000              2019-01-03      -0.132308      -0.278940      0.281110      -0.123113      -0.190242      -0.096013      -0.126329      -0.210193      -0.138905      -0.100851      ...      -0.029680      -0.171040      -0.072336      0.002954      -0.197347      -0.215205      -0.106702      -0.114809      -0.028157      -0.119316              2019-01-04      0.119732      0.228224      -0.230359      0.047508      0.179889      0.075565      0.183646      0.121119      0.152796      0.119270      ...      0.073418      0.202423      0.171038      0.162583      0.234564      0.217075      0.197830      0.074699      0.155477      0.130757              2019-01-07      0.075996      0.104891      0.132851      -0.002583      0.084121      0.047115      0.022400      0.065364      0.014180      0.034905      ...      -0.032803      0.120826      0.025013      0.103462      -0.085687      -0.009355      0.002306      0.028457      0.020812      0.020839              2019-01-08      0.053579      -0.061091      -0.063861      0.022071      0.027177      0.071120      0.059623      -0.053189      0.103600      0.038885      ...      0.087476      -0.089442      0.035154      -0.039906      0.053662      -0.016842      -0.134387      0.012757      0.047745      0.051895              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              2021-03-24      -0.063369      -0.053879      0.126777      -0.078644      -0.129928      0.148182      -0.072635      -0.164107      0.045411      -0.098804      ...      0.023227      -0.116853      0.087134      0.063104      0.063236      -0.089631      0.076719      0.014228      -0.074936      -0.025267              2021-03-25      0.058922      0.112443      0.225893      0.016050      0.059855      0.182449      -0.001153      0.068434      0.031668      -0.005913      ...      0.069681      0.003769      -0.012448      0.045150      0.073033      0.029227      0.025183      -0.031278      0.227436      -0.105625              2021-03-26      0.207894      0.018741      0.096235      0.019902      0.153287      0.128733      0.119576      0.200649      0.363291      0.205795      ...      0.051431      0.167471      0.119032      0.109865      0.252053      0.066249      0.215845      0.192574      0.145927      0.135448              2021-03-29      -0.012229      -0.002343      -0.130234      0.005778      0.054745      0.029636      0.071976      0.010631      -0.036747      0.002615      ...      0.114477      -0.049003      -0.024117      0.117390      -0.043642      0.112039      -0.065105      -0.121310      -0.257674      0.092785              2021-03-30      -0.032241      0.141726      0.058202      -0.047828      0.004379      0.000926      0.067694      -0.164771      -0.029578      -0.043888      ...      -0.164249      -0.103929      -0.055237      0.222739      0.054329      0.054558      0.059240      0.095183      0.265562      -0.055920      565 rows × 531 columns%matplotlib inlinefig, ax1 = plt.subplots(figsize=(20, 15))df_delta.plot(ax=ax1, legend=False)plt.tight_layout()plt.show()2.2. Rolling Average of Stock Price Correlationdef calculate_corr(df_stock_returns, returns_window, corr_window_size, corr_method):    stocks_cross_corr_dict = {}    x_days = []    y_mean_corr = []            for i in range(returns_window,len(df_stock_returns),corr_window_size):        dic_key = i        stocks_cross_corr_dict[dic_key]=df_stock_returns.iloc[i:(i+W)].corr(method='pearson')        stocks_cross_corr_dict[dic_key].fillna(0,inplace=True)        x_days.append(dic_key)        y_mean_corr.append(np.mean([abs(j) for j in stocks_cross_corr_dict[dic_key].values.flatten().tolist()]))            return stocks_cross_corr_dict, x_days,y_mean_corr%matplotlib inlinestart = 21end = 126step = 21;plt.figure(figsize=(20, 10))for t in range(start, end, step):    x_days = []    y_mean_corr = []    W = t    _, x_days, y_mean_corr = calculate_corr(df,1,W, 'pearson')    plt.plot(x_days, y_mean_corr)    plt.xlabel('Days')    plt.ylabel('Mean Correlation')    l = list(range(start, end, step))    plt.legend(l, loc='upper left')     plt.show()3. Network Analysis3.1. Build Graph with Correlation table# craetes a graph from correlation matrixcor_matrix = df.corr()cor_matrix            symbol      A      AAL      AAP      AAPL      ABBV      ABC      ABMD      ABT      ACN      ADBE      ...      XEL      XLNX      XOM      XRAY      XYL      YUM      ZBH      ZBRA      ZION      ZTS              symbol                                                                                                                                                A      1.000000      -0.512211      0.340607      0.936847      0.901146      0.866659      0.476208      0.932064      0.916575      0.876950      ...      0.581891      0.661159      -0.481706      0.276888      0.819616      0.232105      0.731162      0.947770      0.196208      0.830023              AAL      -0.512211      1.000000      0.452672      -0.709976      -0.567905      -0.636546      0.067317      -0.620271      -0.515571      -0.761706      ...      -0.685099      0.040970      0.950026      0.414638      -0.104279      0.401344      -0.116920      -0.402034      0.675397      -0.720983              AAP      0.340607      0.452672      1.000000      0.132583      0.218725      0.129889      0.598790      0.206132      0.304547      0.062110      ...      -0.102458      0.592191      0.471304      0.536131      0.517752      0.604171      0.464449      0.358634      0.691259      0.045798              AAPL      0.936847      -0.709976      0.132583      1.000000      0.883271      0.900174      0.311099      0.946722      0.931212      0.971310      ...      0.752885      0.464395      -0.694573      0.138280      0.700491      0.103699      0.668931      0.859869      -0.067884      0.937015              ABBV      0.901146      -0.567905      0.218725      0.883271      1.000000      0.842135      0.379425      0.820668      0.826314      0.834858      ...      0.525801      0.430849      -0.525965      0.167883      0.625416      0.017315      0.617159      0.868875      0.131018      0.756742              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              YUM      0.232105      0.401344      0.604171      0.103699      0.017315      0.148166      0.135065      0.246568      0.391225      0.042001      ...      0.150926      0.517301      0.446745      0.791644      0.591224      1.000000      0.622313      0.311883      0.601129      0.211623              ZBH      0.731162      -0.116920      0.464449      0.668931      0.617159      0.647128      0.138407      0.709774      0.815626      0.581120      ...      0.583386      0.473123      -0.072924      0.677935      0.793259      0.622313      1.000000      0.754226      0.495103      0.715109              ZBRA      0.947770      -0.402034      0.358634      0.859869      0.868875      0.842147      0.398625      0.902315      0.895845      0.774380      ...      0.477856      0.635769      -0.332037      0.410140      0.824975      0.311883      0.754226      1.000000      0.353050      0.754373              ZION      0.196208      0.675397      0.691259      -0.067884      0.131018      0.003435      0.274263      0.036986      0.155021      -0.207014      ...      -0.314988      0.431878      0.711608      0.768845      0.486766      0.601129      0.495103      0.353050      1.000000      -0.148741              ZTS      0.830023      -0.720983      0.045798      0.937015      0.756742      0.869275      0.075617      0.902965      0.917018      0.930572      ...      0.901222      0.331572      -0.696158      0.206471      0.652798      0.211623      0.715109      0.754373      -0.148741      1.000000      531 rows × 531 columnsmat_pos = cor_matrix[cor_matrix&gt;=0]mat_pos = mat_pos.fillna(0)symbols = cor_matrix.index.valuesmat_pos = np.asmatrix(mat_pos)G_pos = nx.from_numpy_matrix(mat_pos)G_pos = nx.relabel_nodes(G_pos,lambda x: symbols[x])G_pos.remove_edges_from(nx.selfloop_edges(G_pos))mat_neg = cor_matrix[cor_matrix&lt;0]mat_neg = mat_neg.fillna(0)mat_neg = abs(mat_neg)symbols = cor_matrix.index.valuesmat_neg = np.asmatrix(mat_neg)G_neg = nx.from_numpy_matrix(mat_neg)G_neg = nx.relabel_nodes(G_neg,lambda x: symbols[x])G_neg.remove_edges_from(nx.selfloop_edges(G_neg))list(G_pos.edges(data=True))[:5], list(G_neg.edges(data=True))[:5]([('A', 'AAP', {'weight': 0.3406072572702165}),  ('A', 'AAPL', {'weight': 0.9368465656977406}),  ('A', 'ABBV', {'weight': 0.9011464914155253}),  ('A', 'ABC', {'weight': 0.8666593549791352}),  ('A', 'ABMD', {'weight': 0.476208419378593})], [('A', 'AAL', {'weight': 0.5122106576033231}),  ('A', 'AEP', {'weight': 0.010066159905771194}),  ('A', 'AFL', {'weight': 0.14527226301899576}),  ('A', 'AIG', {'weight': 0.21700976911380374}),  ('A', 'ALK', {'weight': 0.13546044591421322})])symbol =  df.columnsdf_sector = df_SP500[df_SP500['symbol'].isin(symbol)]df_sector = df_sector[[\"symbol\",\"sector\"]]tmp = pd.DataFrame({\"symbol\":df_indices[\"symbol\"].unique(), \"sector\":\"Macroeconomic Indices\"})df_sector = pd.concat([df_sector,tmp],ignore_index=True)df_sector['sec_idx']= 0for i,sec in enumerate(df_sector[\"sector\"].unique()):    df_sector.loc[df_sector['sector']==sec,'sec_idx'] = i+1df_sector                  symbol      sector      sec_idx                  0      AAPL      Information Technology      1              1      MSFT      Information Technology      1              2      AMZN      Consumer Discretionary      2              3      FB      Communication Services      3              4      GOOGL      Communication Services      3              ...      ...      ...      ...              526      GOLDAMGBD228NLBM      Macroeconomic Indices      12              527      IOER      Macroeconomic Indices      12              528      IORR      Macroeconomic Indices      12              529      THREEFY1      Macroeconomic Indices      12              530      THREEFY5      Macroeconomic Indices      12      531 rows × 3 columnsnx.set_node_attributes(G_pos, df_sector.set_index('symbol')['sec_idx'],'sec_idx')nx.set_node_attributes(G_neg, df_sector.set_index('symbol')['sec_idx'],'sec_idx')3.2. Setting threshold on weightsdef set_threshold(G,threshold):    edges_rm = list(filter(lambda e: abs(e[2]) &lt; threshold, (e for e in G.edges.data('weight'))))        ids_rm = list(e[:2] for e in edges_rm)    H = G.copy()    H.remove_edges_from(ids_rm)    return HH_pos = set_threshold(G_pos,0.5)H_neg = set_threshold(G_neg,0.5)list(H_pos.edges(data=True))[:5], list(H_neg.edges(data=True))[:5]([('A', 'AAPL', {'weight': 0.9368465656977406}),  ('A', 'ABBV', {'weight': 0.9011464914155253}),  ('A', 'ABC', {'weight': 0.8666593549791352}),  ('A', 'ABT', {'weight': 0.932064482315709}),  ('A', 'ACN', {'weight': 0.916574717968791})], [('A', 'AAL', {'weight': 0.5122106576033231}),  ('A', 'BA', {'weight': 0.5064885797862092}),  ('A', 'BXP', {'weight': 0.522527131570438}),  ('A', 'CCL', {'weight': 0.5559250497113872}),  ('A', 'DEXCAUS', {'weight': 0.6382900166453468})])3.3. Community DetectionLouvain Algorithm# grid search# for positive weightsnp.random.seed(2021)t=0for cor_thresold in np.linspace(0.8,0.85,20):    H_pos = set_threshold(G_pos,cor_thresold)    partition = louvain.best_partition(H_pos)    modularity = louvain.modularity(partition, H_pos)    values = [partition.get(node) for node in H_pos.nodes()]    communities = []    tmp = list(partition.items())    for i in range(len(set(values))):        communities.append([n for n,c in tmp if c==i])    sum_comm_nodes = 0    k=0    print(\"{}th Total number of Communities = {}\".format(t ,len(communities)))    for i, comm_nodes in enumerate(communities):        if len(comm_nodes)&gt;=10:            k+=1            print('community {}th: '.format(i),len(comm_nodes))        sum_comm_nodes+=len(comm_nodes)    t+=1    print('n_big_communities: ',k)# best partition with 5 big communities at cor_threshold = np.linspace(0.8,0.85,20)[8] 0th Total number of Communities = 28community 0th:  168community 1th:  193community 6th:  36community 7th:  102n_big_communities:  41th Total number of Communities = 29community 0th:  166community 2th:  189community 4th:  105community 6th:  40n_big_communities:  42th Total number of Communities = 31community 0th:  157community 2th:  188community 4th:  103community 6th:  50n_big_communities:  43th Total number of Communities = 31community 0th:  161community 1th:  191community 4th:  112community 6th:  33n_big_communities:  44th Total number of Communities = 34community 0th:  167community 2th:  192community 4th:  115community 8th:  18n_big_communities:  45th Total number of Communities = 36community 0th:  162community 1th:  188community 4th:  103community 6th:  35n_big_communities:  46th Total number of Communities = 36community 0th:  172community 1th:  180community 3th:  113community 4th:  25n_big_communities:  47th Total number of Communities = 36community 0th:  152community 3th:  112community 4th:  46community 6th:  180n_big_communities:  48th Total number of Communities = 41community 2th:  77community 4th:  107community 5th:  157community 7th:  37community 9th:  108n_big_communities:  59th Total number of Communities = 41community 0th:  153community 4th:  103community 6th:  43community 7th:  79community 9th:  108n_big_communities:  510th Total number of Communities = 41community 0th:  146community 2th:  84community 4th:  109community 6th:  41community 9th:  106n_big_communities:  511th Total number of Communities = 42community 0th:  145community 2th:  178community 4th:  126community 8th:  35n_big_communities:  412th Total number of Communities = 46community 0th:  147community 2th:  79community 4th:  103community 7th:  48community 28th:  105n_big_communities:  513th Total number of Communities = 48community 1th:  104community 2th:  74community 4th:  107community 5th:  43community 7th:  152n_big_communities:  514th Total number of Communities = 48community 1th:  103community 2th:  78community 6th:  33community 7th:  104community 38th:  162n_big_communities:  515th Total number of Communities = 48community 0th:  150community 1th:  102community 2th:  75community 4th:  106community 7th:  47n_big_communities:  516th Total number of Communities = 50community 0th:  145community 1th:  103community 2th:  77community 7th:  50community 8th:  104n_big_communities:  517th Total number of Communities = 51community 1th:  103community 4th:  106community 6th:  40community 8th:  156community 9th:  74n_big_communities:  518th Total number of Communities = 51community 1th:  103community 2th:  73community 5th:  110community 6th:  36community 41th:  156n_big_communities:  519th Total number of Communities = 51community 1th:  101community 4th:  108community 5th:  42community 7th:  150community 32th:  77n_big_communities:  5np.random.seed(2021)cor_thresold = np.linspace(0.8,0.85,20)[8]H_pos = set_threshold(G_pos,cor_thresold)partition = louvain.best_partition(H_pos)values = [partition.get(node) for node in H_pos.nodes()]communities = []tmp = list(partition.items())for i in range(len(set(values))):    communities.append([n for n,c in tmp if c==i])sum_comm_nodes = 0k=0print(\"Total number of Communities = {}\".format(len(communities)))for i, comm_nodes in enumerate(communities):    if len(comm_nodes)&gt;=10:        k+=1        print('community {}th: '.format(i),len(comm_nodes))    sum_comm_nodes+=len(comm_nodes)print('n_big_communities: ',k)Total number of Communities = 41community 0th:  164community 2th:  79community 4th:  105community 7th:  30community 9th:  108n_big_communities:  5nx.set_node_attributes(H_pos,partition,'community')values = [partition.get(node) for node in H_pos.nodes()]plt.figure(figsize=(10,10))nx.draw_spring(H_pos, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False)np.random.seed(2021)# for negative weightst=0for cor_thresold in np.linspace(0.6,0.65,20):    H_neg = set_threshold(G_neg,cor_thresold)    partition = louvain.best_partition(H_neg)    modularity = louvain.modularity(partition, H_neg)    values = [partition.get(node) for node in H_neg.nodes()]    communities = []    tmp = list(partition.items())    for i in range(len(set(values))):        communities.append([n for n,c in tmp if c==i])    sum_comm_nodes = 0    k=0        print(\"{}th Total number of Communities = {}\".format(t ,len(communities)))    for i, comm_nodes in enumerate(communities):        if len(comm_nodes)&gt;=10:            k+=1            print('community {}th: '.format(i),len(comm_nodes))        sum_comm_nodes+=len(comm_nodes)    t+=1    print('n_big_communities: ',k)# best partition with 5 big communities at cor_threshold = np.linspace(0.6,0.65,20)[11] 0th Total number of Communities = 29community 0th:  213community 1th:  110community 2th:  161community 7th:  22n_big_communities:  41th Total number of Communities = 34community 0th:  101community 1th:  90community 2th:  164community 3th:  14community 7th:  133n_big_communities:  52th Total number of Communities = 35community 0th:  100community 1th:  92community 2th:  162community 7th:  133community 8th:  14n_big_communities:  53th Total number of Communities = 35community 0th:  97community 1th:  95community 2th:  175community 6th:  120community 7th:  14n_big_communities:  54th Total number of Communities = 35community 0th:  107community 1th:  84community 3th:  177community 6th:  119community 7th:  14n_big_communities:  55th Total number of Communities = 36community 0th:  107community 1th:  82community 5th:  120community 6th:  16community 21th:  175n_big_communities:  56th Total number of Communities = 35community 0th:  134community 1th:  111community 2th:  240community 6th:  15n_big_communities:  47th Total number of Communities = 36community 0th:  106community 1th:  80community 5th:  120community 6th:  16community 21th:  178n_big_communities:  58th Total number of Communities = 35community 1th:  170community 2th:  192community 5th:  123community 6th:  15n_big_communities:  49th Total number of Communities = 37community 0th:  104community 1th:  83community 2th:  177community 5th:  119community 6th:  16n_big_communities:  510th Total number of Communities = 40community 0th:  103community 1th:  83community 2th:  177community 3th:  14community 6th:  119n_big_communities:  511th Total number of Communities = 41community 0th:  104community 1th:  83community 2th:  176community 3th:  13community 6th:  119n_big_communities:  512th Total number of Communities = 42community 0th:  103community 1th:  81community 3th:  13community 6th:  130community 11th:  167n_big_communities:  513th Total number of Communities = 42community 0th:  106community 1th:  81community 2th:  176community 6th:  118community 7th:  13n_big_communities:  514th Total number of Communities = 43community 0th:  204community 1th:  165community 2th:  18community 5th:  105n_big_communities:  415th Total number of Communities = 44community 0th:  105community 1th:  80community 2th:  186community 7th:  107community 8th:  14n_big_communities:  516th Total number of Communities = 44community 0th:  101community 1th:  81community 2th:  167community 6th:  130community 7th:  13n_big_communities:  517th Total number of Communities = 45community 1th:  156community 2th:  185community 6th:  15community 14th:  134n_big_communities:  418th Total number of Communities = 48community 0th:  99community 1th:  74community 2th:  171community 5th:  128community 6th:  16n_big_communities:  519th Total number of Communities = 48community 0th:  155community 1th:  183community 6th:  136community 7th:  13n_big_communities:  4np.random.seed(2021)cor_thresold = np.linspace(0.6,0.65,20)[11] H_neg_otim = set_threshold(G_neg,cor_thresold)partition = louvain.best_partition(H_neg_otim)modularity = louvain.modularity(partition, H_neg)values = [partition.get(node) for node in H_neg.nodes()]communities = []tmp = list(partition.items())for i in range(len(set(values))):    communities.append([n for n,c in tmp if c==i])sum_comm_nodes = 0k=0print(\"Total number of Communities = {}\".format(len(communities)))for i, comm_nodes in enumerate(communities):    if len(comm_nodes)&gt;=10:        k+=1        print('community {}th: '.format(i),len(comm_nodes))    sum_comm_nodes+=len(comm_nodes)print('n_big_communities: ',k)Total number of Communities = 41community 0th:  102community 1th:  83community 3th:  13community 6th:  130community 11th:  167n_big_communities:  5nx.set_node_attributes(H_neg, partition,'community')values = [partition.get(node) for node in H_neg.nodes()]plt.figure(figsize=(10,10))nx.draw_spring(H_neg, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False)nx.write_graphml(H_pos, \"model_H_pos.graphml\")nx.write_graphml(H_neg, \"model_H_neg.graphml\")",
        "url": "/DM_final"
    }
    ,
    
    "appml-assginment1": {
        "title": "AppML - Assignment1",
        "author": "Darron Kwon",
        "category": "",
        "content": "2021-1. Applied Machine Learning# This Python 3 environment comes with many helpful analytics libraries installed# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python# For example, here's several helpful packages to loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files are available in the read-only \"../input/\" directory# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport osfor dirname, _, filenames in os.walk('/kaggle/input'):    for filename in filenames:        print(os.path.join(dirname, filename))# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session/kaggle/input/dsc3011/sample_submission.csv/kaggle/input/dsc3011/X_test.csv/kaggle/input/dsc3011/y_train.csv/kaggle/input/dsc3011/X_train.csv1. Load DataX_train = pd.read_csv(\"/kaggle/input/dsc3011/X_train.csv\")y_train = pd.read_csv(\"/kaggle/input/dsc3011/y_train.csv\")X_test = pd.read_csv(\"/kaggle/input/dsc3011/X_test.csv\")sample = pd.read_csv(\"/kaggle/input/dsc3011/sample_submission.csv\")X_train.head(5)                   id      age      sex      cp      trestbps      chol      fbs      restecg      thalach      exang      oldpeak      slope      ca      thal                  0      1      63      1      3      145      233      1      0      150      0      2.3      0      0      1              1      2      37      1      2      130      250      0      1      187      0      3.5      0      0      2              2      3      41      0      1      130      204      0      0      172      0      1.4      2      0      2              3      4      56      1      1      120      236      0      1      178      0      0.8      2      0      2              4      5      57      0      0      120      354      0      1      163      1      0.6      2      0      2      y_train.head()                  id      target                  0      1      1              1      2      1              2      3      1              3      4      1              4      5      1      2. Statistics &amp; Visualization# data typeslen(X_train), X_train.dtypes(233, id            int64 age           int64 sex           int64 cp            int64 trestbps      int64 chol          int64 fbs           int64 restecg       int64 thalach       int64 exang         int64 oldpeak     float64 slope         int64 ca            int64 thal          int64 dtype: object)# no missing valuesX_train.isnull().sum(axis=0)id          0age         0sex         0cp          0trestbps    0chol        0fbs         0restecg     0thalach     0exang       0oldpeak     0slope       0ca          0thal        0dtype: int64y_train['target'].value_counts()1    1330    100Name: target, dtype: int64# count uniques for categorical variablescats = ['sex','cp','fbs','restecg','exang','slope','ca','thal']for c in cats:    print(X_train[c].value_counts())1    1580     75Name: sex, dtype: int640    1072     721     363     18Name: cp, dtype: int640    1971     36Name: fbs, dtype: int640    1251    108Name: restecg, dtype: int640    1541     79Name: exang, dtype: int642    1101    1060     17Name: slope, dtype: int640    1371     492     273     184      2Name: ca, dtype: int642    1273     951     100      1Name: thal, dtype: int64import matplotlib.pyplot as pltfig, axes = plt.subplots(2, 4, figsize=(15, 10))i=0for c in cats:    axes[i//4,i%4].hist(X_train[c])    i+=1# statistics for numerical variablesnums = ['age','trestbps','chol','thalach', 'oldpeak']fig, axes = plt.subplots(2, 3, figsize=(15, 10))i=0for c in nums:    axes[i//3,i%3].hist(X_train.loc[:,c])    i+=1pd.concat([X_train[nums],y_train['target']],axis=1).corr()                  age      trestbps      chol      thalach      oldpeak      target                  age      1.000000      0.293608      0.309546      -0.386512      0.200013      -0.246499              trestbps      0.293608      1.000000      0.154624      -0.055758      0.203551      -0.157073              chol      0.309546      0.154624      1.000000      -0.054981      0.029328      -0.115616              thalach      -0.386512      -0.055758      -0.054981      1.000000      -0.346992      0.435763              oldpeak      0.200013      0.203551      0.029328      -0.346992      1.000000      -0.420039              target      -0.246499      -0.157073      -0.115616      0.435763      -0.420039      1.000000      3. Preprocessing &amp; Classification Accuracy3.1. Naive Bayes w/o preprocessing as a baselinefrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.naive_bayes import GaussianNBX_train = pd.read_csv(\"/kaggle/input/dsc3011/X_train.csv\")y_train = pd.read_csv(\"/kaggle/input/dsc3011/y_train.csv\")X_test = pd.read_csv(\"/kaggle/input/dsc3011/X_test.csv\")X_tr = X_train.drop(columns=[\"id\"])X_te = X_test.drop(columns=[\"id\"])y_tr = y_train['target'].valuesX_t,X_v,y_t,y_v = train_test_split(X_tr,y_tr, test_size=0.2, random_state=2021 )gnb = GaussianNB()gnb.fit(X_t, y_t)y_pred = gnb.predict(X_v)print(accuracy_score(y_v,y_pred))0.851063829787234gnb=GaussianNB()gnb.fit(X_tr,y_tr)y_pred = gnb.predict(X_te)submission0 = pd.DataFrame(X_test['id'])submission0['target'] = y_predsubmission0.to_csv(\"submission0.csv\",sep=',',index=False)3.2. One-hot encodingX_train = pd.read_csv(\"/kaggle/input/dsc3011/X_train.csv\",dtype={c:str for c in cats})y_train = pd.read_csv(\"/kaggle/input/dsc3011/y_train.csv\")X_test = pd.read_csv(\"/kaggle/input/dsc3011/X_test.csv\",dtype={c:str for c in cats})X_train = pd.concat([X_train,pd.get_dummies(X_train[cats],drop_first=True)],axis=1).drop(columns=cats)X_test = pd.concat([X_test,pd.get_dummies(X_test[cats],drop_first=True)],axis=1).drop(columns=cats)print(X_train.columns, X_test.columns)X_train['restecg_2']=0X_train.shape, X_test.shapeIndex(['id', 'age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'sex_1', 'cp_1',       'cp_2', 'cp_3', 'fbs_1', 'restecg_1', 'exang_1', 'slope_1', 'slope_2',       'ca_1', 'ca_2', 'ca_3', 'ca_4', 'thal_1', 'thal_2', 'thal_3'],      dtype='object') Index(['id', 'age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'sex_1', 'cp_1',       'cp_2', 'cp_3', 'fbs_1', 'restecg_1', 'restecg_2', 'exang_1', 'slope_1',       'slope_2', 'ca_1', 'ca_2', 'ca_3', 'ca_4', 'thal_1', 'thal_2',       'thal_3'],      dtype='object')((233, 23), (70, 23))X_tr = X_train.drop(columns=[\"id\"])X_te = X_test.drop(columns=[\"id\"])y_tr = y_train['target'].valuesX_t, X_v, y_t, y_v = train_test_split(X_tr,y_tr,test_size = 0.2,random_state=2021)gnb = GaussianNB()gnb.fit(X_t, y_t)y_pred = gnb.predict(X_v)print(accuracy_score(y_v,y_pred))0.8085106382978723gnb=GaussianNB()gnb.fit(X_tr,y_tr)y_pred = gnb.predict(X_te)submission1 = pd.DataFrame(X_test['id'])submission1['target'] = y_predsubmission1.to_csv(\"submission1.csv\",sep=',',index=False)3.2.1. One-Hot + StandardScalerfrom sklearn.preprocessing import StandardScalersc = StandardScaler()X_tr_pp = X_tr.copy()X_te_pp = X_te.copy()X_tr_pp[nums] = sc.fit_transform(X_tr_pp[nums])X_te_pp[nums] = sc.fit_transform(X_te_pp[nums])X_tr_pp[nums].describe()                  age      trestbps      chol      thalach      oldpeak                  count      2.330000e+02      2.330000e+02      2.330000e+02      2.330000e+02      2.330000e+02              mean      -9.309434e-17      -7.881154e-16      -1.405647e-16      1.038749e-16      1.777310e-16              std      1.002153e+00      1.002153e+00      1.002153e+00      1.002153e+00      1.002153e+00              min      -2.812990e+00      -2.108738e+00      -2.400765e+00      -2.841943e+00      -9.056098e-01              25%      -6.969140e-01      -6.559080e-01      -7.090580e-01      -5.598232e-01      -9.056098e-01              50%      -2.867959e-02      -9.712714e-02      -9.389202e-02      2.008834e-01      -2.149156e-01              75%      7.509272e-01      4.616537e-01      5.020500e-01      7.378527e-01      4.757786e-01              max      2.532886e+00      3.814339e+00      6.019320e+00      2.259266e+00      4.447270e+00      X_t, X_v, y_t, y_v = train_test_split(X_tr_pp,y_tr,test_size = 0.2,random_state=2021)gnb = GaussianNB()gnb.fit(X_t, y_t)y_pred = gnb.predict(X_v)print(accuracy_score(y_v,y_pred))0.8085106382978723gnb=GaussianNB()gnb.fit(X_tr_pp,y_tr)y_pred = gnb.predict(X_te_pp)submission2 = pd.DataFrame(X_test['id'])submission2['target'] = y_predsubmission2.to_csv(\"submission2.csv\",sep=',',index=False)3.2.2 One-hot + Log-transformX_tr_log = X_tr.copy()X_te_log = X_te.copy()for c in nums:    X_tr_log[c] = np.log(X_tr_log[c]+1)    X_te_log[c] = np.log(X_te_log[c]+1)X_tr_log[nums].describe()                  age      trestbps      chol      thalach      oldpeak                  count      233.000000      233.000000      233.000000      233.000000      233.000000              mean      3.998016      4.879579      5.509133      5.015565      0.575909              std      0.170517      0.132206      0.197961      0.156684      0.522850              min      3.401197      4.553877      4.844187      4.488636      0.000000              25%      3.891820      4.795791      5.370638      4.941642      0.000000              50%      4.007333      4.875197      5.509388      5.056246      0.587787              75%      4.127134      4.948760      5.627621      5.129899      0.955511              max      4.356709      5.303305      6.336826      5.313206      1.974081      X_t, X_v, y_t, y_v = train_test_split(X_tr_log,y_tr,test_size = 0.2,random_state=2021)gnb = GaussianNB()gnb.fit(X_t, y_t)y_pred = gnb.predict(X_v)print(accuracy_score(y_v,y_pred))0.8085106382978723gnb=GaussianNB()gnb.fit(X_tr_log,y_tr)y_pred = gnb.predict(X_te_log)submission3 = pd.DataFrame(X_test['id'])submission3['target'] = y_predsubmission3.to_csv(\"submission3.csv\",sep=',',index=False)3.4. Varrying Classification Methodsfrom sklearn.model_selection import cross_val_scorefrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCVset1= X_tr,X_teset2= X_tr_pp, X_te_ppset3= X_tr_log, X_te_logdef grid_selection(set):    knn_grid =[        {'n_neighbors': range(3,15,1), 'p': [1,2]}]    knn = KNeighborsClassifier()    grid_knn = GridSearchCV(knn, knn_grid, cv=5,                               scoring='neg_mean_squared_error',                               return_train_score=True)    grid_knn.fit(X_tr,y_tr)        rf_grid =[        {'n_estimators': [5, 10, 20, 50, 75, 100], 'max_features': [2, 4, 6, 8]}]    rf = RandomForestClassifier()    grid_rf = GridSearchCV(rf, rf_grid, cv=5,                               scoring='neg_mean_squared_error',                               return_train_score=True)    grid_rf.fit(set[0],y_tr)    return grid_knn.best_estimator_, grid_rf.best_estimator_def rmse_scores(set):    gnb = GaussianNB()    svc = SVC()    dt = DecisionTreeClassifier()    knn, rf = grid_selection(set)    models = [gnb, svc, dt, knn, rf]    results = {}    for model in models:        scores = cross_val_score(model, set[0],y_tr,                                scoring=\"neg_mean_squared_error\",cv=10)        results[model] = np.sqrt(-scores).mean()    return resultsset1_result = rmse_scores(set1)set2_result = rmse_scores(set2)set3_result = rmse_scores(set3)set1_result, set2_result, set3_result({GaussianNB(): 0.4178876039491642,  SVC(): 0.5823397149422178,  DecisionTreeClassifier(): 0.4729488489553738,  KNeighborsClassifier(n_neighbors=13, p=1): 0.5371866622228357,  RandomForestClassifier(max_features=2, n_estimators=20): 0.38835845413636216}, {GaussianNB(): 0.4415320745933398,  SVC(): 0.4067235886003079,  DecisionTreeClassifier(): 0.4586168121445854,  KNeighborsClassifier(n_neighbors=13, p=1): 0.413359640076294,  RandomForestClassifier(max_features=2, n_estimators=10): 0.483896282692258}, {GaussianNB(): 0.4415320745933398,  SVC(): 0.3752652644845328,  DecisionTreeClassifier(): 0.46699040892811083,  KNeighborsClassifier(n_neighbors=13, p=1): 0.38902561032241134,  RandomForestClassifier(max_features=4): 0.43207507210776175})print(min(set1_result.items(), key = lambda x: x[1]))print(min(set2_result.items(), key = lambda x: x[1]))print(min(set3_result.items(), key = lambda x: x[1]))(RandomForestClassifier(max_features=2, n_estimators=20), 0.38835845413636216)(SVC(), 0.4067235886003079)(SVC(), 0.3752652644845328)4. Predictions from the best classifiersrf = RandomForestClassifier(max_features=2, n_estimators=50)rf.fit(X_tr_pp,y_tr)y_pred = rf.predict(X_te_pp)submission4 = pd.DataFrame(X_test['id'])submission4['target'] = y_predsubmission4.to_csv(\"submission4.csv\",sep=',',index=False)# grid to SVC# C: float, default=1.0# kernel: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’# gamma: {‘scale’, ‘auto’} or float, default=’scale’X_t, X_v, y_t, y_v = train_test_split(X_tr_log,y_tr,test_size = 0.2,random_state=2021)svc = SVC(C=0.3)svc.fit(X_t, y_t)y_pred = svc.predict(X_v)print(accuracy_score(y_v,y_pred))0.8723404255319149svc = SVC()svc.fit(X_tr_log,y_tr)y_pred = svc.predict(X_te_log)submission5 = pd.DataFrame(X_test['id'])submission5['target'] = y_predsubmission5.to_csv(\"submission5.csv\",sep=',',index=False)5. Other Algorithms: Logistic RegressionX_t, X_v, y_t, y_v = train_test_split(X_tr_pp,y_tr,test_size = 0.2,random_state=2021)lr_clf = LogisticRegression()lr_clf.fit(X_t, y_t)y_pred = lr_clf.predict(X_v)print(accuracy_score(y_v,y_pred))0.8723404255319149lr_clf=LogisticRegression()lr_scores = cross_val_score(lr_clf, X_tr_pp,y_tr,                        scoring=\"neg_mean_squared_error\",cv=10)np.sqrt(-lr_scores).mean()0.3575812829170383lr_clf=LogisticRegression()lr_clf.fit(X_tr_pp, y_tr)y_pred = lr_clf.predict(X_te_pp)submission6 = pd.DataFrame(X_test['id'])submission6['target'] = y_predsubmission6.to_csv(\"submission6.csv\",sep=',',index=False)",
        "url": "/AppML_Assginment1"
    }
    ,
    
    "ds-r-final": {
        "title": "Data Science and R - Final",
        "author": "Darron Kwon",
        "category": "",
        "content": "  1. About  2. Load Data and Preprocess          2.1. Set Attributes      2.2. Handling NA values        3. EDA          3.1. 통합대기환경지수(Comprehensive air-quality index), CAI 계산      3.2. Timestep-wise Visualization                  3.2.1. Hourly          3.2.2. Daily          3.2.3. Montly          3.2.4. Quarterly          3.2.5. Half-yearly                      4. Summary1. About  2020-2, Data Science and R, Final Proejct: 대기환경오염도 데이터 분석2. Load Data and Preprocess  Collected data from Air Korea (https://www.airkorea.or.kr)library(dplyr)library(tidyverse)library(grid)library(vcd)library(ggplot2)library(readxl)data= read_xlsx(\"./final_data/data.xlsx\");head(data,5)nrow(data)날짜시도측정소명측정소코드아황산가스일산화탄소오존이산화질소PM10PM2.5\t2017-07-01 01인천 옹진군  백령도       831492       .0016        .3           .052         .0016        44           35           \t2017-07-01 02인천 옹진군  백령도       831492       .0016        .3           .052         .0017        21           NA           \t2017-07-01 03인천 옹진군  백령도       831492       .0017        .3           .051         .0017        32           18           \t2017-07-01 04인천 옹진군  백령도       831492       .0017        .3           .05          .002         10           NA           \t2017-07-01 05인천 옹진군  백령도       831492       .0016        .3           .048         .0018        24           22           263042.1. Set Attributes#   시도, 측정소명, 측정소코드는 모두 같으므로 제외#   날짜는 시구간별 분석을 위해 연, 월, 일, 시로 분리: int타입으로 반환됨#   측정 대상 오염 물질의 자료형 변환data &lt;- data %&gt;%  select(-c(시도,측정소명,측정소코드)) %&gt;%  separate(col=날짜,           into=c(\"year\",\"month\",\"day\",\"hour\"),           convert=TRUE) %&gt;%  mutate_if(is.character,as.numeric) %&gt;%  rename(SO2=아황산가스,CO=일산화탄소,O3=오존,NO2=이산화질소)head(data,5)yearmonthdayhourSO2COO3NO2PM10PM2.5\t2017  7     1     1     0.00160.3   0.052 0.001644    35    \t2017  7     1     2     0.00160.3   0.052 0.001721    NA    \t2017  7     1     3     0.00170.3   0.051 0.001732    18    \t2017  7     1     4     0.00170.3   0.050 0.002010    NA    \t2017  7     1     5     0.00160.3   0.048 0.001824    22    2.2. Handling NA values# 일평균 값을 확인daily &lt;- data %&gt;%  group_by(year,month,day) %&gt;%  select_if(is.double) %&gt;%  summarise_all(mean,na.rm=TRUE) %&gt;%  ungroup()summary(daily)      year          month             day             SO2            Min.   :2017   Min.   : 1.000   Min.   : 1.00   Min.   :0.0001765   1st Qu.:2018   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.:0.0012553   Median :2018   Median : 7.000   Median :16.00   Median :0.0017042   Mean   :2018   Mean   : 6.522   Mean   :15.73   Mean   :0.0018339   3rd Qu.:2019   3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.:0.0022417   Max.   :2020   Max.   :12.000   Max.   :31.00   Max.   :0.0078375                                                   NA's   :1                 CO                O3               NO2                PM10        Min.   :0.04583   Min.   :0.01188   Min.   :0.000850   Min.   :  2.00   1st Qu.:0.23750   1st Qu.:0.03453   1st Qu.:0.002373   1st Qu.: 20.88   Median :0.32917   Median :0.04290   Median :0.003296   Median : 29.25   Mean   :0.37287   Mean   :0.04405   Mean   :0.003853   Mean   : 35.58   3rd Qu.:0.46667   3rd Qu.:0.05192   3rd Qu.:0.004764   3rd Qu.: 42.54   Max.   :1.39167   Max.   :0.09971   Max.   :0.020209   Max.   :217.04   NA's   :2                                              NA's   :5            PM2.5         Min.   :  1.826   1st Qu.: 10.042   Median : 14.625   Mean   : 18.736   3rd Qu.: 22.375   Max.   :126.375   NA's   :23       # NA: 하루 전체 관측치가 NA인 경우에 발생# 월평균 값을 확인monthly &lt;- data %&gt;%  group_by(year,month) %&gt;%  select_if(is.double) %&gt;%  summarise_all(mean,na.rm=TRUE) %&gt;%  ungroup()summary(monthly)      year          month            SO2                 CO         Min.   :2017   Min.   : 1.00   Min.   :0.001099   Min.   :0.1911   1st Qu.:2018   1st Qu.: 3.75   1st Qu.:0.001394   1st Qu.:0.2646   Median :2018   Median : 6.50   Median :0.001789   Median :0.3562   Mean   :2018   Mean   : 6.50   Mean   :0.001832   Mean   :0.3730   3rd Qu.:2019   3rd Qu.: 9.25   3rd Qu.:0.002084   3rd Qu.:0.4491   Max.   :2020   Max.   :12.00   Max.   :0.003140   Max.   :0.7588         O3               NO2                PM10           PM2.5       Min.   :0.02447   Min.   :0.001857   Min.   :20.24   Min.   :10.62   1st Qu.:0.03821   1st Qu.:0.002876   1st Qu.:27.31   1st Qu.:13.94   Median :0.04175   Median :0.004066   Median :35.42   Median :18.41   Mean   :0.04410   Mean   :0.003850   Mean   :35.69   Mean   :18.92   3rd Qu.:0.05154   3rd Qu.:0.004658   3rd Qu.:42.46   3rd Qu.:22.84   Max.   :0.06422   Max.   :0.005683   Max.   :58.00   Max.   :36.51  # NA값들을 일평균 또는 월평균 값으로 대체for (i in (1:nrow(data))){  for (j in c(\"SO2\",\"CO\",\"O3\",\"NO2\",\"PM10\",\"PM2.5\")){    if (is.na(data[i,][[j]])){      tmp&lt;-data[i,]      y&lt;-tmp$year      m&lt;-tmp$month      d&lt;-tmp$day      if (!is.nan(daily[daily$year==y &amp; daily$month==m &amp; daily$day==d,][[j]])){        data[i,][[j]]&lt;-daily[daily$year==y &amp; daily$month==m &amp; daily$day==d,][[j]]      }else{        data[i,][[j]]&lt;-monthly[monthly$year==y &amp; monthly$month==m,][[j]]      }    }  }}summary(data)      year          month             day             hour       Min.   :2017   Min.   : 1.000   Min.   : 1.00   Min.   : 1.00   1st Qu.:2018   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.: 6.75   Median :2018   Median : 7.000   Median :16.00   Median :12.50   Mean   :2018   Mean   : 6.522   Mean   :15.73   Mean   :12.50   3rd Qu.:2019   3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.:18.25   Max.   :2020   Max.   :12.000   Max.   :31.00   Max.   :24.00        SO2                 CO               O3               NO2           Min.   :0.000000   Min.   :0.0000   Min.   :0.00100   Min.   :0.000300   1st Qu.:0.001200   1st Qu.:0.2000   1st Qu.:0.03400   1st Qu.:0.002100   Median :0.001700   Median :0.3000   Median :0.04300   Median :0.003000   Mean   :0.001834   Mean   :0.3726   Mean   :0.04405   Mean   :0.003853   3rd Qu.:0.002300   3rd Qu.:0.5000   3rd Qu.:0.05200   3rd Qu.:0.004600   Max.   :0.034700   Max.   :2.5000   Max.   :0.13200   Max.   :0.045900        PM10            PM2.5        Min.   :  1.00   Min.   :  0.00   1st Qu.: 19.00   1st Qu.:  9.00   Median : 28.00   Median : 14.00   Mean   : 35.57   Mean   : 18.78   3rd Qu.: 43.00   3rd Qu.: 22.77   Max.   :461.00   Max.   :219.00  3. EDA3.1. 통합대기환경지수(Comprehensive air-quality index), CAI 계산### 통합대기환경지수CAI 지수를 계산# 항목별 계산 함수를 정의SO2_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,0.02,0.05,0.15,1,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0.02,0.02,0.05,0.15,1,1))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,0.021,0.051,0.151,0.151))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (((I_hi-I_lo)/(BP_hi-BP_lo)) * (C_p - BP_lo)) + I_lo  return(I_p)}CO_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,2,9,15,50,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(2,2,9,15,50,50))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,2.01,9.01,15.01,15.01))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}O3_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,0.03,0.09,0.15,0.6,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0.03,0.03,0.09,0.15,0.6,0.6))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,0.031,0.091,0.151,0.151))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}NO2_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,0.03,0.06,0.2,2,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0.03,0.03,0.06,0.2,2,2))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,0.031,0.061,0.201,0.201))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}# PM10, PM2.5의 이동평균을 구하기 위한 함수 정의C12_cal &lt;- function(PM){  C12 &lt;- c()  for (i in (1:length(PM))){    if (i&lt;12){      tmp &lt;- sum(PM[1:i])/i    }else{      tmp &lt;- sum(PM[(i-11):i])/12    }    C12[i] &lt;- tmp  }  return (C12)}C_ai_cal &lt;- function(C_i,M,C12){  if (C_i &lt; M){    C_ai &lt;- C_i  }else if(0.9&lt;= (C_i/C12) &amp;&amp; (C_i/C12) &lt;=1.7){    C_ai &lt;- 0.75*C_i    }else {    C_ai &lt;- C_i  }  return (C_ai)}PM_cal &lt;- function(PM,m,PM_C12){  M=m  C24E=c()  for (i in (1:length(PM))){    if (i&lt;4){      C4 &lt;-0      for (j in (i:1)){        C4 &lt;- C4+ C_ai_cal(PM[j],M,PM_C12[j])      }      C4/i    } else{      C4 &lt;- sum(C_ai_cal(PM[i],M,PM_C12[i]),C_ai_cal(PM[i-1],M,PM_C12[i-1]),              C_ai_cal(PM[i-2],M,PM_C12[i-2]),C_ai_cal(PM[i-3],M,PM_C12[i-3]))/4    }    C12&lt;-PM_C12[i]    C24E[i] = (C12*12+C4*12)/24  }  return (C24E)}PM10_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,30,80,150,600,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE,                                       labels=c(30,30,80,150,600,600))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE,                                       labels=c(0,0,31,81,151,151))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE,                                      labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE,                                      labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}PM2.5_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,15,35,75,500,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(15,15,35,75,500,500))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,16,36,76,76))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}PM10_C12&lt;-C12_cal(data$PM10)PM10_C_p &lt;- PM_cal(data$PM10,70,PM10_C12)PM2.5_C12 &lt;- C12_cal(data$PM2.5)PM2.5_C_p &lt;- PM_cal(data$PM2.5,30,PM2.5_C12)data &lt;- data %&gt;%  bind_cols(\"SO2_I_p\" = SO2_I_p(data$SO2)) %&gt;%  bind_cols(\"CO_I_p\" = CO_I_p(data$CO)) %&gt;%  bind_cols(\"O3_I_p\" = O3_I_p(data$O3)) %&gt;%  bind_cols(\"NO2_I_p\" = NO2_I_p(data$NO2)) %&gt;%  bind_cols(\"PM10_I_p\" = PM10_I_p(PM10_C_p)) %&gt;%  bind_cols(\"PM2.5_I_p\" = PM2.5_I_p(PM2.5_C_p))CAI_table &lt;- data %&gt;%  select(ends_with(\"I_p\")) %&gt;%  mutate(BAD = rowSums(. &gt;100)) %&gt;%  mutate(CAI = apply(., 1, max) + case_when(BAD&gt;=3 ~ 75, BAD&gt;=2 ~ 50,TRUE ~ 0)) %&gt;%  mutate(CAI_index = case_when(CAI&gt;250 ~ 3, CAI&gt;100 ~ 2,                               CAI&gt;50 ~ 1, TRUE ~ 0)) %&gt;%  select(starts_with(\"CAI\"))data &lt;- data %&gt;%  bind_cols(CAI_table)head(data[c(11:18)],5)SO2_I_pCO_I_pO3_I_pNO2_I_pPM10_I_pPM2.5_I_pCAICAI_index\t4.00     7.5      68.44068 2.666667 64.00000  88.71711 88.717111        \t4.00     7.5      68.44068 2.833333 68.75000 103.38782103.387822        \t4.25     7.5      67.61017 2.833333 84.66667 131.82942131.829422        \t4.25     7.5      66.77966 3.333333 44.58333  66.44682 66.779661        \t4.00     7.5      65.11864 3.000000 39.95833  64.79737 65.118641        3.2. Timestep-wise Visualization  시구간별 시각화 및 분석3.2.1. Hourly# 시간대(주간: 07~18, 야간: 19~06)에 따른 CAI 지수 분포CAI_hourly &lt;- data %&gt;%  mutate(time_tmp = case_when(    06&lt;hour &amp; hour&lt;19 ~ \"daytime\", TRUE ~ \"nighttime\")) %&gt;%  group_by(year,month,day,time_tmp) %&gt;%  summarise(mean_CAI=mean(CAI)) %&gt;%  mutate(CAI_idx = case_when(mean_CAI&gt;250 ~ 3, mean_CAI&gt;100 ~ 2,                             mean_CAI&gt;50 ~ 1, TRUE ~ 0)) %&gt;% # 해당 시간대의 평균 오염도  ungroup()# 시간에 따른 오염도의 분포는 거의 차이가 없음을 확인CAI_hourly %&gt;%  group_by(time_tmp,CAI_idx)%&gt;%  tally()`summarise()` has grouped output by 'year', 'month', 'day'. You can override using the `.groups` argument.time_tmpCAI_idxn\tdaytime  0        100      \tdaytime  1        886      \tdaytime  2         93      \tdaytime  3         17      \tnighttime0         73      \tnighttime1        895      \tnighttime2        115      \tnighttime3         13      3.2.2. Daily### 일단위: 일평균 CAI 지수의 분포CAI_daily &lt;- data %&gt;%  group_by(year,month,day) %&gt;%  summarise_at(.vars=\"CAI\", .funs=mean) %&gt;%  ungroup() %&gt;%  mutate(CAI_idx = case_when(CAI&gt;250 ~ 3, CAI&gt;100 ~ 2,                             CAI&gt;50 ~ 1, TRUE ~ 0))summary(factor(CAI_daily$CAI_idx))# 평균 수준보다 오염도가 높은 날들은 연중 특정 시기에 집중되어있음CAI_daily %&gt;%  unite(\"date\",1:3,sep=\"/\") %&gt;%  mutate_at(\"date\",as.Date) %&gt;%  ggplot() +  geom_point(mapping=aes(x=date,y=CAI))3.2.3. Montly### 월단위: 오염 수준 나쁨 이상인 \"일수\" 확인CAI_monthly &lt;- CAI_daily %&gt;%  group_by(year,month) %&gt;%  summarise(mean_CAI = mean(CAI),sum_CAI_idx = sum(CAI_idx&gt;1)) %&gt;%  mutate(m_tmp = case_when(month&lt;10 ~ paste(\"0\",as.character(month),sep=\"\"),                           TRUE ~ paste(as.character(month))))%&gt;%  mutate(y_tmp=as.character(year)) %&gt;%  unite(date,y_tmp,m_tmp,sep=\"\")# 6월~9월은 장마 영향으로 추정. 겨울~봄에 집중적.CAI_monthly %&gt;%  ggplot() +  geom_col(mapping=aes(x=date,y=sum_CAI_idx))`summarise()` has grouped output by 'year'. You can override using the `.groups` argument.3.2.4. Quarterly### 분기 단위CAI_qtr &lt;- CAI_monthly %&gt;%  mutate(qtr_tmp = case_when(    month &lt; 4 ~ \"Q1\", month &lt; 7 ~ \"Q2\", month &lt; 10 ~ \"Q3\", TRUE ~ \"Q4\")) %&gt;%  mutate(y_tmp=as.character(year)) %&gt;%  unite(Qtr, y_tmp, qtr_tmp, sep=\"-\") %&gt;%  group_by(Qtr) %&gt;%  summarise(mean_CAI = mean(mean_CAI), sum_CAI_idx = sum(sum_CAI_idx))CAI_qtr %&gt;%  ggplot() +  geom_col(aes(x=Qtr,y=sum_CAI_idx))3.2.5. Half-yearly### 반기 단위: 2019년 상반기가 매우 심한 것이었음을 알 수 있음CAI_half &lt;- CAI_monthly %&gt;%  transform(date=as.integer(date)) %&gt;%  mutate(year = case_when(    date &lt; 201801 ~ \"2017_H2\", date &lt; 201807 ~ \"2018-H1\",    date &lt; 201901 ~ \"2018_H2\", date &lt; 201907 ~ \"2019-H1\",    date &lt; 202001 ~ \"2019-H2\", TRUE ~ \"2020-H1\")) %&gt;%  group_by(year) %&gt;%  summarise(mean_CAI = mean(mean_CAI), sum_CAI_idx = sum(sum_CAI_idx))CAI_half %&gt;%  ggplot()+  geom_col(aes(x=year,y=sum_CAI_idx))4. Summary대기오염도는 대체적으로 여름보다 겨울에 높았고, 2019년이 특히 심했다는 것을 알 수 있다. 2020년을 기준으로 비교했을 때, 코로나의 영향은 거의 없었던 것으로 추정된다.",
        "url": "/ds_R_final"
    }
    ,
    
    "nlp-final": {
        "title": "NLP - Final",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-1. NLP# 한국어 자료import sysimport osimport numpy as npimport nltkimport konlpyimport pandas as pdimport reimport randomimport itertoolsimport warningswarnings.filterwarnings(action='ignore')from sklearn.metrics import classification_report,f1_score,precision_score,recall_scorerandom.seed(1)# 데이터 로드train_data=pd.read_csv('final_train_data.csv')test_data=pd.read_csv('final_test_data.csv')# 384 duplicates in content, 240 in titleprint(train_data.describe())       category                                            content  titlecount     10686                                              10686  10686unique        7                                              10302  10124top        정치개혁  개인회생 36개월 단축소급 전국 적용을 위해 춘천지방법원의 법원에 바란다에 글을 올...   경남제약freq       3094                                                 16     21# preprocess: duplicatetrain_data = train_data.drop_duplicates(['content'],keep='first')# all duplicates removedtrain_data.duplicated().sum()0train_data['document']=train_data.iloc[:,1]+train_data.iloc[:,2]train_data['document']test_data['document']=test_data.iloc[:,1]+test_data.iloc[:,2]train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행tokenizing process with konlpy-Okt# tokenizefrom konlpy.tag import Oktokt = Okt() #형태소 분석기tokenized_data = []stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']for sentence in train_data['document']:    temp_X = okt.morphs(sentence, norm=True, stem=True) # 형태소 추출    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거    tokenized_data.append(temp_X)x_train=tokenized_dataokt = Okt() #형태소 분석기tokenized_data = []stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']for sentence in test_data['document']:    temp_X = okt.morphs(sentence, norm=True, stem=True) # 형태소 추출 - 토큰화 #norm=True : 근사어    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거 #https://www.ranks.nl/stopwords/korean    tokenized_data.append(temp_X)x_test=tokenized_datafor this tokenizing process takes a long time, save and import preprocessed data.import picklewith open('nlp_final_x_tr.data', 'wb') as f:    pickle.dump(x_train, f)with open('nlp_final_x_te.data', 'wb') as f:    pickle.dump(x_test, f)import picklewith open('nlp_final_x_tr.data', 'rb') as f:    x_train = pickle.load(f)with open('nlp_final_x_te.data', 'rb') as f:    x_test = pickle.load(f)from tensorflow.keras.preprocessing.text import Tokenizertokenizer = Tokenizer()tokenizer.fit_on_texts(x_train)threshold = 3total_cnt = len(tokenizer.word_index) # 단어의 수rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.for key, value in tokenizer.word_counts.items():    total_freq = total_freq + value    # 단어의 등장 빈도수가 threshold보다 작으면    if(value &lt; threshold):        rare_cnt = rare_cnt + 1        rare_freq = rare_freq + valueprint('단어 집합(vocabulary)의 크기 :',total_cnt)print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)단어 집합(vocabulary)의 크기 : 34537등장 빈도가 2번 이하인 희귀 단어의 수: 15483단어 집합에서 희귀 단어의 비율: 44.830182123519705전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.1132793250941202vocab_size = total_cnt - rare_cnt + 1 # 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거. 0번 패딩 토큰을 고려하여 +1print('단어 집합의 크기 :',vocab_size)단어 집합의 크기 : 19055tokenizer = Tokenizer(vocab_size) tokenizer.fit_on_texts(x_train)X_train = tokenizer.texts_to_sequences(x_train)X_test = tokenizer.texts_to_sequences(x_test)y_train=np.array(train_data.category)y_test=np.array(test_data.category)drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) &lt; 1]drop_test = [index for index, sentence in enumerate(X_test) if len(sentence) &lt; 1]X_train = np.delete(X_train, drop_train, axis=0)y_train = np.delete(y_train, drop_train, axis=0)print(len(X_train))print(len(y_train))1030110301X_test = np.delete(X_test, drop_test, axis=0)y_test = np.delete(y_test, drop_test, axis=0)print(len(X_train))print(len(X_test))print(len(y_train))print(len(y_test))103011158103011158import matplotlib.pyplot as pltprint('리뷰의 최대 길이 :',max(len(l) for l in X_train))print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))plt.hist([len(s) for s in X_train], bins=50)plt.xlabel('length of samples')plt.ylabel('number of samples')plt.show()리뷰의 최대 길이 : 9032리뷰의 평균 길이 : 170.45791670711583def below_threshold_len(max_len, nested_list):  cnt = 0  for s in nested_list:    if(len(s) &lt;= max_len):        cnt = cnt + 1  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))max_len = 600below_threshold_len(max_len, X_train)전체 샘플 중 길이가 600 이하인 샘플의 비율: 96.02951169789341from tensorflow.keras.preprocessing.sequence import pad_sequencesX_train = pad_sequences(X_train, maxlen = max_len)X_test = pad_sequences(X_test, maxlen = max_len)y_train=np.array(train_data.category)y_test=np.array(test_data.category)y_train = np.delete(y_train, drop_train, axis=0)y_test = np.delete(y_test, drop_test, axis=0)print(len(y_train),len(y_test))10301 1158t={'경제민주화': 1, '교통/건축/국토': 2, '보건복지': 3, '육아/교육': 4, '인권/성평등': 5, '일자리': 6, '정치개혁': 7}print(t['보건복지'])index1=np.zeros([10301,7])for i in range(len(y_train)):  index1[i][t[y_train[i]]-1]=1y_train=index1index1=np.zeros([1158,7])for i in range(len(y_test)):  index1[i][t[y_test[i]]-1]=1y_test=index1print(y_train.shape,y_test.shape)3(10301, 7) (1158, 7)X_train=np.array(X_train)X_test=np.array(X_test)y_train=np.array(y_train)y_test=np.array(y_test)print(X_train.shape)print(X_test.shape)print(y_train.shape)print(y_test.shape)print(vocab_size)(10301, 600)(1158, 600)(10301, 7)(1158, 7)19055from tensorflow.keras.layers import Embedding, Dense, LSTMfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.models import load_modelfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpointimport tensorflow_addons as tfaf1 = tfa.metrics.F1Score(num_classes=7,threshold=0.5)from tensorflow import keraskeras.__version__'2.6.0'model = Sequential()model.add(Embedding(vocab_size, 128))model.add(LSTM(128))model.add(Dense(7, activation='sigmoid'))es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=4)mc = ModelCheckpoint('best_model.h5', monitor=f1, mode='max', verbose=2, save_best_only=True)model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc',f1])model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=512, validation_split=0.2)Epoch 1/1517/17 [==============================] - 39s 2s/step - loss: 1.8842 - acc: 0.2830 - f1_score: 0.2583 - val_loss: 1.7326 - val_acc: 0.3081 - val_f1_score: 0.2726WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 2/1517/17 [==============================] - 38s 2s/step - loss: 1.6177 - acc: 0.4615 - f1_score: 0.3981 - val_loss: 1.5555 - val_acc: 0.4639 - val_f1_score: 0.3776WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 3/1517/17 [==============================] - 38s 2s/step - loss: 1.3590 - acc: 0.5495 - f1_score: 0.4018 - val_loss: 1.3879 - val_acc: 0.5075 - val_f1_score: 0.4232WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 4/1517/17 [==============================] - 38s 2s/step - loss: 1.1364 - acc: 0.6211 - f1_score: 0.4351 - val_loss: 1.3140 - val_acc: 0.5303 - val_f1_score: 0.4173WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 5/1517/17 [==============================] - 38s 2s/step - loss: 0.9084 - acc: 0.7212 - f1_score: 0.4893 - val_loss: 1.1852 - val_acc: 0.6487 - val_f1_score: 0.4386WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 6/1517/17 [==============================] - 38s 2s/step - loss: 0.7336 - acc: 0.7920 - f1_score: 0.5183 - val_loss: 1.1071 - val_acc: 0.6473 - val_f1_score: 0.4536WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 7/1517/17 [==============================] - 39s 2s/step - loss: 0.5796 - acc: 0.8381 - f1_score: 0.5395 - val_loss: 1.0900 - val_acc: 0.6458 - val_f1_score: 0.4837WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 8/1517/17 [==============================] - 38s 2s/step - loss: 0.4641 - acc: 0.8757 - f1_score: 0.5575 - val_loss: 0.9854 - val_acc: 0.6982 - val_f1_score: 0.4781WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 9/1517/17 [==============================] - 38s 2s/step - loss: 0.3588 - acc: 0.9039 - f1_score: 0.5759 - val_loss: 1.2205 - val_acc: 0.6458 - val_f1_score: 0.4993WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 10/1517/17 [==============================] - 38s 2s/step - loss: 0.3082 - acc: 0.9181 - f1_score: 0.5908 - val_loss: 1.0990 - val_acc: 0.6870 - val_f1_score: 0.5031WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 11/1517/17 [==============================] - 39s 2s/step - loss: 0.2331 - acc: 0.9408 - f1_score: 0.6076 - val_loss: 1.8883 - val_acc: 0.5793 - val_f1_score: 0.4935WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 12/1517/17 [==============================] - 39s 2s/step - loss: 0.2625 - acc: 0.9380 - f1_score: 0.6111 - val_loss: 1.3714 - val_acc: 0.6642 - val_f1_score: 0.5215WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 00012: early stopping&lt;keras.callbacks.History at 0x7f1bf1402a60&gt;# evaluating function: report f1_macrodef evaluate(test_x,test_y,model):    predictions=model.predict(test_x)    y_pred=max(predictions)    print(classification_report(test_y,y_pred))model.evaluate(X_test, y_test)37/37 [==============================] - 6s 149ms/step - loss: 1.1846 - acc: 0.6986 - f1_score: 0.5419[1.1846439838409424, 0.6986182928085327, array([0.5124555 , 0.59907836, 0.48712873, 0.45633796, 0.5854922 ,        0.4855967 , 0.6674057 ], dtype=float32)]# from saved best modelloaded_model = load_model('best_model.h5')loaded_model.evaluate(X_test, y_test)",
        "url": "/NLP_final"
    }
    ,
    
    "nlp-midterm": {
        "title": "NLP - Midterm",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-1. NLP  1. Data Info  2. Preprocessing          2.1 duplicated data found in train_data        3. Comparing classification models          3.1 With Tf-idf vectorizer                  3.1.1 MultinomialNB          3.1.2 LogisticRegression                    3.2 With CountVectorizer                  3.2.1 MultinomialNB                      4. Balanced sampling approach - imblearn  5. Result: Best Model  commentimport sysimport pandas as pdimport osimport numpy as npimport reimport randomimport itertoolsimport warningswarnings.filterwarnings(action='ignore')from sklearn.metrics import classification_reportfrom sklearn.metrics import f1_scorerandom.seed(1)train_data=pd.read_csv('midterm_train.csv')test_data=pd.read_csv('midterm_test.csv')# evaluating functiondef evaluate(test_x,test_y,model):    predictions=model.predict(test_x)    print(classification_report(test_y,predictions))1. Data Infotrain_data.head(3)                  text      senti                  0      J brand is by far the best premium denim line ...      pos              1      I loved this dress. i kept putting it on tryin...      pos              2      I found this at my local store and ended up bu...      pos      print(train_data.groupby('senti').count())print(test_data.groupby('senti').count())print(train_data.describe())print(test_data.describe())        textsenti       neg     2139pos    11279       textsenti      neg     231pos    1260                                                     text  senticount                                               13418  13418unique                                              13414      2top     Perfect fit and i've gotten so many compliment...    posfreq                                                    2  11279                                                     text senticount                                                1491  1491unique                                               1491     2top     Have to disagree with previous posters. i foun...   posfreq                                                    1  12602. Preprocessing2.1 duplicated data found in train_data# remove duplicated dataprint(train_data.text.duplicated().sum())train_data = train_data.drop_duplicates(['text'],keep='first')train_data.duplicated().sum()40print(train_data.describe())                                                     text  senticount                                               13414  13414unique                                              13414      2top     J brand is by far the best premium denim line ...    posfreq                                                    1  11276# train-test split unduplicated datax_train=np.array(train_data.text)x_test=np.array(test_data.text)y_train=np.array(train_data.senti)y_test=np.array(test_data.senti)x_train[0]'J brand is by far the best premium denim line retailer sells! the fit on these jeans is amazing..worth every penny..also, considering it is a crop jean - warm weather wear - the denim weight is light and not too thick...the color is different from ordinary regular denim blue..lighter wash for spring/summer!'# preprocessing: remove non-alphabet charactersx_train_clean=np.array([re.sub('[^a-zA-Z]',' ',text) for text in x_train])x_test_clean=np.array([re.sub('[^a-zA-Z]',' ',text) for text in x_test])x_train_clean[0]'J brand is by far the best premium denim line retailer sells  the fit on these jeans is amazing  worth every penny  also  considering it is a crop jean   warm weather wear   the denim weight is light and not too thick   the color is different from ordinary regular denim blue  lighter wash for spring summer '3. Comparing classification models# tuning parameter setsngram_range= [(1, 1), (1, 2),(2,2)]stop_words=[None,'english']clf__alpha=[0.005,0.01,0.05,0.1]params = dict(ngram_range=ngram_range,              stop_words=stop_words,              clf__alpha=clf__alpha)keys=params.keys()values = (params[key] for key in keys)combinations = [dict(zip(keys, combination)) for combination in itertools.product(*values)]# tuning functiondef tuning_model(params,vectorizer,classifier,x_train,x_test,y_train,y_test):    ngram_range=params['ngram_range']    stop_words=params['stop_words']    vec = vectorizer(ngram_range=ngram_range,stop_words=stop_words)    vec_train = vec.fit_transform(x_train)    vec_test = vec.transform(x_test)    if classifier==MultinomialNB:        alpha=params['clf__alpha']        clf=classifier(alpha)    else:        clf=classifier(random_state=1,max_iter=500)    clf.fit(vec_train, y_train)    pred=clf.predict(vec_test)    return f1_score(y_test,pred,average='macro'),params# get best score &amp; parametersdef get_result(combinations,vectorizer,classifier,x_train,x_test,y_train,y_test):    results=[]    for params in combinations:        results.append(tuning_model(params,vectorizer,classifier,x_train,x_test,y_train,y_test))    return max(results,key=lambda item: item[0])3.1 With Tf-idf vectorizer3.1.1 MultinomialNBpreprocessing seems to have no effect in ngram_range: (1, 2) and also stop words are not important featuresf1-score macro avg: 0.85from sklearn.naive_bayes import MultinomialNBfrom sklearn.feature_extraction.text import TfidfVectorizerget_result(combinations,TfidfVectorizer,MultinomialNB,x_train,x_test,y_train,y_test)(0.8528815948449455, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.01})get_result(combinations,TfidfVectorizer,MultinomialNB,x_train_clean,x_test_clean,y_train,y_test)(0.8528815948449455, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.005})3.1.2 LogisticRegressionfrom sklearn.linear_model import LogisticRegressionget_result(combinations,TfidfVectorizer,LogisticRegression,x_train,x_test,y_train,y_test)(0.8811180515581001, {'ngram_range': (1, 1), 'stop_words': None, 'clf__alpha': 0.005})get_result(combinations,TfidfVectorizer,LogisticRegression,x_train_clean,x_test_clean,y_train,y_test)(0.8823175752378818, {'ngram_range': (1, 1), 'stop_words': None, 'clf__alpha': 0.005})3.2 With CountVectorizer3.2.1 MultinomialNBfrom sklearn.feature_extraction.text import CountVectorizerget_result(combinations,CountVectorizer,MultinomialNB,x_train,x_test,y_train,y_test)(0.9366865264206945, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.1})get_result(combinations,CountVectorizer,MultinomialNB,x_train_clean,x_test_clean,y_train,y_test)(0.935509934324805, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.1})4. Balanced sampling approach - imblearnfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.linear_model import LogisticRegression# tuning parameter sets for all casesvectorizer=[CountVectorizer,TfidfVectorizer]classifier=[MultinomialNB,LogisticRegression]ngram_range= [(1, 1), (1, 2),(2,2)]stop_words=[None,'english']clf__alpha=[0.005,0.01,0.05,0.1]params = dict(ngram_range=ngram_range,              vectorizer=vectorizer,              classifier=classifier,              stop_words=stop_words,              clf__alpha=clf__alpha)keys=params.keys()values = (params[key] for key in keys)combinations = [dict(zip(keys, combination)) for combination in itertools.product(*values)]from imblearn.over_sampling import SMOTEdef smote_tuning(params,x_train,x_test,y_train,y_test):    ngram_range=params['ngram_range']    stop_words=params['stop_words']    classifier=params['classifier']    vec=params['vectorizer'](ngram_range=ngram_range,stop_words=stop_words)    x_train_imb=vec.fit_transform(x_train)    y_train_imb=np.array(train_data['senti']=='pos').astype('int')    y_test_imb=np.array(test_data['senti']=='pos').astype('int')    vec_train_over, y_train_over = SMOTE(random_state=1).fit_resample(x_train_imb,y_train_imb)    vec_test = vec.transform(x_test)    if classifier==MultinomialNB:        alpha=params['clf__alpha']        clf=classifier(alpha)    else:        clf=classifier(random_state=1,max_iter=500)    clf.fit(vec_train_over, y_train_over)    pred=clf.predict(vec_test)    return f1_score(y_test_imb,pred,average='macro'),params# get best score &amp; parametersdef get_smote_result(combinations,x_train,x_test,y_train,y_test):    results=[]    for params in combinations:        results.append(smote_tuning(params,x_train,x_test,y_train,y_test))    return max(results,key=lambda item: item[0])get_smote_result(combinations,x_train,x_test,y_train,y_test)(0.9285352359562871, {'ngram_range': (1, 2),  'vectorizer': sklearn.feature_extraction.text.TfidfVectorizer,  'classifier': sklearn.naive_bayes.MultinomialNB,  'stop_words': None,  'clf__alpha': 0.1})get_smote_result(combinations,x_train_clean,x_test_clean,y_train,y_test)(0.9276401547886131, {'ngram_range': (1, 2),  'vectorizer': sklearn.feature_extraction.text.TfidfVectorizer,  'classifier': sklearn.naive_bayes.MultinomialNB,  'stop_words': None,  'clf__alpha': 0.1})5. Result: Best Model# 'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.1}vectorizer=CountVectorizer(ngram_range=(1,2))vectors_train=vectorizer.fit_transform(x_train)vectors_test=vectorizer.transform(x_test)clf=MultinomialNB(alpha=0.1)clf.fit(vectors_train,y_train)evaluate(vectors_test,y_test,clf)              precision    recall  f1-score   support         neg       0.90      0.88      0.89       231         pos       0.98      0.98      0.98      1260    accuracy                           0.97      1491   macro avg       0.94      0.93      0.94      1491weighted avg       0.97      0.97      0.97      1491comment데이터사이언스 복수전공 후 첫 강의. 중간고사까지 배웠던 기본적인 전처리와 ML 함수들을 적용했다. 추가적으로 imblanced data case를 SMOTE 패키지를 사용해 간단히 처리해보았다. 당시 수업에서 가장 높은 accuracy를 기록했다.",
        "url": "/NLP_midterm"
    }
    ,
    
    "stat-ch3": {
        "title": "Statistical Mining - Chapter 3",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-04-03 Week 3Ch3. Linear RegressionModel:$ f(X) = \\beta_0 + \\sum_{j=1}^p \\beta_j X_j \\quad \\Rightarrow $ Prediction of Y from X  (X= p input variables)f : Regression function E(Y|X)input variable X:  Quantitative or Transformationse of quantitative inputs  Basis expansions leading to polynomial representation  Dummy coding of qualitative variables : G grops -&gt; G-1 dummy variables required  Interactions between inputs ($X_3 = X_1 X_2 $ )How to estimate $\\beta = (\\beta_0, \\beta_1, \\ldots , \\beta_p)^T \\Rightarrow$ Least Squares      $\\beta$ minimizing RSSwhen Y (n by 1) / X ( n by (p + 1) matrics)RSS = $ ( y - X\\hat\\beta)^T ( y - X\\hat\\beta) $        Least square estimator (LSE):$ \\hat\\beta = (X^T X)^{-1} X^T Y $  Assumption:1) $Y_i$ ‘s are uncorrelated &amp; Var ($Y_i$ ) = $\\sigma^2$$\\quad$($\\equiv$ $\\epsilon_i$ ‘s are independent &amp; Var ($\\epsilon_i$ ) = $\\sigma^2$)2) X = $(X_1, \\ldots, X_p)^T$ is fixed (not random)  By (1) &amp; (2)          for OLS estimator $\\hat\\beta$$\\begin{matrix}\\hat\\beta &amp;=&amp; (X^TX)^{-1}X^TY \\quad\\leftarrow Y=X\\beta + \\epsilon          &amp;=&amp; \\beta + (X^TX)^{-1}X^T\\epsilon \\end{matrix}$$\\begin{matrix}E(\\hat\\beta) &amp;=&amp; E(\\beta) + E(\\epsilon)               &amp;=&amp; \\beta\\end{matrix}$$\\begin{matrix}Var(\\hat\\beta)&amp;=&amp; E[\\hat\\beta - E(\\hat\\beta)]^2 &amp;=&amp; E[ (X^TX)^{-1}X^T\\epsilon ]^2 &amp;=&amp; (X^TX)^{-1}X^TE(\\epsilon^2)X(X^TX)^{-1}&amp;=&amp; \\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}&amp;=&amp;\\sigma^2(X^T X)^{-1}\\end{matrix}$      $\\hat\\sigma^2$ as Estimator of Variance of $\\epsilon$ (= $\\sigma^2$)$\\begin{matrix}\\hat\\sigma^2 &amp;=&amp; \\frac 1 {n-p-1} \\sum_{i=1}^n (y_i - \\hat y_i)^2 &amp;=&amp; \\frac1{n-p-1}\\sum_{i=1}^n \\epsilon_i^2 \\Rightarrow E(\\hat\\sigma^2) &amp;=&amp; \\sigma^2\\end{matrix}$ $\\ \\triangleright \\text{ unbiased estimator; }\\frac{RSS}{n-p-1}$      3) $\\epsilon \\sim ^{iid} N(0,\\sigma^2)$ : normal distribution assumption of  error  By (3),          $\\hat\\beta\\sim MVN(\\beta,\\sigma^2(X^TX)^{-1}); \\quad (Y\\sim MVN(X\\beta,\\sigma^2 I)$      $\\frac{(n-p-1)\\hat\\sigma^2}{\\sigma^2}\\sim\\chi^2_{n-p-1}$      $\\hat\\beta \\text{ &amp; } \\hat\\sigma^2$ are independent      2020-04-14Hypothesis test for $\\beta_j$:  $H_0: \\beta_j = 0$ vs . $H_1: B_j \\not= 0$  Test for partial effect of individual $\\beta_j$  $\\hat\\beta_j \\sim N(\\beta_j,\\sigma^2 v_j)$, where $v_j$ is the j th diagonal element of $(X^TX)^{-1}$$\\Rightarrow \\frac{\\hat\\beta_j - \\beta_j}{\\sigma\\sqrt{v_j}}\\sim N(0,1)$  ($\\leftarrow z=\\frac{x-\\mu}{\\sigma}$)  under $H_0$, $t_j = \\frac{\\hat\\beta_j}{\\hat\\sigma\\sqrt{v_j}}\\sim t_{n-p-1}$                              if $          t_j          &gt; t_{(1-\\alpha/2), n-p-1}$ , we reject $H_0$ with level of significance : $\\alpha$                    Test for a group of coefficients:  E.g. test for a categorical variable with k levels (k-1 dummy variables)  $H_0: \\beta_{j+1} = \\beta_{j+2} = \\cdots = \\beta{j+l} = 0$  $H_1$: At least one of those is not zero  Full model: $Y=\\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p + \\epsilon$  Reduced model: under $H_0: Y= \\beta_0  + \\beta_1X_1 + \\cdots + \\beta_jX_j + \\beta_{j+l+1}X_{j+l+1} + \\cdots + \\beta_pX_p + \\epsilon$  Test statistic: F = $\\frac{(RSS_R - RSS_F)/ l}{RSS_F/(n-p-1)} \\sim F_{l,n-p-1}$where $RSS_F$ is from full model and $RSS_R$ is from reduced model  if $ F &gt; F_{(1-\\alpha),l,n-p-1}$ we reject $H_0$.Test for all coefficients:  $H_0$: all $\\beta$ coefficients are zero  Reduced model: $Y=\\beta_0 + \\epsilon \\Rightarrow$ LSE $\\hat\\beta_0 =  \\bar y$  Test statistic: $ F = \\frac{(TSS-RSS)/p}{RSS/(n-p-1)}$ where TSS = $\\sum_{i=1}^n(y_i-\\bar y)^2$ = $RSS_R$  if $ F &gt; F_{(1-\\alpha),p,n-p-1}$ we reject $H_0$ : at least 1 coefficient is not zero  Check F-test first before t-tests for each $\\beta_j$  F-test : test for all coef, T-test: test for indiv. coeff  when F-test is significant ($H_1$ for all coeff is true) $\\Rightarrow$ perform t-test but still need to control $\\alpha$but if F-test is not sig $\\Rightarrow$ cant trust t-test result : all coeffs are ZEROGauss-Markov Theorem  Assumption: $E(\\epsilon_i) = 0, Var(\\epsilon_i) = \\sigma^2 &lt; \\infty\\text{,  } \\epsilon_i$’s are independent  among all linear unbiased estimator $\\tilde{\\beta} = Cy$ &amp; $E(\\tilde{\\beta}) = \\beta)$Then, $Var(\\hat\\beta) \\le Var(\\tilde{\\beta})$ when  LSE $\\hat\\beta$ = BLUE  G-M theorem says that LSE  $\\hat\\beta$ is the best linear unbiased estimator (BLUE)  Conversely, there may exist biased estimators with smaller MSE than LSE.Properties of Good estimators  unbiaseness  efficiency(small variance)  consistancy(as n goes infinity, estimator goes true parameter)  sufficiencyModel fit  Model evaluation based on training data  Measure of model fit: $R^2$, Residual Standard Error(RSE)\\(R^2 = 1 - \\frac{RSS}{TSS}\\) which means the proportion of total variation that your model explains\\(RSE = \\sqrt{\\frac{RSS}{n-p-1}} = \\hat\\sigma^2\\)  As number of predictors increases, $R^2$ increases and always the most complex model selected - is the problemPrediction1) Uncertainty between $\\hat Y$ &amp; $f(X) = X^T\\beta$  Variation due to $\\hat\\beta$ (Model variance):in ideal situation, we can drive number of training datasets from population and have several f(x) and $\\hat\\beta$  Confidence interval (C.I.) for Y  $E(\\hat Y) = x^T\\beta = f(x)$, $Var(\\hat Y) = \\sigma^2 x^T( X^TX)^{-1}x$  $(1-\\alpha)100$% C.I. for Y= $\\hat Y \\pm t_{(1-\\alpha/2, n-p-1)} \\hat\\sigma\\sqrt{x^T(X^TX)^{-1}x}$2) Model bias:  caused by assuming a linear model for f(x).(1)(2) are reducible error3) Uncertainty between Y &amp; $\\hat Y$:  Prediction interval (P.I.) = reducible + irreducible error  (new unseen obs.) test obs. $x_0 = (1, x_{01}, \\ldots, x_{op})^T$  $\\hat Y_0 = x_0^T \\beta$,$Var(\\hat Y_0) =  \\sigma^2 + \\sigma^2 x_0^T( X^TX)^{-1}x_0$ (irreducible + reducible)  $(1-\\alpha)100$% P.I. for $Y_0$=  $\\hat Y_0 \\pm t_{(1-\\alpha/2, n-p-1)} \\hat\\sigma\\sqrt{1+x^T(X^TX)^{-1}x}$K-Nearest Neighbor Regression  Nonparameric method: No assumption of f(x)\\(\\hat f(x_0) = \\frac{1}{K}\\sum_{x_i\\in\\mathcal N_k(x_0)} y_i\\)          ($x_i,y_i$): training data      $\\mathcal N_k(x_0)$ : neighborhood of $x_0$        Preferable situations to linear regression:          True f(x) is nonlinear      Goal: Prediction &gt; Inference (interpretation)      Low dimensions (small p)(curse of demensionality)      Curse of Dimensionality  to capture 10% data for neighbor space from variable X…          p=1 : X range (0,1), edge rank $e_1(0.1) = 0.1$      p=2 : $X_1, X_2$ range (0,1), edge rank $e_2(0.1) = \\sqrt{0.1} = 0.1^{1/2} \\approx 0.316 $      p=3 : $e_3(0.1) = 0.1^{1/3} \\approx 0.464 $      p=10 : $e_{10}(0.1) = 0.1^{1/10} \\approx 0.794 $        Reduction of r : Average using fewer obs. small K : higher variance of the fit, poor prediction  For the smae density, when p=1, n=100 -&gt; when p=10, n = $100^{10}$",
        "url": "/stat_ch3"
    }
    ,
    
    "stat-hw2": {
        "title": "Statistical Mining - HW2",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-1. Statistical Mining#2set.seed(1)x1=runif(100)x2=0.5*x1 + rnorm(100)/10y= 2+ 2*x1 + 0.3*x2 + rnorm(100)fit = lm(y ~  x1 + x2,data=y)Error in eval(predvars, data, env): 길이가 1이 아닌 수치형 'envir' 인자입니다Traceback:1. lm(y ~ x1 + x2, data = y)2. eval(mf, parent.frame())3. eval(mf, parent.frame())4. stats::model.frame(formula = y ~ x1 + x2, data = y, drop.unused.levels = TRUE)5. model.frame.default(formula = y ~ x1 + x2, data = y, drop.unused.levels = TRUE)6. eval(predvars, data, env)cor(x1,x2)0.835121242463113plot(x1,x2)fit &lt;- lm(y~x1+x2)summary(fit)Call:lm(formula = y ~ x1 + x2)Residuals:    Min      1Q  Median      3Q     Max -2.8311 -0.7273 -0.0537  0.6338  2.3359 Coefficients:            Estimate Std. Error t value Pr(&gt;|t|)    (Intercept)   2.1305     0.2319   9.188 7.61e-15 ***x1            1.4396     0.7212   1.996   0.0487 *  x2            1.0097     1.1337   0.891   0.3754    ---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1Residual standard error: 1.056 on 97 degrees of freedomMultiple R-squared:  0.2088,\tAdjusted R-squared:  0.1925 F-statistic:  12.8 on 2 and 97 DF,  p-value: 1.164e-05fit2 &lt;- lm(y~x1)summary(fit2)Call:lm(formula = y ~ x1)Residuals:     Min       1Q   Median       3Q      Max -2.89495 -0.66874 -0.07785  0.59221  2.45560 Coefficients:            Estimate Std. Error t value Pr(&gt;|t|)    (Intercept)   2.1124     0.2307   9.155 8.27e-15 ***x1            1.9759     0.3963   4.986 2.66e-06 ***---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1Residual standard error: 1.055 on 98 degrees of freedomMultiple R-squared:  0.2024,\tAdjusted R-squared:  0.1942 F-statistic: 24.86 on 1 and 98 DF,  p-value: 2.661e-06fit3 &lt;- lm(y~x2)summary(fit3)Call:lm(formula = y ~ x2)Residuals:     Min       1Q   Median       3Q      Max -2.62687 -0.75156 -0.03598  0.72383  2.44890 Coefficients:            Estimate Std. Error t value Pr(&gt;|t|)    (Intercept)   2.3899     0.1949   12.26  &lt; 2e-16 ***x2            2.8996     0.6330    4.58 1.37e-05 ***---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1Residual standard error: 1.072 on 98 degrees of freedomMultiple R-squared:  0.1763,\tAdjusted R-squared:  0.1679 F-statistic: 20.98 on 1 and 98 DF,  p-value: 1.366e-05",
        "url": "/stat_HW2"
    }
    ,
    
    "stat-ch2": {
        "title": "Statistical Mining - Chapter 2",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-03-16 Week 2Ch 2. Supervised LearningFormat of Supervised Learning:  $ Y=f(X)+\\epsilon $$f$ : Fixed but UNKNOWN function of predictors X$\\rightarrow$ Supervised Learning: A set of methods estimating $f$.  Why do we estimate $f$?1) Prediction: Y2) Inference: Relationship between X &amp; Y1) Prediction:$\\rightarrow\\text{Prediction of Y} : \\hat{Y} = \\hat{f}(X)\\quad (\\hat{f} : \\text{Estimate of }f) $$\\rightarrow\\text{Accuracy of }\\hat{Y}\\text{ depends on reducible &amp; irreducible errors}$  $E[Y-\\hat{Y}]^2 = [f(X)-\\hat{f}(X)]^2 + Var(\\epsilon)$$\\quad (MSE)\\quad\\quad (reducible)\\quad (irreducible)$  Reducible error : controlable, can be reduced by selecting a better model  Irreducible error: uncontrolable, nature of data2) Inference:  Characteristics of liner &amp; nonlinear models:1) Linear : Relatively Simple &amp; interpretable inference but poor prediction (when true function $f$ is nonlinear)2) Nonlinear : More accurate but difficult to interpret$\\rightarrow$ Depends on your Goal of analysis3) Estimation F: By Using Training Data  Estimation Methods:1) Parametric: Assume specific function $f$ (linear&amp;non- both)2) Nonparametric: No Assumption; Data-driven method (nonlinear)4) Parametric Methods: Model-based approach  Step 1: Assume about the functional form of $f$$\\quad\\text{e.x) when } X_j = ( x_{1j}, \\ldots , x_{nj} ) ^T : n * 1 \\space \\text{obs. ; vector for the j th predictor} \\quad\\quad \\rightarrow linear: f(X)= \\beta_0 + \\beta_1X_1 + … + \\beta_pX_p$  Step 2: Estimate Parameters$\\quad\\quad\\quad Y \\approx \\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + … + \\hat{\\beta}_pX_p = \\hat{f}(X)$  Disadvantage: Specific form of $f$$\\quad\\quad\\quad\\rightarrow$ Not match with the true $f$: poor estimation &amp; poor prediction$\\quad\\quad\\quad\\rightarrow$ Flextible parametric model : model with more parameters$\\quad\\quad\\quad\\quad$ Flexible model $\\equiv$ Complex model$\\quad\\quad\\quad\\quad$ but overfitting problem still remains: model follows errors or noisesUnderfitting vs Overfitting  under: estimation has not enough accuracy, miss to capture the true structure  over: more complexity than true $f$5) Nonparametric Methods: Data-driven approach  No assumptions about functional form of $f \\Rightarrow $ wider range of possible shapes of $f$  Disadvantage:$\\quad\\quad\\quad\\rightarrow$ A vary large # of obs. is required to get an accurae $f$ (relatively to parametric method)$\\quad\\quad\\quad\\rightarrow$ Overfitting $\\Leftrightarrow$ Level of smoothness; Model complexity (how to determine it?)    Example of Nonparametric model: KNN Reg.    Idea: Similar inputs have Similar output  Step: when predicting $\\hat{Y}_0$ with input $X_0$1) Determine $K$ (level of smoothness) #if K==N: Y = Var(Y) / if K==1: Y = perfect fit; no training error2) find $K$ closest training obs. from the target input point($X_0$) using Euclidean distance\\(\\rightarrow \\hat{Y}_0 = \\frac{1}{K}\\sum_{x_i\\in \\mathcal N}^K y_i \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\)3) average of these #k Y values = $\\hat{Y}_0$  Effect of $K$ (tuning parameter/ model flexibility parameter)          decides the level of smoothness (under or overfitting)      As $K \\uparrow \\Rightarrow \\text{flexibility} \\downarrow$ : the fitted line is simple      As $K \\downarrow \\Rightarrow \\text{flexibility} \\uparrow$ : the fitted line is wiggly      6) Trade-off between Prediction Accuray &amp; Interpretability  Restrictive model(Simple model) vs. Flexible model  our goal: inference ($\\rightarrow$ restrictive model) or prediction ($\\rightarrow$ flexible model)7) Model Assessment &amp; Selection  Evaluate the performance of models on a given dataset  and Select the best model  Criterion: Better prediction of $Y$  #When Regression problem: Mean squared error (MSE)      training MSE(used for building model) is not our interest; cannot be a measure for selectione.g) $\\begin{align} R^2 = 1 - \\frac{RSS}{TSS} \\end {align}\\text{ (Residual Sum of Squares)  / (Total Sum of Square)} \\\\quad\\quad \\text{training RSS}= \\sum(y_i - \\hat{y_i})^2  \\quad \\text{&amp;} \\quad \\text{MSE} =\\frac{RSS}{N}$                  when more X variables added $\\rightarrow \\text{RSS&amp;MSE}\\downarrow , \\space R^2\\uparrow$          even X are not important variables        complex model has smaller training error (regardless of existence of test data)                            Test MSE $= \\begin{align}\\frac{1}{m}\\sum_{i=1}^m(y_i^0 - \\hat{f}(x_i^0))^2 \\end{align}$        with low test MSE $\\Rightarrow$ better model$\\rightarrow$ How to find optimal Flexibility?$\\rightarrow$ Test MSE(error rate) is always larger than Training MSE; We estimated model function $f$ to minimize its training error$\\rightarrow$ No test data: Sample re-use methods (e.g., bootstrap, cross-validation)              Classification problem: Misclassification rate8) Bias-Variance Trade-off 편향분산 교차  Expected test MSE(ideal measure): $E[y_0-\\hat{f}(x_0)]^2 \\quad ((x_0,y_0) \\text{ is a test obs})$      in population: #K training sets &amp; #K function $f_k$Estimation of expected test MSE:\\(\\hat{E}[y_0 - \\hat{f}(x_0)]^2 = \\frac{1}{K} \\sum_{k=1}^K[y_0-\\hat{f}_k(x_0)]^2\\)    For given $x_0$,  (when $\\epsilon$ is irreducible error)$\\begin{matrix}\\begin{align}\\hat{E}[y_0 - \\hat{f}(x_0)]^2 &amp;= Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon) \\               \\text{} \\quad&amp;= E [ \\{ y_0 -E(\\hat{f}(x_0)) \\} - \\{ \\hat{f}(x_0) - E(\\hat{f}(x_0)) \\} ]^2 \\\\                                             &amp;= \\quad E [  y_0 -E(\\hat{f}(x_0)) ] ^2 \\quad \\quad \\quad &amp; \\rightarrow (1) \\\\                                             &amp; \\quad \\; + E [\\hat{f}(x_0) - E(\\hat{f}(x_0)) ] ^2 &amp; \\rightarrow (2)\\\\                                             &amp; \\quad \\; - 2E[ \\{ y_0 -E(\\hat{f}(x_0)) \\} \\{ \\hat{f}(x_0) - E(\\hat{f}(x_0)) \\} ] &amp; \\rightarrow (3)\\end{align}\\end{matrix}$      $\\begin {matrix} \\begin{align}(1) &amp;= E [(f(x_0) + \\epsilon - E(\\hat{f}(x_0)) ] ^2    &amp;= E[ f(x_0)^2 +\\epsilon^2 + [E(\\hat{f}(x_0)) ]^2 + 2f(x_0)\\epsilon -2f(x_0)E(\\hat{f}(x_0)) - 2\\epsilon E(\\hat{f}(x_0)) ]           \\quad \\leftarrow \\text{when } E(\\epsilon) \\text{ goes to } 0     &amp;= E[E(\\hat{f}(x_0)) - f(x_0)] ^2 + E(\\epsilon^2)  \\quad  \\leftarrow \\text{ in E[ ] is Bias }     &amp;= [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon^2)\\end{align}\\end{matrix} $    $(2) = Var(\\hat{f}(x_0)) \\leftarrow \\text { model variance (different fit’s from randomness of training sets)} $  $(3) = 0$$\\therefore$ Expected test MSE is consisted of Reducible Error(Var+ Bias$^2$) and Irreducible Error(Var($\\epsilon$))2020-04-03 Week 3      $Var(\\hat{f(x_0})$ : model variance        $ [Bias(\\hat{f(x_0})]$ : Systematic model error ( caused by model assumptions; e.g. true f: nonlinear)        $ Var(\\epsilon)$ : Irreducible error  Model Flextibility $\\uparrow$ $\\Rightarrow$ model variance $\\uparrow$ &amp; model bias $\\downarrow$Model Flextibility $\\downarrow$ $\\Rightarrow$ model variance $\\downarrow$ &amp; model bias $\\uparrow$$\\rightarrow$ optimal model flextibility is different for each datasetsTrade-off in KNN Reg.  given) Y = f (X) + $\\epsilon$ with E($\\epsilon$) = 0 and Var ($\\epsilon$) = $\\sigma^2$      Expected test MSE at $x_0$:$E[ (Y- \\hat{f_k}(x_0) )^2 | X = x_0 ] = \\sigma^2 + Bias^2(\\hat{f_k} (x_0)) + Var (\\hat{f_k} (x_0)) \\rightarrow Bias(\\hat{f_k} (x_0)) =  f(x_0) - E(\\hat{f}(x_0))   \\text{ : given “f” is KNN func.}\\ \\quad= { f(x_0) - E[\\frac1 K \\sum_{i \\in \\mathcal N (x_0)} ^ K Y_i ] }  \\quad= { f(x_0)- \\frac 1 K \\sum E (f(x_i)+\\epsilon_i)}  \\quad= f(x_0) - \\frac 1 K \\sum E (f(x_i)  \\rightarrow Var(\\hat{f_k} (x_0)) = Var(\\frac 1 K \\sum_{i \\in \\mathcal N (x_0)} Yi )\\quad= \\frac 1 {K^2} \\sum  Var( Yi ) \\quad=  \\frac 1 {K^2} \\sum Var( f( x_i) + \\epsilon_i ) \\quad=  \\frac 1 {K^2} \\sum \\sigma^2 = \\frac 1 {K^2} * K * \\sigma^2 = \\frac {\\sigma^2} K $\\(\\therefore \\text{ Expected test MSE at } x_0 = \\sigma ^2 + [f(x_0) - \\frac 1 K \\sum_{x_i \\in \\mathcal N (x_0)}^k f(x_i) ]^2 + \\frac {\\sigma^2} K\\)    when we use large number of K :as K $\\uparrow\\rightarrow$ model complexity$\\downarrow$ model variance $\\downarrow \\rightarrow$ bias $\\uparrow$ :simpler modelin Classification: Using Misclassification rate\\(= \\frac 1 N \\sum_{i=1}^n I(y_i \\not = \\hat y_i )\\)  Bayes Classifier  KNN Classifier$min_\\hat Y E( I(Y \\not = \\hat Y) = min_\\hat Y E[ E(I(Y \\not = \\hat Y | X) ] = min_{\\hat f (x)} E[ \\sum_{g=1}^G I( Y= g\\not= \\hat f (x) ) P ( Y= g| X) ] \\equiv min_{\\hat f (x)} \\sum_{g=1}^G I ( Y=g \\not= \\hat f (x) ) P ( Y= g| X) \\Rightarrow \\hat f (x) = min_g \\sum_{g=1}^G I ( Y=g \\not= \\hat f (x) ) P ( Y= g| X) \\quad \\equiv min_g ( 1- P ( Y= g| X) )$E.g)in 3 groups -&gt; Y = 1, 2, 3P(Y=1 | X) = 0.3 , P(Y=2 | X) = 0.5, P(Y=3 | X) =0.2$\\hat f(x) = 1 \\Rightarrow$ EPE(expected prediction error; miss classification rate) = 0 * 0.3 + 1 * 0.5 + 1 * 0.2 = 0.7$\\hat f(x) = 2 \\Rightarrow$ EPE = 0.5 : min value$\\hat f(x) = 3 \\Rightarrow$ EPE = 0.8            $\\therefore min_g ( 1- P ( Y= g      X) ) \\              \\quad  \\equiv max_g P(Y = g      X)$                  Bayes error rate at X = $x_0$ : $1 - max_g P(Y=g      X=x_0)$              Overall Bayes error rate = $1 - E[ max_g P(Y=g      X= x) ]$      ",
        "url": "/stat_ch2"
    }
    
    
    };
</script>
<script src="assets/js/lunr.js"></script>
<script src="assets/js/search.js"></script>
            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Darron's Devlog</a> &copy; 2021</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search Darron's Devlog</h1>
                <p class="subscribe-overlay-description">
				lunr.js를 이용한 posts 검색 </p>
                <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
