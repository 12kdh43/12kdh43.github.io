<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Search Result</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
	
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="" />
    <link rel="shortcut icon" href="http://0.0.0.0:4000/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="http://0.0.0.0:4000/search" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Darron's Devlog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Search Result" />
    <meta property="og:description" content="" />
    <meta property="og:url" content="http://0.0.0.0:4000/search" />
    <meta property="og:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Search Result" />
    <meta name="twitter:description" content="" />
    <meta name="twitter:url" content="http://0.0.0.0:4000/" />
    <meta name="twitter:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Darron's Devlog" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="666" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Darron's Devlog",
        "logo": "http://0.0.0.0:4000/"
    },
    "url": "http://0.0.0.0:4000/search",
    "image": {
        "@type": "ImageObject",
        "url": "http://0.0.0.0:4000/assets/built/images/blog-cover1.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://0.0.0.0:4000/search"
    },
    "description": ""
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Search Result" href="/feed.xml" />


</head>
<body class="page-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="/">Darron's Devlog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/jekyll/">Jekyll</a></li>
    <li class="nav-projects" role="menuitem"><a href="/tag/projects/">Projects</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/studies/">Studies</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post page no-image">

            <header class="post-full-header">
                <h1 class="post-full-title">Search Result</h1>
            </header>

            

            <section class="post-full-content">
                <form action="/search" method="get" hidden="hidden">
    <label for="search-box"></label>
    <input type="text" id="search-box" name="query">
</form>

<ul class="mylist" id="search-results"></ul>

<script>
    window.store = {
    
    "helloworld": {
        "title": "Hello, World!",
        "author": "Darron Kwon",
        "category": "",
        "content": "My First Notebook from Docker-workspace.import tensorflow as tftf.__version__'2.4.1'",
        "url": "/helloworld"
    }
    ,
    
    "appml-assginment1": {
        "title": "AppML - Assignment1",
        "author": "Darron Kwon",
        "category": "",
        "content": "2021-1. Applied Machine Learning# This Python 3 environment comes with many helpful analytics libraries installed# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python# For example, here's several helpful packages to loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files are available in the read-only \"../input/\" directory# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport osfor dirname, _, filenames in os.walk('/kaggle/input'):    for filename in filenames:        print(os.path.join(dirname, filename))# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session/kaggle/input/dsc3011/sample_submission.csv/kaggle/input/dsc3011/X_test.csv/kaggle/input/dsc3011/y_train.csv/kaggle/input/dsc3011/X_train.csv1. Load DataX_train = pd.read_csv(\"/kaggle/input/dsc3011/X_train.csv\")y_train = pd.read_csv(\"/kaggle/input/dsc3011/y_train.csv\")X_test = pd.read_csv(\"/kaggle/input/dsc3011/X_test.csv\")sample = pd.read_csv(\"/kaggle/input/dsc3011/sample_submission.csv\")X_train.head(5)                   id      age      sex      cp      trestbps      chol      fbs      restecg      thalach      exang      oldpeak      slope      ca      thal                  0      1      63      1      3      145      233      1      0      150      0      2.3      0      0      1              1      2      37      1      2      130      250      0      1      187      0      3.5      0      0      2              2      3      41      0      1      130      204      0      0      172      0      1.4      2      0      2              3      4      56      1      1      120      236      0      1      178      0      0.8      2      0      2              4      5      57      0      0      120      354      0      1      163      1      0.6      2      0      2      y_train.head()                  id      target                  0      1      1              1      2      1              2      3      1              3      4      1              4      5      1      2. Statistics &amp; Visualization# data typeslen(X_train), X_train.dtypes(233, id            int64 age           int64 sex           int64 cp            int64 trestbps      int64 chol          int64 fbs           int64 restecg       int64 thalach       int64 exang         int64 oldpeak     float64 slope         int64 ca            int64 thal          int64 dtype: object)# no missing valuesX_train.isnull().sum(axis=0)id          0age         0sex         0cp          0trestbps    0chol        0fbs         0restecg     0thalach     0exang       0oldpeak     0slope       0ca          0thal        0dtype: int64y_train['target'].value_counts()1    1330    100Name: target, dtype: int64# count uniques for categorical variablescats = ['sex','cp','fbs','restecg','exang','slope','ca','thal']for c in cats:    print(X_train[c].value_counts())1    1580     75Name: sex, dtype: int640    1072     721     363     18Name: cp, dtype: int640    1971     36Name: fbs, dtype: int640    1251    108Name: restecg, dtype: int640    1541     79Name: exang, dtype: int642    1101    1060     17Name: slope, dtype: int640    1371     492     273     184      2Name: ca, dtype: int642    1273     951     100      1Name: thal, dtype: int64import matplotlib.pyplot as pltfig, axes = plt.subplots(2, 4, figsize=(15, 10))i=0for c in cats:    axes[i//4,i%4].hist(X_train[c])    i+=1# statistics for numerical variablesnums = ['age','trestbps','chol','thalach', 'oldpeak']fig, axes = plt.subplots(2, 3, figsize=(15, 10))i=0for c in nums:    axes[i//3,i%3].hist(X_train.loc[:,c])    i+=1pd.concat([X_train[nums],y_train['target']],axis=1).corr()                  age      trestbps      chol      thalach      oldpeak      target                  age      1.000000      0.293608      0.309546      -0.386512      0.200013      -0.246499              trestbps      0.293608      1.000000      0.154624      -0.055758      0.203551      -0.157073              chol      0.309546      0.154624      1.000000      -0.054981      0.029328      -0.115616              thalach      -0.386512      -0.055758      -0.054981      1.000000      -0.346992      0.435763              oldpeak      0.200013      0.203551      0.029328      -0.346992      1.000000      -0.420039              target      -0.246499      -0.157073      -0.115616      0.435763      -0.420039      1.000000      3. Preprocessing &amp; Classification Accuracy3.1. Naive Bayes w/o preprocessing as a baselinefrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.naive_bayes import GaussianNBX_train = pd.read_csv(\"/kaggle/input/dsc3011/X_train.csv\")y_train = pd.read_csv(\"/kaggle/input/dsc3011/y_train.csv\")X_test = pd.read_csv(\"/kaggle/input/dsc3011/X_test.csv\")X_tr = X_train.drop(columns=[\"id\"])X_te = X_test.drop(columns=[\"id\"])y_tr = y_train['target'].valuesX_t,X_v,y_t,y_v = train_test_split(X_tr,y_tr, test_size=0.2, random_state=2021 )gnb = GaussianNB()gnb.fit(X_t, y_t)y_pred = gnb.predict(X_v)print(accuracy_score(y_v,y_pred))0.851063829787234gnb=GaussianNB()gnb.fit(X_tr,y_tr)y_pred = gnb.predict(X_te)submission0 = pd.DataFrame(X_test['id'])submission0['target'] = y_predsubmission0.to_csv(\"submission0.csv\",sep=',',index=False)3.2. One-hot encodingX_train = pd.read_csv(\"/kaggle/input/dsc3011/X_train.csv\",dtype={c:str for c in cats})y_train = pd.read_csv(\"/kaggle/input/dsc3011/y_train.csv\")X_test = pd.read_csv(\"/kaggle/input/dsc3011/X_test.csv\",dtype={c:str for c in cats})X_train = pd.concat([X_train,pd.get_dummies(X_train[cats],drop_first=True)],axis=1).drop(columns=cats)X_test = pd.concat([X_test,pd.get_dummies(X_test[cats],drop_first=True)],axis=1).drop(columns=cats)print(X_train.columns, X_test.columns)X_train['restecg_2']=0X_train.shape, X_test.shapeIndex(['id', 'age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'sex_1', 'cp_1',       'cp_2', 'cp_3', 'fbs_1', 'restecg_1', 'exang_1', 'slope_1', 'slope_2',       'ca_1', 'ca_2', 'ca_3', 'ca_4', 'thal_1', 'thal_2', 'thal_3'],      dtype='object') Index(['id', 'age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'sex_1', 'cp_1',       'cp_2', 'cp_3', 'fbs_1', 'restecg_1', 'restecg_2', 'exang_1', 'slope_1',       'slope_2', 'ca_1', 'ca_2', 'ca_3', 'ca_4', 'thal_1', 'thal_2',       'thal_3'],      dtype='object')((233, 23), (70, 23))X_tr = X_train.drop(columns=[\"id\"])X_te = X_test.drop(columns=[\"id\"])y_tr = y_train['target'].valuesX_t, X_v, y_t, y_v = train_test_split(X_tr,y_tr,test_size = 0.2,random_state=2021)gnb = GaussianNB()gnb.fit(X_t, y_t)y_pred = gnb.predict(X_v)print(accuracy_score(y_v,y_pred))0.8085106382978723gnb=GaussianNB()gnb.fit(X_tr,y_tr)y_pred = gnb.predict(X_te)submission1 = pd.DataFrame(X_test['id'])submission1['target'] = y_predsubmission1.to_csv(\"submission1.csv\",sep=',',index=False)3.2.1. One-Hot + StandardScalerfrom sklearn.preprocessing import StandardScalersc = StandardScaler()X_tr_pp = X_tr.copy()X_te_pp = X_te.copy()X_tr_pp[nums] = sc.fit_transform(X_tr_pp[nums])X_te_pp[nums] = sc.fit_transform(X_te_pp[nums])X_tr_pp[nums].describe()                  age      trestbps      chol      thalach      oldpeak                  count      2.330000e+02      2.330000e+02      2.330000e+02      2.330000e+02      2.330000e+02              mean      -9.309434e-17      -7.881154e-16      -1.405647e-16      1.038749e-16      1.777310e-16              std      1.002153e+00      1.002153e+00      1.002153e+00      1.002153e+00      1.002153e+00              min      -2.812990e+00      -2.108738e+00      -2.400765e+00      -2.841943e+00      -9.056098e-01              25%      -6.969140e-01      -6.559080e-01      -7.090580e-01      -5.598232e-01      -9.056098e-01              50%      -2.867959e-02      -9.712714e-02      -9.389202e-02      2.008834e-01      -2.149156e-01              75%      7.509272e-01      4.616537e-01      5.020500e-01      7.378527e-01      4.757786e-01              max      2.532886e+00      3.814339e+00      6.019320e+00      2.259266e+00      4.447270e+00      X_t, X_v, y_t, y_v = train_test_split(X_tr_pp,y_tr,test_size = 0.2,random_state=2021)gnb = GaussianNB()gnb.fit(X_t, y_t)y_pred = gnb.predict(X_v)print(accuracy_score(y_v,y_pred))0.8085106382978723gnb=GaussianNB()gnb.fit(X_tr_pp,y_tr)y_pred = gnb.predict(X_te_pp)submission2 = pd.DataFrame(X_test['id'])submission2['target'] = y_predsubmission2.to_csv(\"submission2.csv\",sep=',',index=False)3.2.2 One-hot + Log-transformX_tr_log = X_tr.copy()X_te_log = X_te.copy()for c in nums:    X_tr_log[c] = np.log(X_tr_log[c]+1)    X_te_log[c] = np.log(X_te_log[c]+1)X_tr_log[nums].describe()                  age      trestbps      chol      thalach      oldpeak                  count      233.000000      233.000000      233.000000      233.000000      233.000000              mean      3.998016      4.879579      5.509133      5.015565      0.575909              std      0.170517      0.132206      0.197961      0.156684      0.522850              min      3.401197      4.553877      4.844187      4.488636      0.000000              25%      3.891820      4.795791      5.370638      4.941642      0.000000              50%      4.007333      4.875197      5.509388      5.056246      0.587787              75%      4.127134      4.948760      5.627621      5.129899      0.955511              max      4.356709      5.303305      6.336826      5.313206      1.974081      X_t, X_v, y_t, y_v = train_test_split(X_tr_log,y_tr,test_size = 0.2,random_state=2021)gnb = GaussianNB()gnb.fit(X_t, y_t)y_pred = gnb.predict(X_v)print(accuracy_score(y_v,y_pred))0.8085106382978723gnb=GaussianNB()gnb.fit(X_tr_log,y_tr)y_pred = gnb.predict(X_te_log)submission3 = pd.DataFrame(X_test['id'])submission3['target'] = y_predsubmission3.to_csv(\"submission3.csv\",sep=',',index=False)3.4. Varrying Classification Methodsfrom sklearn.model_selection import cross_val_scorefrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCVset1= X_tr,X_teset2= X_tr_pp, X_te_ppset3= X_tr_log, X_te_logdef grid_selection(set):    knn_grid =[        {'n_neighbors': range(3,15,1), 'p': [1,2]}]    knn = KNeighborsClassifier()    grid_knn = GridSearchCV(knn, knn_grid, cv=5,                               scoring='neg_mean_squared_error',                               return_train_score=True)    grid_knn.fit(X_tr,y_tr)        rf_grid =[        {'n_estimators': [5, 10, 20, 50, 75, 100], 'max_features': [2, 4, 6, 8]}]    rf = RandomForestClassifier()    grid_rf = GridSearchCV(rf, rf_grid, cv=5,                               scoring='neg_mean_squared_error',                               return_train_score=True)    grid_rf.fit(set[0],y_tr)    return grid_knn.best_estimator_, grid_rf.best_estimator_def rmse_scores(set):    gnb = GaussianNB()    svc = SVC()    dt = DecisionTreeClassifier()    knn, rf = grid_selection(set)    models = [gnb, svc, dt, knn, rf]    results = {}    for model in models:        scores = cross_val_score(model, set[0],y_tr,                                scoring=\"neg_mean_squared_error\",cv=10)        results[model] = np.sqrt(-scores).mean()    return resultsset1_result = rmse_scores(set1)set2_result = rmse_scores(set2)set3_result = rmse_scores(set3)set1_result, set2_result, set3_result({GaussianNB(): 0.4178876039491642,  SVC(): 0.5823397149422178,  DecisionTreeClassifier(): 0.4729488489553738,  KNeighborsClassifier(n_neighbors=13, p=1): 0.5371866622228357,  RandomForestClassifier(max_features=2, n_estimators=20): 0.38835845413636216}, {GaussianNB(): 0.4415320745933398,  SVC(): 0.4067235886003079,  DecisionTreeClassifier(): 0.4586168121445854,  KNeighborsClassifier(n_neighbors=13, p=1): 0.413359640076294,  RandomForestClassifier(max_features=2, n_estimators=10): 0.483896282692258}, {GaussianNB(): 0.4415320745933398,  SVC(): 0.3752652644845328,  DecisionTreeClassifier(): 0.46699040892811083,  KNeighborsClassifier(n_neighbors=13, p=1): 0.38902561032241134,  RandomForestClassifier(max_features=4): 0.43207507210776175})print(min(set1_result.items(), key = lambda x: x[1]))print(min(set2_result.items(), key = lambda x: x[1]))print(min(set3_result.items(), key = lambda x: x[1]))(RandomForestClassifier(max_features=2, n_estimators=20), 0.38835845413636216)(SVC(), 0.4067235886003079)(SVC(), 0.3752652644845328)4. Predictions from the best classifiersrf = RandomForestClassifier(max_features=2, n_estimators=50)rf.fit(X_tr_pp,y_tr)y_pred = rf.predict(X_te_pp)submission4 = pd.DataFrame(X_test['id'])submission4['target'] = y_predsubmission4.to_csv(\"submission4.csv\",sep=',',index=False)# grid to SVC# C: float, default=1.0# kernel: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’# gamma: {‘scale’, ‘auto’} or float, default=’scale’X_t, X_v, y_t, y_v = train_test_split(X_tr_log,y_tr,test_size = 0.2,random_state=2021)svc = SVC(C=0.3)svc.fit(X_t, y_t)y_pred = svc.predict(X_v)print(accuracy_score(y_v,y_pred))0.8723404255319149svc = SVC()svc.fit(X_tr_log,y_tr)y_pred = svc.predict(X_te_log)submission5 = pd.DataFrame(X_test['id'])submission5['target'] = y_predsubmission5.to_csv(\"submission5.csv\",sep=',',index=False)5. Other Algorithms: Logistic RegressionX_t, X_v, y_t, y_v = train_test_split(X_tr_pp,y_tr,test_size = 0.2,random_state=2021)lr_clf = LogisticRegression()lr_clf.fit(X_t, y_t)y_pred = lr_clf.predict(X_v)print(accuracy_score(y_v,y_pred))0.8723404255319149lr_clf=LogisticRegression()lr_scores = cross_val_score(lr_clf, X_tr_pp,y_tr,                        scoring=\"neg_mean_squared_error\",cv=10)np.sqrt(-lr_scores).mean()0.3575812829170383lr_clf=LogisticRegression()lr_clf.fit(X_tr_pp, y_tr)y_pred = lr_clf.predict(X_te_pp)submission6 = pd.DataFrame(X_test['id'])submission6['target'] = y_predsubmission6.to_csv(\"submission6.csv\",sep=',',index=False)",
        "url": "/AppML_Assginment1"
    }
    ,
    
    "nlp-final": {
        "title": "NLP - Final",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-1. NLP# 한국어 자료import sysimport osimport numpy as npimport nltkimport konlpyimport pandas as pdimport reimport randomimport itertoolsimport warningswarnings.filterwarnings(action='ignore')from sklearn.metrics import classification_report,f1_score,precision_score,recall_scorerandom.seed(1)# 데이터 로드train_data=pd.read_csv('final_train_data.csv')test_data=pd.read_csv('final_test_data.csv')# 384 duplicates in content, 240 in titleprint(train_data.describe())       category                                            content  titlecount     10686                                              10686  10686unique        7                                              10302  10124top        정치개혁  개인회생 36개월 단축소급 전국 적용을 위해 춘천지방법원의 법원에 바란다에 글을 올...   경남제약freq       3094                                                 16     21# preprocess: duplicatetrain_data = train_data.drop_duplicates(['content'],keep='first')# all duplicates removedtrain_data.duplicated().sum()0train_data['document']=train_data.iloc[:,1]+train_data.iloc[:,2]train_data['document']test_data['document']=test_data.iloc[:,1]+test_data.iloc[:,2]train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행tokenizing process with konlpy-Okt# tokenizefrom konlpy.tag import Oktokt = Okt() #형태소 분석기tokenized_data = []stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']for sentence in train_data['document']:    temp_X = okt.morphs(sentence, norm=True, stem=True) # 형태소 추출    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거    tokenized_data.append(temp_X)x_train=tokenized_dataokt = Okt() #형태소 분석기tokenized_data = []stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']for sentence in test_data['document']:    temp_X = okt.morphs(sentence, norm=True, stem=True) # 형태소 추출 - 토큰화 #norm=True : 근사어    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거 #https://www.ranks.nl/stopwords/korean    tokenized_data.append(temp_X)x_test=tokenized_datafor this tokenizing process takes a long time, save and import preprocessed data.import picklewith open('nlp_final_x_tr.data', 'wb') as f:    pickle.dump(x_train, f)with open('nlp_final_x_te.data', 'wb') as f:    pickle.dump(x_test, f)import picklewith open('nlp_final_x_tr.data', 'rb') as f:    x_train = pickle.load(f)with open('nlp_final_x_te.data', 'rb') as f:    x_test = pickle.load(f)from tensorflow.keras.preprocessing.text import Tokenizertokenizer = Tokenizer()tokenizer.fit_on_texts(x_train)threshold = 3total_cnt = len(tokenizer.word_index) # 단어의 수rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.for key, value in tokenizer.word_counts.items():    total_freq = total_freq + value    # 단어의 등장 빈도수가 threshold보다 작으면    if(value &lt; threshold):        rare_cnt = rare_cnt + 1        rare_freq = rare_freq + valueprint('단어 집합(vocabulary)의 크기 :',total_cnt)print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)단어 집합(vocabulary)의 크기 : 34537등장 빈도가 2번 이하인 희귀 단어의 수: 15483단어 집합에서 희귀 단어의 비율: 44.830182123519705전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.1132793250941202vocab_size = total_cnt - rare_cnt + 1 # 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거. 0번 패딩 토큰을 고려하여 +1print('단어 집합의 크기 :',vocab_size)단어 집합의 크기 : 19055tokenizer = Tokenizer(vocab_size) tokenizer.fit_on_texts(x_train)X_train = tokenizer.texts_to_sequences(x_train)X_test = tokenizer.texts_to_sequences(x_test)y_train=np.array(train_data.category)y_test=np.array(test_data.category)drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) &lt; 1]drop_test = [index for index, sentence in enumerate(X_test) if len(sentence) &lt; 1]X_train = np.delete(X_train, drop_train, axis=0)y_train = np.delete(y_train, drop_train, axis=0)print(len(X_train))print(len(y_train))1030110301X_test = np.delete(X_test, drop_test, axis=0)y_test = np.delete(y_test, drop_test, axis=0)print(len(X_train))print(len(X_test))print(len(y_train))print(len(y_test))103011158103011158import matplotlib.pyplot as pltprint('리뷰의 최대 길이 :',max(len(l) for l in X_train))print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))plt.hist([len(s) for s in X_train], bins=50)plt.xlabel('length of samples')plt.ylabel('number of samples')plt.show()리뷰의 최대 길이 : 9032리뷰의 평균 길이 : 170.45791670711583def below_threshold_len(max_len, nested_list):  cnt = 0  for s in nested_list:    if(len(s) &lt;= max_len):        cnt = cnt + 1  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))max_len = 600below_threshold_len(max_len, X_train)전체 샘플 중 길이가 600 이하인 샘플의 비율: 96.02951169789341from tensorflow.keras.preprocessing.sequence import pad_sequencesX_train = pad_sequences(X_train, maxlen = max_len)X_test = pad_sequences(X_test, maxlen = max_len)y_train=np.array(train_data.category)y_test=np.array(test_data.category)y_train = np.delete(y_train, drop_train, axis=0)y_test = np.delete(y_test, drop_test, axis=0)print(len(y_train),len(y_test))10301 1158t={'경제민주화': 1, '교통/건축/국토': 2, '보건복지': 3, '육아/교육': 4, '인권/성평등': 5, '일자리': 6, '정치개혁': 7}print(t['보건복지'])index1=np.zeros([10301,7])for i in range(len(y_train)):  index1[i][t[y_train[i]]-1]=1y_train=index1index1=np.zeros([1158,7])for i in range(len(y_test)):  index1[i][t[y_test[i]]-1]=1y_test=index1print(y_train.shape,y_test.shape)3(10301, 7) (1158, 7)X_train=np.array(X_train)X_test=np.array(X_test)y_train=np.array(y_train)y_test=np.array(y_test)print(X_train.shape)print(X_test.shape)print(y_train.shape)print(y_test.shape)print(vocab_size)(10301, 600)(1158, 600)(10301, 7)(1158, 7)19055from tensorflow.keras.layers import Embedding, Dense, LSTMfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.models import load_modelfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpointimport tensorflow_addons as tfaf1 = tfa.metrics.F1Score(num_classes=7,threshold=0.5)from tensorflow import keraskeras.__version__'2.6.0'model = Sequential()model.add(Embedding(vocab_size, 128))model.add(LSTM(128))model.add(Dense(7, activation='sigmoid'))es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=4)mc = ModelCheckpoint('best_model.h5', monitor=f1, mode='max', verbose=2, save_best_only=True)model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc',f1])model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=512, validation_split=0.2)Epoch 1/1517/17 [==============================] - 39s 2s/step - loss: 1.8842 - acc: 0.2830 - f1_score: 0.2583 - val_loss: 1.7326 - val_acc: 0.3081 - val_f1_score: 0.2726WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 2/1517/17 [==============================] - 38s 2s/step - loss: 1.6177 - acc: 0.4615 - f1_score: 0.3981 - val_loss: 1.5555 - val_acc: 0.4639 - val_f1_score: 0.3776WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 3/1517/17 [==============================] - 38s 2s/step - loss: 1.3590 - acc: 0.5495 - f1_score: 0.4018 - val_loss: 1.3879 - val_acc: 0.5075 - val_f1_score: 0.4232WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 4/1517/17 [==============================] - 38s 2s/step - loss: 1.1364 - acc: 0.6211 - f1_score: 0.4351 - val_loss: 1.3140 - val_acc: 0.5303 - val_f1_score: 0.4173WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 5/1517/17 [==============================] - 38s 2s/step - loss: 0.9084 - acc: 0.7212 - f1_score: 0.4893 - val_loss: 1.1852 - val_acc: 0.6487 - val_f1_score: 0.4386WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 6/1517/17 [==============================] - 38s 2s/step - loss: 0.7336 - acc: 0.7920 - f1_score: 0.5183 - val_loss: 1.1071 - val_acc: 0.6473 - val_f1_score: 0.4536WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 7/1517/17 [==============================] - 39s 2s/step - loss: 0.5796 - acc: 0.8381 - f1_score: 0.5395 - val_loss: 1.0900 - val_acc: 0.6458 - val_f1_score: 0.4837WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 8/1517/17 [==============================] - 38s 2s/step - loss: 0.4641 - acc: 0.8757 - f1_score: 0.5575 - val_loss: 0.9854 - val_acc: 0.6982 - val_f1_score: 0.4781WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 9/1517/17 [==============================] - 38s 2s/step - loss: 0.3588 - acc: 0.9039 - f1_score: 0.5759 - val_loss: 1.2205 - val_acc: 0.6458 - val_f1_score: 0.4993WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 10/1517/17 [==============================] - 38s 2s/step - loss: 0.3082 - acc: 0.9181 - f1_score: 0.5908 - val_loss: 1.0990 - val_acc: 0.6870 - val_f1_score: 0.5031WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 11/1517/17 [==============================] - 39s 2s/step - loss: 0.2331 - acc: 0.9408 - f1_score: 0.6076 - val_loss: 1.8883 - val_acc: 0.5793 - val_f1_score: 0.4935WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 12/1517/17 [==============================] - 39s 2s/step - loss: 0.2625 - acc: 0.9380 - f1_score: 0.6111 - val_loss: 1.3714 - val_acc: 0.6642 - val_f1_score: 0.5215WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 00012: early stopping&lt;keras.callbacks.History at 0x7f1bf1402a60&gt;# evaluating function: report f1_macrodef evaluate(test_x,test_y,model):    predictions=model.predict(test_x)    y_pred=max(predictions)    print(classification_report(test_y,y_pred))model.evaluate(X_test, y_test)37/37 [==============================] - 6s 149ms/step - loss: 1.1846 - acc: 0.6986 - f1_score: 0.5419[1.1846439838409424, 0.6986182928085327, array([0.5124555 , 0.59907836, 0.48712873, 0.45633796, 0.5854922 ,        0.4855967 , 0.6674057 ], dtype=float32)]# from saved best modelloaded_model = load_model('best_model.h5')loaded_model.evaluate(X_test, y_test)",
        "url": "/NLP_final"
    }
    ,
    
    "nlp-midterm": {
        "title": "NLP - Midterm",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-1. NLPimport sysimport pandas as pdimport osimport numpy as npimport reimport randomimport itertoolsimport warningswarnings.filterwarnings(action='ignore')from sklearn.metrics import classification_reportfrom sklearn.metrics import f1_scorerandom.seed(1)train_data=pd.read_csv('midterm_train.csv')test_data=pd.read_csv('midterm_test.csv')# evaluating functiondef evaluate(test_x,test_y,model):    predictions=model.predict(test_x)    print(classification_report(test_y,predictions))1. Data Infotrain_data.head(3)                  text      senti                  0      J brand is by far the best premium denim line ...      pos              1      I loved this dress. i kept putting it on tryin...      pos              2      I found this at my local store and ended up bu...      pos      print(train_data.groupby('senti').count())print(test_data.groupby('senti').count())print(train_data.describe())print(test_data.describe())        textsenti       neg     2139pos    11279       textsenti      neg     231pos    1260                                                     text  senticount                                               13418  13418unique                                              13414      2top     Perfect fit and i've gotten so many compliment...    posfreq                                                    2  11279                                                     text senticount                                                1491  1491unique                                               1491     2top     Have to disagree with previous posters. i foun...   posfreq                                                    1  12602. Preprocessing2.1 duplicated data found in train_data# remove duplicated dataprint(train_data.text.duplicated().sum())train_data = train_data.drop_duplicates(['text'],keep='first')train_data.duplicated().sum()40print(train_data.describe())                                                     text  senticount                                               13414  13414unique                                              13414      2top     J brand is by far the best premium denim line ...    posfreq                                                    1  11276# train-test split unduplicated datax_train=np.array(train_data.text)x_test=np.array(test_data.text)y_train=np.array(train_data.senti)y_test=np.array(test_data.senti)x_train[0]'J brand is by far the best premium denim line retailer sells! the fit on these jeans is amazing..worth every penny..also, considering it is a crop jean - warm weather wear - the denim weight is light and not too thick...the color is different from ordinary regular denim blue..lighter wash for spring/summer!'# preprocessing: remove non-alphabet charactersx_train_clean=np.array([re.sub('[^a-zA-Z]',' ',text) for text in x_train])x_test_clean=np.array([re.sub('[^a-zA-Z]',' ',text) for text in x_test])x_train_clean[0]'J brand is by far the best premium denim line retailer sells  the fit on these jeans is amazing  worth every penny  also  considering it is a crop jean   warm weather wear   the denim weight is light and not too thick   the color is different from ordinary regular denim blue  lighter wash for spring summer '3. Comparing classification models# tuning parameter setsngram_range= [(1, 1), (1, 2),(2,2)]stop_words=[None,'english']clf__alpha=[0.005,0.01,0.05,0.1]params = dict(ngram_range=ngram_range,              stop_words=stop_words,              clf__alpha=clf__alpha)keys=params.keys()values = (params[key] for key in keys)combinations = [dict(zip(keys, combination)) for combination in itertools.product(*values)]# tuning functiondef tuning_model(params,vectorizer,classifier,x_train,x_test,y_train,y_test):    ngram_range=params['ngram_range']    stop_words=params['stop_words']    vec = vectorizer(ngram_range=ngram_range,stop_words=stop_words)    vec_train = vec.fit_transform(x_train)    vec_test = vec.transform(x_test)    if classifier==MultinomialNB:        alpha=params['clf__alpha']        clf=classifier(alpha)    else:        clf=classifier(random_state=1,max_iter=500)    clf.fit(vec_train, y_train)    pred=clf.predict(vec_test)    return f1_score(y_test,pred,average='macro'),params# get best score &amp; parametersdef get_result(combinations,vectorizer,classifier,x_train,x_test,y_train,y_test):    results=[]    for params in combinations:        results.append(tuning_model(params,vectorizer,classifier,x_train,x_test,y_train,y_test))    return max(results,key=lambda item: item[0])3.1 With Tf-idf vectorizer3.1.1 MultinomialNBpreprocessing seems to have no effect in ngram_range: (1, 2) and also stop words are not important featuresf1-score macro avg: 0.85from sklearn.naive_bayes import MultinomialNBfrom sklearn.feature_extraction.text import TfidfVectorizerget_result(combinations,TfidfVectorizer,MultinomialNB,x_train,x_test,y_train,y_test)(0.8528815948449455, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.01})get_result(combinations,TfidfVectorizer,MultinomialNB,x_train_clean,x_test_clean,y_train,y_test)(0.8528815948449455, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.005})3.1.2 LogisticRegressionfrom sklearn.linear_model import LogisticRegressionget_result(combinations,TfidfVectorizer,LogisticRegression,x_train,x_test,y_train,y_test)(0.8811180515581001, {'ngram_range': (1, 1), 'stop_words': None, 'clf__alpha': 0.005})get_result(combinations,TfidfVectorizer,LogisticRegression,x_train_clean,x_test_clean,y_train,y_test)(0.8823175752378818, {'ngram_range': (1, 1), 'stop_words': None, 'clf__alpha': 0.005})3.2 With CountVectorizer3.2.1 MultinomialNBfrom sklearn.feature_extraction.text import CountVectorizerget_result(combinations,CountVectorizer,MultinomialNB,x_train,x_test,y_train,y_test)(0.9366865264206945, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.1})get_result(combinations,CountVectorizer,MultinomialNB,x_train_clean,x_test_clean,y_train,y_test)(0.935509934324805, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.1})4. Balanced sampling approach - imblearnfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.linear_model import LogisticRegression# tuning parameter sets for all casesvectorizer=[CountVectorizer,TfidfVectorizer]classifier=[MultinomialNB,LogisticRegression]ngram_range= [(1, 1), (1, 2),(2,2)]stop_words=[None,'english']clf__alpha=[0.005,0.01,0.05,0.1]params = dict(ngram_range=ngram_range,              vectorizer=vectorizer,              classifier=classifier,              stop_words=stop_words,              clf__alpha=clf__alpha)keys=params.keys()values = (params[key] for key in keys)combinations = [dict(zip(keys, combination)) for combination in itertools.product(*values)]from imblearn.over_sampling import SMOTEdef smote_tuning(params,x_train,x_test,y_train,y_test):    ngram_range=params['ngram_range']    stop_words=params['stop_words']    classifier=params['classifier']    vec=params['vectorizer'](ngram_range=ngram_range,stop_words=stop_words)    x_train_imb=vec.fit_transform(x_train)    y_train_imb=np.array(train_data['senti']=='pos').astype('int')    y_test_imb=np.array(test_data['senti']=='pos').astype('int')    vec_train_over, y_train_over = SMOTE(random_state=1).fit_resample(x_train_imb,y_train_imb)    vec_test = vec.transform(x_test)    if classifier==MultinomialNB:        alpha=params['clf__alpha']        clf=classifier(alpha)    else:        clf=classifier(random_state=1,max_iter=500)    clf.fit(vec_train_over, y_train_over)    pred=clf.predict(vec_test)    return f1_score(y_test_imb,pred,average='macro'),params# get best score &amp; parametersdef get_smote_result(combinations,x_train,x_test,y_train,y_test):    results=[]    for params in combinations:        results.append(smote_tuning(params,x_train,x_test,y_train,y_test))    return max(results,key=lambda item: item[0])get_smote_result(combinations,x_train,x_test,y_train,y_test)(0.9285352359562871, {'ngram_range': (1, 2),  'vectorizer': sklearn.feature_extraction.text.TfidfVectorizer,  'classifier': sklearn.naive_bayes.MultinomialNB,  'stop_words': None,  'clf__alpha': 0.1})get_smote_result(combinations,x_train_clean,x_test_clean,y_train,y_test)(0.9276401547886131, {'ngram_range': (1, 2),  'vectorizer': sklearn.feature_extraction.text.TfidfVectorizer,  'classifier': sklearn.naive_bayes.MultinomialNB,  'stop_words': None,  'clf__alpha': 0.1})5. Result: Best Model# 'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.1}vectorizer=CountVectorizer(ngram_range=(1,2))vectors_train=vectorizer.fit_transform(x_train)vectors_test=vectorizer.transform(x_test)clf=MultinomialNB(alpha=0.1)clf.fit(vectors_train,y_train)evaluate(vectors_test,y_test,clf)              precision    recall  f1-score   support         neg       0.90      0.88      0.89       231         pos       0.98      0.98      0.98      1260    accuracy                           0.97      1491   macro avg       0.94      0.93      0.94      1491weighted avg       0.97      0.97      0.97      1491comment데이터사이언스 복수전공 후 첫 강의. 중간고사까지 배웠던 기본적인 전처리와 ML 함수들을 적용했다. 추가적으로 imblanced data case를 SMOTE 패키지를 사용해 간단히 처리해보았다. 당시 수업에서 가장 높은 accuracy를 기록했다.",
        "url": "/NLP_midterm"
    }
    ,
    
    "stat-hw2": {
        "title": "Statistical Mining - HW2",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-1. Statistical Mining#2set.seed(1)x1=runif(100)x2=0.5*x1 + rnorm(100)/10y= 2+ 2*x1 + 0.3*x2 + rnorm(100)fit = lm(y ~  x1 + x2,data=y)Error in eval(predvars, data, env): 길이가 1이 아닌 수치형 'envir' 인자입니다Traceback:1. lm(y ~ x1 + x2, data = y)2. eval(mf, parent.frame())3. eval(mf, parent.frame())4. stats::model.frame(formula = y ~ x1 + x2, data = y, drop.unused.levels = TRUE)5. model.frame.default(formula = y ~ x1 + x2, data = y, drop.unused.levels = TRUE)6. eval(predvars, data, env)cor(x1,x2)0.835121242463113plot(x1,x2)fit &lt;- lm(y~x1+x2)summary(fit)Call:lm(formula = y ~ x1 + x2)Residuals:    Min      1Q  Median      3Q     Max -2.8311 -0.7273 -0.0537  0.6338  2.3359 Coefficients:            Estimate Std. Error t value Pr(&gt;|t|)    (Intercept)   2.1305     0.2319   9.188 7.61e-15 ***x1            1.4396     0.7212   1.996   0.0487 *  x2            1.0097     1.1337   0.891   0.3754    ---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1Residual standard error: 1.056 on 97 degrees of freedomMultiple R-squared:  0.2088,\tAdjusted R-squared:  0.1925 F-statistic:  12.8 on 2 and 97 DF,  p-value: 1.164e-05fit2 &lt;- lm(y~x1)summary(fit2)Call:lm(formula = y ~ x1)Residuals:     Min       1Q   Median       3Q      Max -2.89495 -0.66874 -0.07785  0.59221  2.45560 Coefficients:            Estimate Std. Error t value Pr(&gt;|t|)    (Intercept)   2.1124     0.2307   9.155 8.27e-15 ***x1            1.9759     0.3963   4.986 2.66e-06 ***---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1Residual standard error: 1.055 on 98 degrees of freedomMultiple R-squared:  0.2024,\tAdjusted R-squared:  0.1942 F-statistic: 24.86 on 1 and 98 DF,  p-value: 2.661e-06fit3 &lt;- lm(y~x2)summary(fit3)Call:lm(formula = y ~ x2)Residuals:     Min       1Q   Median       3Q      Max -2.62687 -0.75156 -0.03598  0.72383  2.44890 Coefficients:            Estimate Std. Error t value Pr(&gt;|t|)    (Intercept)   2.3899     0.1949   12.26  &lt; 2e-16 ***x2            2.8996     0.6330    4.58 1.37e-05 ***---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1Residual standard error: 1.072 on 98 degrees of freedomMultiple R-squared:  0.1763,\tAdjusted R-squared:  0.1679 F-statistic: 20.98 on 1 and 98 DF,  p-value: 1.366e-05",
        "url": "/stat_HW2"
    }
    ,
    
    "stat-week2": {
        "title": "Statistical Mining - Week2",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-1. Statistical MiningCh 2. Supervised LearningFormat of Supervised Learning:  $ Y=f(X)+\\epsilon $$f$ : Fixed but UNKNOWN function of predictors X$\\rightarrow$ Supervised Learning: A set of methods estimating $f$.  Why do we estimate $f$?1) Prediction: Y2) Inference: Relationship between X &amp; Y1) Prediction:$\\rightarrow\\text{Prediction of Y} : \\hat{Y} = \\hat{f}(X)\\quad (\\hat{f} : \\text{Estimate of }f) $$\\rightarrow\\text{Accuracy of }\\hat{Y}\\text{ depends on reducible &amp; irreducible errors}$  $E[Y-\\hat{Y}]^2 = [f(X)-\\hat{f}(X)]^2 + Var(\\epsilon)$$\\quad (MSE)\\quad\\quad (reducible)\\quad (irreducible)$  Reducible error : controlable, can be reduced by selecting a better model  Irreducible error: uncontrolable, nature of data2) Inference:  Characteristics of liner &amp; nonlinear models:1) Linear : Relatively Simple &amp; interpretable inference but poor prediction (when true function $f$ is nonlinear)2) Nonlinear : More accurate but difficult to interpret$\\rightarrow$ Depends on your Goal of analysis3) Estimation F: By Using Training Data  Estimation Methods:1) Parametric: Assume specific function $f$ (linear&amp;non- both)2) Nonparametric: No Assumption; Data-driven method (nonlinear)4) Parametric Methods: Model-based approach  Step 1: Assume about the functional form of $f$$\\quad\\text{e.x) when } X_j = ( x_{1j}, \\ldots , x_{nj} ) ^T : n * 1 \\space \\text{obs. ; vector for the j th predictor} \\quad\\quad \\rightarrow linear: f(X)= \\beta_0 + \\beta_1X_1 + … + \\beta_pX_p$  Step 2: Estimate Parameters$\\quad\\quad\\quad Y \\approx \\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + … + \\hat{\\beta}_pX_p = \\hat{f}(X)$  Disadvantage: Specific form of $f$$\\quad\\quad\\quad\\rightarrow$ Not match with the true $f$: poor estimation &amp; poor prediction$\\quad\\quad\\quad\\rightarrow$ Flextible parametric model : model with more parameters$\\quad\\quad\\quad\\quad$ Flexible model $\\equiv$ Complex model$\\quad\\quad\\quad\\quad$ but overfitting problem still remains: model follows errors or noisesUnderfitting vs Overfitting  under: estimation has not enough accuracy, miss to capture the true structure  over: more complexity than true $f$5) Nonparametric Methods: Data-driven approach  No assumptions about functional form of $f \\Rightarrow $ wider range of possible shapes of $f$  Disadvantage:$\\quad\\quad\\quad\\rightarrow$ A vary large # of obs. is required to get an accurae $f$ (relatively to parametric method)$\\quad\\quad\\quad\\rightarrow$ Overfitting $\\Leftrightarrow$ Level of smoothness; Model complexity (how to determine it?)    Example of Nonparametric model: KNN Reg.    Idea: Similar inputs have Similar output  Step: when predicting $\\hat{Y}_0$ with input $X_0$1) Determine $K$ (level of smoothness) #if K==N: Y = Var(Y) / if K==1: Y = perfect fit; no training error2) find $K$ closest training obs. from the target input point($X_0$) using Euclidean distance\\(\\rightarrow \\hat{Y}_0 = \\frac{1}{K}\\sum_{x_i\\in \\mathcal N}^K y_i \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\)3) average of these #k Y values = $\\hat{Y}_0$  Effect of $K$ (tuning parameter/ model flexibility parameter)          decides the level of smoothness (under or overfitting)      As $K \\uparrow \\Rightarrow \\text{flexibility} \\downarrow$ : the fitted line is simple      As $K \\downarrow \\Rightarrow \\text{flexibility} \\uparrow$ : the fitted line is wiggly      6) Trade-off between Prediction Accuray &amp; Interpretability  Restrictive model(Simple model) vs. Flexible model  our goal: inference ($\\rightarrow$ restrictive model) or prediction ($\\rightarrow$ flexible model)7) Model Assessment &amp; Selection  Evaluate the performance of models on a given dataset  and Select the best model  Criterion: Better prediction of $Y$  #When Regression problem: Mean squared error (MSE)      training MSE(used for building model) is not our interest; cannot be a measure for selectione.g) $\\begin{align} R^2 = 1 - \\frac{RSS}{TSS} \\end {align}\\text{ (Residual Sum of Squares)  / (Total Sum of Square)} \\\\quad\\quad \\text{training RSS}= \\sum(y_i - \\hat{y_i})^2  \\quad \\text{&amp;} \\quad \\text{MSE} =\\frac{RSS}{N}$                  when more X variables added $\\rightarrow \\text{RSS&amp;MSE}\\downarrow , \\space R^2\\uparrow$          even X are not important variables        complex model has smaller training error (regardless of existence of test data)                            Test MSE $= \\begin{align}\\frac{1}{m}\\sum_{i=1}^m(y_i^0 - \\hat{f}(x_i^0))^2 \\end{align}$        with low test MSE $\\Rightarrow$ better model$\\rightarrow$ How to find optimal Flexibility?$\\rightarrow$ Test MSE(error rate) is always larger than Training MSE; We estimated model function $f$ to minimize its training error$\\rightarrow$ No test data: Sample re-use methods (e.g., bootstrap, cross-validation)              Classification problem: Misclassification rate8) Bias-Variance Trade-off 편향분산 교차  Expected test MSE(ideal measure): $E[y_0-\\hat{f}(x_0)]^2 \\quad ((x_0,y_0) \\text{ is a test obs})$      in population: #K training sets &amp; #K function $f_k$Estimation of expected test MSE:\\(\\hat{E}[y_0 - \\hat{f}(x_0)]^2 = \\frac{1}{K} \\sum_{k=1}^K[y_0-\\hat{f}_k(x_0)]^2\\)    For given $x_0$,  (when $\\epsilon$ is irreducible error)$\\begin{matrix}\\begin{align}\\hat{E}[y_0 - \\hat{f}(x_0)]^2 &amp;= Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon) \\               \\text{} \\quad&amp;= E [ \\{ y_0 -E(\\hat{f}(x_0)) \\} - \\{ \\hat{f}(x_0) - E(\\hat{f}(x_0)) \\} ]^2 \\\\                                             &amp;= \\quad E [  y_0 -E(\\hat{f}(x_0)) ] ^2 \\quad \\quad \\quad &amp; \\rightarrow (1) \\\\                                             &amp; \\quad \\; + E [\\hat{f}(x_0) - E(\\hat{f}(x_0)) ] ^2 &amp; \\rightarrow (2)\\\\                                             &amp; \\quad \\; - 2E[ \\{ y_0 -E(\\hat{f}(x_0)) \\} \\{ \\hat{f}(x_0) - E(\\hat{f}(x_0)) \\} ] &amp; \\rightarrow (3)\\end{align}\\end{matrix}$      $\\begin {matrix} \\begin{align}(1) &amp;= E [(f(x_0) + \\epsilon - E(\\hat{f}(x_0)) ] ^2    &amp;= E[ f(x_0)^2 +\\epsilon^2 + [E(\\hat{f}(x_0)) ]^2 + 2f(x_0)\\epsilon -2f(x_0)E(\\hat{f}(x_0)) - 2\\epsilon E(\\hat{f}(x_0)) ]           \\quad \\leftarrow \\text{when } E(\\epsilon) \\text{ goes to } 0     &amp;= E[E(\\hat{f}(x_0)) - f(x_0)] ^2 + E(\\epsilon^2)  \\quad  \\leftarrow \\text{ in E[ ] is Bias }     &amp;= [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon^2)\\end{align}\\end{matrix} $    $(2) = Var(\\hat{f}(x_0)) \\leftarrow \\text { model variance (different fit’s from randomness of training sets)} $  $(3) = 0$$\\therefore$ Expected test MSE is consisted of Reducible Error(Var+ Bias$^2$) and Irreducible Error(Var($\\epsilon$))2020-04-03 Week 3      $Var(\\hat{f(x_0})$ : model variance        $ [Bias(\\hat{f(x_0})]$ : Systematic model error ( caused by model assumptions; e.g. true f: nonlinear)        $ Var(\\epsilon)$ : Irreducible error  Model Flextibility $\\uparrow$ $\\Rightarrow$ model variance $\\uparrow$ &amp; model bias $\\downarrow$Model Flextibility $\\downarrow$ $\\Rightarrow$ model variance $\\downarrow$ &amp; model bias $\\uparrow$$\\rightarrow$ optimal model flextibility is different for each datasetsTrade-off in KNN Reg.  given) Y = f (X) + $\\epsilon$ with E($\\epsilon$) = 0 and Var ($\\epsilon$) = $\\sigma^2$      Expected test MSE at $x_0$:$E[ (Y- \\hat{f_k}(x_0) )^2 | X = x_0 ] = \\sigma^2 + Bias^2(\\hat{f_k} (x_0)) + Var (\\hat{f_k} (x_0)) \\rightarrow Bias(\\hat{f_k} (x_0)) =  f(x_0) - E(\\hat{f}(x_0))   \\text{ : given “f” is KNN func.}\\ \\quad= { f(x_0) - E[\\frac1 K \\sum_{i \\in \\mathcal N (x_0)} ^ K Y_i ] }  \\quad= { f(x_0)- \\frac 1 K \\sum E (f(x_i)+\\epsilon_i)}  \\quad= f(x_0) - \\frac 1 K \\sum E (f(x_i)  \\rightarrow Var(\\hat{f_k} (x_0)) = Var(\\frac 1 K \\sum_{i \\in \\mathcal N (x_0)} Yi )\\quad= \\frac 1 {K^2} \\sum  Var( Yi ) \\quad=  \\frac 1 {K^2} \\sum Var( f( x_i) + \\epsilon_i ) \\quad=  \\frac 1 {K^2} \\sum \\sigma^2 = \\frac 1 {K^2} * K * \\sigma^2 = \\frac {\\sigma^2} K $\\(\\therefore \\text{ Expected test MSE at } x_0 = \\sigma ^2 + [f(x_0) - \\frac 1 K \\sum_{x_i \\in \\mathcal N (x_0)}^k f(x_i) ]^2 + \\frac {\\sigma^2} K\\)    when we use large number of K :as K $\\uparrow\\rightarrow$ model complexity$\\downarrow$ model variance $\\downarrow \\rightarrow$ bias $\\uparrow$ :simpler modelin Classification: Using Misclassification rate\\(= \\frac 1 N \\sum_{i=1}^n I(y_i \\not = \\hat y_i )\\)  Bayes Classifier  KNN Classifier$min_\\hat Y E( I(Y \\not = \\hat Y) = min_\\hat Y E[ E(I(Y \\not = \\hat Y | X) ] = min_{\\hat f (x)} E[ \\sum_{g=1}^G I( Y= g\\not= \\hat f (x) ) P ( Y= g| X) ] \\equiv min_{\\hat f (x)} \\sum_{g=1}^G I ( Y=g \\not= \\hat f (x) ) P ( Y= g| X) \\Rightarrow \\hat f (x) = min_g \\sum_{g=1}^G I ( Y=g \\not= \\hat f (x) ) P ( Y= g| X) \\quad \\equiv min_g ( 1- P ( Y= g| X) )$E.g)in 3 groups -&gt; Y = 1, 2, 3P(Y=1 | X) = 0.3 , P(Y=2 | X) = 0.5, P(Y=3 | X) =0.2$\\hat f(x) = 1 \\Rightarrow$ EPE(expected prediction error; miss classification rate) = 0 * 0.3 + 1 * 0.5 + 1 * 0.2 = 0.7$\\hat f(x) = 2 \\Rightarrow$ EPE = 0.5 : min value$\\hat f(x) = 3 \\Rightarrow$ EPE = 0.8            $\\therefore min_g ( 1- P ( Y= g      X) ) \\              \\quad  \\equiv max_g P(Y = g      X)$                  Bayes error rate at X = $x_0$ : $1 - max_g P(Y=g      X=x_0)$              Overall Bayes error rate = $1 - E[ max_g P(Y=g      X= x) ]$      ",
        "url": "/stat_week2"
    }
    
    
    };
</script>
<script src="assets/js/lunr.js"></script>
<script src="assets/js/search.js"></script>
            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Darron's Devlog</a> &copy; 2021</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search Darron's Devlog</h1>
                <p class="subscribe-overlay-description">
				lunr.js를 이용한 posts 검색 </p>
                <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
