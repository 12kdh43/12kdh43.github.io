<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
	
	<!-- On Post front-matter YAML, set "use_math: true" to use LaTex -->
	

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Search Result</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
	
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="" />
    <link rel="shortcut icon" href="http://0.0.0.0:4000/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="http://0.0.0.0:4000/search" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Darron's Devlog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Search Result" />
    <meta property="og:description" content="" />
    <meta property="og:url" content="http://0.0.0.0:4000/search" />
    <meta property="og:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Search Result" />
    <meta name="twitter:description" content="" />
    <meta name="twitter:url" content="http://0.0.0.0:4000/" />
    <meta name="twitter:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Darron's Devlog" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="666" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Darron's Devlog",
        "logo": "http://0.0.0.0:4000/"
    },
    "url": "http://0.0.0.0:4000/search",
    "image": {
        "@type": "ImageObject",
        "url": "http://0.0.0.0:4000/assets/built/images/blog-cover1.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://0.0.0.0:4000/search"
    },
    "description": ""
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Search Result" href="/feed.xml" />


</head>
<body class="page-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="/">Darron's Devlog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-projects" role="menuitem"><a href="/tag/projects/">Projects</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/studies/">Studies</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post page no-image">

            <header class="post-full-header">
                <h1 class="post-full-title">Search Result</h1>
            </header>

            

            <section class="post-full-content">
                <form action="/search" method="get" hidden="hidden">
    <label for="search-box"></label>
    <input type="text" id="search-box" name="query">
</form>

<ul class="mylist" id="search-results"></ul>

<script>
    window.store = {
    
    "fashion-gnn": {
        "title": "Fashion Coordinator Chatbot Dataset with GNN",
        "author": "Darron Kwon",
        "category": "",
        "content": "1. About1.1. Workspace  Windows 11 &gt; docker - ubuntu kernel &gt; CUDA on WSLimport randomrandom.seed(2021)import numpy as npnp.random.seed(2021)import tensorflow as tftf.random.set_seed(2021)tf.device('/device:GPU:0')import networkx as nximport stellargraph as sgimport argparsefrom graph_model import *import osdef str2bool(v):    \"\"\"    function: convert into bool type(True or False)    \"\"\"    if isinstance(v, bool):         return v     if v.lower() in ('yes', 'true', 't', 'y', '1'):         return True     elif v.lower() in ('no', 'false', 'f', 'n', '0'):         return False     else:         raise argparse.ArgumentTypeError('Boolean value expected.')# input optionsparser = argparse.ArgumentParser(description='AI Fashion Coordinator.')parser.add_argument('--mode', type=str,                     default='train',                     help='training or eval or test mode')parser.add_argument('--in_file_trn_dialog', type=str,                     default='./data/ddata.wst.txt',                     help='training dialog DB')parser.add_argument('--in_file_tst_dialog', type=str,                     default='./data/ac_eval_t1.wst.dev',                     help='test dialog DB')parser.add_argument('--in_file_fashion', type=str,                     default='./data/mdata.wst.txt',                     help='fashion item metadata')parser.add_argument('--in_file_img_feats', type=str,                     default='./data/extracted_feat.json',                     help='fashion item image features')parser.add_argument('--model_path', type=str,                     default='./gAIa_model',                     help='path to save/read model')parser.add_argument('--model_file', type=str,                     default=None,                     help='model file name')parser.add_argument('--eval_node', type=str,                     default='[6000,6000,6000,200][2000,2000]',                     help='nodes of evaluation network')parser.add_argument('--subWordEmb_path', type=str,                     default='./sstm_v0p5_deploy/sstm_v4p49_np_final_n36134_d128_r_eng_upper.dat',                     help='path of subword embedding')parser.add_argument('--learning_rate', type=float,                    default=0.0001,                     help='learning rate')parser.add_argument('--max_grad_norm', type=float,                    default=40.0,                     help='clip gradients to this norm')parser.add_argument('--zero_prob', type=float,                    default=0.0,                     help='dropout prob.')parser.add_argument('--corr_thres', type=float,                    default=0.7,                     help='correlation threshold')parser.add_argument('--batch_size', type=int,                    default=100,                       help='batch size for training')parser.add_argument('--epochs', type=int,                    default=10,                       help='epochs to training')parser.add_argument('--save_freq', type=int,                    default=2,                       help='evaluate and save results per # epochs')parser.add_argument('--hops', type=int,                    default=3,                       help='number of hops in the MemN2N')parser.add_argument('--mem_size', type=int,                    default=16,                       help='memory size for the MemN2N')parser.add_argument('--key_size', type=int,                    default=300,                       help='memory size for the MemN2N')parser.add_argument('--permutation_iteration', type=int,                    default=3,                       help='# of permutation iteration')parser.add_argument('--evaluation_iteration', type=int,                    default=10,                       help='# of test iteration')parser.add_argument('--num_augmentation', type=int,                    default=3,                       help='# of data augmentation')parser.add_argument('--use_batch_norm', type=str2bool,                     default=False,                     help='use batch normalization')parser.add_argument('--use_dropout', type=str2bool,                     default=False,                     help='use dropout')parser.add_argument('--use_multimodal', type=str2bool,                    default=True,                     help='use multimodal input')_StoreAction(option_strings=['--use_multimodal'], dest='use_multimodal', nargs=None, const=None, default=True, type=&lt;function str2bool at 0x7f85a894e378&gt;, choices=None, help='use multimodal input', metavar=None)args = parser.parse_args(args=[])if __name__ == '__predict__':        print('\\n')    print('-'*60)    print('\\t\\tAI Fashion Coordinator')    print('-'*60)    print('\\n')g_model = graph_model(args)&lt;Initialize subword embedding&gt;loading= ./sstm_v0p5_deploy/sstm_v4p49_np_final_n36134_d128_r_eng_upper.dat&lt;Make metadata&gt;loading fashion item metadatavectorizing data&lt;Make input &amp; output data&gt;loading dialog DB# of dialog: 7236 setsvectorizing datamemorizing data&lt;Make input &amp; output data&gt;loading dialog DB# of dialog: 200 setsvectorizing datamemorizing datag_model.train()link_regression: using 'concat' method to combine node embeddings into edge embeddings14/14 [==============================] - 3s 92ms/step - loss: 0.1245 - root_mean_square_error: 0.3521 - mean_absolute_error: 0.2666Untrained model's Test Evaluation:\tloss: 0.1245\troot_mean_square_error: 0.3521\tmean_absolute_error: 0.2666Epoch 1/1033/33 [==============================] - 8s 222ms/step - loss: 0.0947 - root_mean_square_error: 0.2792 - mean_absolute_error: 0.2281 - val_loss: 0.0451 - val_root_mean_square_error: 0.2123 - val_mean_absolute_error: 0.1991Epoch 2/1033/33 [==============================] - 7s 217ms/step - loss: 0.0441 - root_mean_square_error: 0.2093 - mean_absolute_error: 0.1761 - val_loss: 0.0427 - val_root_mean_square_error: 0.2062 - val_mean_absolute_error: 0.1558Epoch 3/1033/33 [==============================] - 8s 226ms/step - loss: 0.0426 - root_mean_square_error: 0.2062 - mean_absolute_error: 0.1684 - val_loss: 0.0423 - val_root_mean_square_error: 0.2053 - val_mean_absolute_error: 0.1613Epoch 4/1033/33 [==============================] - 8s 224ms/step - loss: 0.0425 - root_mean_square_error: 0.2053 - mean_absolute_error: 0.1683 - val_loss: 0.0422 - val_root_mean_square_error: 0.2050 - val_mean_absolute_error: 0.1620Epoch 5/1033/33 [==============================] - 8s 234ms/step - loss: 0.0423 - root_mean_square_error: 0.2053 - mean_absolute_error: 0.1679 - val_loss: 0.0425 - val_root_mean_square_error: 0.2058 - val_mean_absolute_error: 0.1805Epoch 6/1033/33 [==============================] - 8s 231ms/step - loss: 0.0423 - root_mean_square_error: 0.2057 - mean_absolute_error: 0.1682 - val_loss: 0.0420 - val_root_mean_square_error: 0.2047 - val_mean_absolute_error: 0.1751Epoch 7/1033/33 [==============================] - 8s 233ms/step - loss: 0.0421 - root_mean_square_error: 0.2046 - mean_absolute_error: 0.1701 - val_loss: 0.0417 - val_root_mean_square_error: 0.2039 - val_mean_absolute_error: 0.1685Epoch 8/1033/33 [==============================] - 8s 229ms/step - loss: 0.0421 - root_mean_square_error: 0.2050 - mean_absolute_error: 0.1662 - val_loss: 0.0415 - val_root_mean_square_error: 0.2033 - val_mean_absolute_error: 0.1686Epoch 9/1033/33 [==============================] - 8s 238ms/step - loss: 0.0415 - root_mean_square_error: 0.2036 - mean_absolute_error: 0.1679 - val_loss: 0.0411 - val_root_mean_square_error: 0.2025 - val_mean_absolute_error: 0.1695Epoch 10/1033/33 [==============================] - 8s 232ms/step - loss: 0.0411 - root_mean_square_error: 0.2023 - mean_absolute_error: 0.1668 - val_loss: 0.0409 - val_root_mean_square_error: 0.2017 - val_mean_absolute_error: 0.153514/14 [==============================] - 2s 121ms/step - loss: 0.0408 - root_mean_square_error: 0.2016 - mean_absolute_error: 0.1535Test Evaluation:\tloss: 0.0408\troot_mean_square_error: 0.2016\tmean_absolute_error: 0.1535Mean Baseline Test set metrics:\troot_mean_square_error =  0.2054230809528016\tmean_absolute_error =  0.1687945779971711Model Test set metrics:\troot_mean_square_error =  0.20211308444911932\tmean_absolute_error =  0.15357324988306925link_regression: using 'concat' method to combine node embeddings into edge embeddings14/14 [==============================] - 3s 126ms/step - loss: 0.0841 - root_mean_square_error: 0.2878 - mean_absolute_error: 0.1999Untrained model's Test Evaluation:\tloss: 0.0841\troot_mean_square_error: 0.2878\tmean_absolute_error: 0.1999Epoch 1/1032/32 [==============================] - 9s 238ms/step - loss: 0.0750 - root_mean_square_error: 0.2574 - mean_absolute_error: 0.2070 - val_loss: 0.0436 - val_root_mean_square_error: 0.2081 - val_mean_absolute_error: 0.1893Epoch 2/1032/32 [==============================] - 7s 225ms/step - loss: 0.0430 - root_mean_square_error: 0.2071 - mean_absolute_error: 0.1727 - val_loss: 0.0418 - val_root_mean_square_error: 0.2035 - val_mean_absolute_error: 0.1678Epoch 3/1032/32 [==============================] - 7s 224ms/step - loss: 0.0426 - root_mean_square_error: 0.2061 - mean_absolute_error: 0.1698 - val_loss: 0.0417 - val_root_mean_square_error: 0.2033 - val_mean_absolute_error: 0.1609Epoch 4/1032/32 [==============================] - 8s 233ms/step - loss: 0.0424 - root_mean_square_error: 0.2059 - mean_absolute_error: 0.1690 - val_loss: 0.0421 - val_root_mean_square_error: 0.2046 - val_mean_absolute_error: 0.1809Epoch 5/1032/32 [==============================] - 8s 233ms/step - loss: 0.0426 - root_mean_square_error: 0.2063 - mean_absolute_error: 0.1698 - val_loss: 0.0413 - val_root_mean_square_error: 0.2023 - val_mean_absolute_error: 0.1663Epoch 6/1032/32 [==============================] - 7s 223ms/step - loss: 0.0423 - root_mean_square_error: 0.2053 - mean_absolute_error: 0.1686 - val_loss: 0.0412 - val_root_mean_square_error: 0.2023 - val_mean_absolute_error: 0.1734Epoch 7/1032/32 [==============================] - 8s 227ms/step - loss: 0.0419 - root_mean_square_error: 0.2042 - mean_absolute_error: 0.1682 - val_loss: 0.0412 - val_root_mean_square_error: 0.2022 - val_mean_absolute_error: 0.1757Epoch 8/1032/32 [==============================] - 7s 224ms/step - loss: 0.0416 - root_mean_square_error: 0.2036 - mean_absolute_error: 0.1680 - val_loss: 0.0405 - val_root_mean_square_error: 0.2003 - val_mean_absolute_error: 0.1557Epoch 9/1032/32 [==============================] - 7s 225ms/step - loss: 0.0410 - root_mean_square_error: 0.2025 - mean_absolute_error: 0.1644 - val_loss: 0.0398 - val_root_mean_square_error: 0.1988 - val_mean_absolute_error: 0.1653Epoch 10/1032/32 [==============================] - 8s 238ms/step - loss: 0.0396 - root_mean_square_error: 0.1985 - mean_absolute_error: 0.1610 - val_loss: 0.0388 - val_root_mean_square_error: 0.1964 - val_mean_absolute_error: 0.161014/14 [==============================] - 2s 128ms/step - loss: 0.0387 - root_mean_square_error: 0.1962 - mean_absolute_error: 0.1607Test Evaluation:\tloss: 0.0387\troot_mean_square_error: 0.1962\tmean_absolute_error: 0.1607Mean Baseline Test set metrics:\troot_mean_square_error =  0.2037237830436417\tmean_absolute_error =  0.16601351653248084Model Test set metrics:\troot_mean_square_error =  0.19741210353956345\tmean_absolute_error =  0.1613392177040303link_regression: using 'concat' method to combine node embeddings into edge embeddings18/18 [==============================] - 3s 139ms/step - loss: 0.5363 - root_mean_square_error: 0.7332 - mean_absolute_error: 0.6958Untrained model's Test Evaluation:\tloss: 0.5363\troot_mean_square_error: 0.7332\tmean_absolute_error: 0.6958Epoch 1/1041/41 [==============================] - 11s 241ms/step - loss: 0.0752 - root_mean_square_error: 0.2546 - mean_absolute_error: 0.2105 - val_loss: 0.0452 - val_root_mean_square_error: 0.2128 - val_mean_absolute_error: 0.1887Epoch 2/1041/41 [==============================] - 9s 221ms/step - loss: 0.0441 - root_mean_square_error: 0.2101 - mean_absolute_error: 0.1735 - val_loss: 0.0448 - val_root_mean_square_error: 0.2118 - val_mean_absolute_error: 0.1801Epoch 3/1041/41 [==============================] - 10s 233ms/step - loss: 0.0440 - root_mean_square_error: 0.2094 - mean_absolute_error: 0.1756 - val_loss: 0.0447 - val_root_mean_square_error: 0.2117 - val_mean_absolute_error: 0.1763Epoch 4/1041/41 [==============================] - 9s 221ms/step - loss: 0.0438 - root_mean_square_error: 0.2093 - mean_absolute_error: 0.1745 - val_loss: 0.0448 - val_root_mean_square_error: 0.2117 - val_mean_absolute_error: 0.1754Epoch 5/1041/41 [==============================] - 9s 225ms/step - loss: 0.0439 - root_mean_square_error: 0.2096 - mean_absolute_error: 0.1754 - val_loss: 0.0447 - val_root_mean_square_error: 0.2115 - val_mean_absolute_error: 0.1823Epoch 6/1041/41 [==============================] - 10s 231ms/step - loss: 0.0437 - root_mean_square_error: 0.2091 - mean_absolute_error: 0.1748 - val_loss: 0.0446 - val_root_mean_square_error: 0.2114 - val_mean_absolute_error: 0.1794Epoch 7/1041/41 [==============================] - 10s 233ms/step - loss: 0.0437 - root_mean_square_error: 0.2086 - mean_absolute_error: 0.1746 - val_loss: 0.0446 - val_root_mean_square_error: 0.2113 - val_mean_absolute_error: 0.1751Epoch 8/1041/41 [==============================] - 10s 228ms/step - loss: 0.0438 - root_mean_square_error: 0.2086 - mean_absolute_error: 0.1751 - val_loss: 0.0452 - val_root_mean_square_error: 0.2128 - val_mean_absolute_error: 0.1634Epoch 9/1041/41 [==============================] - 10s 226ms/step - loss: 0.0442 - root_mean_square_error: 0.2099 - mean_absolute_error: 0.1727 - val_loss: 0.0445 - val_root_mean_square_error: 0.2112 - val_mean_absolute_error: 0.1838Epoch 10/1041/41 [==============================] - 9s 226ms/step - loss: 0.0436 - root_mean_square_error: 0.2088 - mean_absolute_error: 0.1749 - val_loss: 0.0446 - val_root_mean_square_error: 0.2113 - val_mean_absolute_error: 0.169618/18 [==============================] - 3s 132ms/step - loss: 0.0446 - root_mean_square_error: 0.2114 - mean_absolute_error: 0.1697Test Evaluation:\tloss: 0.0446\troot_mean_square_error: 0.2114\tmean_absolute_error: 0.1697Mean Baseline Test set metrics:\troot_mean_square_error =  0.21115701980168747\tmean_absolute_error =  0.1783491631950995Model Test set metrics:\troot_mean_square_error =  0.21133401478765093\tmean_absolute_error =  0.16970877193269276link_regression: using 'concat' method to combine node embeddings into edge embeddings15/15 [==============================] - 3s 143ms/step - loss: 1.1537 - root_mean_square_error: 1.0739 - mean_absolute_error: 1.0498Untrained model's Test Evaluation:\tloss: 1.1537\troot_mean_square_error: 1.0739\tmean_absolute_error: 1.0498Epoch 1/1035/35 [==============================] - 9s 234ms/step - loss: 0.0790 - root_mean_square_error: 0.2416 - mean_absolute_error: 0.2106 - val_loss: 0.0444 - val_root_mean_square_error: 0.2104 - val_mean_absolute_error: 0.1774Epoch 2/1035/35 [==============================] - 8s 227ms/step - loss: 0.0459 - root_mean_square_error: 0.2138 - mean_absolute_error: 0.1824 - val_loss: 0.0445 - val_root_mean_square_error: 0.2107 - val_mean_absolute_error: 0.1839Epoch 3/1035/35 [==============================] - 8s 231ms/step - loss: 0.0457 - root_mean_square_error: 0.2141 - mean_absolute_error: 0.1821 - val_loss: 0.0449 - val_root_mean_square_error: 0.2117 - val_mean_absolute_error: 0.1913Epoch 4/1035/35 [==============================] - 8s 230ms/step - loss: 0.0456 - root_mean_square_error: 0.2143 - mean_absolute_error: 0.1824 - val_loss: 0.0449 - val_root_mean_square_error: 0.2118 - val_mean_absolute_error: 0.1932Epoch 5/1035/35 [==============================] - 8s 229ms/step - loss: 0.0459 - root_mean_square_error: 0.2133 - mean_absolute_error: 0.1822 - val_loss: 0.0435 - val_root_mean_square_error: 0.2084 - val_mean_absolute_error: 0.1746Epoch 6/1035/35 [==============================] - 8s 232ms/step - loss: 0.0445 - root_mean_square_error: 0.2095 - mean_absolute_error: 0.1774 - val_loss: 0.0427 - val_root_mean_square_error: 0.2064 - val_mean_absolute_error: 0.1650Epoch 7/1035/35 [==============================] - 8s 220ms/step - loss: 0.0429 - root_mean_square_error: 0.2073 - mean_absolute_error: 0.1712 - val_loss: 0.0445 - val_root_mean_square_error: 0.2108 - val_mean_absolute_error: 0.1963Epoch 8/1035/35 [==============================] - 9s 238ms/step - loss: 0.0417 - root_mean_square_error: 0.2053 - mean_absolute_error: 0.1672 - val_loss: 0.0455 - val_root_mean_square_error: 0.2132 - val_mean_absolute_error: 0.2007Epoch 9/1035/35 [==============================] - 8s 234ms/step - loss: 0.0410 - root_mean_square_error: 0.2026 - mean_absolute_error: 0.1665 - val_loss: 0.0414 - val_root_mean_square_error: 0.2033 - val_mean_absolute_error: 0.1802Epoch 10/1035/35 [==============================] - 8s 229ms/step - loss: 0.0389 - root_mean_square_error: 0.1973 - mean_absolute_error: 0.1607 - val_loss: 0.0410 - val_root_mean_square_error: 0.2023 - val_mean_absolute_error: 0.176515/15 [==============================] - 2s 129ms/step - loss: 0.0411 - root_mean_square_error: 0.2025 - mean_absolute_error: 0.1768Test Evaluation:\tloss: 0.0411\troot_mean_square_error: 0.2025\tmean_absolute_error: 0.1768Mean Baseline Test set metrics:\troot_mean_square_error =  0.21047757725548227\tmean_absolute_error =  0.17720323507903052Model Test set metrics:\troot_mean_square_error =  0.20244772001149536\tmean_absolute_error =  0.17655691348749106Done trainingg_model.test()&lt;Evaluate&gt;WARNING:tensorflow:5 out of the last 19 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7f8539bf9d08&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.--------------------------------------------------Prediction Time: 379.64sec# of Test Examples: 200Average Weighted Kendalll Tau Corrleation over iterations: 0.0575Best Weighted Kendalll Tau Corrleation: 0.0914--------------------------------------------------import tensorflow as tftf.keras.backend.clear_session() # not workingfrom numba import cudadevice = cuda.get_current_device()device.reset()",
        "url": "/fashion_GNN"
    }
    ,
    
    "dm-final-exam": {
        "title": "Data Mining - K-POP Fandom Data Analysis with networkX",
        "author": "Darron Kwon",
        "category": "",
        "content": "  1. Introduction  2. Supporting Activities          2.1. Fandom Supporting Ratio (Total Supporting activities in Total activities)      2.2. Case by Gender Type      2.3. Case by Agents (Sum of artists data in a company)        3. Correlation between supporting and supported activities          3.1. by Pearson correlation      3.2. by Spearman rank correlation        4. Hyphothesis test by independent t-test          4.1. Supported activities and gender types      4.2. Supporting activities and gender types        5. Graph Analysis          5.1. Generating graphs with different time periods      5.2. Cumulative Distribution Function(CDF) with G0’s degree      5.3. Top 5 fandoms in G0 sorted by Pagerank and Degree Centrality      5.4. Supporting-supported cases by gender types in G0        6. Comparison between G1 and G2          6.1. Persistence of edges-supporting and gender types      6.2. Graph Role Extraction by RolX algorithm      6.3. Community Detection by Girvan-Newman algorithm      6.4. Page Rank and Degree Centrality in G1, G2        7. Effect of the fandom activities on the monthly chart1. Introduction      In K-POP scene, a fandom plays a critical role in the success of their idol. They even cooperate with other fandoms to achieve their goal. For example, they do “supports”; steam albums or encourage to vote in awards for another idol and get helped in the same way when their idol release an album or be nominated for an award. With these data collected in 2018, we will find out the effect of the fandom activities.    fandom information data: {fandom_id, fandom_name, nov_post, dec_post, nov_support, dec_support, type(gender)}  fandom support activities data: {source, target, nov_support, dec_support}  monthly chart data: {album, artist, rank, title, start, end}  fandoms metadata: {fandom_name, artist, agent, year(debut), #articles}import pandas as pdfrom pandas import DataFrame, Seriesimport matplotlib.pyplot as plt%matplotlib inlineimport numpy as npimport networkx as nx2. Supporting Activities2.1. Fandom Supporting Ratio (Total Supporting activities in Total activities)nodes = pd.read_csv('./dataset/fandom_nodes_2018.csv')nodes['total_support'] = nodes.loc[:,['nov_support','dec_support']].sum(axis=1)nodes['total_activity'] = nodes.loc[:,['nov_post','dec_post','nov_support','dec_support']].sum(axis=1)nodes['support_ratio'] = nodes['total_support'] / nodes['total_activity']nodes['support_ratio'] = nodes['support_ratio'].fillna(0)nodes.loc[:,['fandom_id','total_support','total_activity','support_ratio']].sort_values(by=['support_ratio'],ascending=False).head(10)                  fandom_id      total_support      total_activity      support_ratio                  213      oh_soul      14      28      0.500000              283      tbz1206      1      2      0.500000              196      myname      1      2      0.500000              3      2PM      1      2      0.500000              98      imcenter      1      2      0.500000              259      shinjihoon      921      1848      0.498377              180      livematilda0317      1458      2932      0.497271              315      xtracam      2527      5083      0.497147              303      we100      2848      5759      0.494530              201      namyujin      516      1048      0.492366        5 fandoms with very few activities seems to be not quite active in the period.nodes.describe()                  nov_post      dec_post      nov_support      dec_support      type      total_support      total_activity      support_ratio                  count      330.000000      330.000000      330.000000      330.000000      330.000000      330.000000      330.000000      330.000000              mean      5854.921212      5249.675758      1455.627273      859.090909      0.493939      2314.718182      13419.315152      0.226563              std      17573.678298      16329.999791      2329.251725      1166.459438      0.500723      3310.463410      35436.387630      0.174810              min      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000              25%      250.000000      127.500000      19.750000      7.250000      0.000000      53.750000      656.250000      0.043461              50%      1739.000000      1472.000000      535.500000      403.500000      0.000000      984.500000      4776.000000      0.221352              75%      5047.000000      4258.000000      1736.750000      1187.500000      1.000000      3077.750000      13687.000000      0.397054              max      261517.000000      249246.000000      12462.000000      6989.000000      1.000000      16917.000000      525546.000000      0.500000      2.2. Case by Gender Typeedges = pd.read_csv('./dataset/fandom_edges_2018.csv')edges['total_support'] = edges.loc[:,['nov_support','dec_support']].sum(axis=1)df_supported = edges.groupby(['target']).sum()df_supported = df_supported.reset_index()df_supported.columns = ['fandom_id','nov_supported','dec_supported','total_supported']df = pd.merge(nodes,df_supported,on='fandom_id',how='outer')df = df.fillna(0)# Supporting Top 10 fandoms for both gendersdf.loc[:,['fandom_id','total_support']].sort_values(by=['total_support'],ascending=False).head(10)                  fandom_id      total_support                  308      winner      16917              301      wannaonego      16036              31      btob      15739              181      lovelyz      15328              209      nuest      14895              292      twice      14783              183      mamamoo      14472              32      bts      14006              243      roykim      13149              81      gx9      11954      # Supporting Top 10 fandoms for boy groupsdf.loc[df['type']==1,['fandom_id','total_support']].sort_values(by=['total_support'],ascending=False).head(10)                  fandom_id      total_support                  308      winner      16917              301      wannaonego      16036              31      btob      15739              209      nuest      14895              32      bts      14006              243      roykim      13149              298      vikon      10589              88      highlight      10575              124      jsh      9673              288      tj3579      9314      # Supporting Top 10 fandoms for girl groupsdf.loc[df['type']==0,['fandom_id','total_support']].sort_values(by=['total_support'],ascending=False).head(10)                  fandom_id      total_support                  181      lovelyz      15328              292      twice      14783              183      mamamoo      14472              81      gx9      11954              67      gf      11564              234      real__izo      11104              26      blackpink      10678              214      ohmygirl      9091              236      redvelvetreveluv      8155              64      fromis      8003      # Supported Top 10 fandoms for both gendersdf.loc[:,['fandom_id','total_supported']].sort_values(by=['total_supported'],ascending=False).head(10)                  fandom_id      total_supported                  301      wannaonego      33609.0              181      lovelyz      29083.0              234      real__izo      25459.0              32      bts      24865.0              308      winner      21896.0              209      nuest      19312.0              183      mamamoo      18795.0              67      gf      14899.0              124      jsh      14437.0              243      roykim      14267.0      # Supported Top 10 fandoms for boyt groupsdf.loc[df['type']==1,['fandom_id','total_supported']].sort_values(by=['total_supported'],ascending=False).head(10)                  fandom_id      total_supported                  301      wannaonego      33609.0              32      bts      24865.0              308      winner      21896.0              209      nuest      19312.0              124      jsh      14437.0              243      roykim      14267.0              298      vikon      14245.0              137      kim      13884.0              4      6kies      13565.0              31      btob      11493.0      # Supported Top 10 fandoms for girl groupsdf.loc[df['type']==0,['fandom_id','total_supported']].sort_values(by=['total_supported'],ascending=False).head(10)                  fandom_id      total_supported                  181      lovelyz      29083.0              234      real__izo      25459.0              183      mamamoo      18795.0              67      gf      14899.0              64      fromis      13957.0              292      twice      12942.0              214      ohmygirl      12335.0              26      blackpink      9632.0              262      shitaomiu      8563.0              125      jungchaeyeon      6820.0      2.3. Case by Agents (Sum of artists data in a company)meta = pd.read_json('./dataset/fandom_meta_2018.json')df_meta = meta.copy()df_meta.rename(columns={'fandom_name':'fandom_id'},inplace=True)df_metadf = pd.merge(df,df_meta,on='fandom_id')# Supporting Top 10 Agentsdf.groupby(['agent'])[['total_support','total_supported']].sum().sort_values(by='total_support',ascending=False).head(10)                  total_support      total_supported              agent                              YG Entertainment      67790      81735.0              JYP Entertainment      54025      41464.0              Off The Record      47300      75508.0              Swing Entertainment      41180      73716.0              SM Entertainment      38862      23736.0              PLEDIS Entertainment      38839      43115.0              Cube Entertainment      30991      20990.0              Woollim Entertainment      22780      35219.0              Jellyfish Entertainment      17606      7583.0              Fantagio Music      15853      8202.0      # Supported Top 10 Agentsdf.groupby(['agent'])[['total_support','total_supported']].sum().sort_values(by='total_supported',ascending=False).head(10)                  total_support      total_supported              agent                              YG Entertainment      67790      81735.0              Off The Record      47300      75508.0              Swing Entertainment      41180      73716.0              PLEDIS Entertainment      38839      43115.0              JYP Entertainment      54025      41464.0              Woollim Entertainment      22780      35219.0              Big Hit Entertainment      14006      24865.0              SM Entertainment      38862      23736.0              Cube Entertainment      30991      20990.0              AKS      8554      19646.0      3. Correlation between supporting and supported activities3.1. by Pearson correlationfrom scipy import statsprint(stats.pearsonr(df['total_activity'],df['total_support']))print(stats.pearsonr(df['total_activity'],df['total_supported']))(0.538005581735917, 3.725917667518625e-26)(0.5114446175904475, 2.1548883750451727e-23)3.2. by Spearman rank correlationprint(stats.spearmanr(df['total_activity'],df['total_support']))print(stats.spearmanr(df['total_activity'],df['total_supported']))SpearmanrResult(correlation=0.8461473515842707, pvalue=1.1711298194140583e-91)SpearmanrResult(correlation=0.8244727920052708, pvalue=4.2257040130501645e-83)  Spearman rank method shows better correlation and confidence level in average.4. Hyphothesis test by independent t-test4.1. Supported activities and gender types  H_0: There is a difference in supported activities between girl and boy groups.  H_1: There is no difference in supported activities between girl and boy groups.boy = df.loc[df['type']==1,['fandom_id','total_supported','total_support']]girl = df.loc[df['type']==0,['fandom_id','total_supported','total_support']]len(boy),len(girl)(163, 167)# t-teststats.ttest_ind(boy['total_supported'], girl['total_supported'])Ttest_indResult(statistic=1.494285307111671, pvalue=0.1360624474957505)4.2. Supporting activities and gender types  H_0: There is a difference in supporting activities between girl and boy groups.  H_1: There is no difference in supporting activities between girl and boy groups.# t-teststats.ttest_ind(boy['total_support'], girl['total_support'])Ttest_indResult(statistic=1.33823756824641, pvalue=0.1817459557355244)  both H_0’s would not be rejected5. Graph Analysis5.1. Generating graphs with different time periods  G0; graph with data in entire periodG1; grpah with data in novemberG2; grpah with data in decembertotal_edges = edges[['source','target','total_support']].sort_values(by='total_support',ascending=False)total_edges = total_edges.iloc[:int(len(total_edges)*0.01),]G0 = nx.DiGraph(total_edges.loc[:,('source','target')].values.tolist())nx.set_edge_attributes(G0, total_edges.set_index(['source', 'target'])['total_support'], 'weight')nx.set_node_attributes(G0,df.set_index(['fandom_id'])['agent'],'agent')nx.set_node_attributes(G0,df.set_index(['fandom_id'])['year(debut)'],'debut_year')nx.set_node_attributes(G0,df.set_index(['fandom_id'])['type'],'gender_type')nov_edges = edges[['source','target','nov_support']].sort_values(by='nov_support',ascending=False)nov_edges = nov_edges.iloc[:int(len(nov_edges)*0.01),]G1 = nx.DiGraph(nov_edges.loc[:,('source','target')].values.tolist())nx.set_edge_attributes(G1, nov_edges.set_index(['source', 'target'])['nov_support'], 'weight')nx.set_node_attributes(G1,df.set_index(['fandom_id'])['agent'],'agent')nx.set_node_attributes(G1,df.set_index(['fandom_id'])['year(debut)'],'debut_year')nx.set_node_attributes(G1,df.set_index(['fandom_id'])['type'],'gender_type')dec_edges = edges[['source','target','dec_support']].sort_values(by='dec_support',ascending=False)dec_edges = dec_edges.iloc[:int(len(dec_edges)*0.01),]G2 = nx.DiGraph(dec_edges.loc[:,('source','target')].values.tolist())nx.set_edge_attributes(G2, dec_edges.set_index(['source', 'target'])['dec_support'], 'weight')nx.set_node_attributes(G2,df.set_index(['fandom_id'])['agent'],'agent')nx.set_node_attributes(G2,df.set_index(['fandom_id'])['year(debut)'],'debut_year')nx.set_node_attributes(G2,df.set_index(['fandom_id'])['type'],'gender_type')print(len(G0.nodes),len(G0.edges)) print(len(G1.nodes),len(G1.edges)) print(len(G2.nodes),len(G2.edges)) 59 23662 23690 2365.2. Cumulative Distribution Function(CDF) with G0’s degreeG0_degree = dict(nx.degree(G0))h = plt.hist(G0_degree.values())cdf = stats.norm().cdf(sorted(G0_degree.values()))plt.plot(sorted(G0_degree.values()), cdf)plt.show()5.3. Top 5 fandoms in G0 sorted by Pagerank and Degree Centrality  in both results, top fandoms shows very high scores in features - supporting, supported, total activites, while the average of supporting/supported activities for all data is  2,314 and the average of total activities for all data is about 13,419.def viewer(_dict_data, col_name):    result = pd.DataFrame().from_dict(_dict_data, orient='index', columns=[col_name])    return df.set_index('fandom_id').join(result, how='right').sort_values(col_name, ascending=False)pagerank = nx.pagerank(G0)pagerank = viewer(pagerank, 'pagerank')pagerank.head(5)[['type','total_support','total_activity','total_supported','agent','year(debut)','pagerank']]                  type      total_support      total_activity      total_supported      agent      year(debut)      pagerank                  wannaonego      1      16036      77608      33609.0      Swing Entertainment      2017      0.131164              real__izo      0      11104      90757      25459.0      Off The Record      2018      0.081406              lovelyz      0      15328      171752      29083.0      Woollim Entertainment      2014      0.076592              jsh      1      9673      20872      14437.0      Antenna      2016      0.065368              nuest      1      14895      56741      19312.0      PLEDIS Entertainment      2012      0.064216      degree_centrality = nx.degree_centrality(G0)degree_centrality = viewer(degree_centrality, 'degree_centrality')degree_centrality.head(5)[['type','total_support','total_activity','total_supported','agent','year(debut)','degree_centrality']]                  type      total_support      total_activity      total_supported      agent      year(debut)      degree_centrality                  wannaonego      1      16036      77608      33609.0      Swing Entertainment      2017      0.827586              lovelyz      0      15328      171752      29083.0      Woollim Entertainment      2014      0.482759              winner      1      16917      41928      21896.0      YG Entertainment      2014      0.465517              nuest      1      14895      56741      19312.0      PLEDIS Entertainment      2012      0.448276              bts      1      14006      125849      24865.0      Big Hit Entertainment      2013      0.431034      df[['total_support','total_activity','total_supported']].describe()                  total_support      total_activity      total_supported                  count      330.000000      330.000000      330.000000              mean      2314.718182      13419.315152      2314.718182              std      3310.463410      35436.387630      4464.948830              min      0.000000      0.000000      0.000000              25%      53.750000      656.250000      2.000000              50%      984.500000      4776.000000      511.500000              75%      3077.750000      13687.000000      2923.500000              max      16917.000000      525546.000000      33609.000000      5.4. Supporting-supported cases by gender types in G0  Generally, boy group fandoms are more supporting.G0_support = pd.DataFrame().from_dict({e for e in G0.edges})G0_support.columns = ['source','target']tmp = pd.DataFrame().from_dict({n for n in G0.nodes.data('gender_type')})tmp.columns = ['source','source_gender']G0_support = pd.merge(G0_support,tmp,how='left')tmp.columns = ['target','target_gender']G0_support = pd.merge(G0_support,tmp,how='left')G0_support                  source      target      source_gender      target_gender                  0      winner      day6      1      1              1      btob      mamamoo      1      0              2      mmld      ohmygirl      0      0              3      nuest      got7vlive      1      1              4      nuest      ch_freemonth      1      0              ...      ...      ...      ...      ...              231      sanarang      twice      0      0              232      6kies      wannaonego      1      1              233      redvelvetreveluv      wannaonego      0      1              234      mamamoo      twice      0      0              235      gx9      vikon      0      1      236 rows × 4 columns# boy -&gt; boyprint(len(G0_support[(G0_support.source_gender==1) &amp; (G0_support.target_gender==1)]) / len(G0_support))# boy -&gt; girlprint(len(G0_support[(G0_support.source_gender==1) &amp; (G0_support.target_gender==0)]) / len(G0_support))# girl -&gt; boyprint(len(G0_support[(G0_support.source_gender==0) &amp; (G0_support.target_gender==1)]) / len(G0_support))# girl -&gt; girlprint(len(G0_support[(G0_support.source_gender==0) &amp; (G0_support.target_gender==0)]) / len(G0_support))0.32203389830508470.26271186440677970.25423728813559320.161016949152542366. Comparison between G1 and G26.1. Persistence of edges-supporting and gender typesG1_support = set(e for e in G1.edges)G2_support = set(e for e in G2.edges)G1_only = pd.DataFrame.from_dict(G1_support - G2_support)G1_only.columns = ['source','target']tmp = pd.DataFrame().from_dict({n for n in G1.nodes.data('gender_type')})tmp.columns = ['source','source_gender']G1_only = pd.merge(G1_only,tmp,how='left')tmp.columns = ['target','target_gender']G1_only = pd.merge(G1_only,tmp,how='left')# Supporting in november onlyG1_only                  source      target      source_gender      target_gender                  0      winner      day6      1      1              1      mamamoo      wannaonego      0      1              2      btob      mamamoo      1      0              3      mmld      ohmygirl      0      0              4      nuest      got7vlive      1      1              ...      ...      ...      ...      ...              139      anyujin      wannaonego      0      1              140      buzz      mamamoo      1      0              141      redvelvetreveluv      wannaonego      0      1              142      idlesong      lovelyz      0      0              143      gx9      vikon      0      1      144 rows × 4 columnsG2_only = pd.DataFrame.from_dict(G2_support-G1_support)G2_only.columns = ['source','target']tmp = pd.DataFrame().from_dict({n for n in G2.nodes.data('gender_type')})tmp.columns = ['source','source_gender']G2_only = pd.merge(G2_only,tmp,how='left')tmp.columns = ['target','target_gender']G2_only = pd.merge(G2_only,tmp,how='left')# Supporting in december onlyG2_only                  source      target      source_gender      target_gender                  0      wheesung      bts      1      1              1      mkyunghoon      lovelyz      1      0              2      bts      nuest      1      1              3      god      bts      1      1              4      onairpril      lovelyz      0      0              ...      ...      ...      ...      ...              139      jjy      vikon      1      1              140      sanarang      twice      0      0              141      onf      lovelyz      1      0              142      got7vlive      fromis      1      0              143      idlesong      blackpink      0      0      144 rows × 4 columnsG1_n_G2 = pd.DataFrame.from_dict(G1_support &amp; G2_support)G1_n_G2.columns = ['source','target']tmp = pd.DataFrame().from_dict({n for n in G1.nodes.data('gender_type')})tmp.columns = ['source','source_gender']G1_n_G2 = pd.merge(G1_n_G2,tmp,how='left')tmp.columns = ['target','target_gender']G1_n_G2 = pd.merge(G1_n_G2,tmp,how='left')# Supporting in both monthsG1_n_G2                  source      target      source_gender      target_gender                  0      twice      roykim      0      1              1      guckkasten      wannaonego      1      1              2      bigbang      wannaonego      1      1              3      btob      bts      1      1              4      ohmygirl      fromis      0      0              ...      ...      ...      ...      ...              87      real__izo      wannaonego      0      1              88      tj3579      jsh      1      1              89      nuest      jsh      1      1              90      6kies      wannaonego      1      1              91      mamamoo      twice      0      0      92 rows × 4 columns# boy -&gt; boyprint(len(G1_n_G2[(G1_n_G2.source_gender==1) &amp; (G1_n_G2.target_gender==1)]) / len(G1_n_G2))# boy -&gt; girlprint(len(G1_n_G2[(G1_n_G2.source_gender==1) &amp; (G1_n_G2.target_gender==0)]) / len(G1_n_G2))# girl -&gt; boyprint(len(G1_n_G2[(G1_n_G2.source_gender==0) &amp; (G1_n_G2.target_gender==1)]) / len(G1_n_G2))# girl -&gt; girlprint(len(G1_n_G2[(G1_n_G2.source_gender==0) &amp; (G1_n_G2.target_gender==0)]) / len(G1_n_G2))0.402173913043478270.217391304347826080.250.130434782608695656.2. Graph Role Extraction by RolX algorithmfrom pprint import pprintimport seaborn as snsfrom graphrole import RecursiveFeatureExtractor, RoleExtractorfeature_extractor = RecursiveFeatureExtractor(G1)features = feature_extractor.extract_features()role_extractor = RoleExtractor(n_roles=None)role_extractor.extract_role_factors(features)G1_roles = role_extractor.rolesG1_role_groups = {}for n,r in G1_roles.items():    if r in G1_role_groups:        G1_role_groups[r].append(n)    else:        G1_role_groups[r] = [n]for r,g in G1_role_groups.items():    print(r,g)role_2 ['6kies', 'ahnhyungsub', 'anyujin', 'apink', 'boa', 'dmlwlsska', 'girlsgeneration_new', 'girlsofthemonth', 'haonkim', 'highfiveofteenager', 'idlesong', 'jjy', 'jungsewoon', 'jungyeon', 'kgkg', 'kim', 'kimsejeong', 'mkyunghoon', 'mmld', 'nth0510', 'onairpril', 'paka', 'pjb', 'sakura0319', 'samkim', 'shinhwa', 'taeyeon_new1', 'unitb', 'wheesung', 'yuseonho']role_4 ['bigbang', 'blackpink', 'ch_freemonth', 'chungha', 'fromis', 'guckkasten', 'iu_tv', 'redvelvetreveluv', 'tj3579', 'vikon']role_1 ['btob', 'bts', 'gx9', 'jsh', 'mamamoo', 'nuest', 'real__izo', 'roykim', 'straykids', 'wannaonego', 'winner', 'zico']role_3 ['buzz', 'got7vlive', 'kimdonghan', 'madewg', 'twice']role_0 ['day6', 'gf', 'lovelyz', 'miyazakimiho', 'ohmygirl']feature_extractor = RecursiveFeatureExtractor(G2)features = feature_extractor.extract_features()role_extractor = RoleExtractor(n_roles=5) # set 'n_roles' same as G1`srole_extractor.extract_role_factors(features)G2_roles = role_extractor.rolesG2_role_groups = {}for n,r in G2_roles.items():    if r in G2_role_groups:        G2_role_groups[r].append(n)    else:        G2_role_groups[r] = [n]for r,g in G2_role_groups.items():    print(r,g)role_2 ['6kies', 'ahnhyungsub', 'berrygood', 'blackpink', 'buzz', 'dickpunks', 'fromis', 'highfiveofteenager', 'jjy', 'jungchaeyeon', 'kgkg', 'kimsejeong', 'lovelyz', 'mmld', 'pjb', 'real__izo', 'sanarang', 'wheesung', 'wjsnvlive', 'woojinyoung', 'zico']role_4 ['astro', 'b1a4', 'bigbang', 'doakim', 'doitamazing7', 'dongbang_new', 'fortediquattro', 'girllaboum', 'girlsgeneration_new', 'god', 'godgayoung', 'goldenchild', 'guckkasten', 'gx9', 'highlight', 'hotshot', 'in2it', 'infinite', 'ioi', 'iu_tv', 'jsh', 'kanghyewon', 'kim', 'kimchaewon', 'kimdonghan', 'ksy', 'ladiescode', 'mamamoo', 'mino0330', 'mkyunghoon', 'nojisun', 'nth0510', 'nuest', 'ohmygirl', 'onairpril', 'onf', 'paka', 'pentagon', 'pledis', 'pushkang', 'redvelvetreveluv', 'sonamoo', 'taewookim', 'tj3579', 'vikon', 'wekimeki']role_1 ['btob', 'bts', 'day6', 'gf', 'got7vlive', 'idlesong', 'madewg', 'roykim', 'straykids', 'twice', 'wannaonego', 'winner']role_0 ['dmlwlsska', 'eunjiwon', 'jdh', 'jeonsomi', 'jungsewoon', 'kdani', 'leechaeyeon', 'miyazakimiho', 'yabukinako']role_3 ['shitaomiu', 'take_miyu']6.3. Community Detection by Girvan-Newman algorithmfrom networkx.algorithms import communitycomp = community.girvan_newman(G1.to_undirected())communities = tuple(sorted(c) for c in next(comp))print([len(c) for c in communities])for c in communities:    print(c)[39, 23]['6kies', 'apink', 'bigbang', 'blackpink', 'btob', 'bts', 'ch_freemonth', 'chungha', 'day6', 'fromis', 'gf', 'girlsgeneration_new', 'girlsofthemonth', 'got7vlive', 'guckkasten', 'gx9', 'haonkim', 'idlesong', 'iu_tv', 'jsh', 'jungyeon', 'kimdonghan', 'lovelyz', 'madewg', 'mamamoo', 'mkyunghoon', 'mmld', 'nuest', 'ohmygirl', 'pjb', 'real__izo', 'redvelvetreveluv', 'roykim', 'straykids', 'tj3579', 'twice', 'vikon', 'winner', 'zico']['ahnhyungsub', 'anyujin', 'boa', 'buzz', 'dmlwlsska', 'highfiveofteenager', 'jjy', 'jungsewoon', 'kgkg', 'kim', 'kimsejeong', 'miyazakimiho', 'nth0510', 'onairpril', 'paka', 'sakura0319', 'samkim', 'shinhwa', 'taeyeon_new1', 'unitb', 'wannaonego', 'wheesung', 'yuseonho']comp = community.girvan_newman(G2.to_undirected())communities = tuple(sorted(c) for c in next(comp))print([len(c) for c in communities])for c in communities:    print(c)[84, 6]['6kies', 'ahnhyungsub', 'astro', 'b1a4', 'berrygood', 'bigbang', 'blackpink', 'btob', 'bts', 'buzz', 'day6', 'dickpunks', 'doakim', 'doitamazing7', 'dongbang_new', 'eunjiwon', 'fortediquattro', 'fromis', 'gf', 'girllaboum', 'girlsgeneration_new', 'god', 'godgayoung', 'goldenchild', 'got7vlive', 'guckkasten', 'gx9', 'highfiveofteenager', 'highlight', 'hotshot', 'in2it', 'infinite', 'ioi', 'iu_tv', 'jdh', 'jjy', 'jsh', 'jungchaeyeon', 'jungsewoon', 'kanghyewon', 'kdani', 'kgkg', 'kim', 'kimchaewon', 'kimdonghan', 'kimsejeong', 'ksy', 'ladiescode', 'leechaeyeon', 'lovelyz', 'madewg', 'mamamoo', 'mino0330', 'mkyunghoon', 'mmld', 'nojisun', 'nth0510', 'nuest', 'ohmygirl', 'onairpril', 'onf', 'paka', 'pentagon', 'pjb', 'pledis', 'pushkang', 'real__izo', 'redvelvetreveluv', 'roykim', 'sanarang', 'sonamoo', 'straykids', 'taewookim', 'tj3579', 'twice', 'vikon', 'wannaonego', 'wekimeki', 'wheesung', 'winner', 'wjsnvlive', 'woojinyoung', 'yabukinako', 'zico']['dmlwlsska', 'idlesong', 'jeonsomi', 'miyazakimiho', 'shitaomiu', 'take_miyu']6.4. Page Rank and Degree Centrality in G1, G2pagerank = nx.pagerank(G1)pagerank = viewer(pagerank, 'pagerank')pagerank.head(10).indexIndex(['wannaonego', 'real__izo', 'jsh', 'nuest', 'winner', 'roykim',       'lovelyz', 'ohmygirl', 'gf', 'twice'],      dtype='object')pagerank = nx.pagerank(G2)pagerank = viewer(pagerank, 'pagerank')pagerank.head(10).indexIndex(['lovelyz', 'bts', 'jsh', 'wannaonego', 'fromis', 'nuest', 'gf', 'twice',       'highfiveofteenager', 'roykim'],      dtype='object')degree_centrality = nx.degree_centrality(G1)degree_centrality = viewer(degree_centrality, 'degree_centrality')degree_centrality.head(10).indexIndex(['wannaonego', 'nuest', 'real__izo', 'winner', 'mamamoo', 'roykim',       'jsh', 'bts', 'lovelyz', 'twice'],      dtype='object')degree_centrality = nx.degree_centrality(G2)degree_centrality = viewer(degree_centrality, 'degree_centrality')degree_centrality.head(10).indexIndex(['lovelyz', 'bts', 'wannaonego', 'winner', 'twice', 'jsh', 'fromis',       'gf', 'nuest', 'btob'],      dtype='object')7. Effect of the fandom activities on the monthly chartmonthly_chart = pd.read_json('./dataset/monthly_chart_2018.json')m11 = monthly_chart[monthly_chart.start==201811]m12 = monthly_chart[monthly_chart.start==201812]df_chart = df[['fandom_id','chart_name','total_activity','nov_supported','dec_supported','total_supported']]in_nov = df_chart['chart_name'].isin(m11['artist'])df_nov = df_chart[in_nov].groupby(['chart_name']).sum()rank_nov = m11[m11['artist'].isin(df_chart['chart_name'])].groupby(['artist'])['rank'].mean()chart_nov = pd.merge(df_nov,rank_nov,left_index=True,right_index=True,how='left')chart_nov.sort_values(by='rank')                  total_activity      nov_supported      dec_supported      total_supported      rank              chart_name                                                바이브      1088      70.0      7.0      77.0      4.000000              임창정      1649      6.0      1.0      7.0      7.000000              벤      304      4.0      0.0      4.0      16.000000              TWICE (트와이스)      606440      8341.0      5549.0      13890.0      17.500000              아이유      130843      2467.0      1407.0      3874.0      19.000000              선미      718      36.0      5.0      41.0      20.000000              IZ*ONE (아이즈원)      309295      37770.0      18416.0      56186.0      22.000000              로이킴      30003      10979.0      3288.0      14267.0      29.500000              iKON      33653      9430.0      5150.0      14580.0      39.500000              비투비      47090      6551.0      4942.0      11493.0      41.250000              BLACKPINK      104426      7314.0      2318.0      9632.0      45.500000              Red Velvet (레드벨벳)      194521      6117.0      3132.0      9249.0      50.000000              EXO      28649      5.0      1.0      6.0      55.272727              방탄소년단      125849      14519.0      10346.0      24865.0      63.300000              Apink (에이핑크)      89709      507.0      492.0      999.0      71.000000              하이라이트 (Highlight)      29392      4317.0      3357.0      7674.0      75.000000              Wanna One (워너원)      427571      47720.0      25996.0      73716.0      77.800000              정승환      20872      9704.0      4733.0      14437.0      78.000000              지코 (ZICO)      19315      2272.0      566.0      2838.0      84.000000              (여자)아이들      22860      1500.0      830.0      2330.0      91.000000      df_chart = df[['fandom_id','chart_name','total_activity','nov_supported','dec_supported','total_supported']]in_dec = df_chart['chart_name'].isin(m12['artist'])df_dec = df_chart[in_dec].groupby(['chart_name']).sum()rank_dec = m12[m12['artist'].isin(df_chart['chart_name'])].groupby(['artist'])['rank'].mean()chart_dec = pd.merge(df_dec,rank_dec,left_index=True,right_index=True,how='left')chart_dec.sort_values(by='rank')                  total_activity      nov_supported      dec_supported      total_supported      rank              chart_name                                                바이브      1088      70.0      7.0      77.0      6.000000              벤      304      4.0      0.0      4.0      12.000000              임창정      1649      6.0      1.0      7.0      15.000000              허각      336      0.0      0.0      0.0      17.000000              WINNER      69689      17153.0      11363.0      28516.0      21.000000              TWICE (트와이스)      606440      8341.0      5549.0      13890.0      25.500000              아이유      130843      2467.0      1407.0      3874.0      33.000000              IZ*ONE (아이즈원)      309295      37770.0      18416.0      56186.0      37.000000              비투비      47090      6551.0      4942.0      11493.0      38.666667              BLACKPINK      104426      7314.0      2318.0      9632.0      39.000000              선미      718      36.0      5.0      41.0      40.000000              로이킴      30003      10979.0      3288.0      14267.0      45.000000              EXO      28649      5.0      1.0      6.0      60.000000              Red Velvet (레드벨벳)      194521      6117.0      3132.0      9249.0      66.000000              EXID      28513      19.0      6.0      25.0      66.000000              방탄소년단      125849      14519.0      10346.0      24865.0      67.875000              iKON      33653      9430.0      5150.0      14580.0      71.000000              Wanna One (워너원)      427571      47720.0      25996.0      73716.0      73.000000      print(stats.spearmanr(chart_nov['nov_supported'],chart_nov['rank']))print(stats.spearmanr(chart_dec['dec_supported'],chart_dec['rank']))SpearmanrResult(correlation=0.2721804511278195, pvalue=0.2456688782803774)SpearmanrResult(correlation=0.42480625827858726, pvalue=0.07887457444015573)common = pd.merge(chart_nov[chart_nov.index.isin(chart_dec.index)],chart_dec['rank'],left_index=True,right_index=True,how='left')common.sort_values(by=['rank_x','rank_y'])                  total_activity      nov_supported      dec_supported      total_supported      rank_x      rank_y              chart_name                                                      바이브      1088      70.0      7.0      77.0      4.000000      6.000000              임창정      1649      6.0      1.0      7.0      7.000000      15.000000              벤      304      4.0      0.0      4.0      16.000000      12.000000              TWICE (트와이스)      606440      8341.0      5549.0      13890.0      17.500000      25.500000              아이유      130843      2467.0      1407.0      3874.0      19.000000      33.000000              선미      718      36.0      5.0      41.0      20.000000      40.000000              IZ*ONE (아이즈원)      309295      37770.0      18416.0      56186.0      22.000000      37.000000              로이킴      30003      10979.0      3288.0      14267.0      29.500000      45.000000              iKON      33653      9430.0      5150.0      14580.0      39.500000      71.000000              비투비      47090      6551.0      4942.0      11493.0      41.250000      38.666667              BLACKPINK      104426      7314.0      2318.0      9632.0      45.500000      39.000000              Red Velvet (레드벨벳)      194521      6117.0      3132.0      9249.0      50.000000      66.000000              EXO      28649      5.0      1.0      6.0      55.272727      60.000000              방탄소년단      125849      14519.0      10346.0      24865.0      63.300000      67.875000              Wanna One (워너원)      427571      47720.0      25996.0      73716.0      77.800000      73.000000      print(stats.spearmanr(common['nov_supported'],common['rank_x']))print(stats.spearmanr(common['dec_supported'],common['rank_y']))SpearmanrResult(correlation=0.4892857142857142, pvalue=0.06416038631454055)SpearmanrResult(correlation=0.5004470273579545, pvalue=0.057440003802729525)",
        "url": "/DM_final_exam"
    }
    ,
    
    "dm-final-proj": {
        "title": "Data Mining - Stock Market Network Analysis",
        "author": "Darron Kwon",
        "category": "",
        "content": "  1. Data Load and Preprocessing  2. Data Visualization          2.1. Stock Price Volatility      2.2. Rolling Average of Stock Price Correlation        3. Network Analysis          3.1. Build Graph with Correlation table      3.2. Setting threshold on weights      3.3. Community Detection      3.4. Visualization with Gephi      1. Data Load and Preprocessingimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltfrom matplotlib.collections import LineCollectionfrom sklearn import cluster, covariance, manifoldfrom community import community_louvain as louvainimport matplotlib.cm as cmimport networkx as nximport networkx.algorithms.community as nxcomfrom importlib import reloadimport csvimport osimport re%matplotlib inlinedf_prices = pd.read_csv(\"SP500_prices.csv\", index_col = 0)df_indices = pd.read_csv(\"indices.csv\")df_SP500 = pd.read_csv(\"SP500.csv\",index_col = 0)df_prices.describe()                  open      high      low      close      volume      adjusted                  count      282821.000000      282821.000000      282821.000000      282821.000000      2.828210e+05      282821.000000              mean      139.384227      141.184891      137.569435      139.425703      4.983235e+06      137.510842              std      249.635882      253.080583      246.310869      249.739199      1.180107e+07      249.704273              min      3.220000      3.290000      3.020000      3.120000      0.000000e+00      3.092542              25%      47.780000      48.419998      47.120000      47.790001      1.010000e+06      46.309925              50%      86.430000      87.480003      85.320000      86.420000      2.034800e+06      84.330002              75%      151.500000      153.380005      149.600006      151.570000      4.528900e+06      148.930000              max      4742.610000      4832.800000      4700.000000      4776.410000      4.286171e+08      4776.410000      df_prices.head()                  symbol      date      open      high      low      close      volume      adjusted                  1      AAPL      2019-01-02      38.722500      39.712502      38.557499      39.480000      148158800.0      38.505024              2      AAPL      2019-01-03      35.994999      36.430000      35.500000      35.547501      365248800.0      34.669640              3      AAPL      2019-01-04      36.132500      37.137501      35.950001      37.064999      234428400.0      36.149662              4      AAPL      2019-01-07      37.174999      37.207500      36.474998      36.982498      219111200.0      36.069202              5      AAPL      2019-01-08      37.389999      37.955002      37.130001      37.687500      164101200.0      36.756794      df_prices_adj = df_prices[['symbol','date', 'adjusted']]df_prices_adj.columns = ['symbol','date','price']df_prices_adj.tail()                  symbol      date      price                  282817      NWS      2021-03-24      23.69              282818      NWS      2021-03-25      24.42              282819      NWS      2021-03-26      24.01              282820      NWS      2021-03-29      23.39              282821      NWS      2021-03-30      23.83      df_indices.describe()                  Unnamed: 0      price                  count      16385.000000      16215.000000              mean      8193.000000      588.995521              std      4730.086416      3309.895206              min      1.000000      -36.980000              25%      4097.000000      1.284350              50%      8193.000000      4.227200              75%      12289.000000      107.426300              max      16385.000000      59221.230000      df_indices.head()                  Unnamed: 0      symbol      date      price                  0      1      DPROPANEMBTX      2019-01-02      0.641              1      2      DPROPANEMBTX      2019-01-03      0.630              2      3      DPROPANEMBTX      2019-01-04      0.635              3      4      DPROPANEMBTX      2019-01-07      0.623              4      5      DPROPANEMBTX      2019-01-08      0.628      df_indices = df_indices[[\"symbol\",\"date\",\"price\"]]df_indices.tail()                  symbol      date      price                  16380      THREEFY5      2021-03-24      0.8123              16381      THREEFY5      2021-03-25      0.8126              16382      THREEFY5      2021-03-26      0.8354              16383      THREEFY5      2021-03-29      0.8687              16384      THREEFY5      2021-03-30      0.8818      df_SP500.head()                  symbol      company      identifier      sedol      weight      sector      shares_held      local_currency      exchange                  1      AAPL      Apple Inc.      03783310      2046251      0.059338      Information Technology      161340980      USD      SP500              2      MSFT      Microsoft Corporation      59491810      2588173      0.055094      Information Technology      77110660      USD      SP500              3      AMZN      Amazon.com Inc.      02313510      2000019      0.040733      Consumer Discretionary      4376067      USD      SP500              4      FB      Facebook Inc. Class A      30303M10      B7TL820      0.021718      Communication Services      24592958      USD      SP500              5      GOOGL      Alphabet Inc. Class A      02079K30      BYVY8G0      0.019521      Communication Services      3074670      USD      SP500      df = pd.concat([df_prices_adj,df_indices])df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='ignore')df.set_index(['date','symbol'],inplace=True)df=df.unstack()['price']df.fillna(method='bfill',inplace=True)df            symbol      A      AAL      AAP      AAPL      ABBV      ABC      ABMD      ABT      ACN      ADBE      ...      XEL      XLNX      XOM      XRAY      XYL      YUM      ZBH      ZBRA      ZION      ZTS              date                                                                                                                                                2019-01-02      64.511734      31.96316      156.2589      38.505024      79.101799      71.46416      309.96      67.034943      136.179626      224.570007      ...      45.400452      84.360565      60.557911      37.21114      64.63606      87.819199      100.576180      156.24      38.71991      83.337715              2019-01-03      62.135132      29.58167      161.1371      34.669640      76.495514      70.42746      302.29      63.871284      131.530212      215.699997      ...      45.221561      81.184296      59.628124      37.23077      62.42029      85.610275      98.756989      146.88      38.50573      80.457184              2019-01-04      64.285828      31.53016      157.1396      36.149662      78.959961      71.24338      313.44      65.694260      136.644577      226.190002      ...      45.664082      84.943359      61.826595      38.31106      65.05392      87.838394      102.129860      152.97      39.68837      83.613907              2019-01-07      65.650917      32.42568      159.4450      36.069202      80.112411      71.75211      314.80      66.678070      137.119217      229.259995      ...      45.466366      87.187141      62.148109      38.99852      64.09184      87.742371      102.169182      155.29      39.84668      84.117012              2019-01-08      66.613335      31.90411      158.3368      36.756794      80.484734      72.52003      318.42      65.877518      140.586914      232.679993      ...      45.993618      85.526176      62.599968      38.73336      64.69435      87.569496      99.877983      156.33      40.20985      85.369850              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              2021-03-24      120.656403      21.81000      181.7300      120.089996      103.059998      115.37000      294.21      118.019997      267.549988      451.510010      ...      65.580002      119.959999      56.340000      60.25000      101.11000      107.080002      157.210648      463.81      53.01000      155.429993              2021-03-25      121.714798      22.77000      185.6500      120.589996      103.879997      117.34000      294.14      119.050003      268.609985      450.989990      ...      66.000000      120.029999      56.180000      60.55000      101.93000      107.379997      157.639999      461.26      54.74000      152.880005              2021-03-26      125.449112      22.93000      187.3200      121.209999      105.980003      118.73000      301.40      122.070000      280.769989      469.089996      ...      66.309998      123.139999      57.709999      61.28000      104.76000      108.059998      161.320007      476.96      55.85000      156.149994              2021-03-29      125.229446      22.91000      185.0600      121.389999      106.730003      119.05000      305.77      122.230003      279.540009      469.320007      ...      67.000000      122.230003      57.400002      62.06000      104.27000      109.209999      160.210007      467.07      53.89000      158.389999              2021-03-30      124.650322      24.12000      186.0700      119.900002      106.790001      119.06000      309.88      119.750000      278.549988      465.459991      ...      66.010002      120.300003      56.689999      63.54000      104.88000      109.769997      161.220001      474.83      55.91000      157.039993      565 rows × 531 columnsdf = (df-df.mean())/df.std()df.describe()             symbol      A      AAL      AAP      AAPL      ABBV      ABC      ABMD      ABT      ACN      ADBE      ...      XEL      XLNX      XOM      XRAY      XYL      YUM      ZBH      ZBRA      ZION      ZTS                  count      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      ...      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02      5.650000e+02              mean      -5.533430e-16      -1.156990e-15      -2.339132e-15      2.364284e-15      -3.420666e-15      -1.307902e-15      7.797106e-16      4.426744e-15      8.551665e-16      4.326136e-15      ...      -1.237476e-14      4.099769e-15      7.545587e-16      -1.760637e-16      -7.646194e-15      -1.936701e-15      -2.678683e-15      -1.810941e-15      -2.867323e-15      -3.068539e-15              std      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      ...      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00      1.000000e+00              min      -1.402956e+00      -1.578205e+00      -4.368070e+00      -1.422373e+00      -1.783790e+00      -1.920594e+00      -1.960001e+00      -1.883874e+00      -2.071674e+00      -1.646058e+00      ...      -2.597860e+00      -2.077320e+00      -1.906102e+00      -2.730556e+00      -2.111907e+00      -4.089893e+00      -3.106090e+00      -1.373928e+00      -2.269724e+00      -2.059833e+00              25%      -7.205020e-01      -1.081580e+00      -3.269484e-01      -9.160365e-01      -8.075055e-01      -6.871508e-01      -9.956515e-01      -7.258871e-01      -7.138452e-01      -9.190687e-01      ...      -5.309481e-01      -7.727544e-01      -9.708038e-01      -8.461749e-01      -5.681917e-01      -6.355256e-01      -7.359661e-01      -6.979548e-01      -1.004871e+00      -7.099711e-01              50%      -3.988059e-01      3.186508e-01      2.191719e-01      -2.702986e-01      -7.196453e-02      -2.771314e-01      2.134506e-01      -3.168558e-01      -2.189815e-01      -2.961142e-01      ...      6.798383e-02      -1.999376e-01      3.236947e-01      1.573233e-01      -1.819963e-01      3.273311e-02      1.208611e-01      -2.155373e-01      2.149392e-01      -1.179682e-01              75%      6.010351e-01      8.994444e-01      5.329562e-01      1.102648e+00      7.246279e-01      7.769495e-01      8.128134e-01      8.944433e-01      7.914156e-01      1.047349e+00      ...      6.942542e-01      6.930862e-01      8.373723e-01      8.655397e-01      3.657484e-01      7.911210e-01      7.965623e-01      2.228456e-01      6.929752e-01      1.075490e+00              max      2.370841e+00      1.631285e+00      2.129579e+00      2.053239e+00      2.192993e+00      2.759439e+00      1.856881e+00      2.525851e+00      2.386994e+00      1.970695e+00      ...      2.121844e+00      2.439377e+00      1.533337e+00      2.122636e+00      2.542459e+00      1.801157e+00      1.843039e+00      3.113057e+00      2.442046e+00      1.818189e+00      8 rows × 531 columns2. Data Visualization2.1. Stock Price Volatility%matplotlib inlinefig, ax1 = plt.subplots(figsize=(20, 15))df.iloc[:,:20].plot(ax=ax1, legend=False)plt.tight_layout()plt.show()df_delta = df.copy()for column in df_delta.columns.values.tolist():    df_delta[column] = df_delta[column]- df_delta[column].shift(1)df_delta.iloc[0]=0df_delta            symbol      A      AAL      AAP      AAPL      ABBV      ABC      ABMD      ABT      ACN      ADBE      ...      XEL      XLNX      XOM      XRAY      XYL      YUM      ZBH      ZBRA      ZION      ZTS              date                                                                                                                                                2019-01-02      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      ...      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000              2019-01-03      -0.132308      -0.278940      0.281110      -0.123113      -0.190242      -0.096013      -0.126329      -0.210193      -0.138905      -0.100851      ...      -0.029680      -0.171040      -0.072336      0.002954      -0.197347      -0.215205      -0.106702      -0.114809      -0.028157      -0.119316              2019-01-04      0.119732      0.228224      -0.230359      0.047508      0.179889      0.075565      0.183646      0.121119      0.152796      0.119270      ...      0.073418      0.202423      0.171038      0.162583      0.234564      0.217075      0.197830      0.074699      0.155477      0.130757              2019-01-07      0.075996      0.104891      0.132851      -0.002583      0.084121      0.047115      0.022400      0.065364      0.014180      0.034905      ...      -0.032803      0.120826      0.025013      0.103462      -0.085687      -0.009355      0.002306      0.028457      0.020812      0.020839              2019-01-08      0.053579      -0.061091      -0.063861      0.022071      0.027177      0.071120      0.059623      -0.053189      0.103600      0.038885      ...      0.087476      -0.089442      0.035154      -0.039906      0.053662      -0.016842      -0.134387      0.012757      0.047745      0.051895              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              2021-03-24      -0.063369      -0.053879      0.126777      -0.078644      -0.129928      0.148182      -0.072635      -0.164107      0.045411      -0.098804      ...      0.023227      -0.116853      0.087134      0.063104      0.063236      -0.089631      0.076719      0.014228      -0.074936      -0.025267              2021-03-25      0.058922      0.112443      0.225893      0.016050      0.059855      0.182449      -0.001153      0.068434      0.031668      -0.005913      ...      0.069681      0.003769      -0.012448      0.045150      0.073033      0.029227      0.025183      -0.031278      0.227436      -0.105625              2021-03-26      0.207894      0.018741      0.096235      0.019902      0.153287      0.128733      0.119576      0.200649      0.363291      0.205795      ...      0.051431      0.167471      0.119032      0.109865      0.252053      0.066249      0.215845      0.192574      0.145927      0.135448              2021-03-29      -0.012229      -0.002343      -0.130234      0.005778      0.054745      0.029636      0.071976      0.010631      -0.036747      0.002615      ...      0.114477      -0.049003      -0.024117      0.117390      -0.043642      0.112039      -0.065105      -0.121310      -0.257674      0.092785              2021-03-30      -0.032241      0.141726      0.058202      -0.047828      0.004379      0.000926      0.067694      -0.164771      -0.029578      -0.043888      ...      -0.164249      -0.103929      -0.055237      0.222739      0.054329      0.054558      0.059240      0.095183      0.265562      -0.055920      565 rows × 531 columns%matplotlib inlinefig, ax1 = plt.subplots(figsize=(20, 15))df_delta.plot(ax=ax1, legend=False)plt.tight_layout()plt.show()2.2. Rolling Average of Stock Price Correlationdef calculate_corr(df_stock_returns, returns_window, corr_window_size, corr_method):    stocks_cross_corr_dict = {}    x_days = []    y_mean_corr = []            for i in range(returns_window,len(df_stock_returns),corr_window_size):        dic_key = i        stocks_cross_corr_dict[dic_key]=df_stock_returns.iloc[i:(i+W)].corr(method='pearson')        stocks_cross_corr_dict[dic_key].fillna(0,inplace=True)        x_days.append(dic_key)        y_mean_corr.append(np.mean([abs(j) for j in stocks_cross_corr_dict[dic_key].values.flatten().tolist()]))            return stocks_cross_corr_dict, x_days,y_mean_corr%matplotlib inlinestart = 21end = 126step = 21;plt.figure(figsize=(20, 10))for t in range(start, end, step):    x_days = []    y_mean_corr = []    W = t    _, x_days, y_mean_corr = calculate_corr(df,1,W, 'pearson')    plt.plot(x_days, y_mean_corr)    plt.xlabel('Days')    plt.ylabel('Mean Correlation')    l = list(range(start, end, step))    plt.legend(l, loc='upper left')     plt.show()3. Network Analysis3.1. Build Graph with Correlation table# craetes a graph from correlation matrixcor_matrix = df.corr()cor_matrix            symbol      A      AAL      AAP      AAPL      ABBV      ABC      ABMD      ABT      ACN      ADBE      ...      XEL      XLNX      XOM      XRAY      XYL      YUM      ZBH      ZBRA      ZION      ZTS              symbol                                                                                                                                                A      1.000000      -0.512211      0.340607      0.936847      0.901146      0.866659      0.476208      0.932064      0.916575      0.876950      ...      0.581891      0.661159      -0.481706      0.276888      0.819616      0.232105      0.731162      0.947770      0.196208      0.830023              AAL      -0.512211      1.000000      0.452672      -0.709976      -0.567905      -0.636546      0.067317      -0.620271      -0.515571      -0.761706      ...      -0.685099      0.040970      0.950026      0.414638      -0.104279      0.401344      -0.116920      -0.402034      0.675397      -0.720983              AAP      0.340607      0.452672      1.000000      0.132583      0.218725      0.129889      0.598790      0.206132      0.304547      0.062110      ...      -0.102458      0.592191      0.471304      0.536131      0.517752      0.604171      0.464449      0.358634      0.691259      0.045798              AAPL      0.936847      -0.709976      0.132583      1.000000      0.883271      0.900174      0.311099      0.946722      0.931212      0.971310      ...      0.752885      0.464395      -0.694573      0.138280      0.700491      0.103699      0.668931      0.859869      -0.067884      0.937015              ABBV      0.901146      -0.567905      0.218725      0.883271      1.000000      0.842135      0.379425      0.820668      0.826314      0.834858      ...      0.525801      0.430849      -0.525965      0.167883      0.625416      0.017315      0.617159      0.868875      0.131018      0.756742              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              YUM      0.232105      0.401344      0.604171      0.103699      0.017315      0.148166      0.135065      0.246568      0.391225      0.042001      ...      0.150926      0.517301      0.446745      0.791644      0.591224      1.000000      0.622313      0.311883      0.601129      0.211623              ZBH      0.731162      -0.116920      0.464449      0.668931      0.617159      0.647128      0.138407      0.709774      0.815626      0.581120      ...      0.583386      0.473123      -0.072924      0.677935      0.793259      0.622313      1.000000      0.754226      0.495103      0.715109              ZBRA      0.947770      -0.402034      0.358634      0.859869      0.868875      0.842147      0.398625      0.902315      0.895845      0.774380      ...      0.477856      0.635769      -0.332037      0.410140      0.824975      0.311883      0.754226      1.000000      0.353050      0.754373              ZION      0.196208      0.675397      0.691259      -0.067884      0.131018      0.003435      0.274263      0.036986      0.155021      -0.207014      ...      -0.314988      0.431878      0.711608      0.768845      0.486766      0.601129      0.495103      0.353050      1.000000      -0.148741              ZTS      0.830023      -0.720983      0.045798      0.937015      0.756742      0.869275      0.075617      0.902965      0.917018      0.930572      ...      0.901222      0.331572      -0.696158      0.206471      0.652798      0.211623      0.715109      0.754373      -0.148741      1.000000      531 rows × 531 columnsmat_pos = cor_matrix[cor_matrix&gt;=0]mat_pos = mat_pos.fillna(0)symbols = cor_matrix.index.valuesmat_pos = np.asmatrix(mat_pos)G_pos = nx.from_numpy_matrix(mat_pos)G_pos = nx.relabel_nodes(G_pos,lambda x: symbols[x])G_pos.remove_edges_from(nx.selfloop_edges(G_pos))mat_neg = cor_matrix[cor_matrix&lt;0]mat_neg = mat_neg.fillna(0)mat_neg = abs(mat_neg)symbols = cor_matrix.index.valuesmat_neg = np.asmatrix(mat_neg)G_neg = nx.from_numpy_matrix(mat_neg)G_neg = nx.relabel_nodes(G_neg,lambda x: symbols[x])G_neg.remove_edges_from(nx.selfloop_edges(G_neg))list(G_pos.edges(data=True))[:5], list(G_neg.edges(data=True))[:5]([('A', 'AAP', {'weight': 0.3406072572702165}),  ('A', 'AAPL', {'weight': 0.9368465656977406}),  ('A', 'ABBV', {'weight': 0.9011464914155253}),  ('A', 'ABC', {'weight': 0.8666593549791352}),  ('A', 'ABMD', {'weight': 0.476208419378593})], [('A', 'AAL', {'weight': 0.5122106576033231}),  ('A', 'AEP', {'weight': 0.010066159905771194}),  ('A', 'AFL', {'weight': 0.14527226301899576}),  ('A', 'AIG', {'weight': 0.21700976911380374}),  ('A', 'ALK', {'weight': 0.13546044591421322})])symbol =  df.columnsdf_sector = df_SP500[df_SP500['symbol'].isin(symbol)]df_sector = df_sector[[\"symbol\",\"sector\"]]tmp = pd.DataFrame({\"symbol\":df_indices[\"symbol\"].unique(), \"sector\":\"Macroeconomic Indices\"})df_sector = pd.concat([df_sector,tmp],ignore_index=True)df_sector['sec_idx']= 0for i,sec in enumerate(df_sector[\"sector\"].unique()):    df_sector.loc[df_sector['sector']==sec,'sec_idx'] = i+1df_sector                  symbol      sector      sec_idx                  0      AAPL      Information Technology      1              1      MSFT      Information Technology      1              2      AMZN      Consumer Discretionary      2              3      FB      Communication Services      3              4      GOOGL      Communication Services      3              ...      ...      ...      ...              526      GOLDAMGBD228NLBM      Macroeconomic Indices      12              527      IOER      Macroeconomic Indices      12              528      IORR      Macroeconomic Indices      12              529      THREEFY1      Macroeconomic Indices      12              530      THREEFY5      Macroeconomic Indices      12      531 rows × 3 columnsnx.set_node_attributes(G_pos, df_sector.set_index('symbol')['sec_idx'],'sec_idx')nx.set_node_attributes(G_neg, df_sector.set_index('symbol')['sec_idx'],'sec_idx')3.2. Setting threshold on weightsdef set_threshold(G,threshold):    edges_rm = list(filter(lambda e: abs(e[2]) &lt; threshold, (e for e in G.edges.data('weight'))))        ids_rm = list(e[:2] for e in edges_rm)    H = G.copy()    H.remove_edges_from(ids_rm)    return HH_pos = set_threshold(G_pos,0.5)H_neg = set_threshold(G_neg,0.5)list(H_pos.edges(data=True))[:5], list(H_neg.edges(data=True))[:5]([('A', 'AAPL', {'weight': 0.9368465656977406}),  ('A', 'ABBV', {'weight': 0.9011464914155253}),  ('A', 'ABC', {'weight': 0.8666593549791352}),  ('A', 'ABT', {'weight': 0.932064482315709}),  ('A', 'ACN', {'weight': 0.916574717968791})], [('A', 'AAL', {'weight': 0.5122106576033231}),  ('A', 'BA', {'weight': 0.5064885797862092}),  ('A', 'BXP', {'weight': 0.522527131570438}),  ('A', 'CCL', {'weight': 0.5559250497113872}),  ('A', 'DEXCAUS', {'weight': 0.6382900166453468})])3.3. Community Detection  with Louvain Algorithm# grid search# for positive weightsnp.random.seed(2021)t=0for cor_thresold in np.linspace(0.8,0.85,20):    H_pos = set_threshold(G_pos,cor_thresold)    partition = louvain.best_partition(H_pos)    modularity = louvain.modularity(partition, H_pos)    values = [partition.get(node) for node in H_pos.nodes()]    communities = []    tmp = list(partition.items())    for i in range(len(set(values))):        communities.append([n for n,c in tmp if c==i])    sum_comm_nodes = 0    k=0    print(\"{}th Total number of Communities = {}\".format(t ,len(communities)))    for i, comm_nodes in enumerate(communities):        if len(comm_nodes)&gt;=10:            k+=1            print('community {}th: '.format(i),len(comm_nodes))        sum_comm_nodes+=len(comm_nodes)    t+=1    print('n_big_communities: ',k)# best partition with 5 big communities at cor_threshold = np.linspace(0.8,0.85,20)[8] ...5th Total number of Communities = 36community 0th:  162community 1th:  188community 4th:  103community 6th:  35n_big_communities:  46th Total number of Communities = 36community 0th:  172community 1th:  180community 3th:  113community 4th:  25n_big_communities:  47th Total number of Communities = 36community 0th:  152community 3th:  112community 4th:  46community 6th:  180n_big_communities:  48th Total number of Communities = 41community 2th:  77community 4th:  107community 5th:  157community 7th:  37community 9th:  108n_big_communities:  59th Total number of Communities = 41community 0th:  153community 4th:  103community 6th:  43community 7th:  79community 9th:  108n_big_communities:  510th Total number of Communities = 41community 0th:  146community 2th:  84community 4th:  109community 6th:  41community 9th:  106n_big_communities:  511th Total number of Communities = 42community 0th:  145community 2th:  178community 4th:  126community 8th:  35n_big_communities:  4...np.random.seed(2021)cor_thresold = np.linspace(0.8,0.85,20)[8]H_pos = set_threshold(G_pos,cor_thresold)partition = louvain.best_partition(H_pos)values = [partition.get(node) for node in H_pos.nodes()]communities = []tmp = list(partition.items())for i in range(len(set(values))):    communities.append([n for n,c in tmp if c==i])sum_comm_nodes = 0k=0print(\"Total number of Communities = {}\".format(len(communities)))for i, comm_nodes in enumerate(communities):    if len(comm_nodes)&gt;=10:        k+=1        print('community {}th: '.format(i),len(comm_nodes))    sum_comm_nodes+=len(comm_nodes)print('n_big_communities: ',k)Total number of Communities = 41community 0th:  164community 2th:  79community 4th:  105community 7th:  30community 9th:  108n_big_communities:  5nx.set_node_attributes(H_pos,partition,'community')values = [partition.get(node) for node in H_pos.nodes()]plt.figure(figsize=(10,10))nx.draw_spring(H_pos, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False)np.random.seed(2021)# for negative weightst=0for cor_thresold in np.linspace(0.6,0.65,20):    H_neg = set_threshold(G_neg,cor_thresold)    partition = louvain.best_partition(H_neg)    modularity = louvain.modularity(partition, H_neg)    values = [partition.get(node) for node in H_neg.nodes()]    communities = []    tmp = list(partition.items())    for i in range(len(set(values))):        communities.append([n for n,c in tmp if c==i])    sum_comm_nodes = 0    k=0        print(\"{}th Total number of Communities = {}\".format(t ,len(communities)))    for i, comm_nodes in enumerate(communities):        if len(comm_nodes)&gt;=10:            k+=1            print('community {}th: '.format(i),len(comm_nodes))        sum_comm_nodes+=len(comm_nodes)    t+=1    print('n_big_communities: ',k)# best partition with 5 big communities at cor_threshold = np.linspace(0.6,0.65,20)[11] ...9th Total number of Communities = 37community 0th:  104community 1th:  83community 2th:  177community 5th:  119community 6th:  16n_big_communities:  510th Total number of Communities = 40community 0th:  103community 1th:  83community 2th:  177community 3th:  14community 6th:  119n_big_communities:  511th Total number of Communities = 41community 0th:  104community 1th:  83community 2th:  176community 3th:  13community 6th:  119n_big_communities:  512th Total number of Communities = 42community 0th:  103community 1th:  81community 3th:  13community 6th:  130community 11th:  167n_big_communities:  513th Total number of Communities = 42community 0th:  106community 1th:  81community 2th:  176community 6th:  118community 7th:  13n_big_communities:  514th Total number of Communities = 43community 0th:  204community 1th:  165community 2th:  18community 5th:  105n_big_communities:  4...np.random.seed(2021)cor_thresold = np.linspace(0.6,0.65,20)[11] H_neg_otim = set_threshold(G_neg,cor_thresold)partition = louvain.best_partition(H_neg_otim)modularity = louvain.modularity(partition, H_neg)values = [partition.get(node) for node in H_neg.nodes()]communities = []tmp = list(partition.items())for i in range(len(set(values))):    communities.append([n for n,c in tmp if c==i])sum_comm_nodes = 0k=0print(\"Total number of Communities = {}\".format(len(communities)))for i, comm_nodes in enumerate(communities):    if len(comm_nodes)&gt;=10:        k+=1        print('community {}th: '.format(i),len(comm_nodes))    sum_comm_nodes+=len(comm_nodes)print('n_big_communities: ',k)Total number of Communities = 41community 0th:  102community 1th:  83community 3th:  13community 6th:  130community 11th:  167n_big_communities:  5nx.set_node_attributes(H_neg, partition,'community')values = [partition.get(node) for node in H_neg.nodes()]plt.figure(figsize=(10,10))nx.draw_spring(H_neg, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False)3.4. Visualization with Gephinx.write_graphml(H_pos, \"model_H_pos.graphml\")nx.write_graphml(H_neg, \"model_H_neg.graphml\")",
        "url": "/DM_final_proj"
    }
    ,
    
    "ds-r-final": {
        "title": "Data Science and R - Air Pollution Data Analysis",
        "author": "Darron Kwon",
        "category": "",
        "content": "  1. About  2. Load Data and Preprocess          2.1. Set Attributes      2.2. Handling NA values        3. EDA          3.1. 통합대기환경지수(Comprehensive air-quality index), CAI 계산      3.2. Timestep-wise Visualization                  3.2.1. Hourly          3.2.2. Daily          3.2.3. Montly          3.2.4. Quarterly          3.2.5. Half-yearly                      4. Summary1. About  2020-2, Data Science and R, Final Proejct: Air Pollution Data Analysis2. Load Data and Preprocess  Collected data from Air Korea (https://www.airkorea.or.kr)library(dplyr)library(tidyverse)library(grid)library(vcd)library(ggplot2)library(readxl)data= read_xlsx(\"./final_data/data.xlsx\");head(data,5)nrow(data)\t\t날짜시도측정소명측정소코드아황산가스일산화탄소오존이산화질소PM10PM2.5\t\t\t2017-07-01 01인천 옹진군  백령도       831492       .0016        .3           .052         .0016        44           35           \t\t2017-07-01 02인천 옹진군  백령도       831492       .0016        .3           .052         .0017        21           NA           \t\t2017-07-01 03인천 옹진군  백령도       831492       .0017        .3           .051         .0017        32           18           \t\t2017-07-01 04인천 옹진군  백령도       831492       .0017        .3           .05          .002         10           NA           \t\t2017-07-01 05인천 옹진군  백령도       831492       .0016        .3           .048         .0018        24           22           \t\t263042.1. Set Attributes#   시도, 측정소명, 측정소코드는 모두 같으므로 제외#   날짜는 시구간별 분석을 위해 연, 월, 일, 시로 분리: int타입으로 반환됨#   측정 대상 오염 물질의 자료형 변환data &lt;- data %&gt;%  select(-c(시도,측정소명,측정소코드)) %&gt;%  separate(col=날짜,           into=c(\"year\",\"month\",\"day\",\"hour\"),           convert=TRUE) %&gt;%  mutate_if(is.character,as.numeric) %&gt;%  rename(SO2=아황산가스,CO=일산화탄소,O3=오존,NO2=이산화질소)head(data,5)\t\tyearmonthdayhourSO2COO3NO2PM10PM2.5\t\t\t2017  7     1     1     0.00160.3   0.052 0.001644    35    \t\t2017  7     1     2     0.00160.3   0.052 0.001721    NA    \t\t2017  7     1     3     0.00170.3   0.051 0.001732    18    \t\t2017  7     1     4     0.00170.3   0.050 0.002010    NA    \t\t2017  7     1     5     0.00160.3   0.048 0.001824    22    \t\t2.2. Handling NA values# 일평균 값을 확인daily &lt;- data %&gt;%  group_by(year,month,day) %&gt;%  select_if(is.double) %&gt;%  summarise_all(mean,na.rm=TRUE) %&gt;%  ungroup()summary(daily)      year          month             day             SO2            Min.   :2017   Min.   : 1.000   Min.   : 1.00   Min.   :0.0001765   1st Qu.:2018   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.:0.0012553   Median :2018   Median : 7.000   Median :16.00   Median :0.0017042   Mean   :2018   Mean   : 6.522   Mean   :15.73   Mean   :0.0018339   3rd Qu.:2019   3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.:0.0022417   Max.   :2020   Max.   :12.000   Max.   :31.00   Max.   :0.0078375                                                   NA's   :1                 CO                O3               NO2                PM10        Min.   :0.04583   Min.   :0.01188   Min.   :0.000850   Min.   :  2.00   1st Qu.:0.23750   1st Qu.:0.03453   1st Qu.:0.002373   1st Qu.: 20.88   Median :0.32917   Median :0.04290   Median :0.003296   Median : 29.25   Mean   :0.37287   Mean   :0.04405   Mean   :0.003853   Mean   : 35.58   3rd Qu.:0.46667   3rd Qu.:0.05192   3rd Qu.:0.004764   3rd Qu.: 42.54   Max.   :1.39167   Max.   :0.09971   Max.   :0.020209   Max.   :217.04   NA's   :2                                              NA's   :5            PM2.5         Min.   :  1.826   1st Qu.: 10.042   Median : 14.625   Mean   : 18.736   3rd Qu.: 22.375   Max.   :126.375   NA's   :23       # NA: 하루 전체 관측치가 NA인 경우에 발생# 월평균 값을 확인monthly &lt;- data %&gt;%  group_by(year,month) %&gt;%  select_if(is.double) %&gt;%  summarise_all(mean,na.rm=TRUE) %&gt;%  ungroup()summary(monthly)      year          month            SO2                 CO         Min.   :2017   Min.   : 1.00   Min.   :0.001099   Min.   :0.1911   1st Qu.:2018   1st Qu.: 3.75   1st Qu.:0.001394   1st Qu.:0.2646   Median :2018   Median : 6.50   Median :0.001789   Median :0.3562   Mean   :2018   Mean   : 6.50   Mean   :0.001832   Mean   :0.3730   3rd Qu.:2019   3rd Qu.: 9.25   3rd Qu.:0.002084   3rd Qu.:0.4491   Max.   :2020   Max.   :12.00   Max.   :0.003140   Max.   :0.7588         O3               NO2                PM10           PM2.5       Min.   :0.02447   Min.   :0.001857   Min.   :20.24   Min.   :10.62   1st Qu.:0.03821   1st Qu.:0.002876   1st Qu.:27.31   1st Qu.:13.94   Median :0.04175   Median :0.004066   Median :35.42   Median :18.41   Mean   :0.04410   Mean   :0.003850   Mean   :35.69   Mean   :18.92   3rd Qu.:0.05154   3rd Qu.:0.004658   3rd Qu.:42.46   3rd Qu.:22.84   Max.   :0.06422   Max.   :0.005683   Max.   :58.00   Max.   :36.51  # NA값들을 일평균 또는 월평균 값으로 대체for (i in (1:nrow(data))){  for (j in c(\"SO2\",\"CO\",\"O3\",\"NO2\",\"PM10\",\"PM2.5\")){    if (is.na(data[i,][[j]])){      tmp&lt;-data[i,]      y&lt;-tmp$year      m&lt;-tmp$month      d&lt;-tmp$day      if (!is.nan(daily[daily$year==y &amp; daily$month==m &amp; daily$day==d,][[j]])){        data[i,][[j]]&lt;-daily[daily$year==y &amp; daily$month==m &amp; daily$day==d,][[j]]      }else{        data[i,][[j]]&lt;-monthly[monthly$year==y &amp; monthly$month==m,][[j]]      }    }  }}summary(data)      year          month             day             hour       Min.   :2017   Min.   : 1.000   Min.   : 1.00   Min.   : 1.00   1st Qu.:2018   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.: 6.75   Median :2018   Median : 7.000   Median :16.00   Median :12.50   Mean   :2018   Mean   : 6.522   Mean   :15.73   Mean   :12.50   3rd Qu.:2019   3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.:18.25   Max.   :2020   Max.   :12.000   Max.   :31.00   Max.   :24.00        SO2                 CO               O3               NO2           Min.   :0.000000   Min.   :0.0000   Min.   :0.00100   Min.   :0.000300   1st Qu.:0.001200   1st Qu.:0.2000   1st Qu.:0.03400   1st Qu.:0.002100   Median :0.001700   Median :0.3000   Median :0.04300   Median :0.003000   Mean   :0.001834   Mean   :0.3726   Mean   :0.04405   Mean   :0.003853   3rd Qu.:0.002300   3rd Qu.:0.5000   3rd Qu.:0.05200   3rd Qu.:0.004600   Max.   :0.034700   Max.   :2.5000   Max.   :0.13200   Max.   :0.045900        PM10            PM2.5        Min.   :  1.00   Min.   :  0.00   1st Qu.: 19.00   1st Qu.:  9.00   Median : 28.00   Median : 14.00   Mean   : 35.57   Mean   : 18.78   3rd Qu.: 43.00   3rd Qu.: 22.77   Max.   :461.00   Max.   :219.00  3. EDA3.1. 통합대기환경지수(Comprehensive air-quality index), CAI 계산### 통합대기환경지수CAI 지수를 계산# 항목별 계산 함수를 정의SO2_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,0.02,0.05,0.15,1,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0.02,0.02,0.05,0.15,1,1))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,0.021,0.051,0.151,0.151))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (((I_hi-I_lo)/(BP_hi-BP_lo)) * (C_p - BP_lo)) + I_lo  return(I_p)}CO_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,2,9,15,50,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(2,2,9,15,50,50))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,2.01,9.01,15.01,15.01))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}O3_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,0.03,0.09,0.15,0.6,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0.03,0.03,0.09,0.15,0.6,0.6))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,0.031,0.091,0.151,0.151))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}NO2_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,0.03,0.06,0.2,2,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0.03,0.03,0.06,0.2,2,2))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,0.031,0.061,0.201,0.201))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}# PM10, PM2.5의 이동평균을 구하기 위한 함수 정의C12_cal &lt;- function(PM){  C12 &lt;- c()  for (i in (1:length(PM))){    if (i&lt;12){      tmp &lt;- sum(PM[1:i])/i    }else{      tmp &lt;- sum(PM[(i-11):i])/12    }    C12[i] &lt;- tmp  }  return (C12)}C_ai_cal &lt;- function(C_i,M,C12){  if (C_i &lt; M){    C_ai &lt;- C_i  }else if(0.9&lt;= (C_i/C12) &amp;&amp; (C_i/C12) &lt;=1.7){    C_ai &lt;- 0.75*C_i    }else {    C_ai &lt;- C_i  }  return (C_ai)}PM_cal &lt;- function(PM,m,PM_C12){  M=m  C24E=c()  for (i in (1:length(PM))){    if (i&lt;4){      C4 &lt;-0      for (j in (i:1)){        C4 &lt;- C4+ C_ai_cal(PM[j],M,PM_C12[j])      }      C4/i    } else{      C4 &lt;- sum(C_ai_cal(PM[i],M,PM_C12[i]),C_ai_cal(PM[i-1],M,PM_C12[i-1]),              C_ai_cal(PM[i-2],M,PM_C12[i-2]),C_ai_cal(PM[i-3],M,PM_C12[i-3]))/4    }    C12&lt;-PM_C12[i]    C24E[i] = (C12*12+C4*12)/24  }  return (C24E)}PM10_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,30,80,150,600,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE,                                       labels=c(30,30,80,150,600,600))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE,                                       labels=c(0,0,31,81,151,151))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE,                                      labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE,                                      labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}PM2.5_I_p &lt;- function(C_p){  cut &lt;- c(-Inf,0,15,35,75,500,Inf)  BP_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(15,15,35,75,500,500))))  BP_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,16,36,76,76))))  I_hi &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(50,50,100,250,500,500))))  I_lo &lt;- as.numeric(as.character(cut(C_p, breaks=cut,right=TRUE,                include.lowest = FALSE, labels=c(0,0,51,101,251,251))))  I_p &lt;- (I_hi-I_lo)/(BP_hi-BP_lo) * (C_p - BP_lo) + I_lo  return(I_p)}PM10_C12&lt;-C12_cal(data$PM10)PM10_C_p &lt;- PM_cal(data$PM10,70,PM10_C12)PM2.5_C12 &lt;- C12_cal(data$PM2.5)PM2.5_C_p &lt;- PM_cal(data$PM2.5,30,PM2.5_C12)data &lt;- data %&gt;%  bind_cols(\"SO2_I_p\" = SO2_I_p(data$SO2)) %&gt;%  bind_cols(\"CO_I_p\" = CO_I_p(data$CO)) %&gt;%  bind_cols(\"O3_I_p\" = O3_I_p(data$O3)) %&gt;%  bind_cols(\"NO2_I_p\" = NO2_I_p(data$NO2)) %&gt;%  bind_cols(\"PM10_I_p\" = PM10_I_p(PM10_C_p)) %&gt;%  bind_cols(\"PM2.5_I_p\" = PM2.5_I_p(PM2.5_C_p))CAI_table &lt;- data %&gt;%  select(ends_with(\"I_p\")) %&gt;%  mutate(BAD = rowSums(. &gt;100)) %&gt;%  mutate(CAI = apply(., 1, max) + case_when(BAD&gt;=3 ~ 75, BAD&gt;=2 ~ 50,TRUE ~ 0)) %&gt;%  mutate(CAI_index = case_when(CAI&gt;250 ~ 3, CAI&gt;100 ~ 2,                               CAI&gt;50 ~ 1, TRUE ~ 0)) %&gt;%  select(starts_with(\"CAI\"))data &lt;- data %&gt;%  bind_cols(CAI_table)head(data[c(11:18)],5)\t\tSO2_I_pCO_I_pO3_I_pNO2_I_pPM10_I_pPM2.5_I_pCAICAI_index\t\t\t4.00     7.5      68.44068 2.666667 64.00000  88.71711 88.717111        \t\t4.00     7.5      68.44068 2.833333 68.75000 103.38782103.387822        \t\t4.25     7.5      67.61017 2.833333 84.66667 131.82942131.829422        \t\t4.25     7.5      66.77966 3.333333 44.58333  66.44682 66.779661        \t\t4.00     7.5      65.11864 3.000000 39.95833  64.79737 65.118641        \t\t3.2. Timestep-wise Visualization  시구간별 시각화 및 분석3.2.1. Hourly# 시간대(주간: 07~18, 야간: 19~06)에 따른 CAI 지수 분포CAI_hourly &lt;- data %&gt;%  mutate(time_tmp = case_when(    06&lt;hour &amp; hour&lt;19 ~ \"daytime\", TRUE ~ \"nighttime\")) %&gt;%  group_by(year,month,day,time_tmp) %&gt;%  summarise(mean_CAI=mean(CAI)) %&gt;%  mutate(CAI_idx = case_when(mean_CAI&gt;250 ~ 3, mean_CAI&gt;100 ~ 2,                             mean_CAI&gt;50 ~ 1, TRUE ~ 0)) %&gt;% # 해당 시간대의 평균 오염도  ungroup()# 시간에 따른 오염도의 분포는 거의 차이가 없음을 확인CAI_hourly %&gt;%  group_by(time_tmp,CAI_idx)%&gt;%  tally()`summarise()` has grouped output by 'year', 'month', 'day'. You can override using the `.groups` argument.\t\ttime_tmpCAI_idxn\t\t\tdaytime  0        100      \t\tdaytime  1        886      \t\tdaytime  2         93      \t\tdaytime  3         17      \t\tnighttime0         73      \t\tnighttime1        895      \t\tnighttime2        115      \t\tnighttime3         13      \t\t3.2.2. Daily### 일단위: 일평균 CAI 지수의 분포CAI_daily &lt;- data %&gt;%  group_by(year,month,day) %&gt;%  summarise_at(.vars=\"CAI\", .funs=mean) %&gt;%  ungroup() %&gt;%  mutate(CAI_idx = case_when(CAI&gt;250 ~ 3, CAI&gt;100 ~ 2,                             CAI&gt;50 ~ 1, TRUE ~ 0))summary(factor(CAI_daily$CAI_idx))# 평균 수준보다 오염도가 높은 날들은 연중 특정 시기에 집중되어있음CAI_daily %&gt;%  unite(\"date\",1:3,sep=\"/\") %&gt;%  mutate_at(\"date\",as.Date) %&gt;%  ggplot() +  geom_point(mapping=aes(x=date,y=CAI))3.2.3. Montly### 월단위: 오염 수준 나쁨 이상인 \"일수\" 확인CAI_monthly &lt;- CAI_daily %&gt;%  group_by(year,month) %&gt;%  summarise(mean_CAI = mean(CAI),sum_CAI_idx = sum(CAI_idx&gt;1)) %&gt;%  mutate(m_tmp = case_when(month&lt;10 ~ paste(\"0\",as.character(month),sep=\"\"),                           TRUE ~ paste(as.character(month))))%&gt;%  mutate(y_tmp=as.character(year)) %&gt;%  unite(date,y_tmp,m_tmp,sep=\"\")# 6월~9월은 장마 영향으로 추정. 겨울~봄에 집중적.CAI_monthly %&gt;%  ggplot() +  geom_col(mapping=aes(x=date,y=sum_CAI_idx))`summarise()` has grouped output by 'year'. You can override using the `.groups` argument.3.2.4. Quarterly### 분기 단위CAI_qtr &lt;- CAI_monthly %&gt;%  mutate(qtr_tmp = case_when(    month &lt; 4 ~ \"Q1\", month &lt; 7 ~ \"Q2\", month &lt; 10 ~ \"Q3\", TRUE ~ \"Q4\")) %&gt;%  mutate(y_tmp=as.character(year)) %&gt;%  unite(Qtr, y_tmp, qtr_tmp, sep=\"-\") %&gt;%  group_by(Qtr) %&gt;%  summarise(mean_CAI = mean(mean_CAI), sum_CAI_idx = sum(sum_CAI_idx))CAI_qtr %&gt;%  ggplot() +  geom_col(aes(x=Qtr,y=sum_CAI_idx))3.2.5. Half-yearly### 반기 단위: 2019년 상반기가 매우 심한 것이었음을 알 수 있음CAI_half &lt;- CAI_monthly %&gt;%  transform(date=as.integer(date)) %&gt;%  mutate(year = case_when(    date &lt; 201801 ~ \"2017_H2\", date &lt; 201807 ~ \"2018-H1\",    date &lt; 201901 ~ \"2018_H2\", date &lt; 201907 ~ \"2019-H1\",    date &lt; 202001 ~ \"2019-H2\", TRUE ~ \"2020-H1\")) %&gt;%  group_by(year) %&gt;%  summarise(mean_CAI = mean(mean_CAI), sum_CAI_idx = sum(sum_CAI_idx))CAI_half %&gt;%  ggplot()+  geom_col(aes(x=year,y=sum_CAI_idx))4. Summary대기오염도는 대체적으로 여름보다 겨울에 높았고, 2019년이 특히 심했다는 것을 알 수 있다. 2020년을 기준으로 비교했을 때, 코로나의 영향은 거의 없었던 것으로 추정된다.",
        "url": "/ds_R_final"
    }
    ,
    
    "nlp-final": {
        "title": "NLP - Korean Language Text Analysis with RNN",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-1. NLP# 한국어 자료import sysimport osimport numpy as npimport nltkimport konlpyimport pandas as pdimport reimport randomimport itertoolsimport warningswarnings.filterwarnings(action='ignore')from sklearn.metrics import classification_report,f1_score,precision_score,recall_scorerandom.seed(1)# 데이터 로드train_data=pd.read_csv('final_train_data.csv')test_data=pd.read_csv('final_test_data.csv')# 384 duplicates in content, 240 in titleprint(train_data.describe())       category                                            content  titlecount     10686                                              10686  10686unique        7                                              10302  10124top        정치개혁  개인회생 36개월 단축소급 전국 적용을 위해 춘천지방법원의 법원에 바란다에 글을 올...   경남제약freq       3094                                                 16     21# preprocess: duplicatetrain_data = train_data.drop_duplicates(['content'],keep='first')# all duplicates removedtrain_data.duplicated().sum()0train_data['document']=train_data.iloc[:,1]+train_data.iloc[:,2]train_data['document']test_data['document']=test_data.iloc[:,1]+test_data.iloc[:,2]train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행tokenizing process with konlpy-Okt# tokenizefrom konlpy.tag import Oktokt = Okt() #형태소 분석기tokenized_data = []stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']for sentence in train_data['document']:    temp_X = okt.morphs(sentence, norm=True, stem=True) # 형태소 추출    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거    tokenized_data.append(temp_X)x_train=tokenized_dataokt = Okt() #형태소 분석기tokenized_data = []stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']for sentence in test_data['document']:    temp_X = okt.morphs(sentence, norm=True, stem=True) # 형태소 추출 - 토큰화 #norm=True : 근사어    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거 #https://www.ranks.nl/stopwords/korean    tokenized_data.append(temp_X)x_test=tokenized_datafor this tokenizing process takes a long time, save and import preprocessed data.import picklewith open('nlp_final_x_tr.data', 'wb') as f:    pickle.dump(x_train, f)with open('nlp_final_x_te.data', 'wb') as f:    pickle.dump(x_test, f)import picklewith open('nlp_final_x_tr.data', 'rb') as f:    x_train = pickle.load(f)with open('nlp_final_x_te.data', 'rb') as f:    x_test = pickle.load(f)from tensorflow.keras.preprocessing.text import Tokenizertokenizer = Tokenizer()tokenizer.fit_on_texts(x_train)threshold = 3total_cnt = len(tokenizer.word_index) # 단어의 수rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.for key, value in tokenizer.word_counts.items():    total_freq = total_freq + value    # 단어의 등장 빈도수가 threshold보다 작으면    if(value &lt; threshold):        rare_cnt = rare_cnt + 1        rare_freq = rare_freq + valueprint('단어 집합(vocabulary)의 크기 :',total_cnt)print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)단어 집합(vocabulary)의 크기 : 34537등장 빈도가 2번 이하인 희귀 단어의 수: 15483단어 집합에서 희귀 단어의 비율: 44.830182123519705전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.1132793250941202vocab_size = total_cnt - rare_cnt + 1 # 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거. 0번 패딩 토큰을 고려하여 +1print('단어 집합의 크기 :',vocab_size)단어 집합의 크기 : 19055tokenizer = Tokenizer(vocab_size) tokenizer.fit_on_texts(x_train)X_train = tokenizer.texts_to_sequences(x_train)X_test = tokenizer.texts_to_sequences(x_test)y_train=np.array(train_data.category)y_test=np.array(test_data.category)drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) &lt; 1]drop_test = [index for index, sentence in enumerate(X_test) if len(sentence) &lt; 1]X_train = np.delete(X_train, drop_train, axis=0)y_train = np.delete(y_train, drop_train, axis=0)print(len(X_train))print(len(y_train))1030110301X_test = np.delete(X_test, drop_test, axis=0)y_test = np.delete(y_test, drop_test, axis=0)print(len(X_train))print(len(X_test))print(len(y_train))print(len(y_test))103011158103011158import matplotlib.pyplot as pltprint('리뷰의 최대 길이 :',max(len(l) for l in X_train))print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))plt.hist([len(s) for s in X_train], bins=50)plt.xlabel('length of samples')plt.ylabel('number of samples')plt.show()리뷰의 최대 길이 : 9032리뷰의 평균 길이 : 170.45791670711583def below_threshold_len(max_len, nested_list):  cnt = 0  for s in nested_list:    if(len(s) &lt;= max_len):        cnt = cnt + 1  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))max_len = 600below_threshold_len(max_len, X_train)전체 샘플 중 길이가 600 이하인 샘플의 비율: 96.02951169789341from tensorflow.keras.preprocessing.sequence import pad_sequencesX_train = pad_sequences(X_train, maxlen = max_len)X_test = pad_sequences(X_test, maxlen = max_len)y_train=np.array(train_data.category)y_test=np.array(test_data.category)y_train = np.delete(y_train, drop_train, axis=0)y_test = np.delete(y_test, drop_test, axis=0)print(len(y_train),len(y_test))10301 1158t={'경제민주화': 1, '교통/건축/국토': 2, '보건복지': 3, '육아/교육': 4, '인권/성평등': 5, '일자리': 6, '정치개혁': 7}print(t['보건복지'])index1=np.zeros([10301,7])for i in range(len(y_train)):  index1[i][t[y_train[i]]-1]=1y_train=index1index1=np.zeros([1158,7])for i in range(len(y_test)):  index1[i][t[y_test[i]]-1]=1y_test=index1print(y_train.shape,y_test.shape)3(10301, 7) (1158, 7)X_train=np.array(X_train)X_test=np.array(X_test)y_train=np.array(y_train)y_test=np.array(y_test)print(X_train.shape)print(X_test.shape)print(y_train.shape)print(y_test.shape)print(vocab_size)(10301, 600)(1158, 600)(10301, 7)(1158, 7)19055from tensorflow.keras.layers import Embedding, Dense, LSTMfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.models import load_modelfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpointimport tensorflow_addons as tfaf1 = tfa.metrics.F1Score(num_classes=7,threshold=0.5)from tensorflow import keraskeras.__version__'2.6.0'model = Sequential()model.add(Embedding(vocab_size, 128))model.add(LSTM(128))model.add(Dense(7, activation='sigmoid'))es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=4)mc = ModelCheckpoint('best_model.h5', monitor=f1, mode='max', verbose=2, save_best_only=True)model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc',f1])model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=512, validation_split=0.2)Epoch 1/1517/17 [==============================] - 39s 2s/step - loss: 1.8842 - acc: 0.2830 - f1_score: 0.2583 - val_loss: 1.7326 - val_acc: 0.3081 - val_f1_score: 0.2726WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 2/1517/17 [==============================] - 38s 2s/step - loss: 1.6177 - acc: 0.4615 - f1_score: 0.3981 - val_loss: 1.5555 - val_acc: 0.4639 - val_f1_score: 0.3776WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 3/1517/17 [==============================] - 38s 2s/step - loss: 1.3590 - acc: 0.5495 - f1_score: 0.4018 - val_loss: 1.3879 - val_acc: 0.5075 - val_f1_score: 0.4232WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 4/1517/17 [==============================] - 38s 2s/step - loss: 1.1364 - acc: 0.6211 - f1_score: 0.4351 - val_loss: 1.3140 - val_acc: 0.5303 - val_f1_score: 0.4173WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 5/1517/17 [==============================] - 38s 2s/step - loss: 0.9084 - acc: 0.7212 - f1_score: 0.4893 - val_loss: 1.1852 - val_acc: 0.6487 - val_f1_score: 0.4386WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 6/1517/17 [==============================] - 38s 2s/step - loss: 0.7336 - acc: 0.7920 - f1_score: 0.5183 - val_loss: 1.1071 - val_acc: 0.6473 - val_f1_score: 0.4536WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 7/1517/17 [==============================] - 39s 2s/step - loss: 0.5796 - acc: 0.8381 - f1_score: 0.5395 - val_loss: 1.0900 - val_acc: 0.6458 - val_f1_score: 0.4837WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 8/1517/17 [==============================] - 38s 2s/step - loss: 0.4641 - acc: 0.8757 - f1_score: 0.5575 - val_loss: 0.9854 - val_acc: 0.6982 - val_f1_score: 0.4781WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 9/1517/17 [==============================] - 38s 2s/step - loss: 0.3588 - acc: 0.9039 - f1_score: 0.5759 - val_loss: 1.2205 - val_acc: 0.6458 - val_f1_score: 0.4993WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 10/1517/17 [==============================] - 38s 2s/step - loss: 0.3082 - acc: 0.9181 - f1_score: 0.5908 - val_loss: 1.0990 - val_acc: 0.6870 - val_f1_score: 0.5031WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 11/1517/17 [==============================] - 39s 2s/step - loss: 0.2331 - acc: 0.9408 - f1_score: 0.6076 - val_loss: 1.8883 - val_acc: 0.5793 - val_f1_score: 0.4935WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 12/1517/17 [==============================] - 39s 2s/step - loss: 0.2625 - acc: 0.9380 - f1_score: 0.6111 - val_loss: 1.3714 - val_acc: 0.6642 - val_f1_score: 0.5215WARNING:tensorflow:Can save best model only with &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x7f1bd3f444f0&gt; available, skipping.Epoch 00012: early stopping&lt;keras.callbacks.History at 0x7f1bf1402a60&gt;# evaluating function: report f1_macrodef evaluate(test_x,test_y,model):    predictions=model.predict(test_x)    y_pred=max(predictions)    print(classification_report(test_y,y_pred))model.evaluate(X_test, y_test)37/37 [==============================] - 6s 149ms/step - loss: 1.1846 - acc: 0.6986 - f1_score: 0.5419[1.1846439838409424, 0.6986182928085327, array([0.5124555 , 0.59907836, 0.48712873, 0.45633796, 0.5854922 ,        0.4855967 , 0.6674057 ], dtype=float32)]# from saved best modelloaded_model = load_model('best_model.h5')loaded_model.evaluate(X_test, y_test)",
        "url": "/NLP_final"
    }
    ,
    
    "nlp-midterm": {
        "title": "NLP - Text Analysis with ML algorithms",
        "author": "Darron Kwon",
        "category": "",
        "content": "2020-1. NLP  1. Data Info  2. Preprocessing          2.1 duplicated data found in train_data        3. Comparing classification models          3.1 With Tf-idf vectorizer                  3.1.1 MultinomialNB          3.1.2 LogisticRegression                    3.2 With CountVectorizer                  3.2.1 MultinomialNB                      4. Balanced sampling approach - imblearn  5. Result: Best Model  commentimport sysimport pandas as pdimport osimport numpy as npimport reimport randomimport itertoolsimport warningswarnings.filterwarnings(action='ignore')from sklearn.metrics import classification_reportfrom sklearn.metrics import f1_scorerandom.seed(1)train_data=pd.read_csv('midterm_train.csv')test_data=pd.read_csv('midterm_test.csv')# evaluating functiondef evaluate(test_x,test_y,model):    predictions=model.predict(test_x)    print(classification_report(test_y,predictions))1. Data Infotrain_data.head(3)                  text      senti                  0      J brand is by far the best premium denim line ...      pos              1      I loved this dress. i kept putting it on tryin...      pos              2      I found this at my local store and ended up bu...      pos      print(train_data.groupby('senti').count())print(test_data.groupby('senti').count())print(train_data.describe())print(test_data.describe())        textsenti       neg     2139pos    11279       textsenti      neg     231pos    1260                                                     text  senticount                                               13418  13418unique                                              13414      2top     Perfect fit and i've gotten so many compliment...    posfreq                                                    2  11279                                                     text senticount                                                1491  1491unique                                               1491     2top     Have to disagree with previous posters. i foun...   posfreq                                                    1  12602. Preprocessing2.1 duplicated data found in train_data# remove duplicated dataprint(train_data.text.duplicated().sum())train_data = train_data.drop_duplicates(['text'],keep='first')train_data.duplicated().sum()40print(train_data.describe())                                                     text  senticount                                               13414  13414unique                                              13414      2top     J brand is by far the best premium denim line ...    posfreq                                                    1  11276# train-test split unduplicated datax_train=np.array(train_data.text)x_test=np.array(test_data.text)y_train=np.array(train_data.senti)y_test=np.array(test_data.senti)x_train[0]'J brand is by far the best premium denim line retailer sells! the fit on these jeans is amazing..worth every penny..also, considering it is a crop jean - warm weather wear - the denim weight is light and not too thick...the color is different from ordinary regular denim blue..lighter wash for spring/summer!'# preprocessing: remove non-alphabet charactersx_train_clean=np.array([re.sub('[^a-zA-Z]',' ',text) for text in x_train])x_test_clean=np.array([re.sub('[^a-zA-Z]',' ',text) for text in x_test])x_train_clean[0]'J brand is by far the best premium denim line retailer sells  the fit on these jeans is amazing  worth every penny  also  considering it is a crop jean   warm weather wear   the denim weight is light and not too thick   the color is different from ordinary regular denim blue  lighter wash for spring summer '3. Comparing classification models# tuning parameter setsngram_range= [(1, 1), (1, 2),(2,2)]stop_words=[None,'english']clf__alpha=[0.005,0.01,0.05,0.1]params = dict(ngram_range=ngram_range,              stop_words=stop_words,              clf__alpha=clf__alpha)keys=params.keys()values = (params[key] for key in keys)combinations = [dict(zip(keys, combination)) for combination in itertools.product(*values)]# tuning functiondef tuning_model(params,vectorizer,classifier,x_train,x_test,y_train,y_test):    ngram_range=params['ngram_range']    stop_words=params['stop_words']    vec = vectorizer(ngram_range=ngram_range,stop_words=stop_words)    vec_train = vec.fit_transform(x_train)    vec_test = vec.transform(x_test)    if classifier==MultinomialNB:        alpha=params['clf__alpha']        clf=classifier(alpha)    else:        clf=classifier(random_state=1,max_iter=500)    clf.fit(vec_train, y_train)    pred=clf.predict(vec_test)    return f1_score(y_test,pred,average='macro'),params# get best score &amp; parametersdef get_result(combinations,vectorizer,classifier,x_train,x_test,y_train,y_test):    results=[]    for params in combinations:        results.append(tuning_model(params,vectorizer,classifier,x_train,x_test,y_train,y_test))    return max(results,key=lambda item: item[0])3.1 With Tf-idf vectorizer3.1.1 MultinomialNBpreprocessing seems to have no effect in ngram_range: (1, 2) and also stop words are not important featuresf1-score macro avg: 0.85from sklearn.naive_bayes import MultinomialNBfrom sklearn.feature_extraction.text import TfidfVectorizerget_result(combinations,TfidfVectorizer,MultinomialNB,x_train,x_test,y_train,y_test)(0.8528815948449455, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.01})get_result(combinations,TfidfVectorizer,MultinomialNB,x_train_clean,x_test_clean,y_train,y_test)(0.8528815948449455, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.005})3.1.2 LogisticRegressionfrom sklearn.linear_model import LogisticRegressionget_result(combinations,TfidfVectorizer,LogisticRegression,x_train,x_test,y_train,y_test)(0.8811180515581001, {'ngram_range': (1, 1), 'stop_words': None, 'clf__alpha': 0.005})get_result(combinations,TfidfVectorizer,LogisticRegression,x_train_clean,x_test_clean,y_train,y_test)(0.8823175752378818, {'ngram_range': (1, 1), 'stop_words': None, 'clf__alpha': 0.005})3.2 With CountVectorizer3.2.1 MultinomialNBfrom sklearn.feature_extraction.text import CountVectorizerget_result(combinations,CountVectorizer,MultinomialNB,x_train,x_test,y_train,y_test)(0.9366865264206945, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.1})get_result(combinations,CountVectorizer,MultinomialNB,x_train_clean,x_test_clean,y_train,y_test)(0.935509934324805, {'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.1})4. Balanced sampling approach - imblearnfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.linear_model import LogisticRegression# tuning parameter sets for all casesvectorizer=[CountVectorizer,TfidfVectorizer]classifier=[MultinomialNB,LogisticRegression]ngram_range= [(1, 1), (1, 2),(2,2)]stop_words=[None,'english']clf__alpha=[0.005,0.01,0.05,0.1]params = dict(ngram_range=ngram_range,              vectorizer=vectorizer,              classifier=classifier,              stop_words=stop_words,              clf__alpha=clf__alpha)keys=params.keys()values = (params[key] for key in keys)combinations = [dict(zip(keys, combination)) for combination in itertools.product(*values)]from imblearn.over_sampling import SMOTEdef smote_tuning(params,x_train,x_test,y_train,y_test):    ngram_range=params['ngram_range']    stop_words=params['stop_words']    classifier=params['classifier']    vec=params['vectorizer'](ngram_range=ngram_range,stop_words=stop_words)    x_train_imb=vec.fit_transform(x_train)    y_train_imb=np.array(train_data['senti']=='pos').astype('int')    y_test_imb=np.array(test_data['senti']=='pos').astype('int')    vec_train_over, y_train_over = SMOTE(random_state=1).fit_resample(x_train_imb,y_train_imb)    vec_test = vec.transform(x_test)    if classifier==MultinomialNB:        alpha=params['clf__alpha']        clf=classifier(alpha)    else:        clf=classifier(random_state=1,max_iter=500)    clf.fit(vec_train_over, y_train_over)    pred=clf.predict(vec_test)    return f1_score(y_test_imb,pred,average='macro'),params# get best score &amp; parametersdef get_smote_result(combinations,x_train,x_test,y_train,y_test):    results=[]    for params in combinations:        results.append(smote_tuning(params,x_train,x_test,y_train,y_test))    return max(results,key=lambda item: item[0])get_smote_result(combinations,x_train,x_test,y_train,y_test)(0.9285352359562871, {'ngram_range': (1, 2),  'vectorizer': sklearn.feature_extraction.text.TfidfVectorizer,  'classifier': sklearn.naive_bayes.MultinomialNB,  'stop_words': None,  'clf__alpha': 0.1})get_smote_result(combinations,x_train_clean,x_test_clean,y_train,y_test)(0.9276401547886131, {'ngram_range': (1, 2),  'vectorizer': sklearn.feature_extraction.text.TfidfVectorizer,  'classifier': sklearn.naive_bayes.MultinomialNB,  'stop_words': None,  'clf__alpha': 0.1})5. Result: Best Model# 'ngram_range': (1, 2), 'stop_words': None, 'clf__alpha': 0.1}vectorizer=CountVectorizer(ngram_range=(1,2))vectors_train=vectorizer.fit_transform(x_train)vectors_test=vectorizer.transform(x_test)clf=MultinomialNB(alpha=0.1)clf.fit(vectors_train,y_train)evaluate(vectors_test,y_test,clf)              precision    recall  f1-score   support         neg       0.90      0.88      0.89       231         pos       0.98      0.98      0.98      1260    accuracy                           0.97      1491   macro avg       0.94      0.93      0.94      1491weighted avg       0.97      0.97      0.97      1491comment데이터사이언스 복수전공 후 첫 강의. 중간고사까지 배웠던 기본적인 전처리와 ML 함수들을 적용했다. 추가적으로 imblanced data case를 SMOTE 패키지를 사용해 간단히 처리해보았다. 당시 수업에서 가장 높은 accuracy를 기록했다.",
        "url": "/NLP_midterm"
    }
    ,
    
    "stat-ch4": {
        "title": "Statistical Mining - Chapter 4",
        "author": "Darron Kwon",
        "category": "",
        "content": "  Chapter 4. Classification          4.1. Problem of linear regression in classification:      4.2. Logistic Regression      4.3. Log-Likelyhood Function: MLE      4.4. Multinomial Logistic Regression: K classes (K&gt;2)      4.5. LDA      4.6. QDA      4.7. LDA vs. Logistic Regression      2020-04-15Chapter 4. Classification  Response Y : Qualitative (categorical) variable.4.1. Problem of linear regression in classification:      Two group: Y= 0 or 1        Model prediction: $\\hat y_i = x_i^T \\hat\\beta$; $\\quad$ where b is LSE        Decision rule: if $\\hat y_i &gt; 0.5$; $\\quad$ i goes to group 1        possible value of $\\hat Y &gt;1 \\text{ or } \\hat Y &lt;0$: $\\quad$ do not fit for the range of Y  4.2. Logistic Regression      Supoose that Y has 2 classes:    $ log \\frac{p(X)}{1-p(X)} = \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p = X^T \\beta $where $p(X) = P(Y=1|X)$, $\\quad X=(1,X_1, \\ldots, X_p)^T$, $\\quad$ $\\beta = (\\beta_0,\\beta_1,\\ldots, \\beta_p)^T $        this model has no error term: GLM        $ E(Y | X) = ? = X^T\\beta $; $\\quad$ a black box - link function    as X is fixed, Y is random variable    $ Y | X ~ \\mathit{Bernoulli} (p(X))$, $\\quad$ $0 &lt; E(Y | X)=p(X) &lt; 1$    so we use log transformation, “logistic fuction”        again, $ log\\frac{p(X)}{1-p(X)} = \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p = X^T \\beta $we don’t know the left side : we can’t use LSE  4.3. Log-Likelyhood Function: MLE      when $log\\frac{p(X)}{1-p(X)}$, $\\quad$ $p(X)= \\frac{exp(x^T\\beta)}{1+exp(x^T\\beta)}$, $\\quad$ $exp(x^T\\beta) = \\frac{p(X)}{1-p(X)}$    $Y_i | X_i \\sim \\mathit{Bernoulli}_{(p(X_i))}$    $\\prod_{i=1}^N p(x_i)^{y_i} (1-p(x_i))^{1-y_i}$    $\\prod_{i:y_i=1}p(x_i) \\prod_{i:y_i=0}(1-p(x_i))$        MLE: \\(\\hat\\beta = \\text{max}_\\beta [\\prod_{i:y_i=1} \\frac{exp(x^T\\beta)}{1+exp(x^T\\beta)} \\prod_{i:y_i=0} \\frac{1}{1+exp(x^T\\beta)}]\\)    when take log, log-like function $l(\\beta) = \\sum_{i:y=1}log(\\frac{exp(x^T\\beta}{1+exp(x^T\\beta)}) + \\sum_{i:y_i=0} (\\frac{1}{1+exp(x^T\\beta)})$    $\\Rightarrow \\frac{\\partial \\; l(\\beta)}{\\partial\\beta} = 0 $        if $\\hat\\beta &gt;0$ , p(X) increasesif $\\hat\\beta &lt;0$ , p(X) decreases(not a linear relationship, but just about direction)  4.4. Multinomial Logistic Regression: K classes (K&gt;2)      $ log\\frac{P(Y=k | X=x)}{P(Y=K | X=x)} = x^T\\beta_k $    for $ k=1,\\cdots, K-1$ with $\\sum_{k=1}^K P(Y=k | X=x) = 1 $    the choice of denominator the $K_{th}$ class is arbitary        we can have K-1 equations        By soloving for $P(Y=k | X=x)$,    $P(Y=k | X=x) = \\frac{exp(x^T\\beta_k)}{1+\\sum_{j=1}^{K-1} exp(x^T\\beta_j)}$    $P(Y=K | X=x)= \\frac 1{1+\\sum_{j=1}^{K-1} exp(x^T\\beta_j)}$    $\\Rightarrow$ ML estimation(by numerical method): we can have K-1 number of $\\hat\\beta_k$ (MLE)  2020-04-164.5. LDA      Suppose $f_k(x)$ is the density of X in class k,    and let $\\pi_k$ be the prior probability of class k with $\\sum_{k=1}^K \\pi_k =1$    $f_k(\\underline{x}) = P(\\underline X=\\underline x | Y=k)$    ($f_k(x)$는 k 그룹 안에 있는 X 의 밀도함수, $\\pi_k$는 $P(Y=k)$로 k 그룹일 확률)        Bayes Theorem:    $ P(Y=k | X=x) = \\frac{f_k(x)\\pi_k}{\\sum_{j=1}^K f_j(x)\\pi_j}$    Assumptions:                  $f_k(x) = MVN(\\mu_k,\\Sigma_k)$ $\\scriptstyle\\text{(p by 1, p by p)}$                    $\\Sigma_k = \\Sigma$ for all k, groups have same var-covariance sturcture.                  Find k maximizing $logP(Y=k | X=x)$:\\(\\begin{align*} ln P(Y=k \\| X=x) &amp;= ln\\frac{f_k(x)\\pi_k}{\\sum_{j=1}^K f_j(x)\\pi_j}\\\\                    \t\t\t    &amp;= ln(f_k(x)\\pi_k) + C_1 \\\\                                  &amp;= \\cdots \\\\                                  &amp;= ln\\pi_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + x^T\\Sigma^{-1}\\mu_k + C_3\\\\                                  &amp;= \\delta_k(x) + C_3 \\quad \\scriptstyle\\text{ : Linear discriminant function (linear in X)} \\\\               \\Rightarrow \\hat Y &amp;= \\text{argmax}_k \\delta_k(x)\\end{align*}\\)        Decision boundary is also in a linear form.  when solving $ { \\underline{x} : \\delta_l(\\underline{x}) = \\delta_m(\\underline{x}) }$,we can have linear form of decision boundary.    Estimate model parameter : mu, pi, sigma$\\hat\\pi_k = n_k/n$$\\hat\\mu_k = \\sum_{y_i=k}x_i / n_k $\\(\\begin{align*}\\hat\\Sigma_k = \\sum_{k=1}^K\\sum_{y_i=k}(x_i-\\hat\\mu_k)(x_i-\\hat\\mu_k)^T / (n-K) \\end{align*}\\)4.6. QDA      Assumption: Same as LDA except for “$\\Sigma_k = \\Sigma$”        Quadratic discriminant function: $\\delta_k(x)=  ln\\pi_k - \\frac{1}{2}ln | \\Sigma_k | - \\frac{1}{2}(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k) $        Decision boundary is in quadratic line        Estimation of parameters:$\\hat\\pi_k = n_k/n$$\\hat\\mu_k = \\sum_{y_i=k}x_i / n_k $\\(\\begin{align*}\\hat\\Sigma_k =\\sum_{y_i=k}(x_i-\\hat\\mu_k)(x_i-\\hat\\mu_k)^T / (n-1) \\end{align*}\\) (only obs. in $k_{th}$ group)        which is more complex?$\\rightarrow$ quadratic: number of parameters, sigma &amp; decision boundary is more flexible.  4.7. LDA vs. Logistic Regression      LDA:MVN assumption neededQualitative(categorical) inputs are not available        Logistic:No assumption needed and Y ~ Bernoulli is evident; $\\rightarrow$ robust modelQualitative are availablecan run F-test to choose more important features  ",
        "url": "/stat_ch4"
    }
    ,
    
    "stat-ch3": {
        "title": "Statistical Mining - Chapter 3",
        "author": "Darron Kwon",
        "category": "",
        "content": "  Chapter 3. Linear Regression          3.1. Least Squares      3.2. Hypothesis test for $\\beta_j$:                  3.2.1. Test for a group of coefficients:          3.2.2. Test for all coefficients:                    3.3. Gauss-Markov Theorem      3.4. Properties of Good estimators      3.5. Model fit      3.6. Prediction Errors      3.7. in K-Nearest Neighbor Regression      3.8. Curse of Dimensionality      Chapter 3. Linear Regression      Model: $ f(X) = \\beta_0 + \\sum_{j=1}^p \\beta_j X_j $$\\Rightarrow$ Prediction of Y from X  (X= p input variables)$\\quad$ f : Regression function E(Y|X)        input variable X:          Quantitative or Transformations of quantitative inputs      Basis expansions leading to polynomial representation      Dummy coding of qualitative variables : G grops -&gt; G-1 dummy variables required      Interactions between inputs ($X_3 = X_1 X_2 $ )      3.1. Least Squares      How to estimate $\\beta = (\\beta_0, \\beta_1, \\ldots , \\beta_p)^T$Least Squares : $\\beta$ minimizing RSSwhen Y (n by 1) / X ( n by (p + 1) matrix),RSS = $ ( y - X\\hat\\beta)^T ( y - X\\hat\\beta) $,Least square estimator (LSE): $ \\hat\\beta = (X^T X)^{-1} X^T Y $        With Assumption:1) $Y_i$ ‘s are uncorrelated &amp; Var ($Y_i$ ) = $\\sigma^2$$\\quad$($\\equiv$ $\\epsilon_i$ ‘s are independent &amp; Var ($\\epsilon_i$ ) = $\\sigma^2$)2) X = $(X_1, \\ldots, X_p)^T$ is fixed (not random)        By (1) &amp; (2), for OLS estimator $\\hat\\beta$;\\(\\\\ \\begin{align*}\\hat\\beta &amp;= (X^TX)^{-1}X^TY \\quad\\leftarrow Y=X\\beta + \\epsilon \\\\                             &amp;= \\beta + (X^TX)^{-1}X^T\\epsilon \\\\\\end{align*} \\\\\\)\\(\\begin{align*}E(\\hat\\beta) &amp;= E(\\beta) + E(\\epsilon) \\\\                            &amp;= \\beta\\end{align*} \\\\\\)\\(\\begin{align*} Var(\\hat\\beta) &amp;= E[\\hat\\beta - E(\\hat\\beta)]^2 \\\\                                &amp;= E[ (X^TX)^{-1}X^T\\epsilon ]^2 \\\\                                &amp;= (X^TX)^{-1}X^TE(\\epsilon^2)X(X^TX)^{-1}\\\\                                &amp;= \\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}\\\\                                &amp;=\\sigma^2(X^T X)^{-1}\\end{align*} \\\\\\)while $\\hat\\sigma^2$ as Estimator of Variance of $\\epsilon$ (= $\\sigma^2$):\\(\\begin{align*} \\hat\\sigma^2 &amp;= \\frac 1 {n-p-1} \\sum_{i=1}^n (y_i - \\hat y_i)^2 \\\\                              &amp;= \\frac1{n-p-1}\\sum_{i=1}^n \\epsilon_i^2 \\\\  \\rightarrow E(\\hat\\sigma^2) &amp;= \\sigma^2\\end{align*} \\\\\\)$\\therefore \\text{ unbiased estimator; }\\frac{RSS}{n-p-1}$        With Assumption:3) $\\epsilon \\sim ^{iid} N(0,\\sigma^2)$ : normal distribution assumption of  error          $\\hat\\beta\\sim MVN(\\beta,\\sigma^2(X^TX)^{-1}), \\quad (Y\\sim MVN(X\\beta,\\sigma^2 I)$ $\\frac{(n-p-1)\\hat\\sigma^2}{\\sigma^2}\\sim\\chi^2_{n-p-1}$$\\hat\\beta \\text{ &amp; } \\hat\\sigma^2$ are independent      3.2. Hypothesis test for $\\beta_j$:  $H_0: \\beta_j = 0$ vs . $H_1: B_j \\not= 0$  Test for partial effect of individual $\\beta_j$  $\\hat\\beta_j \\sim N(\\beta_j,\\sigma^2 v_j)$, where $v_j$ is the j th diagonal element of $(X^TX)^{-1}$$\\Rightarrow \\frac{\\hat\\beta_j - \\beta_j}{\\sigma\\sqrt{v_j}}\\sim N(0,1)$  ($\\leftarrow z=\\frac{x-\\mu}{\\sigma}$)  under $H_0$, $t_j = \\frac{\\hat\\beta_j}{\\hat\\sigma\\sqrt{v_j}}\\sim t_{n-p-1}$  if $ \\left\\vert t_j \\right\\vert &gt; t_{(1-\\alpha/2), n-p-1}$ , we reject $H_0$ with level of significance : $\\alpha$3.2.1. Test for a group of coefficients:  E.g. test for a categorical variable with k levels (k-1 dummy variables)  $H_0: \\beta_{j+1} = \\beta_{j+2} = \\cdots = \\beta{j+l} = 0$  $H_1$: At least one of those is not zero  Full model: $Y=\\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p + \\epsilon$  Reduced model: under $H_0: Y= \\beta_0  + \\beta_1X_1 + \\cdots + \\beta_jX_j + \\beta_{j+l+1}X_{j+l+1} + \\cdots + \\beta_pX_p + \\epsilon$  Test statistic: F = $\\frac{(RSS_R - RSS_F)/ l}{RSS_F/(n-p-1)} \\sim F_{l,n-p-1}$where $RSS_F$ is from full model and $RSS_R$ is from reduced model  if $ F &gt; F_{(1-\\alpha),l,n-p-1}$ we reject $H_0$.3.2.2. Test for all coefficients:  $H_0$: all $\\beta$ coefficients are zero  Reduced model: $Y=\\beta_0 + \\epsilon \\Rightarrow$ LSE $\\hat\\beta_0 =  \\bar y$  Test statistic: $ F = \\frac{(TSS-RSS)/p}{RSS/(n-p-1)}$ where TSS = $\\sum_{i=1}^n(y_i-\\bar y)^2$ = $RSS_R$  if $ F &gt; F_{(1-\\alpha),p,n-p-1}$ we reject $H_0$ : at least 1 coefficient is not zero  Check F-test first before t-tests for each $\\beta_j$  F-test : test for all coef, T-test: test for indiv. coeff  when F-test is significant ($H_1$ for all coeff is true) $\\Rightarrow$ perform t-test but still need to control $\\alpha$but if F-test is not sig $\\Rightarrow$ cant trust t-test result : all coeffs are ZERO3.3. Gauss-Markov Theorem  Assumption: $E(\\epsilon_i) = 0, Var(\\epsilon_i) = \\sigma^2 &lt; \\infty\\text{,  } \\epsilon_i$’s are independent  among all linear unbiased estimator $\\tilde{\\beta} = Cy$ &amp; $E(\\tilde{\\beta}) = \\beta)$Then, $Var(\\hat\\beta) \\le Var(\\tilde{\\beta})$ when  LSE $\\hat\\beta$ = BLUE  G-M theorem says that LSE  $\\hat\\beta$ is the best linear unbiased estimator (BLUE)  Conversely, there may exist biased estimators with smaller MSE than LSE.3.4. Properties of Good estimators  unbiaseness  efficiency(small variance)  consistancy(as n goes infinity, estimator goes true parameter)  sufficiency3.5. Model fit  Model evaluation based on training data  Measure of model fit: $R^2$, $\\scriptstyle\\text{Residual Standard Error(RSE)}$\\(R^2 = 1 - \\frac{RSS}{TSS}\\)which means the proportion of total variation that your model explains\\(RSE = \\sqrt{\\frac{RSS}{n-p-1}} = \\hat\\sigma^2\\)  Problem: As number of predictors increases, $R^2$ increases and always the most complex model selected3.6. Prediction Errors      1) Uncertainty between $\\hat Y$ &amp; $f(X) = X^T\\beta$  Variation due to $\\hat\\beta$ (Model variance):  in ideal situation, we can drive number of training datasets from population and have several f(x) and $\\hat\\beta$  $\\scriptstyle\\text{Confidence interval}$ C.I. for Y:  $E(\\hat Y) = x^T\\beta = f(x)$, $Var(\\hat Y) = \\sigma^2 x^T( X^TX)^{-1}x$  $\\therefore (1-\\alpha)100$% C.I. for Y = $\\hat Y \\pm t_{(1-\\alpha/2, n-p-1)} \\hat\\sigma\\sqrt{x^T(X^TX)^{-1}x}$        2) Model bias:caused by assuming a linear model for f(x).$\\rightarrow$ (1)(2) are reducible error        3) Uncertainty between Y &amp; $\\hat Y$:$\\scriptstyle\\text{Prediction interval}$ P.I. = reducible + irreducible errorfor (new, unseen) test obs. $x_0 = (1, x_{01}, \\ldots, x_{op})^T$,$\\hat Y_0 = x_0^T \\beta$,$Var(\\hat Y_0) =  \\sigma^2 + \\sigma^2 x_0^T( X^TX)^{-1}x_0$ $\\scriptstyle\\text{(irreducible + reducible)}$$\\therefore (1-\\alpha)100$% P.I. for $Y_0$ =  $\\hat Y_0 \\pm t_{(1-\\alpha/2, n-p-1)} \\hat\\sigma\\sqrt{1+x^T(X^TX)^{-1}x}$  3.7. in K-Nearest Neighbor Regression  Nonparameric method: No assumption of f(x)\\(\\hat f(x_0) = \\frac{1}{K}\\sum_{x_i\\in\\mathcal N_k(x_0)} y_i\\)          ($x_i,y_i$): training data      $\\mathcal N_k(x_0)$ : neighborhood of $x_0$        Preferable situations to linear regression:          True f(x) is nonlinear      Goal: Prediction &gt; Inference (interpretation)      Low dimensions (small p)(curse of demensionality)      3.8. Curse of Dimensionality  to capture 10% data for neighbor space from variable X…          p=1 : X range (0,1), edge rank $e_1(0.1) = 0.1$      p=2 : $X_1, X_2$ range (0,1), edge rank $e_2(0.1) = \\sqrt{0.1} = 0.1^{1/2} \\approx 0.316 $      p=3 : $e_3(0.1) = 0.1^{1/3} \\approx 0.464 $      p=10 : $e_{10}(0.1) = 0.1^{1/10} \\approx 0.794 $        Reduction of r : Average using fewer obs. small K : higher variance of the fit, poor prediction  For the smae density, when p=1, n=100 -&gt; when p=10, n = $100^{10}$",
        "url": "/stat_ch3"
    }
    ,
    
    "stat-ch2": {
        "title": "Statistical Mining - Chapter 2",
        "author": "Darron Kwon",
        "category": "",
        "content": "  Chapter 2. Supervised Learning          2.1. Prediction      2.2. Inference      2.3. Estimation F: By Using Training Data      2.4. Parametric Methods: Model-based approach      2.5. Nonparametric Methods: Data-driven approach                  2.5.1. Example of Nonparametric model: KNN Reg.                    2.6. Trade-off between Prediction Accuray &amp; Interpretability      2.7. Model Assessment &amp; Selection                  2.7.1. Mean squared error (MSE)                    2.8. Bias-Variance Trade-off 편향분산 교차                  2.8.1. Ex) Trade-off in KNN Reg.                    Chapter 2. Supervised Learning      Format of Supervised Learning:$ Y=f(X)+\\epsilon $$f$ : Fixed but UNKNOWN function of predictors X$\\rightarrow$ Supervised Learning: A set of methods estimating $f$.        Why do we estimate $f$ ?1) Prediction: Y2) Inference: Relationship between X &amp; Y  2.1. Prediction      $\\text{Prediction of } \\; Y : \\hat{Y} = \\hat{f}(X)\\quad (\\hat{f} : \\text{Estimate of }f) $$\\text{Accuracy of } \\; \\hat{Y} \\text{ depends on reducible &amp; irreducible errors}$        $E[Y-\\hat{Y}]^2 = [f(X)-\\hat{f}(X)]^2 + Var(\\epsilon)$$\\quad (MSE)\\quad\\quad (reducible)\\quad (irreducible)$    Reducible error : controlable, can be reduced by selecting a better model  Irreducible error: uncontrolable, nature of data  2.2. Inference  Characteristics of linear &amp; nonlinear models:1) Linear : Relatively Simple &amp; interpretable inference but poor prediction (when true function $f$ is nonlinear)2) Nonlinear : More accurate but difficult to interpret$\\rightarrow$ Depends on your Goal of analysis2.3. Estimation F: By Using Training Data  Estimation Methods:1) Parametric: Assume specific function $f$ (linear&amp;non- both)2) Nonparametric: No Assumption; Data-driven method (nonlinear)2.4. Parametric Methods: Model-based approach      Step 1: Assume about the functional form of $f$$\\text{e.x) when } X_j = ( x_{1j}, \\ldots , x_{nj} ) ^T : n \\ast 1$ obs.; $\\;$ vector for the $j^{th}$ predictor$\\rightarrow linear: f(X)= \\beta_0 + \\beta_1X_1 + … + \\beta_pX_p$        Step 2: Estimate Parameters$Y \\approx \\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + … + \\hat{\\beta}_pX_p = \\hat{f}(X)$    Disadvantage: Specific form of $f$$\\rightarrow\\;$ Not match with the true $f$: poor estimation &amp; poor prediction$\\rightarrow\\;$ Flextible parametric model : model with more parameters          Flexible model $\\equiv$ Complex modelbut overfitting problem still remains: model follows errors or noises        Underfitting vs Overfittingunder: estimation has not enough accuracy, miss to capture the true structureover: more complexity than true $f$2.5. Nonparametric Methods: Data-driven approach  No assumptions about functional form of $f$$\\Rightarrow $ wider range of possible shapes of $f$  Disadvantage:          1) A vary large # of obs. is required to get an accurate $f$(relative to a parametric method)      2) Overfitting $\\Leftrightarrow$ Level of smoothness; $\\;$ Model complexity(how to determine it?)      2.5.1. Example of Nonparametric model: KNN Reg.  Idea: Similar inputs have Similar outputStep: when predicting $\\hat{Y}_0$ with input $X_0$          1) Determine $K$ (level of smoothness)              if K==N: Y = Var(Y)if K==1: Y = perfect fit; no training error                    2) find $K$ closest training obs. from the target input point($X_0$) using Euclidean distance\\(\\rightarrow\\hat{Y}_0 = \\frac{1}{K}\\sum_{x_i\\in \\mathcal N}^K y_i\\)            3) average of these #k Y values = $\\hat{Y}_0$        Effect of $K$ (tuning parameter/ model flexibility parameter)          decides the level of smoothness (under or overfitting)      As $K \\uparrow \\; \\Rightarrow \\text{flexibility} \\downarrow$ : the fitted line is simple      As $K \\downarrow \\; \\Rightarrow \\text{flexibility} \\uparrow$ : the fitted line is wiggly      2.6. Trade-off between Prediction Accuray &amp; Interpretability  Restrictive model(Simple model) vs. Flexible model  our goal:inference ($\\rightarrow$ restrictive model)prediction ($\\rightarrow$ flexible model)2.7. Model Assessment &amp; Selection  Evaluate the performance of models on a given dataset  and Select the best model  Criterion: Better prediction of $Y$2.7.1. Mean squared error (MSE)  training MSE(used for building model) is not our interest; cannot be a measure for model selection          e.g.)\\(\\\\ R^2 = 1 - \\frac{RSS}{TSS}\\)training RSS = \\(\\sum(y_i - \\hat{y_i})^2  \\quad \\text{&amp;} \\quad \\text{MSE} =\\frac{RSS}{N}\\\\\\)when more X variables added, $\\;$ RSS &amp; MSE $\\downarrow$ , $R^2 \\uparrow$even X are not important variablescomplex model has smaller training error regardless of existence of test data            Test MSE \\(= \\frac{1}{m}\\sum_{i=1}^m(y_i^0 - \\hat{f}(x_i^0))^2\\)Models with low test MSE are better.$\\rightarrow$ How to find optimal Flexibility?$\\rightarrow$ Test MSE(error rate) is always larger than Training MSE; We estimated model function $f$ to minimize its training error$\\rightarrow$ No test data: Sample re-use methods (e.g., bootstrap, cross-validation)    Classification problem: Misclassification rate2.8. Bias-Variance Trade-off 편향분산 교차  Expected test MSE(ideal measure): $E[y_0-\\hat{f}(x_0)]^2$ when $((x_0,y_0)$ is a test obs.      in population: #K training sets &amp; #K function $f_k$Estimation of expected test MSE:  \\(\\\\ \\hat{E}[y_0 - \\hat{f}(x_0)]^2 = \\frac{1}{K} \\sum_{k=1}^K[y_0-\\hat{f}_k(x_0)]^2\\)        For given $x_0$,  (when $\\epsilon$ is irreducible error) \\(\\\\ \\begin{align*}\\hat{E}[y_0 - \\hat{f}(x_0)]^2 &amp;= Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon) \\\\                      &amp;= E [ \\{ y_0 -E(\\hat{f}(x_0)) \\} - \\{ \\hat{f}(x_0) - E(\\hat{f}(x_0)) \\} ]^2 \\\\          (1) \\cdots\t&amp;= E [  y_0 -E(\\hat{f}(x_0)) ] ^2 \\\\          (2) \\cdots\t&amp; \\; + E [\\hat{f}(x_0) - E(\\hat{f}(x_0)) ] ^2 \\\\          (3) \\cdots\t&amp; \\; - 2E[ \\{ y_0 -E(\\hat{f}(x_0)) \\} \\{ \\hat{f}(x_0) - E(\\hat{f}(x_0)) \\} ]\\end{align*}\\)        (1) \\(\\\\ \\begin{align*}   &amp;= E [(f(x_0) + \\epsilon - E(\\hat{f}(x_0)) ] ^2 \\\\  &amp;= E[ f(x_0)^2 +\\epsilon^2 + [E(\\hat{f}(x_0)) ]^2 + 2f(x_0)\\epsilon -2f(x_0)E(\\hat{f}(x_0)) - 2\\epsilon E(\\hat{f}(x_0)) ] \\\\  &amp; \\quad \\scriptstyle\\text{when } E(\\epsilon) \\text{ goes to } 0 \\\\  &amp;= E[E(\\hat{f}(x_0)) - f(x_0)] ^2 + E(\\epsilon^2)  \\\\  &amp; \\quad \\scriptstyle{E(\\hat{f}(x_0)) - f(x_0) \\text{is a Bias}} \\\\  &amp;= [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon^2)\\end{align*}\\)    $(2) = Var(\\hat{f}(x_0)) \\leftarrow $ model variance$\\scriptstyle\\text{different fit’s from randomness of training sets}$  $(3) = 0$$\\therefore$ Expected test MSE is consisted of Reducible Error(Var+ Bias$^2$) and Irreducible Error(Var($\\epsilon$))      $Var(\\hat{f(x_0})$ : model variance        $ [Bias(\\hat{f(x_0})]$ : Systematic model error ( caused by model assumptions; e.g. true f: nonlinear)        $ Var(\\epsilon)$ : Irreducible error  Model Flextibility $\\uparrow$ $\\Rightarrow$ model variance $\\uparrow$ &amp; model bias $\\downarrow$Model Flextibility $\\downarrow$ $\\Rightarrow$ model variance $\\downarrow$ &amp; model bias $\\uparrow$$\\rightarrow$ optimal model flextibility is different for each datasets2.8.1. Ex) Trade-off in KNN Reg.  given) Y = f (X) + $\\epsilon$ with E($\\epsilon$) = 0 and Var ($\\epsilon$) = $\\sigma^2$      Expected test MSE at $x_0$: \\(\\\\E[ (Y- \\hat{f_k}(x_0) )^2 | X = x_0 ] \\\\= \\sigma^2 + Bias^2(\\hat{f_k} (x_0)) + Var (\\hat{f_k} (x_0)) \\\\\\rightarrow Bias(\\hat{f_k} (x_0)) =  f(x_0) - E(\\hat{f}(x_0))   \\text{ : given \"f\" is KNN func.}\\\\ \\quad= { f(x_0) - E[\\frac1 K \\sum_{i \\in \\mathcal N (x_0)} ^ K Y_i ] }  \\\\\\quad= { f(x_0)- \\frac 1 K \\sum E (f(x_i)+\\epsilon_i)}  \\\\\\quad= f(x_0) - \\frac 1 K \\sum E (f(x_i)  \\\\\\rightarrow Var(\\hat{f_k} (x_0)) = Var(\\frac 1 K \\sum_{i \\in \\mathcal N (x_0)} Yi )\\\\\\quad= \\frac 1 {K^2} \\sum  Var( Yi ) \\\\\\quad=  \\frac 1 {K^2} \\sum Var( f( x_i) + \\epsilon_i ) \\\\\\quad=  \\frac 1 {K^2} \\sum \\sigma^2 = \\frac 1 {K^2} * K * \\sigma^2 = \\frac {\\sigma^2} K\\)\\(\\therefore\\) Expected test MSE at \\(x_0 = \\sigma ^2 + [f(x_0) - \\frac 1 K \\sum_{x_i \\in \\mathcal N (x_0)}^k f(x_i) ]^2 + \\frac {\\sigma^2} K\\)        when we use large number of K :as K increases, model complexity and model variance decreases, then bias increases;which is a simpler model        Using Misclassification rate \\(= \\frac 1 N \\sum_{i=1}^n I(y_i \\not = \\hat y_i )\\)(in Bayes Classifier &amp; KNN Classifier)\\(min_\\hat Y E( I(Y \\not = \\hat Y) \\\\= min_\\hat Y E[ E(I(Y \\not = \\hat Y | X) ] \\\\= min_{\\hat f (x)} E[ \\sum_{g=1}^G I( Y= g\\not= \\hat f (x) ) P ( Y= g| X) ] \\\\\\equiv min_{\\hat f (x)} \\sum_{g=1}^G I ( Y=g \\not= \\hat f (x) ) P ( Y= g| X) \\\\\\Rightarrow \\hat f (x) = min_g \\sum_{g=1}^G I ( Y=g \\not= \\hat f (x) ) P ( Y= g| X) \\\\\\quad \\equiv min_g ( 1- P ( Y= g| X) )\\)    E.g)in 3 groups -&gt; Y = 1, 2, 3P(Y=1 | X) = 0.3 , P(Y=2 | X) = 0.5, P(Y=3 | X) =0.2$\\hat f(x) = 1 \\Rightarrow$ EPE(expected prediction error; miss classification rate) = 0 * 0.3 + 1 * 0.5 + 1 * 0.2 = 0.7$\\hat f(x) = 2 \\Rightarrow$ EPE = 0.5 : min value$\\hat f(x) = 3 \\Rightarrow$ EPE = 0.8\\(\\therefore min_g ( 1- P ( Y= g| X) ) \\\\\\quad  \\equiv max_g P(Y = g | X)\\)Bayes error rate at X = $x_0$; $\\quad 1 - max_g P(Y=g|X=x_0)$Overall Bayes error rate; $\\quad 1 - E[ max_g P(Y=g | X= x) ]$",
        "url": "/stat_ch2"
    }
    
    
    };
</script>
<script src="assets/js/lunr.js"></script>
<script src="assets/js/search.js"></script>
            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Darron's Devlog</a> &copy; 2021</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search Darron's Devlog</h1>
                <p class="subscribe-overlay-description">
				lunr.js를 이용한 posts 검색 </p>
                <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

 </script>

	
    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
