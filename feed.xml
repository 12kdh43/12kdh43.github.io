<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2022-03-15T10:20:18+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Darron’s Devlog</title><entry><title type="html">cs224n - Lecture 4. Dependency Parsing</title><link href="http://0.0.0.0:4000/cs224n_lec4" rel="alternate" type="text/html" title="cs224n - Lecture 4. Dependency Parsing" /><published>2022-03-11T00:00:00+00:00</published><updated>2022-03-11T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec4</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec4">&lt;h3 id=&quot;two-views-of-linguistic-structure-phrase-structure&quot;&gt;Two views of linguistic structure: Phrase structure&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Constituency = phrase structure grammar = context-free grammers(CFGs)&lt;br /&gt;
  &lt;strong&gt;Phrase structure&lt;/strong&gt; organizes words into nested constituents&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Starting unit: &lt;strong&gt;words&lt;/strong&gt; (noun, preposition, adjective, determiner, …)&lt;br /&gt;
  the, $\ $ cat, $\ $ cuddly, $\ $ by, $\ $ door&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Words combine into &lt;strong&gt;phrases&lt;/strong&gt;&lt;br /&gt;
  the cuddly cat(noun phrase),&lt;br /&gt;
  by the door(prepositional phrase; preposition(by) + noun phrase)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Phrases can combine into bigger phrases&lt;br /&gt;
  the cuddly cat by the door(noun phrase)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Lexicon:&lt;br /&gt;
  $\text{N} \rightarrow \text{cat}$&lt;br /&gt;
  $\text{N} \rightarrow \text{door}$&lt;br /&gt;
  $\text{Det} \rightarrow \text{the}$&lt;br /&gt;
  $P \rightarrow \text{by}$&lt;br /&gt;
  $\text{Adj} \rightarrow \text{cuddly}$&lt;br /&gt;
  $\vdots$&lt;/li&gt;
  &lt;li&gt;Grammar:&lt;br /&gt;
  $\text{NP} \rightarrow \text{Det } \  \text{ (Adj)}^{\ast} \  \text{ N } \  \text{ (PP)}$&lt;br /&gt;
  $\text{PP} \rightarrow \text{P } \  \text{ NP}$&lt;br /&gt;
  $\vdots$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;two-views-of-linguistic-structure-dependency-structure&quot;&gt;Two views of linguistic structure: Dependency structure&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Dependency structure shows which words depend on (modify, attach to, or are arguments of) which other words.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;why-do-we-need-sentence-structure&quot;&gt;Why do we need sentence structure?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Humans communicate complex ideas by composing words together into bigger units to convey complex meanings&lt;/li&gt;
  &lt;li&gt;Listeners need to work out what modifies &lt;em&gt;attaches to&lt;/em&gt; what&lt;/li&gt;
  &lt;li&gt;A model needs to understand sentence structure in order to be able to interpret language correctly&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ambiguities&quot;&gt;Ambiguities&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Prepositional phrase ambiguity
    &lt;ul&gt;
      &lt;li&gt;A key parsing decision is how we ‘attach’ various constituents
        &lt;ul&gt;
          &lt;li&gt;&lt;em&gt;PP&lt;/em&gt;s, adverbial or participial phrases, infinitives, coordinations&lt;br /&gt;
  e.g.&lt;br /&gt;
  \(\begin{align*}
  \text{The board approved [its acquisition]} &amp;amp; \text{[by Royal Trustco Ltd.]} \\
                                      &amp;amp; \text{[of Toronto]} \\
                                      &amp;amp; \text{[for \$27 a share]} \\
                                      &amp;amp; \text{[at its monthly meeting].}
  \end{align*}\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;With a sentence of &lt;em&gt;k&lt;/em&gt; prepositional phrases at the end of it, the number of parses is given by the Catalan numbers; $C_n = (2n)!/[(n+1)!n!]$, an exponential series growing as the number of prepositional phrases.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Coordination scopre ambiguity&lt;br /&gt;
  e.g. &lt;strong&gt;Shuttle veteran&lt;/strong&gt; and longtime NASA executive__ Fred Gregory appointed to board&lt;br /&gt;
      $\rightarrow$ 1 or 2 person?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adjectival/Adverbial Modifier ambiguity&lt;br /&gt;
  e.g. Students get &lt;strong&gt;first hand job experience&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Verb Phrase(VP) attachment ambiguity&lt;br /&gt;
  e.g. Mutilated body washes up on Rio beach &lt;strong&gt;to be used for Olympics beach volleyball&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dependency-paths-help-extract-semantic-interpretation&quot;&gt;Dependency paths help extract semantic interpretation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;simple practical example: extracting protein-protein interaction&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec4_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;dependency-grammar-and-dependency-structure&quot;&gt;Dependency Grammar and Dependency Structure&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Dependency syntax postulates that syntactic structure consists of relations between lexical items, normally binary asymmetric relations (“arrows”) called &lt;strong&gt;dependencies&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec4_1.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The arrows are commonly &lt;strong&gt;typed&lt;/strong&gt; with the name of grammatical relations (subject, prepositional object, apposition, etc.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;An arrow connects a &lt;strong&gt;head&lt;/strong&gt;(governor, superior, regent) with a &lt;strong&gt;dependent&lt;/strong&gt;(modifier, inferior, subordinate)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Usually, dependencies form a tree(a connected, acyclic, single-root graph)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Check: some people draw the arrows one way; some the other way&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Usually add a fake ROOT so every word is a dependent of precisely 1 other node&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-rise-of-annotated-data--universal-dependencies-treebanks&quot;&gt;The rise of annotated data &amp;amp; Universal Dependencies treebanks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Advantages of treebank
    &lt;ul&gt;
      &lt;li&gt;Reusability of the labor
        &lt;ul&gt;
          &lt;li&gt;Many parsers, part-of-speech taggers, etc. can be built on it&lt;/li&gt;
          &lt;li&gt;Valuable resource for linguistic&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Broad coverage, not just a few intuitions&lt;/li&gt;
      &lt;li&gt;Frequencies and distributional information(statistics)&lt;/li&gt;
      &lt;li&gt;A way to evaluate NLP systems&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dependency-conditioning-preferences&quot;&gt;Dependency Conditioning Preferences&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec4_2.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The sources of information for dependency parsing
    &lt;ol&gt;
      &lt;li&gt;Bilexical affinities: The dependency $\text{discussion}\rightarrow\text{issues}$ is plausible&lt;/li&gt;
      &lt;li&gt;Dependency distance: Most dependencies are between nearby words&lt;/li&gt;
      &lt;li&gt;Intervening material: Dependencies rarely span intervening verbs or punctuation&lt;/li&gt;
      &lt;li&gt;Valency of heads: How many dependents on which side are usual for a head?&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dependency-parsing&quot;&gt;Dependency Parsing&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A sentence is parsed by choosing for each word what other word (including ROOT) it is a dependent of&lt;/li&gt;
  &lt;li&gt;Usually some constraints:
    &lt;ul&gt;
      &lt;li&gt;Only one word is a dependent of ROOT&lt;/li&gt;
      &lt;li&gt;Don’t want cycles $A\rightarrow B$, $B\rightarrow A$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;This makes the dependencies a tree&lt;/li&gt;
  &lt;li&gt;Final issue is whether arrows can cross(be &lt;strong&gt;non-projective&lt;/strong&gt;) or not&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;projectivity&quot;&gt;Projectivity&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Definition of a &lt;strong&gt;projective parse&lt;/strong&gt;: There are no crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words&lt;/li&gt;
  &lt;li&gt;Dependencies corresponding to a CFG tree must be &lt;strong&gt;projective&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;i.e., by forming dependencies by taking 1 child of each category as head&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Most syntactic structure is projective like this, but dependency theory normally does allow non-projective structures to account for displaced constituents
    &lt;ul&gt;
      &lt;li&gt;You can’t easily get the semantics of certain constructions right without these nonprojective dependencies&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;e.g.&lt;br /&gt;
  &lt;strong&gt;From&lt;/strong&gt; who did Bill buy the coffee yesterday?&lt;br /&gt;
  Who did Bill buy the coffee &lt;strong&gt;from&lt;/strong&gt; yesterday&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;methods-of-dependency-parsing&quot;&gt;Methods of Dependency Parsing&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Dynamic programming, &lt;em&gt;Eisner(1996)&lt;/em&gt;: $O(n^3)$ complexity&lt;/li&gt;
  &lt;li&gt;Graph algorithms, &lt;em&gt;McDonald et al.(2005)&lt;/em&gt;: creating a Minimun Spanning Tree&lt;/li&gt;
  &lt;li&gt;Constraint Satisfaction, &lt;em&gt;Karlsson(1990)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;“Transition-based parsing” or “deterministic dependency parsing”&lt;br /&gt;
 Greedy choice of attachments guided by good machine learning classifiers&lt;br /&gt;
 E.g., MaltParser, &lt;em&gt;Nivre et al.(2008)&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;greedy-transition-based-parsing-nivre-2003&quot;&gt;Greedy transition-based parsing, Nivre 2003&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A simple form of greedy discriminative dependency parser&lt;/li&gt;
  &lt;li&gt;The parser does a sequence of bottom-up actions
    &lt;ul&gt;
      &lt;li&gt;Roughly like “shift” or “reduce” in a shift-reduce parser, but the “reduce” actions are specialized to create dependencies with head on left or right&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The parser has:
    &lt;ul&gt;
      &lt;li&gt;a stack $\sigma$, written with top to the right
        &lt;ul&gt;
          &lt;li&gt;which starts with the ROOT symbol&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;a buffer $\beta$, written with top to the left
        &lt;ul&gt;
          &lt;li&gt;which starts with the input sentence&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;a set of dependency arcs A
        &lt;ul&gt;
          &lt;li&gt;which starts off empty&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;a set of actions&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;basic-transition-based-dependency-parser&quot;&gt;Basic transition-based dependency parser&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec4_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Arc-standard transition-based parser&lt;br /&gt;
  E.g., Analysis of “I ate fish”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec4_4.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec4_5.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;maltparser-nivre-and-hall-2005&quot;&gt;MaltParser, Nivre and Hall 2005&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;How we choose the next action?&lt;br /&gt;
  Answer: Machine Learning!&lt;/li&gt;
  &lt;li&gt;Each action is predicted by a discriminative classifier (e.g., softmax classifier) over legal move
    &lt;ul&gt;
      &lt;li&gt;Max of 3 untyped choices; max of $\lvert R \rvert \times 2 + 1 $ when typed&lt;/li&gt;
      &lt;li&gt;Features: top of stack word, POS; first in buffer word, POS; etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;There is NO search(in the simplest form)
    &lt;ul&gt;
      &lt;li&gt;But you can profitably do a beam search if you wish(slower but better): You keep &lt;em&gt;k&lt;/em&gt; good parse prefixes at each time step&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The model’s accuracy is &lt;em&gt;fractionally&lt;/em&gt; below the state of the art in dependency parsing, but it provides &lt;strong&gt;very fast linear time parsing&lt;/strong&gt;, with high accuracy, great for parsing the web&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conventional-feature-representation&quot;&gt;Conventional Feature Representation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec4_6.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;evaluation-of-dependency-parsing-labeled-dependency-accuracy&quot;&gt;Evaluation of Dependency Parsing: (labeled) dependency accuracy&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec4_7.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">Two views of linguistic structure: Phrase structure Constituency = phrase structure grammar = context-free grammers(CFGs) Phrase structure organizes words into nested constituents Starting unit: words (noun, preposition, adjective, determiner, …) the, $\ $ cat, $\ $ cuddly, $\ $ by, $\ $ door Words combine into phrases the cuddly cat(noun phrase), by the door(prepositional phrase; preposition(by) + noun phrase) Phrases can combine into bigger phrases the cuddly cat by the door(noun phrase) Lexicon: $\text{N} \rightarrow \text{cat}$ $\text{N} \rightarrow \text{door}$ $\text{Det} \rightarrow \text{the}$ $P \rightarrow \text{by}$ $\text{Adj} \rightarrow \text{cuddly}$ $\vdots$ Grammar: $\text{NP} \rightarrow \text{Det } \ \text{ (Adj)}^{\ast} \ \text{ N } \ \text{ (PP)}$ $\text{PP} \rightarrow \text{P } \ \text{ NP}$ $\vdots$ Two views of linguistic structure: Dependency structure Dependency structure shows which words depend on (modify, attach to, or are arguments of) which other words. Why do we need sentence structure? Humans communicate complex ideas by composing words together into bigger units to convey complex meanings Listeners need to work out what modifies attaches to what A model needs to understand sentence structure in order to be able to interpret language correctly Ambiguities Prepositional phrase ambiguity A key parsing decision is how we ‘attach’ various constituents PPs, adverbial or participial phrases, infinitives, coordinations e.g. \(\begin{align*} \text{The board approved [its acquisition]} &amp;amp; \text{[by Royal Trustco Ltd.]} \\ &amp;amp; \text{[of Toronto]} \\ &amp;amp; \text{[for \$27 a share]} \\ &amp;amp; \text{[at its monthly meeting].} \end{align*}\) With a sentence of k prepositional phrases at the end of it, the number of parses is given by the Catalan numbers; $C_n = (2n)!/[(n+1)!n!]$, an exponential series growing as the number of prepositional phrases. Coordination scopre ambiguity e.g. Shuttle veteran and longtime NASA executive__ Fred Gregory appointed to board $\rightarrow$ 1 or 2 person? Adjectival/Adverbial Modifier ambiguity e.g. Students get first hand job experience Verb Phrase(VP) attachment ambiguity e.g. Mutilated body washes up on Rio beach to be used for Olympics beach volleyball Dependency paths help extract semantic interpretation simple practical example: extracting protein-protein interaction Dependency Grammar and Dependency Structure Dependency syntax postulates that syntactic structure consists of relations between lexical items, normally binary asymmetric relations (“arrows”) called dependencies The arrows are commonly typed with the name of grammatical relations (subject, prepositional object, apposition, etc.) An arrow connects a head(governor, superior, regent) with a dependent(modifier, inferior, subordinate) Usually, dependencies form a tree(a connected, acyclic, single-root graph) Check: some people draw the arrows one way; some the other way Usually add a fake ROOT so every word is a dependent of precisely 1 other node The rise of annotated data &amp;amp; Universal Dependencies treebanks Advantages of treebank Reusability of the labor Many parsers, part-of-speech taggers, etc. can be built on it Valuable resource for linguistic Broad coverage, not just a few intuitions Frequencies and distributional information(statistics) A way to evaluate NLP systems Dependency Conditioning Preferences The sources of information for dependency parsing Bilexical affinities: The dependency $\text{discussion}\rightarrow\text{issues}$ is plausible Dependency distance: Most dependencies are between nearby words Intervening material: Dependencies rarely span intervening verbs or punctuation Valency of heads: How many dependents on which side are usual for a head? Dependency Parsing A sentence is parsed by choosing for each word what other word (including ROOT) it is a dependent of Usually some constraints: Only one word is a dependent of ROOT Don’t want cycles $A\rightarrow B$, $B\rightarrow A$ This makes the dependencies a tree Final issue is whether arrows can cross(be non-projective) or not Projectivity Definition of a projective parse: There are no crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words Dependencies corresponding to a CFG tree must be projective i.e., by forming dependencies by taking 1 child of each category as head Most syntactic structure is projective like this, but dependency theory normally does allow non-projective structures to account for displaced constituents You can’t easily get the semantics of certain constructions right without these nonprojective dependencies e.g. From who did Bill buy the coffee yesterday? Who did Bill buy the coffee from yesterday Methods of Dependency Parsing Dynamic programming, Eisner(1996): $O(n^3)$ complexity Graph algorithms, McDonald et al.(2005): creating a Minimun Spanning Tree Constraint Satisfaction, Karlsson(1990) “Transition-based parsing” or “deterministic dependency parsing” Greedy choice of attachments guided by good machine learning classifiers E.g., MaltParser, Nivre et al.(2008) Greedy transition-based parsing, Nivre 2003 A simple form of greedy discriminative dependency parser The parser does a sequence of bottom-up actions Roughly like “shift” or “reduce” in a shift-reduce parser, but the “reduce” actions are specialized to create dependencies with head on left or right The parser has: a stack $\sigma$, written with top to the right which starts with the ROOT symbol a buffer $\beta$, written with top to the left which starts with the input sentence a set of dependency arcs A which starts off empty a set of actions Basic transition-based dependency parser Arc-standard transition-based parser E.g., Analysis of “I ate fish” MaltParser, Nivre and Hall 2005 How we choose the next action? Answer: Machine Learning! Each action is predicted by a discriminative classifier (e.g., softmax classifier) over legal move Max of 3 untyped choices; max of $\lvert R \rvert \times 2 + 1 $ when typed Features: top of stack word, POS; first in buffer word, POS; etc. There is NO search(in the simplest form) But you can profitably do a beam search if you wish(slower but better): You keep k good parse prefixes at each time step The model’s accuracy is fractionally below the state of the art in dependency parsing, but it provides very fast linear time parsing, with high accuracy, great for parsing the web Conventional Feature Representation Evaluation of Dependency Parsing: (labeled) dependency accuracy</summary></entry><entry><title type="html">cs224n - Lecture 3. Backprop and Neural Networks</title><link href="http://0.0.0.0:4000/cs224n_lec3" rel="alternate" type="text/html" title="cs224n - Lecture 3. Backprop and Neural Networks" /><published>2022-03-08T00:00:00+00:00</published><updated>2022-03-08T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec3</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec3">&lt;h3 id=&quot;named-entity-recognitionner&quot;&gt;Named Entity Recognition(NER)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Task: &lt;strong&gt;find&lt;/strong&gt; and &lt;strong&gt;classify&lt;/strong&gt; names in text&lt;/li&gt;
  &lt;li&gt;Possible uses:
    &lt;ul&gt;
      &lt;li&gt;Tracking mentions of particular entities in documents&lt;/li&gt;
      &lt;li&gt;For question answering, answers are usually named entities&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Often followed by Named Entity Linking/Canonicalization into Knowledge Base&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;simple-ner-window-classification-using-binary-logistic-classifier&quot;&gt;Simple NER: Window classification using binary logistic classifier&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using word vectors, build a context window of word vectors, then pass through a neural network and feed it to logistic classifier.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;idea: classify each word in its context window of neighboring words&lt;/li&gt;
  &lt;li&gt;Train logistic classifier on hand-labeled data to classify center word {yes/no} for each class based on a concatenation of word vectors in a window
    &lt;ul&gt;
      &lt;li&gt;In practice, we usually use multi-class softmax&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Example: Classify “Paris” as +/- location(binary for is_location) in context of sentence with window length 2:&lt;br /&gt;
  sentence $=$ “$\mbox{ the  museums  in  Paris  are  amazing  to  see   . }$”
  $x_{\text{window}} = [x_{\text{museums}}\quad x_{\text{in}}\quad x_{\text{Paris}}\quad x_{\text{are}}\quad x_{\text{amazing}}]^T$&lt;/li&gt;
  &lt;li&gt;Resulting vector $x_{\text{window}} = x\in \mathbb{R}^{5d}$, a column vector.&lt;/li&gt;
  &lt;li&gt;To classify all words: run classifier for each class on the vector centered on each word in the sentence&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec3_0.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;computing-gradients-by-hand&quot;&gt;Computing Gradients by Hand&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Matrix calculus: Fully vectorized gradients
    &lt;ul&gt;
      &lt;li&gt;“Multivariable calculus is just like single-variable calculus if you use matrices”&lt;/li&gt;
      &lt;li&gt;Much faster and more usful than non-vectorized gradients&lt;/li&gt;
      &lt;li&gt;But doing a non-vectorized gradient can be good for intuition; partial derivative for one parameter&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Vector gradient&lt;br /&gt;
  Given a function with &lt;em&gt;1&lt;/em&gt; output and &lt;em&gt;n&lt;/em&gt; inputs&lt;br /&gt;
  $f(x) = f(x_1, x_2, \ldots, x_n)$&lt;br /&gt;
  Its gradient is a vector of partial derivatives w.r.t each input&lt;br /&gt;
  \(\frac{\partial f}{\partial x} = \left[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n} \right]\)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jacobian Matrix: Generalization of the Gradient&lt;br /&gt;
  Given a function with &lt;strong&gt;&lt;em&gt;m&lt;/em&gt; outputs&lt;/strong&gt; and &lt;em&gt;n&lt;/em&gt; inputs&lt;br /&gt;
  $f(x) = [f_1(x_1, x_2, \ldots, x_n), \ldots, f_m(x_1, x_2, \ldots, x_n)]$&lt;br /&gt;
  It’s Jacobian is an &lt;strong&gt;$m \times n$&lt;/strong&gt; matrix of partial derivatives&lt;br /&gt;
  \(\begin{align*}
  \frac{\partial f}{\partial x} = 
  \begin{bmatrix}
  \frac{\partial f_1}{\partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\
  \vdots &amp;amp; \ddots &amp;amp; \vdots \\
  \frac{\partial f_m}{\partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial f_m}{\partial x_n}
  \end{bmatrix}
  \end{align*}\) $\quad\quad$ \(\begin{align*}\left( \frac{\partial f}{\partial x} \right)_{ij} = \frac{\partial f_i}{\partial x_j}\end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Chain Rule
    &lt;ul&gt;
      &lt;li&gt;For composition of one-variable functions: &lt;strong&gt;multiply derivatives&lt;/strong&gt;&lt;br /&gt;
  $z = g(y)$, $y = f(x)$&lt;br /&gt;
  \(\begin{align*}\frac{dz}{dx} = \frac{dz}{dy}\frac{dy}{dx}\end{align*}\)&lt;/li&gt;
      &lt;li&gt;For multiple variables at once: &lt;strong&gt;multiply Jacobians&lt;/strong&gt;&lt;br /&gt;
  $h = f(z)$&lt;br /&gt;
  $z = Wx + b$&lt;br /&gt;
  \(\begin{align*}\frac{\partial h}{\partial x} = \frac{\partial h}{\partial z}\frac{\partial z}{\partial x} = \cdots\end{align*}\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Example Jacobian: Elementwise activation function&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec3_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Other Jacobians&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec3_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Write out the Jacobians&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec3_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Derivative with respect to Matrix: Output shape&lt;br /&gt;
  for $\frac{\partial s}{\partial W}$, output $s$ is a scalar and input $W \in \mathbb{R}^{n\times m}$: &lt;em&gt;1&lt;/em&gt; by &lt;em&gt;nm&lt;/em&gt; Jacobian?&lt;br /&gt;
  $\rightarrow$ is inconvenient to do \(\theta^{\text{new}} = \theta^{\text{old}} - \alpha\nabla_\theta J(\theta)\) gradient descent.
    &lt;ul&gt;
      &lt;li&gt;Instead, use the &lt;strong&gt;shape convention&lt;/strong&gt;: the shape of the gradient is the shape of the parameters&lt;br /&gt;
  &lt;em&gt;n&lt;/em&gt; by &lt;em&gt;m&lt;/em&gt; matrix \(\begin{align*}\frac{\partial s}{\partial W} = 
  \begin{bmatrix} 
  \frac{\partial s}{\partial W_{11}} &amp;amp; \cdots &amp;amp; \frac{\partial s}{\partial W_{1m}} \\ 
  \vdots &amp;amp; \ddots &amp;amp; \vdots \\
  \frac{\partial s}{\partial W_{n1}} &amp;amp; \cdots &amp;amp; \frac{\partial s}{\partial W_{nm}}
  \end{bmatrix}\end{align*}\)&lt;/li&gt;
      &lt;li&gt;$\frac{\partial s}{\partial W} = \delta\frac{\partial z}{\partial W}$&lt;br /&gt;
  $z = Wx + b$&lt;br /&gt;
  \(\begin{align*}
  \frac{\partial s}{\partial W} &amp;amp;= \delta^T x^T \\
  &amp;amp; = \begin{bmatrix} \delta_1 \\ \vdots \\ \delta_n \end{bmatrix} [x_1, \ldots, x_m] = 
  \begin{bmatrix} \delta_1 x_1 &amp;amp; \cdots &amp;amp; \delta_1 x_m \\
  \vdots &amp;amp; \ddots &amp;amp; \vdots \\
  \delta_n x_1 &amp;amp; \cdots &amp;amp; \delta_n x_m \end{bmatrix}
  \end{align*}\)&lt;br /&gt;
  denote that $\delta$ is local error signal at &lt;em&gt;z&lt;/em&gt; and &lt;em&gt;x&lt;/em&gt; is local input signal.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-shape-should-derivatives-be&quot;&gt;What shape should derivatives be?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Two options:
    &lt;ol&gt;
      &lt;li&gt;Use Jacobian form as much as possible, reshape to follow the shape convention at the end:
        &lt;ul&gt;
          &lt;li&gt;at the end transpose $\frac{\partial s}{\partial b}$ to make the derivative a column vector, resulting in $\delta^T$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Always follow the shape convention
        &lt;ul&gt;
          &lt;li&gt;Look at dimensions to figure out when to transpose and/or reorder terms&lt;/li&gt;
          &lt;li&gt;The error message $\delta$ that arrives at a hidden layer has the same dimensionality as that hidden layer&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;backpropagation-chain-rule&quot;&gt;Backpropagation: Chain Rule&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Other trick:&lt;br /&gt;
  &lt;strong&gt;re-use&lt;/strong&gt; derivatives computed for higher layers in computing derivatives for lower layers to minimize computation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec4_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">Named Entity Recognition(NER) Task: find and classify names in text Possible uses: Tracking mentions of particular entities in documents For question answering, answers are usually named entities Often followed by Named Entity Linking/Canonicalization into Knowledge Base Simple NER: Window classification using binary logistic classifier Using word vectors, build a context window of word vectors, then pass through a neural network and feed it to logistic classifier. idea: classify each word in its context window of neighboring words Train logistic classifier on hand-labeled data to classify center word {yes/no} for each class based on a concatenation of word vectors in a window In practice, we usually use multi-class softmax Example: Classify “Paris” as +/- location(binary for is_location) in context of sentence with window length 2: sentence $=$ “$\mbox{ the museums in Paris are amazing to see . }$” $x_{\text{window}} = [x_{\text{museums}}\quad x_{\text{in}}\quad x_{\text{Paris}}\quad x_{\text{are}}\quad x_{\text{amazing}}]^T$ Resulting vector $x_{\text{window}} = x\in \mathbb{R}^{5d}$, a column vector. To classify all words: run classifier for each class on the vector centered on each word in the sentence Computing Gradients by Hand Matrix calculus: Fully vectorized gradients “Multivariable calculus is just like single-variable calculus if you use matrices” Much faster and more usful than non-vectorized gradients But doing a non-vectorized gradient can be good for intuition; partial derivative for one parameter Vector gradient Given a function with 1 output and n inputs $f(x) = f(x_1, x_2, \ldots, x_n)$ Its gradient is a vector of partial derivatives w.r.t each input \(\frac{\partial f}{\partial x} = \left[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n} \right]\) Jacobian Matrix: Generalization of the Gradient Given a function with m outputs and n inputs $f(x) = [f_1(x_1, x_2, \ldots, x_n), \ldots, f_m(x_1, x_2, \ldots, x_n)]$ It’s Jacobian is an $m \times n$ matrix of partial derivatives \(\begin{align*} \frac{\partial f}{\partial x} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \frac{\partial f_m}{\partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial f_m}{\partial x_n} \end{bmatrix} \end{align*}\) $\quad\quad$ \(\begin{align*}\left( \frac{\partial f}{\partial x} \right)_{ij} = \frac{\partial f_i}{\partial x_j}\end{align*}\) Chain Rule For composition of one-variable functions: multiply derivatives $z = g(y)$, $y = f(x)$ \(\begin{align*}\frac{dz}{dx} = \frac{dz}{dy}\frac{dy}{dx}\end{align*}\) For multiple variables at once: multiply Jacobians $h = f(z)$ $z = Wx + b$ \(\begin{align*}\frac{\partial h}{\partial x} = \frac{\partial h}{\partial z}\frac{\partial z}{\partial x} = \cdots\end{align*}\) Example Jacobian: Elementwise activation function Other Jacobians Write out the Jacobians Derivative with respect to Matrix: Output shape for $\frac{\partial s}{\partial W}$, output $s$ is a scalar and input $W \in \mathbb{R}^{n\times m}$: 1 by nm Jacobian? $\rightarrow$ is inconvenient to do \(\theta^{\text{new}} = \theta^{\text{old}} - \alpha\nabla_\theta J(\theta)\) gradient descent. Instead, use the shape convention: the shape of the gradient is the shape of the parameters n by m matrix \(\begin{align*}\frac{\partial s}{\partial W} = \begin{bmatrix} \frac{\partial s}{\partial W_{11}} &amp;amp; \cdots &amp;amp; \frac{\partial s}{\partial W_{1m}} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \frac{\partial s}{\partial W_{n1}} &amp;amp; \cdots &amp;amp; \frac{\partial s}{\partial W_{nm}} \end{bmatrix}\end{align*}\) $\frac{\partial s}{\partial W} = \delta\frac{\partial z}{\partial W}$ $z = Wx + b$ \(\begin{align*} \frac{\partial s}{\partial W} &amp;amp;= \delta^T x^T \\ &amp;amp; = \begin{bmatrix} \delta_1 \\ \vdots \\ \delta_n \end{bmatrix} [x_1, \ldots, x_m] = \begin{bmatrix} \delta_1 x_1 &amp;amp; \cdots &amp;amp; \delta_1 x_m \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \delta_n x_1 &amp;amp; \cdots &amp;amp; \delta_n x_m \end{bmatrix} \end{align*}\) denote that $\delta$ is local error signal at z and x is local input signal. What shape should derivatives be? Two options: Use Jacobian form as much as possible, reshape to follow the shape convention at the end: at the end transpose $\frac{\partial s}{\partial b}$ to make the derivative a column vector, resulting in $\delta^T$ Always follow the shape convention Look at dimensions to figure out when to transpose and/or reorder terms The error message $\delta$ that arrives at a hidden layer has the same dimensionality as that hidden layer Backpropagation: Chain Rule Other trick: re-use derivatives computed for higher layers in computing derivatives for lower layers to minimize computation</summary></entry><entry><title type="html">cs224n - Lecture 2. Neural Classifiers</title><link href="http://0.0.0.0:4000/cs224n_lec2" rel="alternate" type="text/html" title="cs224n - Lecture 2. Neural Classifiers" /><published>2022-03-05T00:00:00+00:00</published><updated>2022-03-05T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec2</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec2">&lt;h3 id=&quot;review-main-idea-of-word2vec&quot;&gt;Review: Main idea of word2vec&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Start with random word vectors&lt;/li&gt;
  &lt;li&gt;Iterate through each word in the whole corpus&lt;/li&gt;
  &lt;li&gt;Try to predict surrounding words using word vectors: $P(o\mid c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Learning&lt;/strong&gt;: Update vectors so they can predict actual surrounding words better&lt;/li&gt;
  &lt;li&gt;Doing no more than this, this algorithm learns word vectors that capture well word similarity and meaningful directions in a wordspace.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;A “bag of words” model; doesn’t actually pay any attention to word order or position. The model makes the same predictions at each position; the probability estimate would be the same if it is next to the center word or a bit further away.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We want a model that gives a reasonably high probability estimate to &lt;em&gt;all&lt;/em&gt; words that occur in the context(at all often)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Word2vec maximizes objective function by putting similar words nearby in high dimensional vector space&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimization-gradient-descent&quot;&gt;Optimization: Gradient Descent&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;To learn good word vectors: minimize a cost function $J(\theta)$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Gradient Descent&lt;/strong&gt; is an algorithm to minimize $J(\theta)$ by changing $\theta$&lt;/li&gt;
  &lt;li&gt;idea: from current value of $\theta$, calculate gradient of $J(\theta)$, then take small step in the direction of &lt;em&gt;negative&lt;/em&gt; gradient. Repreat.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_1.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;theta_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;J&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_grad&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;stochastic-gradient-descent&quot;&gt;Stochastic Gradient Descent&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: $J(\theta)$ is a function of &lt;strong&gt;all&lt;/strong&gt; windows in the corpus (often, billions!); so $\nabla_\theta J(\theta)$ is very expensive to compute&lt;/li&gt;
  &lt;li&gt;Solution: Stochastic gradient descent(SGD)
    &lt;ul&gt;
      &lt;li&gt;Repeatedly sample windows, and update after each one, or each small batch&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;theta_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;J&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_grad&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Stochastic gradients with word vectors (Aside)
    &lt;ul&gt;
      &lt;li&gt;iteratively take gradients at each such window for SGD&lt;/li&gt;
      &lt;li&gt;But in each window, we only have at most &lt;em&gt;2m + 1&lt;/em&gt; words,&lt;br /&gt;
  so $\nabla_\theta J(\theta)$ is very sparse:&lt;br /&gt;
  \(\nabla_\theta J_t(\theta) = \begin{bmatrix} 0  \\ \vdots \\ \nabla_{v_{\text{like}}} \\ \vdots 0 \\ \nabla_{u_I} \\ \vdots \\ \nabla_{u_{\text{learning}}} \\ \vdots \end{bmatrix} \in \mathbb{R}^{2dV}\)&lt;/li&gt;
      &lt;li&gt;We might only update the word vectors that actually appear.&lt;/li&gt;
      &lt;li&gt;Solution: either you need sparse matrix update operations to only update certain &lt;strong&gt;rows&lt;/strong&gt;(in most DL packages) of full embedding matrices &lt;em&gt;U&lt;/em&gt; and &lt;em&gt;V&lt;/em&gt;, or you need to keep around a hash for word vectors.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec2_2.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;If you have millions of word vectors and do distributed computing, it is important to not have to send gigantic updates around.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2b-word2vec-algorithm-family-more-details&quot;&gt;2b. Word2vec algorithm family: More details&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Why two vectors? $\rightarrow$ Easier optimization. Average both at the end
    &lt;ul&gt;
      &lt;li&gt;But can implement the algorithm with just one vector per word, and it works slightly better, but it makes the algorithm much more complicated.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Two model variants:
    &lt;ol&gt;
      &lt;li&gt;Skip-grams(SG)&lt;br /&gt;
 Predict context(“outside”) words (position independent) given center word&lt;/li&gt;
      &lt;li&gt;Continuous Bag of Words(CBOW)&lt;br /&gt;
 Predict center word from (bag of) context words&lt;br /&gt;
  &lt;em&gt;We presented: Skip-gram model&lt;/em&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Additional efficiency in training:
    &lt;ul&gt;
      &lt;li&gt;Negative sampling&lt;br /&gt;
  &lt;em&gt;So far: Focus on naive softmax(simpler, but expensive training method)&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-skip-gram-model-with-negative-samplingsgns&quot;&gt;The skip-gram model with negative sampling(SGNS)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The normalization term is computationally expensive, especially on the denominator of $P(o\mid c)$.&lt;/li&gt;
  &lt;li&gt;Main idea: train binary logistic regressions for a true pair (center word and a word in its context window) versus several noise pairs (the center word paired with a random word)&lt;/li&gt;
  &lt;li&gt;From paper: &lt;em&gt;“Distributed Representations of Words and Phrases and their Compositionality” (Mikolov et al. 2013)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Overall objective function(to maximize):&lt;br /&gt;
  \(J(\theta) = \frac{1}{T}\sum_{t=1}^T J_t(\theta)\)&lt;br /&gt;
  \(J_t(\theta) = \log\sigma(u_o^T u_c) + \sum_{i=1}^k \mathbb{E}_{j~P(w)}\left[ \log\sigma(-u_j^T v_c) \right]\)&lt;br /&gt;
  where the logistic/sigmoid function: $\sigma(x) = \frac{1}{1+ e^{-x}}$&lt;/li&gt;
  &lt;li&gt;We maximize the probability of two words co-occuring in first log and minimize probability of noise words:&lt;br /&gt;
  $J_{\text{neg-sample}}(u_o, v_c, U) = -\log \sigma(u_o^T v_c) - \sum_{k\in { \text{K sampled indicies} }} \log \sigma(-u_k^T v_c)$&lt;/li&gt;
  &lt;li&gt;We take &lt;em&gt;k&lt;/em&gt; negative samples (using word probabilities)&lt;/li&gt;
  &lt;li&gt;Maximize probability that real outside word appears, minimize probability that random words appear around center word&lt;/li&gt;
  &lt;li&gt;Another trick: sample with $P(w) = U(w)^{3/4} / Z$, the unigram distribution $U(w)$ raised to the $3/4$ power (We provide this function in the starter code)&lt;/li&gt;
  &lt;li&gt;The power makes less frequent words be sampled more often&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;why-not-capture-co-occurrence-counts-directly&quot;&gt;Why not capture co-occurrence counts directly?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Building a co-occurrence matrix &lt;em&gt;X&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;2 options: windows vs. full document&lt;/li&gt;
      &lt;li&gt;Window: Similar to word2vec, use window around each word and captures some syntactic and semantic information&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec2_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;Word-document co-occurrence matrix will give general topics (all sports terms will have similar entries) learning to “Latent Semantic Analysis”; in tasks like information retrieval&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;co-occurrence-vectors&quot;&gt;Co-occurrence vectors&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Simple count co-occurrence vectors
    &lt;ul&gt;
      &lt;li&gt;Vectors increase in size with vocabulary&lt;/li&gt;
      &lt;li&gt;Very high dimensional: require a lot of storage (though sparse)&lt;/li&gt;
      &lt;li&gt;Subsequent classification models have sparsity issues $\rightarrow$ Models are less robust&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Low-dimensional vectors
    &lt;ul&gt;
      &lt;li&gt;idea: store “most” of the important information in a fixed, small number of directions: a dense vector&lt;/li&gt;
      &lt;li&gt;Usually 25-1000 directions, similar to word2vec&lt;/li&gt;
      &lt;li&gt;How to reduce the dimensionality?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;classic-method-dimensionality-reduction-on-x&quot;&gt;Classic Method: Dimensionality Reduction on X&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Singular Value Decomposition of co-occurrence matrix &lt;em&gt;X&lt;/em&gt;&lt;br /&gt;
  Factorizes &lt;em&gt;X&lt;/em&gt; into $U\Sigma V^T$, where &lt;em&gt;U&lt;/em&gt; and &lt;em&gt;V&lt;/em&gt; are orthonormal&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec2_4.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
  Corresponding to the columns without singular values in $\Sigma$, bottom rows in $V^T$ are ignored. The singular values inside the diagonal matrix $\Sigma$ are ordered from largest down to smallest. Retaining only &lt;em&gt;k&lt;/em&gt; singular values, in order to generalize, the lower dimensional representation $\hat{X}$ is the best rank &lt;em&gt;k&lt;/em&gt; approximation to &lt;em&gt;X&lt;/em&gt;, in terms of least squares.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;hacks-to-x-several-used-in-rohde-et-al-2005-in-coals&quot;&gt;Hacks to X (several used in Rohde et al. 2005 in COALS)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Running an SVD on a raw count co-occurrence matrix works poorly; In the mathematical assumptions of SVD, we are expecting to have normally distributed errors. But there are exceedingly common words like “a”, “the”, and “and”, and there is a very large number of rare words.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Scaling the counts in the cells can help &lt;strong&gt;a lot&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Problem: function words(&lt;em&gt;the, he, has&lt;/em&gt;) are too frequent $\rightarrow$ syntax has too much impact. Some fixes:
        &lt;ul&gt;
          &lt;li&gt;log the frequencies&lt;/li&gt;
          &lt;li&gt;$min(X,t)$, with $t\approx 100$&lt;/li&gt;
          &lt;li&gt;ignore the function words&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Ramped windows that count closer words more than further away words&lt;/li&gt;
  &lt;li&gt;Use Pearson correlations instead of counts, then set negative values to &lt;em&gt;0&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Etc.&lt;/li&gt;
  &lt;li&gt;Result:&lt;br /&gt;
  Interesting semantic patterns emerge in the scaled vectors; something like a word vector analogies.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec2_5.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;towards-glove-count-based-vs-direct-prediction&quot;&gt;Towards GloVe: Count based vs. direct prediction&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_6.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;encoding-meaning-components-in-vector-differences&quot;&gt;Encoding meaning components in vector differences&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Pennington, Socher, and Manning, EMNLP 2014&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;What properties needed to make vector analogies work?&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Crucial insight: Ratios of co-occurrence probabilities can encode meaning components&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_7.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_8.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Q: How can we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space?&lt;/li&gt;
  &lt;li&gt;A: Log-bilinear model: the dot product between two word vectors attempts to approximate the log of the probability of co-occurrence; \(w_i \cdot w_j = \log P(i|j)\)&lt;br /&gt;
  $\rightarrow$ with vector differences \(w_x \cdot (w_a - w_b) = \log \frac{P(x\mid a)}{P(x \mid b)}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;combining-the-best-of-both-worlds-glove&quot;&gt;Combining the best of both worlds: GloVe&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Pennington, Socher, and Manning, EMNLP 2014&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;With \(w_i \cdot w_j = \log P(i|j)\),&lt;br /&gt;
  explicit loss function \(J = \sum_{i,j=1}^V f(X_{ij})(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2\)&lt;br /&gt;
  to make the dot product to be similar to the log of the co-occurrence. To not have very common words dominate, capped the effect of high word counts using $f$ function. Optimize &lt;em&gt;J&lt;/em&gt; directly on the co-occurrence count matrix.
    &lt;ul&gt;
      &lt;li&gt;Fast training&lt;/li&gt;
      &lt;li&gt;Scalable to hugh corpora&lt;/li&gt;
      &lt;li&gt;Good performance even with small corpus and small vectors&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-evaluate-word-vectors&quot;&gt;How to evaluate word vectors?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Related to general evaluation in NLP: intrinsic vs. extrinsic&lt;/li&gt;
  &lt;li&gt;Intrinsic:
    &lt;ul&gt;
      &lt;li&gt;Evaluation on a specific/intermediate subtask&lt;/li&gt;
      &lt;li&gt;Fast to compute&lt;/li&gt;
      &lt;li&gt;Helps to understand that system&lt;/li&gt;
      &lt;li&gt;Not clear if really helpful unless correlation to real task is established&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Extrinsic:
    &lt;ul&gt;
      &lt;li&gt;Evaluation on a real task&lt;/li&gt;
      &lt;li&gt;Can take a long time to compute accuracy&lt;/li&gt;
      &lt;li&gt;Unclear if the subsystem is the problem or its interaction or other subsystems&lt;/li&gt;
      &lt;li&gt;If replacing exactly one subsystem with another improves accuracy $\rightarrow$ Winning!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intrinsic-word-vector-evaluation&quot;&gt;Intrinsic word vector evaluation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Word Vector Analogies&lt;br /&gt;
  |a:b :: c:?| $\rightarrow$ $d = \text{argmax}_i \frac{(x_b -x_a +x_c)^T x_i}{\lVert x_b -x_a +x_c\rVert}$&lt;br /&gt;
  (e.g. man:woman :: king:?)&lt;/li&gt;
  &lt;li&gt;Evalute word vectors by how well their cosine distance after addition captures intuitive semantic and syntactic analogy questions&lt;/li&gt;
  &lt;li&gt;Discarding the input words from the search!&lt;/li&gt;
  &lt;li&gt;Problem: What if the information is there but not linear?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;glove-visualizations&quot;&gt;GloVe Visualizations&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_9.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;analogy-evaluation-and-hyperparameters&quot;&gt;Analogy evaluation and hyperparameters&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_10.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_11.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;another-intrinsic-word-vector-evaluation&quot;&gt;Another intrinsic word vector evaluation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Word vector distances and their correlation with human judgements&lt;/li&gt;
  &lt;li&gt;Example dataset: WordSim353 &lt;a href=&quot;http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/&quot;&gt;http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_12.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_13.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Some ideas from Glove paper have been shown to improve skip-gram(SG) model also (e.g., average both vectors)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;extrinsic-word-vector-evaluation&quot;&gt;Extrinsic word vector evaluation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;All subsequent NLP tasks&lt;/li&gt;
  &lt;li&gt;One example where good word vectors should help directly: &lt;strong&gt;named entity recognition&lt;/strong&gt;: identifying references to a person, organization or location&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_14.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;word-senses&quot;&gt;Word senses&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Most words have lots of meanings
    &lt;ul&gt;
      &lt;li&gt;Especially common words&lt;/li&gt;
      &lt;li&gt;Especially words that have existed for a long time&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Does one vector caputre all these meanings or do we have a mess?&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;linear-algebric-structure-of-word-senses-with-applications-to-polysemy-arora--ma--tacl-2018&quot;&gt;&lt;em&gt;“Linear Algebric Structure of Word Senses, with Applications to Polysemy”, Arora, …, Ma, …, TACL 2018&lt;/em&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Different senses of a word reside in a linear superposition(weighted sum) in standard word embeddings like word2vec&lt;/li&gt;
  &lt;li&gt;\(v_{\text{pike}} = \alpha_1 v_{\text{pike}_2} + \alpha_2 v_{\text{pike}_2} + \alpha_3 v_{\text{pike}_3}\)&lt;br /&gt;
  where $\alpha_1 = \frac{f_1}{f_1+f_2+f_3}$, etc., for frequency &lt;em&gt;f&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Surprising result:&lt;br /&gt;
  Commonly, it is impossible to reconstruct the original components from their sum, but, because of ideas from &lt;em&gt;sparse coding&lt;/em&gt; you can actually separate out the senses(providing they are relatively common)!&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_15.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">Review: Main idea of word2vec Start with random word vectors Iterate through each word in the whole corpus Try to predict surrounding words using word vectors: $P(o\mid c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}$ Learning: Update vectors so they can predict actual surrounding words better Doing no more than this, this algorithm learns word vectors that capture well word similarity and meaningful directions in a wordspace. A “bag of words” model; doesn’t actually pay any attention to word order or position. The model makes the same predictions at each position; the probability estimate would be the same if it is next to the center word or a bit further away. We want a model that gives a reasonably high probability estimate to all words that occur in the context(at all often) Word2vec maximizes objective function by putting similar words nearby in high dimensional vector space Optimization: Gradient Descent To learn good word vectors: minimize a cost function $J(\theta)$ Gradient Descent is an algorithm to minimize $J(\theta)$ by changing $\theta$ idea: from current value of $\theta$, calculate gradient of $J(\theta)$, then take small step in the direction of negative gradient. Repreat. Algorithm: while True: theta_grad = evaluate_gradient(J, corpus, theta) theta = theta - alpha * theta_grad Stochastic Gradient Descent Problem: $J(\theta)$ is a function of all windows in the corpus (often, billions!); so $\nabla_\theta J(\theta)$ is very expensive to compute Solution: Stochastic gradient descent(SGD) Repeatedly sample windows, and update after each one, or each small batch Algorithm: while True: window = sample_window(corpus) theta_grad = evaluate_gradient(J, window, theta) theta = theta - alpha * theta_grad Stochastic gradients with word vectors (Aside) iteratively take gradients at each such window for SGD But in each window, we only have at most 2m + 1 words, so $\nabla_\theta J(\theta)$ is very sparse: \(\nabla_\theta J_t(\theta) = \begin{bmatrix} 0 \\ \vdots \\ \nabla_{v_{\text{like}}} \\ \vdots 0 \\ \nabla_{u_I} \\ \vdots \\ \nabla_{u_{\text{learning}}} \\ \vdots \end{bmatrix} \in \mathbb{R}^{2dV}\) We might only update the word vectors that actually appear. Solution: either you need sparse matrix update operations to only update certain rows(in most DL packages) of full embedding matrices U and V, or you need to keep around a hash for word vectors. If you have millions of word vectors and do distributed computing, it is important to not have to send gigantic updates around. 2b. Word2vec algorithm family: More details Why two vectors? $\rightarrow$ Easier optimization. Average both at the end But can implement the algorithm with just one vector per word, and it works slightly better, but it makes the algorithm much more complicated. Two model variants: Skip-grams(SG) Predict context(“outside”) words (position independent) given center word Continuous Bag of Words(CBOW) Predict center word from (bag of) context words We presented: Skip-gram model Additional efficiency in training: Negative sampling So far: Focus on naive softmax(simpler, but expensive training method) The skip-gram model with negative sampling(SGNS) The normalization term is computationally expensive, especially on the denominator of $P(o\mid c)$. Main idea: train binary logistic regressions for a true pair (center word and a word in its context window) versus several noise pairs (the center word paired with a random word) From paper: “Distributed Representations of Words and Phrases and their Compositionality” (Mikolov et al. 2013) Overall objective function(to maximize): \(J(\theta) = \frac{1}{T}\sum_{t=1}^T J_t(\theta)\) \(J_t(\theta) = \log\sigma(u_o^T u_c) + \sum_{i=1}^k \mathbb{E}_{j~P(w)}\left[ \log\sigma(-u_j^T v_c) \right]\) where the logistic/sigmoid function: $\sigma(x) = \frac{1}{1+ e^{-x}}$ We maximize the probability of two words co-occuring in first log and minimize probability of noise words: $J_{\text{neg-sample}}(u_o, v_c, U) = -\log \sigma(u_o^T v_c) - \sum_{k\in { \text{K sampled indicies} }} \log \sigma(-u_k^T v_c)$ We take k negative samples (using word probabilities) Maximize probability that real outside word appears, minimize probability that random words appear around center word Another trick: sample with $P(w) = U(w)^{3/4} / Z$, the unigram distribution $U(w)$ raised to the $3/4$ power (We provide this function in the starter code) The power makes less frequent words be sampled more often Why not capture co-occurrence counts directly? Building a co-occurrence matrix X 2 options: windows vs. full document Window: Similar to word2vec, use window around each word and captures some syntactic and semantic information Word-document co-occurrence matrix will give general topics (all sports terms will have similar entries) learning to “Latent Semantic Analysis”; in tasks like information retrieval Co-occurrence vectors Simple count co-occurrence vectors Vectors increase in size with vocabulary Very high dimensional: require a lot of storage (though sparse) Subsequent classification models have sparsity issues $\rightarrow$ Models are less robust Low-dimensional vectors idea: store “most” of the important information in a fixed, small number of directions: a dense vector Usually 25-1000 directions, similar to word2vec How to reduce the dimensionality? Classic Method: Dimensionality Reduction on X Singular Value Decomposition of co-occurrence matrix X Factorizes X into $U\Sigma V^T$, where U and V are orthonormal Corresponding to the columns without singular values in $\Sigma$, bottom rows in $V^T$ are ignored. The singular values inside the diagonal matrix $\Sigma$ are ordered from largest down to smallest. Retaining only k singular values, in order to generalize, the lower dimensional representation $\hat{X}$ is the best rank k approximation to X, in terms of least squares. Hacks to X (several used in Rohde et al. 2005 in COALS) Running an SVD on a raw count co-occurrence matrix works poorly; In the mathematical assumptions of SVD, we are expecting to have normally distributed errors. But there are exceedingly common words like “a”, “the”, and “and”, and there is a very large number of rare words. Scaling the counts in the cells can help a lot Problem: function words(the, he, has) are too frequent $\rightarrow$ syntax has too much impact. Some fixes: log the frequencies $min(X,t)$, with $t\approx 100$ ignore the function words Ramped windows that count closer words more than further away words Use Pearson correlations instead of counts, then set negative values to 0 Etc. Result: Interesting semantic patterns emerge in the scaled vectors; something like a word vector analogies. Towards GloVe: Count based vs. direct prediction Encoding meaning components in vector differences Pennington, Socher, and Manning, EMNLP 2014 What properties needed to make vector analogies work? Crucial insight: Ratios of co-occurrence probabilities can encode meaning components Q: How can we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space? A: Log-bilinear model: the dot product between two word vectors attempts to approximate the log of the probability of co-occurrence; \(w_i \cdot w_j = \log P(i|j)\) $\rightarrow$ with vector differences \(w_x \cdot (w_a - w_b) = \log \frac{P(x\mid a)}{P(x \mid b)}\) Combining the best of both worlds: GloVe Pennington, Socher, and Manning, EMNLP 2014 With \(w_i \cdot w_j = \log P(i|j)\), explicit loss function \(J = \sum_{i,j=1}^V f(X_{ij})(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2\) to make the dot product to be similar to the log of the co-occurrence. To not have very common words dominate, capped the effect of high word counts using $f$ function. Optimize J directly on the co-occurrence count matrix. Fast training Scalable to hugh corpora Good performance even with small corpus and small vectors How to evaluate word vectors? Related to general evaluation in NLP: intrinsic vs. extrinsic Intrinsic: Evaluation on a specific/intermediate subtask Fast to compute Helps to understand that system Not clear if really helpful unless correlation to real task is established Extrinsic: Evaluation on a real task Can take a long time to compute accuracy Unclear if the subsystem is the problem or its interaction or other subsystems If replacing exactly one subsystem with another improves accuracy $\rightarrow$ Winning! Intrinsic word vector evaluation Word Vector Analogies |a:b :: c:?| $\rightarrow$ $d = \text{argmax}_i \frac{(x_b -x_a +x_c)^T x_i}{\lVert x_b -x_a +x_c\rVert}$ (e.g. man:woman :: king:?) Evalute word vectors by how well their cosine distance after addition captures intuitive semantic and syntactic analogy questions Discarding the input words from the search! Problem: What if the information is there but not linear? GloVe Visualizations Analogy evaluation and hyperparameters Another intrinsic word vector evaluation Word vector distances and their correlation with human judgements Example dataset: WordSim353 http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/ Some ideas from Glove paper have been shown to improve skip-gram(SG) model also (e.g., average both vectors) Extrinsic word vector evaluation All subsequent NLP tasks One example where good word vectors should help directly: named entity recognition: identifying references to a person, organization or location Word senses Most words have lots of meanings Especially common words Especially words that have existed for a long time Does one vector caputre all these meanings or do we have a mess? “Linear Algebric Structure of Word Senses, with Applications to Polysemy”, Arora, …, Ma, …, TACL 2018 Different senses of a word reside in a linear superposition(weighted sum) in standard word embeddings like word2vec \(v_{\text{pike}} = \alpha_1 v_{\text{pike}_2} + \alpha_2 v_{\text{pike}_2} + \alpha_3 v_{\text{pike}_3}\) where $\alpha_1 = \frac{f_1}{f_1+f_2+f_3}$, etc., for frequency f Surprising result: Commonly, it is impossible to reconstruct the original components from their sum, but, because of ideas from sparse coding you can actually separate out the senses(providing they are relatively common)!</summary></entry><entry><title type="html">DevEnv Setup</title><link href="http://0.0.0.0:4000/DevEnv_Setup" rel="alternate" type="text/html" title="DevEnv Setup" /><published>2022-03-02T00:00:00+00:00</published><updated>2022-03-02T00:00:00+00:00</updated><id>http://0.0.0.0:4000/DevEnv_Setup</id><content type="html" xml:base="http://0.0.0.0:4000/DevEnv_Setup">&lt;ul&gt;
  &lt;li&gt;For purpose of setting local development environment on a new SSD storage, followed instructions below. Post for later use.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Document &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl&quot;&gt;Enable NVIDIA CUDA on WSL&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Install stable version of Windows 11&lt;/li&gt;
      &lt;li&gt;Enable WSL, install Ubuntu(20.04.3 LTS)&lt;br /&gt;
  On Windows &lt;strong&gt;Settings&lt;/strong&gt; app, select &lt;strong&gt;Check for updates&lt;/strong&gt; in the &lt;strong&gt;Windows Update&lt;/strong&gt; section and get the latest kernel(5.10.43.3 or higher)&lt;br /&gt;
  To check the version, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wsl cat /proc/version&lt;/code&gt; command in &lt;strong&gt;Powershell&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Install the GPU driver&lt;br /&gt;
  Download and install the NVIDIA CUDA enabled driver for WSL&lt;br /&gt;
  (Studio version: 511.65-desktop-win10-win11-64bit-international-nsd-dch-whql)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Install Docker Desktop app on Windows
    &lt;ul&gt;
      &lt;li&gt;Run:&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Result:
        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Windowed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Simulation&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Single&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;floating&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simulation&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Devices&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;used&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simulation&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;GPU&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Device&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Ampere&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capability&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;8.6&lt;/span&gt;

  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Compute&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;8.6&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CUDA&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NVIDIA&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GeForce&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RTX&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3070&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;mi&quot;&gt;47104&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bodies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;40.275&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ms&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;550.910&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;billion&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interactions&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;11018.199&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;single&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GFLOP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flops&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interaction&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Setting Docker image for TensorFlow-GPU
    &lt;ul&gt;
      &lt;li&gt;Pull the latest TensorFlow-GPU image
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker run -it --gpus all tensorflow/tensorflow:latest-gpu&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Install Anaconda on user:root(ref: &lt;a href=&quot;https://omhdydy.tistory.com/6&quot;&gt;blog&lt;/a&gt;)
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  # update and install prerequisites
  apt-get update
  apt-get install wget
  # get proper version of anaconda3
  wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh 
  sh Anaconda3-2021.11-Linux-x86_64.sh
  exec bash
  # create anaconda environment and install libraries(for stability)
  conda create -n !env_name pip python=3.7
  conda activate !env_name
  pip install tensorflow-gpu
  pip install ipykernel
  python -m ipykernel install --user --name !env_name --display-name !dispaly_name
  pip install jupyter
  # escape with Ctrl + p, Ctrl + q
  docker commit -m &quot;!message&quot; !container_id !image_name:tag
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;(Optional) Install TensorFlow Object Detection API
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  apt-get install git
  git clone --depth 1 https://github.com/tensorflow/models
  cd models/research/
  apt install -y protobuf-compiler
  # found a symlink err, fixed with running:
  # ln -s /usr/lib/x86_64-linux-gnu/libcuda.so.1 /usr/local/cuda/lib64/libcuda.so
  # and rerun: apt install -y protobuf-compiler
  protoc object_detection/protos/*.proto --python_out=.
  cd models/research/
  # install Object Detection API
  cp object_detection/packages/tf2/setup.py .
  python -m pip install --use-feature=2020-resolver .
  # run test
  python object_detection/builders/model_builder_tf2_test.py
  # rm -rf models (if desired)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;(Optional) Install JupyterLab Extensions and enable TensorBoard within Jupyterlab-Docker container&lt;br /&gt;
  Stable versions worked on my local environment
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  curl -sL https://deb.nodesource.com/setup_12.x | bash -
  apt-get install -y nodejs
  node --version # check: v12.22.10
  npm --version # check: 6.14.16
  pip install jupyterlab==2.3.2 
  pip install git+https://github.com/cliffwoolley/jupyter_tensorboard.git 
  pip install tensorboard==2.2
  jupyter labextension install jupyterlab_tensorboard
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
        &lt;p&gt;Commit and run container with any open port for JupyterLab&lt;br /&gt;
  e.g.&lt;/p&gt;
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  docker run --rm -it --gpus all -p 4000:4000 !image_name:tag
  conda activate !env_name
  jupyter lab --ip='0.0.0.0' --port=4000 --no-browser --allow-root
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
        &lt;p&gt;On your &lt;strong&gt;Windows&lt;/strong&gt;, open &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost:4000&lt;/code&gt; with browser&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Setting Docker image for Jekyll blog
    &lt;ul&gt;
      &lt;li&gt;Get latest Ubuntu image and install packages
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  docker run --rm -it -p 4000:4000 ubuntu
  apt-get update
  apt-get install git
  apt-get install vim ruby-full build-essential zlib1g-dev -y

  echo '# Install Ruby Gems to ~/gems' &amp;gt;&amp;gt; ~/.bashrc
  echo 'export GEM_HOME=&quot;$HOME/gems&quot;' &amp;gt;&amp;gt; ~/.bashrc
  echo 'export PATH=&quot;$HOME/gems/bin:$PATH&quot;' &amp;gt;&amp;gt; ~/.bashrc
  source ~/.bashrc
  gem install jekyll bundler
		
  jekyll -v # 4.2.1
  mkdir -p /root/blog_home
  echo 'export BLOG_HOME=&quot;/root/blog_home&quot;' &amp;gt;&amp;gt; ~/.bashrc
  echo '# Start jekyll' &amp;gt;&amp;gt; ~/.bashrc
  source ~/.bashrc
  cd $BLOG_HOME # Get any jekyll blog template here
  rm Gemfile.lock # if needed
  bundle install
  bundle exec jekyll serve --host 0.0.0.0 -p 4000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Darron Kwon</name></author><category term="blog" /><summary type="html">For purpose of setting local development environment on a new SSD storage, followed instructions below. Post for later use. Document Enable NVIDIA CUDA on WSL Install stable version of Windows 11 Enable WSL, install Ubuntu(20.04.3 LTS) On Windows Settings app, select Check for updates in the Windows Update section and get the latest kernel(5.10.43.3 or higher) To check the version, run wsl cat /proc/version command in Powershell. Install the GPU driver Download and install the NVIDIA CUDA enabled driver for WSL (Studio version: 511.65-desktop-win10-win11-64bit-international-nsd-dch-whql) Install Docker Desktop app on Windows Run: docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark Result: &amp;gt; Windowed mode &amp;gt; Simulation data stored in video memory &amp;gt; Single precision floating point simulation &amp;gt; 1 Devices used for simulation GPU Device 0: &quot;Ampere&quot; with compute capability 8.6 &amp;gt; Compute 8.6 CUDA device: [NVIDIA GeForce RTX 3070] 47104 bodies, total time for 10 iterations: 40.275 ms = 550.910 billion interactions per second = 11018.199 single-precision GFLOP/s at 20 flops per interaction Setting Docker image for TensorFlow-GPU Pull the latest TensorFlow-GPU image docker run -it --gpus all tensorflow/tensorflow:latest-gpu Install Anaconda on user:root(ref: blog) # update and install prerequisites apt-get update apt-get install wget # get proper version of anaconda3 wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh sh Anaconda3-2021.11-Linux-x86_64.sh exec bash # create anaconda environment and install libraries(for stability) conda create -n !env_name pip python=3.7 conda activate !env_name pip install tensorflow-gpu pip install ipykernel python -m ipykernel install --user --name !env_name --display-name !dispaly_name pip install jupyter # escape with Ctrl + p, Ctrl + q docker commit -m &quot;!message&quot; !container_id !image_name:tag (Optional) Install TensorFlow Object Detection API apt-get install git git clone --depth 1 https://github.com/tensorflow/models cd models/research/ apt install -y protobuf-compiler # found a symlink err, fixed with running: # ln -s /usr/lib/x86_64-linux-gnu/libcuda.so.1 /usr/local/cuda/lib64/libcuda.so # and rerun: apt install -y protobuf-compiler protoc object_detection/protos/*.proto --python_out=. cd models/research/ # install Object Detection API cp object_detection/packages/tf2/setup.py . python -m pip install --use-feature=2020-resolver . # run test python object_detection/builders/model_builder_tf2_test.py # rm -rf models (if desired) (Optional) Install JupyterLab Extensions and enable TensorBoard within Jupyterlab-Docker container Stable versions worked on my local environment curl -sL https://deb.nodesource.com/setup_12.x | bash - apt-get install -y nodejs node --version # check: v12.22.10 npm --version # check: 6.14.16 pip install jupyterlab==2.3.2 pip install git+https://github.com/cliffwoolley/jupyter_tensorboard.git pip install tensorboard==2.2 jupyter labextension install jupyterlab_tensorboard Commit and run container with any open port for JupyterLab e.g. docker run --rm -it --gpus all -p 4000:4000 !image_name:tag conda activate !env_name jupyter lab --ip='0.0.0.0' --port=4000 --no-browser --allow-root On your Windows, open localhost:4000 with browser Setting Docker image for Jekyll blog Get latest Ubuntu image and install packages docker run --rm -it -p 4000:4000 ubuntu apt-get update apt-get install git apt-get install vim ruby-full build-essential zlib1g-dev -y echo '# Install Ruby Gems to ~/gems' &amp;gt;&amp;gt; ~/.bashrc echo 'export GEM_HOME=&quot;$HOME/gems&quot;' &amp;gt;&amp;gt; ~/.bashrc echo 'export PATH=&quot;$HOME/gems/bin:$PATH&quot;' &amp;gt;&amp;gt; ~/.bashrc source ~/.bashrc gem install jekyll bundler jekyll -v # 4.2.1 mkdir -p /root/blog_home echo 'export BLOG_HOME=&quot;/root/blog_home&quot;' &amp;gt;&amp;gt; ~/.bashrc echo '# Start jekyll' &amp;gt;&amp;gt; ~/.bashrc source ~/.bashrc cd $BLOG_HOME # Get any jekyll blog template here rm Gemfile.lock # if needed bundle install bundle exec jekyll serve --host 0.0.0.0 -p 4000</summary></entry><entry><title type="html">cs224n - Lecture 1. Word Vectors</title><link href="http://0.0.0.0:4000/cs224n_lec1" rel="alternate" type="text/html" title="cs224n - Lecture 1. Word Vectors" /><published>2022-03-02T00:00:00+00:00</published><updated>2022-03-02T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec1</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec1">&lt;h3 id=&quot;objectives&quot;&gt;Objectives&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;The foundations of the effective modern methods for deep learning applied to NLP; from basics to key methods used in NLP: RNN, Attention, Transformers, etc.)&lt;/li&gt;
  &lt;li&gt;A big picture understanding of human languages and the difficulties in understanding and producing them&lt;/li&gt;
  &lt;li&gt;An understanding of and ability to build systems (in PyTorch) for some of the major problems in NLP: Word meaning, dependency parsing, machine translation, question answering&lt;br /&gt;
&lt;!-- *CS145: translate human language sentences into SQL --&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;NLP tasks:&lt;br /&gt;
  Easy: Spell Checking, Keyword Search, Finding Synonyms&lt;br /&gt;
  Medium: Parsing information from websites, documents, etc.&lt;br /&gt;
  Hard: Machine Translation, Semantic Analysis, Coreference, QA&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;word-meaning&quot;&gt;Word meaning&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Commonest linguistic way of thinking of meaning:&lt;br /&gt;
  signifier (symbol) $\Leftrightarrow$ signified (idea or thing)&lt;br /&gt;
  $=$ denotational semantics&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Common NLP solution&lt;/strong&gt;: Use, e.g., &lt;em&gt;WordNet&lt;/em&gt;, a thesaurus containing lists of &lt;strong&gt;synonym sets&lt;/strong&gt; and &lt;strong&gt;hypernyms&lt;/strong&gt;(“is a” relationships).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Problems with resources like WordNet&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Great as a resource but missing nuance&lt;/li&gt;
      &lt;li&gt;Missing new meanings of words; impossible to keep up-to-date&lt;/li&gt;
      &lt;li&gt;Subjective&lt;/li&gt;
      &lt;li&gt;Requires human labor to create and adapt&lt;/li&gt;
      &lt;li&gt;Can’t compute accurate word similarity&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;representing-words-as-discrete-symbols&quot;&gt;Representing words as discrete symbols&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;In traditional NLP, we regard words as discrete symbols - a &lt;em&gt;localist&lt;/em&gt; representation.&lt;br /&gt;
  $\rightarrow$ in a statistical machine learning systems, such symbols for words are separately represented by one-hot vectors. Thus we need to have huge vector dimension corresponding to the number of words in vocabulary.
    &lt;ul&gt;
      &lt;li&gt;But with discrete symbols, two vectors are orthogonal and there is no natural notion of similarity for one-hot vectors.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Solution:
    &lt;ul&gt;
      &lt;li&gt;Could try  to rely on WordNet’s list of synonyms to get similarity?&lt;br /&gt;
  But it is well-known to fail badly; incompleteness, etc.&lt;/li&gt;
      &lt;li&gt;Instead: learn to encode similarity in the vecotr themselves.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;representing-words-by-their-context&quot;&gt;Representing words by their context&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Distributional semantics: &lt;strong&gt;A word’s meaning is given by the words that frequently appear close-by&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;“You shall know a word by the company it keeps”&lt;/em&gt; (J. R. Firth 1957:11)&lt;/li&gt;
      &lt;li&gt;One of the most successful ideas of modern statistical NLP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;When a word &lt;em&gt;w&lt;/em&gt; appears in a text, its &lt;strong&gt;context&lt;/strong&gt; is the set of words that appear nearby(within a fixed-size window).&lt;/li&gt;
  &lt;li&gt;Use the many contexts of &lt;em&gt;w&lt;/em&gt; to build up a representation of &lt;em&gt;w&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;word-vectors&quot;&gt;Word vectors&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note: as a &lt;em&gt;distributed&lt;/em&gt; representation, &lt;em&gt;word vectors&lt;/em&gt; are also called &lt;em&gt;word embeddings&lt;/em&gt; or &lt;em&gt;(neural) words representations&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;word2vec&quot;&gt;Word2vec&lt;/h2&gt;
&lt;h3 id=&quot;word2vec-overview&quot;&gt;Word2vec: overview&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Word2vec&lt;/em&gt;(Mikolov et al. 2013) is a framework for learning word vectors&lt;/li&gt;
  &lt;li&gt;idea:
    &lt;ul&gt;
      &lt;li&gt;We have a large corpus(“body”) of text&lt;/li&gt;
      &lt;li&gt;Every word in a fixed vocabulary is representated by a vector&lt;/li&gt;
      &lt;li&gt;Go through each position &lt;em&gt;t&lt;/em&gt; in the text, which has a center word &lt;em&gt;c&lt;/em&gt; and context(“outside”) words &lt;em&gt;o&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Use the similarity of the word vectors for &lt;em&gt;c&lt;/em&gt; and &lt;em&gt;o&lt;/em&gt; to calculate the probability of &lt;em&gt;o&lt;/em&gt; given &lt;em&gt;c&lt;/em&gt;(or vice versa)&lt;/li&gt;
      &lt;li&gt;Keep adjusting the word vectors to maximize this probability&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;We can learn these word vectors from just a big pile of text by doing this distributional similarity task of being able to predict what words occur in the context of other words.&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Example windows and process for computing $P(w_{t+j}|w_t)$&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec1_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;word2vec-objective-function&quot;&gt;Word2vec: objective function&lt;/h3&gt;
&lt;p&gt;For each position $t = 1, \ldots, T$, predict context words within a window of fixed size &lt;em&gt;m&lt;/em&gt;, given center word $w_j$. Data likelihood:&lt;br /&gt;
\(\begin{align*}
L(\theta) = \prod_{t=1}^T \prod_{\substack{-m\leqq j\leqq m \\ j\ne 0}} P(w_{t+j}|w_t;\theta)
\end{align*}\)&lt;br /&gt;
where $\theta$ is all variables to be optimized.&lt;/p&gt;

&lt;p&gt;The objective function $J(\theta)$ is the (average) negative log likelihood:&lt;br /&gt;
\(\begin{align*}
J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T}\sum_{t=1}^T \sum_{\substack{-m\leqq j\leqq m \\ j\ne 0}}\log P(w_{t+j}|w_t;\theta)
\end{align*}\)&lt;br /&gt;
Minimizing objective function $\Leftrightarrow$ Maximizing predictive accuracy&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Question: How to calculate $P(w_{t+j}\vert w_t;\theta)$?&lt;/li&gt;
  &lt;li&gt;Answer: We will &lt;em&gt;use&lt;/em&gt; two vectors per word &lt;em&gt;w&lt;/em&gt;:
    &lt;ul&gt;
      &lt;li&gt;$v_w$ when &lt;em&gt;w&lt;/em&gt; is a center word&lt;/li&gt;
      &lt;li&gt;$u_w$ when &lt;em&gt;w&lt;/em&gt; is a context word&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Then for a centor word &lt;em&gt;c&lt;/em&gt; and a context word &lt;em&gt;o&lt;/em&gt;:&lt;br /&gt;
  \(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;word2vec-prediction-function&quot;&gt;Word2vec: prediction function&lt;/h3&gt;
&lt;p&gt;\(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\)&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$u_w^T v_c$: Dot product compares similarity of &lt;em&gt;o&lt;/em&gt; and &lt;em&gt;c&lt;/em&gt;.&lt;br /&gt;
 $u^T v = u\ .v = \sum_{i=1}^n u_i v_i$&lt;br /&gt;
 Larger dot product = larger probability&lt;/li&gt;
  &lt;li&gt;$\exp$: Exponentiation makes anything positive&lt;/li&gt;
  &lt;li&gt;$\sum_{w \in V}\exp(u_w^T v_c)$: Normalize over entire vocabulary to give probability distribution.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This is an example of the softmax function $\mathbb{R}^n \rightarrow (0,1)^n$(Open region) that maps arbitary values $x_i$ to a probability distribution $p_i$&lt;br /&gt;
  \(\mbox{softmax}(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^n\exp(x_j)} = p_i\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To train the model: Optimize value of parameters to minimize loss&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Recall: $\theta$ represents all the model parameters, in one long vector&lt;/li&gt;
      &lt;li&gt;In our case, with &lt;em&gt;d&lt;/em&gt;-dimensional vectors and &lt;em&gt;V&lt;/em&gt;-many words, we have: $\theta \in \mathbb{R}^{2dV}$&lt;/li&gt;
      &lt;li&gt;Remember: every word has two vectors&lt;/li&gt;
      &lt;li&gt;We optimize these parameters by walking down the gradient(gradient descent)&lt;/li&gt;
      &lt;li&gt;We compute &lt;strong&gt;all&lt;/strong&gt; vector gradients&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;word2vec-derivations-of-gradient&quot;&gt;Word2vec derivations of gradient&lt;/h3&gt;
&lt;p&gt;\(\begin{align*}
J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T}\sum_{t=1}^T \sum_{\substack{-m\leqq j\leqq m \\ j\ne 0}}\log P(w_{t+j}|w_t;\theta)
\end{align*}\)&lt;br /&gt;
\(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\)&lt;/p&gt;

&lt;p&gt;\(\frac{\partial}{\partial v_c} \log \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)} = \underbrace{\frac{\partial}{\partial v_c} \log \exp(u_o^T v_c)}_{(1)} - \underbrace{\frac{\partial}{\partial v_c} \log \sum_{w=1}^V \exp(u_w^T v_c)}_{(2)}\)&lt;br /&gt;
\(\begin{align*}\cdots (1) &amp;amp;= \frac{\partial}{\partial v_c} u_o^T v_c \\
							&amp;amp;= u_o \end{align*}\)&lt;br /&gt;
\(\begin{align*}\cdots (2) &amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \frac{\partial}{\partial v_c} \exp(u_x^T v_c) \\
							&amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \exp(u_x^T v_c) \cdot \frac{\partial}{\partial v_c} u_x^T v_c \\
							&amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \exp(u_x^T v_c) \cdot u_x \\
							\end{align*} \\\)&lt;br /&gt;
\(\begin{align*}
\frac{\partial}{\partial v_c} \log P(o|c) &amp;amp;= u_o - \frac{\sum_{x=1}^V \exp(u_x^T v_c)u_x}{\sum_{w=1}^V \exp(u_w^T v_c)} \\
	&amp;amp; = u_o - \sum_{x=1}^V \underbrace{\frac{\exp(u_x^T v_c)}{\sum_{w=1}^V \exp(u_w^T v_c)}}_{\mbox{softmax formula}} u_x \\
	&amp;amp; = u_o - \underbrace{\sum_{x=1}^V P(x|c) u_x}_{\mbox{Expectation}} \\
	&amp;amp; = \mbox{observed} - \mbox{expected} \end{align*}\)&lt;/p&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">Objectives The foundations of the effective modern methods for deep learning applied to NLP; from basics to key methods used in NLP: RNN, Attention, Transformers, etc.) A big picture understanding of human languages and the difficulties in understanding and producing them An understanding of and ability to build systems (in PyTorch) for some of the major problems in NLP: Word meaning, dependency parsing, machine translation, question answering NLP tasks: Easy: Spell Checking, Keyword Search, Finding Synonyms Medium: Parsing information from websites, documents, etc. Hard: Machine Translation, Semantic Analysis, Coreference, QA Word meaning Commonest linguistic way of thinking of meaning: signifier (symbol) $\Leftrightarrow$ signified (idea or thing) $=$ denotational semantics Common NLP solution: Use, e.g., WordNet, a thesaurus containing lists of synonym sets and hypernyms(“is a” relationships). Problems with resources like WordNet Great as a resource but missing nuance Missing new meanings of words; impossible to keep up-to-date Subjective Requires human labor to create and adapt Can’t compute accurate word similarity Representing words as discrete symbols In traditional NLP, we regard words as discrete symbols - a localist representation. $\rightarrow$ in a statistical machine learning systems, such symbols for words are separately represented by one-hot vectors. Thus we need to have huge vector dimension corresponding to the number of words in vocabulary. But with discrete symbols, two vectors are orthogonal and there is no natural notion of similarity for one-hot vectors. Solution: Could try to rely on WordNet’s list of synonyms to get similarity? But it is well-known to fail badly; incompleteness, etc. Instead: learn to encode similarity in the vecotr themselves. Representing words by their context Distributional semantics: A word’s meaning is given by the words that frequently appear close-by “You shall know a word by the company it keeps” (J. R. Firth 1957:11) One of the most successful ideas of modern statistical NLP When a word w appears in a text, its context is the set of words that appear nearby(within a fixed-size window). Use the many contexts of w to build up a representation of w Word vectors A dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts Note: as a distributed representation, word vectors are also called word embeddings or (neural) words representations Word2vec Word2vec: overview Word2vec(Mikolov et al. 2013) is a framework for learning word vectors idea: We have a large corpus(“body”) of text Every word in a fixed vocabulary is representated by a vector Go through each position t in the text, which has a center word c and context(“outside”) words o Use the similarity of the word vectors for c and o to calculate the probability of o given c(or vice versa) Keep adjusting the word vectors to maximize this probability We can learn these word vectors from just a big pile of text by doing this distributional similarity task of being able to predict what words occur in the context of other words. Example windows and process for computing $P(w_{t+j}|w_t)$ Word2vec: objective function For each position $t = 1, \ldots, T$, predict context words within a window of fixed size m, given center word $w_j$. Data likelihood: \(\begin{align*} L(\theta) = \prod_{t=1}^T \prod_{\substack{-m\leqq j\leqq m \\ j\ne 0}} P(w_{t+j}|w_t;\theta) \end{align*}\) where $\theta$ is all variables to be optimized. The objective function $J(\theta)$ is the (average) negative log likelihood: \(\begin{align*} J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T}\sum_{t=1}^T \sum_{\substack{-m\leqq j\leqq m \\ j\ne 0}}\log P(w_{t+j}|w_t;\theta) \end{align*}\) Minimizing objective function $\Leftrightarrow$ Maximizing predictive accuracy Question: How to calculate $P(w_{t+j}\vert w_t;\theta)$? Answer: We will use two vectors per word w: $v_w$ when w is a center word $u_w$ when w is a context word Then for a centor word c and a context word o: \(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\) Word2vec: prediction function \(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\) $u_w^T v_c$: Dot product compares similarity of o and c. $u^T v = u\ .v = \sum_{i=1}^n u_i v_i$ Larger dot product = larger probability $\exp$: Exponentiation makes anything positive $\sum_{w \in V}\exp(u_w^T v_c)$: Normalize over entire vocabulary to give probability distribution. This is an example of the softmax function $\mathbb{R}^n \rightarrow (0,1)^n$(Open region) that maps arbitary values $x_i$ to a probability distribution $p_i$ \(\mbox{softmax}(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^n\exp(x_j)} = p_i\) To train the model: Optimize value of parameters to minimize loss Recall: $\theta$ represents all the model parameters, in one long vector In our case, with d-dimensional vectors and V-many words, we have: $\theta \in \mathbb{R}^{2dV}$ Remember: every word has two vectors We optimize these parameters by walking down the gradient(gradient descent) We compute all vector gradients Word2vec derivations of gradient \(\begin{align*} J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T}\sum_{t=1}^T \sum_{\substack{-m\leqq j\leqq m \\ j\ne 0}}\log P(w_{t+j}|w_t;\theta) \end{align*}\) \(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\) \(\frac{\partial}{\partial v_c} \log \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)} = \underbrace{\frac{\partial}{\partial v_c} \log \exp(u_o^T v_c)}_{(1)} - \underbrace{\frac{\partial}{\partial v_c} \log \sum_{w=1}^V \exp(u_w^T v_c)}_{(2)}\) \(\begin{align*}\cdots (1) &amp;amp;= \frac{\partial}{\partial v_c} u_o^T v_c \\ &amp;amp;= u_o \end{align*}\) \(\begin{align*}\cdots (2) &amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \frac{\partial}{\partial v_c} \exp(u_x^T v_c) \\ &amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \exp(u_x^T v_c) \cdot \frac{\partial}{\partial v_c} u_x^T v_c \\ &amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \exp(u_x^T v_c) \cdot u_x \\ \end{align*} \\\) \(\begin{align*} \frac{\partial}{\partial v_c} \log P(o|c) &amp;amp;= u_o - \frac{\sum_{x=1}^V \exp(u_x^T v_c)u_x}{\sum_{w=1}^V \exp(u_w^T v_c)} \\ &amp;amp; = u_o - \sum_{x=1}^V \underbrace{\frac{\exp(u_x^T v_c)}{\sum_{w=1}^V \exp(u_w^T v_c)}}_{\mbox{softmax formula}} u_x \\ &amp;amp; = u_o - \underbrace{\sum_{x=1}^V P(x|c) u_x}_{\mbox{Expectation}} \\ &amp;amp; = \mbox{observed} - \mbox{expected} \end{align*}\)</summary></entry><entry><title type="html">Mask R-CNN</title><link href="http://0.0.0.0:4000/Mask_R-CNN" rel="alternate" type="text/html" title="Mask R-CNN" /><published>2022-02-15T00:00:00+00:00</published><updated>2022-02-15T00:00:00+00:00</updated><id>http://0.0.0.0:4000/Mask_R-CNN</id><content type="html" xml:base="http://0.0.0.0:4000/Mask_R-CNN">&lt;h2 id=&quot;mask-r-cnn&quot;&gt;Mask R-CNN&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick, ICCV 2017&lt;/em&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/facebookresearch/Detectron&quot;&gt;https://github.com/facebookresearch/Detectron&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;whats-different&quot;&gt;What’s different?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Models so far&lt;br /&gt;
  R-CNN: 2-stage model for Object detection&lt;br /&gt;
  Fast R-CNN: RoI on feature map&lt;br /&gt;
  Faster R-CNN: RPN network&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Instance Segmentation&lt;/em&gt;&lt;br /&gt;
  Combining to tasks:
    &lt;ul&gt;
      &lt;li&gt;Object detection(Fast/Faster R-CNN): classify individual objects and localize each using a bounding box.&lt;/li&gt;
      &lt;li&gt;Semantic segmentation(FCN; Fully Convolutional Network): classify each pixel into a fixed set of categories without differentiating object instances.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Mask R-CNN:&lt;br /&gt;
  1) Model for &lt;em&gt;instance segmentation&lt;/em&gt;: Mask prediction branch&lt;br /&gt;
  2) FPN(feature pyramid network) before RPN&lt;br /&gt;
  3) RoI align&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mask-prediction&quot;&gt;Mask prediction&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Mask loss&lt;br /&gt;
  In the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask(ones to the object and zeros elsewhere) for each RoI. Defined multi-task loss on each sampled RoI: $L = L_{cls} + L_{box} + L_{mask}$&lt;br /&gt;
  The mask branch has a $Km^2$-dimensional output for each RoI, which encodes &lt;em&gt;K&lt;/em&gt; binary masks of resolution $m\times m$, one for each of the &lt;em&gt;K&lt;/em&gt; classes. To this we apply a per-pixel sigmoid, and define $L_{mask}$ as the &lt;strong&gt;average binary cross-entropy loss&lt;/strong&gt;.  For an RoI associated with ground-truth class &lt;em&gt;k&lt;/em&gt;, $L_{mask}$ is only defined on the &lt;em&gt;k&lt;/em&gt;-th mask(other mask outputs do not contribute to the loss).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Decouples&lt;/em&gt; mask and class prediction&lt;br /&gt;
  This definition of mask loss allows the network to generate masks for each class without competition among classes; we rely on the dedicated classification branch to predict the class label used to select the output mask.&lt;br /&gt;
  With a per-pixel sigmoid and a binary loss, masks do not compete across classes; in contrast to FCNs for semantic segmentation using a per-pixel softmax and a multinomial cross-entropy loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mask Representation&lt;br /&gt;
  Unlike class labels or box offsets, extracting the spatial structure of masks can be addressed naturally by the pixel-to-pixel correspondence provided by convolutions.&lt;br /&gt;
  Predicting an $m\times m$ mask from each RoI using an FCN, allows each layer in the mask branch to have $m\times m$ object spatial layout without collapsing it into a vector representation that lacks spatial dimensions.&lt;br /&gt;
  This pixel-to-pixel behavior requires RoI features, small cropped feature maps, to be well aligned to faithfully preserve the explicit per-pixel spatial correspondence; &lt;em&gt;RoIAlign&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;roialign&quot;&gt;RoIAlign&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RoIPool(or RoI Pooling)&lt;br /&gt;
  &lt;em&gt;Quantizes&lt;/em&gt; a floating-number RoI to the discrete granularity(integerize by &lt;em&gt;rounding&lt;/em&gt;) of the feature map, its result is then subdivided into spatial bins, and finally feature values covered by each bin are aggregated(usually by max pooling).
    &lt;ul&gt;
      &lt;li&gt;Problem: Quantizations introduce misalignments between the RoI and the extracted features. This may not impact classification, which is robust to small translations, but it has a large negative effect on predicting pixel-accurate masks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RoIAlign layer&lt;br /&gt;
  Instead of any quantization of the RoI boundaries or bins, use bilinear interpolation(&lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial transformer networks&lt;/a&gt;) to compute the exact values of the input features at four regularly sampled locations in each RoI bin, and aggregate the result(using max or average).&lt;br /&gt;
  e.g.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_1.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  (from CS231n lecture)&lt;br /&gt;
  Feature $f_{xy}$ for point $(x, y)$ is a linear combination of features at its four neighboring grid cells:&lt;br /&gt;
  $f_{xy} = \sum_{i,j=1}^2 f_{i,j} \text{max}(0, 1 - \left\vert x - x_i \right\vert) \text{max}(0, 1 - \left\vert y - y_i \right\vert)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;RoIAlign improves mask accuracy by relative 10% to 50%, showing bigger gains under stricter localization metrics.&lt;br /&gt;
&lt;!--
  [Understanding Region of Interest — (RoI Pooling)](https://towardsdatascience.com/understanding-region-of-interest-part-1-roi-pooling-e4f5dd65bb44)  
  [Understanding Region of Interest — (RoI Align and RoI Warp)](https://towardsdatascience.com/understanding-region-of-interest-part-2-roi-align-and-roi-warp-f795196fc193)  
--&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;backbone&lt;/em&gt;:  Faster R-CNN with an FPN(ResNet-FPN)
    &lt;ul&gt;
      &lt;li&gt;FPN, Feature pyramid network(&lt;a href=&quot;https://arxiv.org/abs/1612.03144&quot;&gt;Lin et al.&lt;/a&gt;):&lt;br /&gt;
  Uses a top-down architecture with lateral connections to build an in-network feature pyramid from a single-scale input.&lt;/li&gt;
      &lt;li&gt;RPN:&lt;br /&gt;
  RoI align on each FPN feature maps&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;head&lt;/em&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  Add a fully convolutional mask prediction branch, extending the Faster R-CNN box heads from the ResNet and FPN papers. Train with additional mask loss.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Comparison to the sota methods in instance segmentation&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/papers/Mask_R-CNN_3.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/papers/Mask_R-CNN_4.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ablations
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_5.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Multinomial vs. Independent Masks&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_6.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Class-Specific vs. Class-Agnostic Masks&lt;/strong&gt;&lt;br /&gt;
  Interestingly, Mask R-CNN with classagnostic masks(predicting a single &lt;em&gt;m×m&lt;/em&gt; output regardless of class)) is nearly as effective as class-specific masks(default; &lt;em&gt;m×m&lt;/em&gt; mask per class).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;RoIAlign&lt;/strong&gt;&lt;br /&gt;
  ResNet50-C4 backbone of stride 16&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_7.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  ResNet-50-C5 backbone of stride 32&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_8.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  Note that with RoIAlign, using stride-32 C5 features is more accurate than using stride-16 C4 features. Used with FPN, which has finer multi-level strides, RoIAlign shows better result.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Mask branch&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_9.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bounding Box Detection Results&lt;/strong&gt;&lt;br /&gt;
  Our approach largely closes the gap between object detection and the more 	challenging instance segmentation task.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/papers/Mask_R-CNN_10.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;mask-r-cnn-for-human-pose-estimation&quot;&gt;Mask R-CNN for Human Pose Estimation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;By modeling a keypoint’s location as a one-hot mask, and adopt Mask R-CNN to predict &lt;em&gt;K&lt;/em&gt; masks, one for each of &lt;em&gt;K&lt;/em&gt; keypoint types, this framework can easily be extended to human pose estimation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Main Results and Ablations&lt;/strong&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_11.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_12.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_13.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$\therefore$ We have &lt;em&gt;a unified model that can simultaneously predict boxes, segments, and keypoints&lt;/em&gt; while running at 5 fps.&lt;/p&gt;

&lt;!-- https://ganghee-lee.tistory.com/40 --&gt;</content><author><name>Darron Kwon</name></author><category term="papers" /><summary type="html">Mask R-CNN Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick, ICCV 2017 https://github.com/facebookresearch/Detectron What’s different? Models so far R-CNN: 2-stage model for Object detection Fast R-CNN: RoI on feature map Faster R-CNN: RPN network Instance Segmentation Combining to tasks: Object detection(Fast/Faster R-CNN): classify individual objects and localize each using a bounding box. Semantic segmentation(FCN; Fully Convolutional Network): classify each pixel into a fixed set of categories without differentiating object instances. Mask R-CNN: 1) Model for instance segmentation: Mask prediction branch 2) FPN(feature pyramid network) before RPN 3) RoI align Mask prediction Mask loss In the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask(ones to the object and zeros elsewhere) for each RoI. Defined multi-task loss on each sampled RoI: $L = L_{cls} + L_{box} + L_{mask}$ The mask branch has a $Km^2$-dimensional output for each RoI, which encodes K binary masks of resolution $m\times m$, one for each of the K classes. To this we apply a per-pixel sigmoid, and define $L_{mask}$ as the average binary cross-entropy loss. For an RoI associated with ground-truth class k, $L_{mask}$ is only defined on the k-th mask(other mask outputs do not contribute to the loss). Decouples mask and class prediction This definition of mask loss allows the network to generate masks for each class without competition among classes; we rely on the dedicated classification branch to predict the class label used to select the output mask. With a per-pixel sigmoid and a binary loss, masks do not compete across classes; in contrast to FCNs for semantic segmentation using a per-pixel softmax and a multinomial cross-entropy loss. Mask Representation Unlike class labels or box offsets, extracting the spatial structure of masks can be addressed naturally by the pixel-to-pixel correspondence provided by convolutions. Predicting an $m\times m$ mask from each RoI using an FCN, allows each layer in the mask branch to have $m\times m$ object spatial layout without collapsing it into a vector representation that lacks spatial dimensions. This pixel-to-pixel behavior requires RoI features, small cropped feature maps, to be well aligned to faithfully preserve the explicit per-pixel spatial correspondence; RoIAlign. RoIAlign RoIPool(or RoI Pooling) Quantizes a floating-number RoI to the discrete granularity(integerize by rounding) of the feature map, its result is then subdivided into spatial bins, and finally feature values covered by each bin are aggregated(usually by max pooling). Problem: Quantizations introduce misalignments between the RoI and the extracted features. This may not impact classification, which is robust to small translations, but it has a large negative effect on predicting pixel-accurate masks. RoIAlign layer Instead of any quantization of the RoI boundaries or bins, use bilinear interpolation(Spatial transformer networks) to compute the exact values of the input features at four regularly sampled locations in each RoI bin, and aggregate the result(using max or average). e.g. (from CS231n lecture) Feature $f_{xy}$ for point $(x, y)$ is a linear combination of features at its four neighboring grid cells: $f_{xy} = \sum_{i,j=1}^2 f_{i,j} \text{max}(0, 1 - \left\vert x - x_i \right\vert) \text{max}(0, 1 - \left\vert y - y_i \right\vert)$ RoIAlign improves mask accuracy by relative 10% to 50%, showing bigger gains under stricter localization metrics. Network Architecture backbone: Faster R-CNN with an FPN(ResNet-FPN) FPN, Feature pyramid network(Lin et al.): Uses a top-down architecture with lateral connections to build an in-network feature pyramid from a single-scale input. RPN: RoI align on each FPN feature maps head: Add a fully convolutional mask prediction branch, extending the Faster R-CNN box heads from the ResNet and FPN papers. Train with additional mask loss. Experiments Comparison to the sota methods in instance segmentation Ablations Architecture Multinomial vs. Independent Masks Class-Specific vs. Class-Agnostic Masks Interestingly, Mask R-CNN with classagnostic masks(predicting a single m×m output regardless of class)) is nearly as effective as class-specific masks(default; m×m mask per class). RoIAlign ResNet50-C4 backbone of stride 16 ResNet-50-C5 backbone of stride 32 Note that with RoIAlign, using stride-32 C5 features is more accurate than using stride-16 C4 features. Used with FPN, which has finer multi-level strides, RoIAlign shows better result. Mask branch Bounding Box Detection Results Our approach largely closes the gap between object detection and the more challenging instance segmentation task. Mask R-CNN for Human Pose Estimation By modeling a keypoint’s location as a one-hot mask, and adopt Mask R-CNN to predict K masks, one for each of K keypoint types, this framework can easily be extended to human pose estimation. Main Results and Ablations: $\therefore$ We have a unified model that can simultaneously predict boxes, segments, and keypoints while running at 5 fps.</summary></entry><entry><title type="html">Starfish detection w/ TF Object Detection API</title><link href="http://0.0.0.0:4000/starfish_detection" rel="alternate" type="text/html" title="Starfish detection w/ TF Object Detection API" /><published>2022-02-14T15:00:00+00:00</published><updated>2022-02-14T15:00:00+00:00</updated><id>http://0.0.0.0:4000/starfish_detection</id><content type="html" xml:base="http://0.0.0.0:4000/starfish_detection">&lt;h2 id=&quot;tensorflow---help-protect-the-great-barrier-reef&quot;&gt;TensorFlow - Help Protect the Great Barrier Reef&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Worked in Feb. 2022. to study object detection model&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Task:&lt;br /&gt;
  Underwater + Small object detection&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Score(IoU=0.50:0.95):&lt;br /&gt;
  &lt;em&gt;mAP@100: 0.364686 / AR@100: 0.491768 / Expected F2: 0.459727&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Direct link: &lt;a href=&quot;https://www.kaggle.com/kwondalhyeon/starfish-detection-w-tf-object-detection-api?scriptVersionId=87885389&quot;&gt;kaggle notebook&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;iframe src=&quot;https://www.kaggle.com/embed/kwondalhyeon/starfish-detection-w-tf-object-detection-api?kernelSessionId=87885389&quot; height=&quot;1200&quot; style=&quot;margin: 0 auto; width: 100%; max-width: 100%;&quot; frameborder=&quot;0&quot; scrolling=&quot;auto&quot; title=&quot;Starfish detection w/ TF Object Detection API&quot;&gt;&lt;/iframe&gt;</content><author><name>Darron Kwon</name></author><category term="projects" /><summary type="html">TensorFlow - Help Protect the Great Barrier Reef Worked in Feb. 2022. to study object detection model Task: Underwater + Small object detection Score(IoU=0.50:0.95): mAP@100: 0.364686 / AR@100: 0.491768 / Expected F2: 0.459727 Direct link: kaggle notebook</summary></entry><entry><title type="html">cs231n - Lecture 15. Detection and Segmentation</title><link href="http://0.0.0.0:4000/cs231n_lec15" rel="alternate" type="text/html" title="cs231n - Lecture 15. Detection and Segmentation" /><published>2022-02-07T00:00:00+00:00</published><updated>2022-02-07T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs231n_lec15</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec15">&lt;h3 id=&quot;computer-vision-tasks&quot;&gt;Computer Vision Tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Image Classification: No spatial extent&lt;/li&gt;
  &lt;li&gt;Semantic Segmentation: No objects, just pixels&lt;/li&gt;
  &lt;li&gt;Object Detection/ Instance Segmentation: Multiple objects&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;semantic-segmentation&quot;&gt;Semantic Segmentation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Paired training data:&lt;br /&gt;
  For each training image, &lt;strong&gt;each pixel is labeled&lt;/strong&gt; with a semantic category.&lt;/li&gt;
  &lt;li&gt;At test time, classify each pixel of a new image.&lt;/li&gt;
  &lt;li&gt;Problem:&lt;br /&gt;
  Classifying with only single pixel does not include context information.&lt;/li&gt;
  &lt;li&gt;Idea:
    &lt;ul&gt;
      &lt;li&gt;Sliding Window&lt;br /&gt;
  Extract patch from full image, classify center pixel with CNN.&lt;br /&gt;
  $\color{red}{(-)}$ Very inefficient, not reusing shared features between overlapping patches.&lt;/li&gt;
      &lt;li&gt;Convolution&lt;br /&gt;
  Encode the entire image with conv net, and do semantic segmentation on top.&lt;br /&gt;
  $\color{red}{(-)}$ CNN architectures often change the spatial sizes, but semantic segmentation requires the output size to be same as input size.&lt;/li&gt;
      &lt;li&gt;Fully Convolutional&lt;br /&gt;
  Design a network with &lt;strong&gt;only&lt;/strong&gt; convolutional layers without downsampling operators&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
  $\color{red}{(-)}$ convolutions at original image resolution is very expensive&lt;br /&gt;
  $\rightarrow$ Design convolutional network with &lt;strong&gt;downsampling and upsampling&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Downsampling: Pooling, strided convolution&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In-Network Upsampling: Unpooling, strided transpose convolution&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unpooling:&lt;br /&gt;
  Nearest Neighbor: copy-paste to extended region&lt;br /&gt;
  “Bed of Nails”: no positional argument, pad with zeros&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Max Unpooling: use positions from poolying layer ahead, pad with zeros&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learnable Downsampling: Strided convolution&lt;br /&gt;
  Output is a dot product between filter and input&lt;br /&gt;
  Stride gives ratio between movement in input and output&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learnable Upsampling: Transposed convolution&lt;br /&gt;
  Input gives weight for filter&lt;br /&gt;
  Output contains copies of the filter weighted by the input, summing at where at overlaps in the output&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Summary&lt;br /&gt;
  Label each pixel in the image with a category label&lt;br /&gt;
  Don’t differentiate instances, only care about pixels&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;object-detection&quot;&gt;Object Detection&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multiple Objects:&lt;br /&gt;
  Each image needs a different number of outputs;&lt;br /&gt;
  $\rightarrow$ Apply a CNN to many different crops of the image, CNN classifies each crop as object or background.&lt;br /&gt;
  $\color{red}{(-)}$ Need to apply CNN to huge number of locations, scales, and aspect ratios, very computationally expensive.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;r-cnn&quot;&gt;R-CNN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_4.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Girshick et al, “Rich feature hierarchies for accurate object detection and
semantic segmentation”, CVPR 2014&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;2-stage Detector: Region Proposal + Region Classification
    &lt;ol&gt;
      &lt;li&gt;Image as input&lt;/li&gt;
      &lt;li&gt;Crop bounding boxes with Selective Search&lt;br /&gt;
 Warp into same size pixels for CNN model&lt;/li&gt;
      &lt;li&gt;Input Warped images into CNN&lt;/li&gt;
      &lt;li&gt;Run classification on each&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;ol&gt;
      &lt;li&gt;Region Proposals: Selective Search&lt;br /&gt;
 Find “blobby” image regions that are likely to contain objects.&lt;br /&gt;
 Relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU.&lt;/li&gt;
      &lt;li&gt;CNN:&lt;br /&gt;
 For a pre-trained CNN architecture, change the number of classes on the last classification layer(detection classes &lt;em&gt;N&lt;/em&gt; + background &lt;em&gt;1&lt;/em&gt;), fine-tune with dataset for Object Detection. From the region proposal input, outputs a fixed-length feature vector.&lt;/li&gt;
      &lt;li&gt;SVM: Category-Specific Linear SVMs&lt;br /&gt;
 Positive: ground-truth boxes&lt;br /&gt;
 Negative: IoU under 0.3&lt;br /&gt;
 Scores each feature vector for classes, classifies whether each one is positive/negative(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_object&lt;/code&gt;).&lt;/li&gt;
      &lt;li&gt;Non-Maximum Suppression: with concept of &lt;strong&gt;IoU&lt;/strong&gt;&lt;br /&gt;
 Intersection over Union; area of intersection divided by area of union&lt;br /&gt;
 If there are two boxes with IoU over 0.5, consider them proposed on the same object, leave one with the highest score.&lt;/li&gt;
      &lt;li&gt;Bounding Box Regression: adjust boxes from Selective Search
        &lt;ul&gt;
          &lt;li&gt;Algorithm:&lt;br /&gt;
 Assume a bounding box $P^i = (P_x^i, P_y^i, P_w^i, P_h^i)$,&lt;br /&gt;
 Ground-truth box $G = (G_x, G_y, G_w, G_h)$.&lt;br /&gt;
 Define a function $d$, mapping $P$ close to $G$;&lt;br /&gt;
 \(\hat{G}_x = P_w d_x(P) + P_x\)&lt;br /&gt;
 \(\hat{G}_y = P_h d_y(P) + P_y\)&lt;br /&gt;
 \(\hat{G}_w = P_w \mbox{exp}(d_w(P))\)&lt;br /&gt;
 \(\hat{G}_h = P_h \mbox{exp}(d_h(P))\)&lt;br /&gt;
 where $d_{\star}(P) = w_{\star}^T \phi_5(P)$, is modeled as a linear function(learnable weight vector &lt;em&gt;w&lt;/em&gt;) of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POOL5&lt;/code&gt; features of proposal &lt;em&gt;P&lt;/em&gt;($\phi_5(P)$). We learn $w_{\star}$ by optimizing the regularized least squares objective(Ridge regression)&lt;br /&gt;
  &lt;em&gt;Learnable parameters on: 2, 3, 5&lt;/em&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Summary:&lt;br /&gt;
  Score: 53.7% on Pascal VOC 2010&lt;br /&gt;
  Problem:&lt;br /&gt;
      1. Low Performance; Warping images into 224x224 size for AlexNet&lt;br /&gt;
      2. Slow; Using all candidates from Selective Search&lt;br /&gt;
      3. Not GPU-optimized; Using Selective Search and SVM&lt;br /&gt;
      4. No Back Propagation; Not sharing computations&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fast-r-cnn&quot;&gt;Fast R-CNN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_5.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Girshick, “Fast R-CNN”, ICCV 2015&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;idea:&lt;br /&gt;
  Pass the image through convnet &lt;strong&gt;before&lt;/strong&gt; cropping. Crop the conv feature instead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;ol&gt;
      &lt;li&gt;Pass the full image through pre-trained CNN and extract feature maps.&lt;/li&gt;
      &lt;li&gt;Get RoIs from a proposal method(Selective Search) and crop by RoI Pooling, get fixed size feature vectors.&lt;/li&gt;
      &lt;li&gt;With RoI feature vectors, pass some fully connected layers and split into two branches.&lt;/li&gt;
      &lt;li&gt;1) pass softmax and classify the class of RoI. no SVM used. 2) Run bounding box regression.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Cropping Features: RoI Pool &lt;!--[](href) link to terminologies --&gt;
    &lt;ol&gt;
      &lt;li&gt;Project RoI proposals(on input image) onto CNN image features.&lt;/li&gt;
      &lt;li&gt;Divide into subregions.&lt;/li&gt;
      &lt;li&gt;Run pooling(Max-pool) within each subregion.&lt;br /&gt;
  $\rightarrow$ Region features always be the same size regardless of input region size&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;faster-r-cnn&quot;&gt;Faster R-CNN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_6.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Ren et al, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”, NIPS 2015&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;idea:&lt;br /&gt;
  Fast R-CNN is not GPU-optimized; runtime dominated by region proposals.&lt;br /&gt;
  By inserting Region Proposal Network(RPN), implemented end-to-end architecture.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;ol&gt;
      &lt;li&gt;Pass the full image through pre-trained CNN and extract feature maps.&lt;/li&gt;
      &lt;li&gt;RPN:&lt;br /&gt;
 For &lt;em&gt;K&lt;/em&gt; different anchor boxes of different size and scale at each point in the feature map, predict whether it contains an object(binary classification), and also predict a corrections from the anchor to the ground-truth box(regress 4 numbers per pixel).&lt;br /&gt;
 &lt;img src=&quot;/assets/images/cs231n_lec15_7.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;Jointly train with 4 losses:&lt;br /&gt;
 1) RPN classify object / not object&lt;br /&gt;
 2) RPN regress box coordinates&lt;br /&gt;
 3) Final classification score (object classes)&lt;br /&gt;
 4) Final box coordinates&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Glossing over many details:
    &lt;ul&gt;
      &lt;li&gt;Ignore overlapping proposals with non-max suppression&lt;/li&gt;
      &lt;li&gt;How are anchors determined?&lt;/li&gt;
      &lt;li&gt;How do we sample positive / negative samples for training the RPN?&lt;/li&gt;
      &lt;li&gt;How to parameterize bounding box regression?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Two-stage object detector:
    &lt;ul&gt;
      &lt;li&gt;First stage: Run once per image
        &lt;ul&gt;
          &lt;li&gt;Backbone network&lt;/li&gt;
          &lt;li&gt;Region proposal network(RPN)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Second stage: Run once per region
        &lt;ul&gt;
          &lt;li&gt;Crop features: RoI pool/ align&lt;/li&gt;
          &lt;li&gt;Predict object class&lt;/li&gt;
          &lt;li&gt;Prediction bbox offset&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;single-stage-object-detectors-yolo--ssd--retinanet&quot;&gt;Single-Stage Object Detectors: YOLO / SSD / RetinaNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Algorithm:
    &lt;ol&gt;
      &lt;li&gt;Divide input imgae into grid&lt;/li&gt;
      &lt;li&gt;Image a set of &lt;strong&gt;base boxes&lt;/strong&gt; centered at each grid cell&lt;/li&gt;
      &lt;li&gt;Within each grid cell:
        &lt;ul&gt;
          &lt;li&gt;Regress from each of the &lt;em&gt;B&lt;/em&gt; base boxes to a final box with 5 numbers(dx, dy, dh, dw, confidence)&lt;/li&gt;
          &lt;li&gt;Predict scores for each of &lt;em&gt;C&lt;/em&gt; classes(including background as a class)&lt;/li&gt;
          &lt;li&gt;Looks a lot like RPN, but category-specific&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_8.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;instance-segmentation-mask-r-cnn&quot;&gt;Instance Segmentation: Mask R-CNN&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;He et al, “Mask R-CNN”, ICCV 2017&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_9.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_10.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_11.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_12.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_13.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;open-source-frameworks&quot;&gt;Open Source Frameworks&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/object_detection&quot;&gt;TensorFlow Detection API&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;&gt;Detectron2(Pytorch)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;beyond-2d-object-detection&quot;&gt;Beyond 2D Object Detection&lt;/h2&gt;

&lt;h3 id=&quot;object-detection--captioning-dense-captioning&quot;&gt;Object Detection + Captioning: Dense Captioning&lt;/h3&gt;

&lt;h3 id=&quot;dense-video-captioning-timestep-t&quot;&gt;Dense Video Captioning: timestep “T”&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_14.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;objects--relationships-scene-graphs&quot;&gt;Objects + Relationships: Scene Graphs&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_15.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_16.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3d-object-detection&quot;&gt;3D Object Detection&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_17.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;2D bounding box: (x, y, w, h)&lt;br /&gt;
  $\rightarrow$ 3D oriented bounding box: (x, y, z, w, h, l, r, p, y)&lt;br /&gt;
  $\rightarrow$ Simplified bbox: no roll &amp;amp; pitch&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Simple Camera Model:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_18.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
  A point on the image plane corresponds to a &lt;strong&gt;ray&lt;/strong&gt; in the 3D space&lt;br /&gt;
  A 2D bounding box on an image is a &lt;strong&gt;frustrum&lt;/strong&gt; in the 3D space&lt;br /&gt;
  Localize an object in 3D: The object can be anywhere in the &lt;strong&gt;camera viewing frustrum&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Monocular Camera:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_19.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;80%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;Same idea as Faster RCNN, but proposals are in 3D&lt;/li&gt;
      &lt;li&gt;3D bounding box proposal, regress 3D box parameters + class score&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;3D Shape Prediction: Mesh R-CNN
&lt;img src=&quot;/assets/images/cs231n_lec15_20.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Computer Vision Tasks Image Classification: No spatial extent Semantic Segmentation: No objects, just pixels Object Detection/ Instance Segmentation: Multiple objects Semantic Segmentation Paired training data: For each training image, each pixel is labeled with a semantic category. At test time, classify each pixel of a new image. Problem: Classifying with only single pixel does not include context information. Idea: Sliding Window Extract patch from full image, classify center pixel with CNN. $\color{red}{(-)}$ Very inefficient, not reusing shared features between overlapping patches. Convolution Encode the entire image with conv net, and do semantic segmentation on top. $\color{red}{(-)}$ CNN architectures often change the spatial sizes, but semantic segmentation requires the output size to be same as input size. Fully Convolutional Design a network with only convolutional layers without downsampling operators $\color{red}{(-)}$ convolutions at original image resolution is very expensive $\rightarrow$ Design convolutional network with downsampling and upsampling Downsampling: Pooling, strided convolution In-Network Upsampling: Unpooling, strided transpose convolution Unpooling: Nearest Neighbor: copy-paste to extended region “Bed of Nails”: no positional argument, pad with zeros Max Unpooling: use positions from poolying layer ahead, pad with zeros Learnable Downsampling: Strided convolution Output is a dot product between filter and input Stride gives ratio between movement in input and output Learnable Upsampling: Transposed convolution Input gives weight for filter Output contains copies of the filter weighted by the input, summing at where at overlaps in the output Summary Label each pixel in the image with a category label Don’t differentiate instances, only care about pixels Object Detection Multiple Objects: Each image needs a different number of outputs; $\rightarrow$ Apply a CNN to many different crops of the image, CNN classifies each crop as object or background. $\color{red}{(-)}$ Need to apply CNN to huge number of locations, scales, and aspect ratios, very computationally expensive. R-CNN Girshick et al, “Rich feature hierarchies for accurate object detection and semantic segmentation”, CVPR 2014 2-stage Detector: Region Proposal + Region Classification Image as input Crop bounding boxes with Selective Search Warp into same size pixels for CNN model Input Warped images into CNN Run classification on each Algorithm: Region Proposals: Selective Search Find “blobby” image regions that are likely to contain objects. Relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU. CNN: For a pre-trained CNN architecture, change the number of classes on the last classification layer(detection classes N + background 1), fine-tune with dataset for Object Detection. From the region proposal input, outputs a fixed-length feature vector. SVM: Category-Specific Linear SVMs Positive: ground-truth boxes Negative: IoU under 0.3 Scores each feature vector for classes, classifies whether each one is positive/negative(is_object). Non-Maximum Suppression: with concept of IoU Intersection over Union; area of intersection divided by area of union If there are two boxes with IoU over 0.5, consider them proposed on the same object, leave one with the highest score. Bounding Box Regression: adjust boxes from Selective Search Algorithm: Assume a bounding box $P^i = (P_x^i, P_y^i, P_w^i, P_h^i)$, Ground-truth box $G = (G_x, G_y, G_w, G_h)$. Define a function $d$, mapping $P$ close to $G$; \(\hat{G}_x = P_w d_x(P) + P_x\) \(\hat{G}_y = P_h d_y(P) + P_y\) \(\hat{G}_w = P_w \mbox{exp}(d_w(P))\) \(\hat{G}_h = P_h \mbox{exp}(d_h(P))\) where $d_{\star}(P) = w_{\star}^T \phi_5(P)$, is modeled as a linear function(learnable weight vector w) of the POOL5 features of proposal P($\phi_5(P)$). We learn $w_{\star}$ by optimizing the regularized least squares objective(Ridge regression) Learnable parameters on: 2, 3, 5 Summary: Score: 53.7% on Pascal VOC 2010 Problem: 1. Low Performance; Warping images into 224x224 size for AlexNet 2. Slow; Using all candidates from Selective Search 3. Not GPU-optimized; Using Selective Search and SVM 4. No Back Propagation; Not sharing computations Fast R-CNN Girshick, “Fast R-CNN”, ICCV 2015 idea: Pass the image through convnet before cropping. Crop the conv feature instead. Algorithm: Pass the full image through pre-trained CNN and extract feature maps. Get RoIs from a proposal method(Selective Search) and crop by RoI Pooling, get fixed size feature vectors. With RoI feature vectors, pass some fully connected layers and split into two branches. 1) pass softmax and classify the class of RoI. no SVM used. 2) Run bounding box regression. Cropping Features: RoI Pool Project RoI proposals(on input image) onto CNN image features. Divide into subregions. Run pooling(Max-pool) within each subregion. $\rightarrow$ Region features always be the same size regardless of input region size Faster R-CNN Ren et al, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”, NIPS 2015 idea: Fast R-CNN is not GPU-optimized; runtime dominated by region proposals. By inserting Region Proposal Network(RPN), implemented end-to-end architecture. Algorithm: Pass the full image through pre-trained CNN and extract feature maps. RPN: For K different anchor boxes of different size and scale at each point in the feature map, predict whether it contains an object(binary classification), and also predict a corrections from the anchor to the ground-truth box(regress 4 numbers per pixel). Jointly train with 4 losses: 1) RPN classify object / not object 2) RPN regress box coordinates 3) Final classification score (object classes) 4) Final box coordinates Glossing over many details: Ignore overlapping proposals with non-max suppression How are anchors determined? How do we sample positive / negative samples for training the RPN? How to parameterize bounding box regression? Two-stage object detector: First stage: Run once per image Backbone network Region proposal network(RPN) Second stage: Run once per region Crop features: RoI pool/ align Predict object class Prediction bbox offset Single-Stage Object Detectors: YOLO / SSD / RetinaNet Algorithm: Divide input imgae into grid Image a set of base boxes centered at each grid cell Within each grid cell: Regress from each of the B base boxes to a final box with 5 numbers(dx, dy, dh, dw, confidence) Predict scores for each of C classes(including background as a class) Looks a lot like RPN, but category-specific Instance Segmentation: Mask R-CNN He et al, “Mask R-CNN”, ICCV 2017 Open Source Frameworks TensorFlow Detection API Detectron2(Pytorch) Beyond 2D Object Detection Object Detection + Captioning: Dense Captioning Dense Video Captioning: timestep “T” Objects + Relationships: Scene Graphs 3D Object Detection 2D bounding box: (x, y, w, h) $\rightarrow$ 3D oriented bounding box: (x, y, z, w, h, l, r, p, y) $\rightarrow$ Simplified bbox: no roll &amp;amp; pitch Simple Camera Model: A point on the image plane corresponds to a ray in the 3D space A 2D bounding box on an image is a frustrum in the 3D space Localize an object in 3D: The object can be anywhere in the camera viewing frustrum Monocular Camera: Same idea as Faster RCNN, but proposals are in 3D 3D bounding box proposal, regress 3D box parameters + class score 3D Shape Prediction: Mesh R-CNN</summary></entry><entry><title type="html">cs231n - Lecture 14. Visualizing and Understanding</title><link href="http://0.0.0.0:4000/cs231n_lec14" rel="alternate" type="text/html" title="cs231n - Lecture 14. Visualizing and Understanding" /><published>2022-02-02T15:00:00+00:00</published><updated>2022-02-02T15:00:00+00:00</updated><id>http://0.0.0.0:4000/cs231n_lec14</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec14">&lt;h2 id=&quot;whats-going-on-inside-convnets&quot;&gt;What’s going on inside ConvNets?&lt;/h2&gt;

&lt;h3 id=&quot;visualizing-what-models-have-learned&quot;&gt;Visualizing what models have learned&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;First layer: Visualize Filters&lt;br /&gt;
  At the first layer, we can visualize the raw weights and see gabor-like features. While the higher layers are about the weights to the activations from the layer before, it is not very interpretable.&lt;/li&gt;
  &lt;li&gt;Last layer: Visualize Representations(feature vector)
    &lt;ul&gt;
      &lt;li&gt;Nearest neighbors in feature space&lt;/li&gt;
      &lt;li&gt;Dimensionality reduction: clustering with similarity; using simple algorithm(PCA) or more complex one(t-SNE)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Visualizing Activations:&lt;br /&gt;
  &lt;em&gt;Yosinski et al, “Understanding Neural Networks Through Deep Visualization”, ICML DL Workshop 2014&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;understanding-input-pixels&quot;&gt;Understanding input pixels&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Maximally Activating Patches&lt;br /&gt;
  Run many images, record values of chosen channel(layer or neuron) and visualize image patches that correspond to maximal activations.&lt;/li&gt;
  &lt;li&gt;Saliency via Occlusion: &lt;em&gt;Zeiler and Fergus, “Visualizing and Understanding Convolutional Networks”, ECCV 2014&lt;/em&gt;.&lt;br /&gt;
  Mask part of the image before feeding to CNN, slide the occluder and check how much predicted probabilities change. Found that when there are multiple objects, the classification performance improved as the false class object masked.&lt;/li&gt;
  &lt;li&gt;Which pixels matter: Saliency via Backprop
    &lt;ul&gt;
      &lt;li&gt;Visualize the data gradient
        &lt;ol&gt;
          &lt;li&gt;foward an image&lt;/li&gt;
          &lt;li&gt;set &lt;em&gt;activations&lt;/em&gt; in layer of interest to all zero, except for a 1.0 for a neuron of interest&lt;/li&gt;
          &lt;li&gt;backprop to image input&lt;/li&gt;
          &lt;li&gt;squish the channels of gradients to get 1-dimensional activation map(or we can run segmentation using grabcut on this heatmap).&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;Can also find biases; to see what false classifier actually see&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Intermediate Features via (guided) backprop
    &lt;ul&gt;
      &lt;li&gt;Deconvolution-based approach:
        &lt;ol&gt;
          &lt;li&gt;Feed image into net&lt;/li&gt;
          &lt;li&gt;Pick a layer, set the gradient there to be all zero except for one 1 for some neuron of interest&lt;/li&gt;
          &lt;li&gt;Backprop to image(use guided backprop to pass the positive influence; activation with positive values, using modified relu or deconvnet)&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Visualizing CNN features: Gradient Ascent
  (Guided) backprop: Find the part of an image that a neuron responds to.&lt;br /&gt;
  Gradient ascent: Generate a synthetic image that maximally activates a neuron.&lt;br /&gt;
  \(I^{\ast} = \mbox{argmax}_I f(I) + R(I)\); (neuron value + regularizer)
    &lt;ul&gt;
      &lt;li&gt;Optimization-based approach: freeze/fix the weights and run “image update” to find images that maximize the score of chosen class.&lt;br /&gt;
  &lt;em&gt;Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, “Deep Inside Convolutional Networks: Visualizing Image Classification Models and Saliency Maps”, Workshop at ICLR, 2014&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;Find images that maximize some class score: \(\mbox{argmax}_I S_c(I) - \lambda {\lVert I \rVert}^2_2\)
        &lt;ol&gt;
          &lt;li&gt;initialize image to zeros&lt;/li&gt;
          &lt;li&gt;forward image to compute current scores&lt;/li&gt;
          &lt;li&gt;set the &lt;em&gt;gradient&lt;/em&gt; of the scores vector to be &lt;em&gt;I&lt;/em&gt;, then backprop to image&lt;/li&gt;
          &lt;li&gt;make a small update to the image&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;Instead of using L2 norm, we can use better regularizer(Gaussian blur image, Clip pixels with small values/gradients to 0, …)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimize in FC6 latent space instead of pixel space:&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec14_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec14_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adversarial-perturbations&quot;&gt;Adversarial perturbations&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Fooling Images / Adversarial Examples
    &lt;ol&gt;
      &lt;li&gt;Start from an arbitrary image&lt;/li&gt;
      &lt;li&gt;Pick an arbitrary class&lt;/li&gt;
      &lt;li&gt;Modify the image to maximize the class&lt;/li&gt;
      &lt;li&gt;Repeat until network is fooled&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Ian Goodfellow, “Explaining and Harnessing Adversarial Examples”, 2014&lt;/em&gt;&lt;br /&gt;
  Classifier is vulnerable to adversarical perturbation because of its linear nature. Check &lt;a href=&quot;https://www.youtube.com/watch?v=CIfsB_EYsVI&quot;&gt;Ian Goodfellow’s lecture&lt;/a&gt; from 2017&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Moosavi-Dezfooli, Seyed-Mohsen, et al. “Universal adversarial perturbations”, IEEE, 2017.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;style-transfer&quot;&gt;Style Transfer&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;DeepDream: Amplify existing features&lt;br /&gt;
  Rather than synthesizing an image to maximize a specific neuron, instead try to &lt;strong&gt;amplify&lt;/strong&gt; the neuron activations at some layer in the network.&lt;/li&gt;
  &lt;li&gt;Choose an image and a layer in a CNN; repeat:
    &lt;ol&gt;
      &lt;li&gt;Forward: compute activations at chosen layer&lt;/li&gt;
      &lt;li&gt;Set gradient of chosen layer &lt;em&gt;equal to its activation&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Backward: Compute gradient on image&lt;/li&gt;
      &lt;li&gt;Update image&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec14_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feature Inversion&lt;br /&gt;
  &lt;em&gt;Mahendran and Vedaldi, “Understanding Deep Image Representations by Inverting Them”, CVPR 2015&lt;/em&gt;&lt;br /&gt;
  Given a CNN feature vector for an image, find a new image that:
    &lt;ul&gt;
      &lt;li&gt;Matches the given feature vector&lt;/li&gt;
      &lt;li&gt;“looks natural” (image prior regularization)&lt;br /&gt;
  \(\begin{align*}
  x^{\ast} &amp;amp;= \underset{x\in\mathbb{R}^{H \times W \times C}}{\mbox{argmin}} l(\Phi(x), \Phi_0) + \lambda \mathcal{R}(x) \\
      &amp;amp; \mbox{where loss } l = {\lVert \Phi(x) - \Phi_0 \rVert}^2
  \end{align*}\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Texture Synthesis: Nearest Neighbor&lt;br /&gt;
  Given a sample patch of some texture, generate a bigger image of the same texture&lt;br /&gt;
  Generate pixels one at a time in scanline order; form neighborhood of already generated pixels and copy nearest neighbor from input&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Neural Texture Synthesis: Gram Matrix&lt;br /&gt;
  a pair-wise statistics; interpret given &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CxHxW&lt;/code&gt; features as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HxW&lt;/code&gt; grid of C-dimensional vectors. By computing outer product and sum up for all spacial locations($G=V^T V$), it gives &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CxC&lt;/code&gt; matrix measuring co-occurrence.
    &lt;ol&gt;
      &lt;li&gt;Pretrain a CNN on ImageNet (VGG-19)&lt;/li&gt;
      &lt;li&gt;Run input texture forward through CNN, record activations on every layer&lt;/li&gt;
      &lt;li&gt;At each layer compute the Gram matrix&lt;/li&gt;
      &lt;li&gt;Initialize generated image from random noise&lt;/li&gt;
      &lt;li&gt;Pass generated image through CNN, compute Gram matrix on each layer&lt;/li&gt;
      &lt;li&gt;Compute loss: weighted sum of L2 distance between Gram matrices&lt;/li&gt;
      &lt;li&gt;Backprop to get gradient on image&lt;/li&gt;
      &lt;li&gt;Make gradient step on image&lt;/li&gt;
      &lt;li&gt;GOTO 5
&lt;img src=&quot;/assets/images/cs231n_lec14_3.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Neural Style Transfer: Feature + Gram Reconstruction
    &lt;ol&gt;
      &lt;li&gt;Extract content targets (ConvNet activations of all layers for the given image)&lt;/li&gt;
      &lt;li&gt;Extract style targets (Gram matrices of ConvNet activations of all layers)&lt;/li&gt;
      &lt;li&gt;Optimize over image to have:
        &lt;ul&gt;
          &lt;li&gt;The content of the content image(activations match content)&lt;/li&gt;
          &lt;li&gt;The style of the stlye image(Gram matrices of activations match style)&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec14_4.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;Problem: requires many forward, backward passes; VGG is very slow&lt;/li&gt;
          &lt;li&gt;Solution: Train another neural network; &lt;strong&gt;Fast Style Transfer&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Fast Style Transfer
  &lt;img src=&quot;/assets/images/cs231n_lec14_5.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">What’s going on inside ConvNets? Visualizing what models have learned First layer: Visualize Filters At the first layer, we can visualize the raw weights and see gabor-like features. While the higher layers are about the weights to the activations from the layer before, it is not very interpretable. Last layer: Visualize Representations(feature vector) Nearest neighbors in feature space Dimensionality reduction: clustering with similarity; using simple algorithm(PCA) or more complex one(t-SNE) Visualizing Activations: Yosinski et al, “Understanding Neural Networks Through Deep Visualization”, ICML DL Workshop 2014. Understanding input pixels Maximally Activating Patches Run many images, record values of chosen channel(layer or neuron) and visualize image patches that correspond to maximal activations. Saliency via Occlusion: Zeiler and Fergus, “Visualizing and Understanding Convolutional Networks”, ECCV 2014. Mask part of the image before feeding to CNN, slide the occluder and check how much predicted probabilities change. Found that when there are multiple objects, the classification performance improved as the false class object masked. Which pixels matter: Saliency via Backprop Visualize the data gradient foward an image set activations in layer of interest to all zero, except for a 1.0 for a neuron of interest backprop to image input squish the channels of gradients to get 1-dimensional activation map(or we can run segmentation using grabcut on this heatmap). Can also find biases; to see what false classifier actually see Intermediate Features via (guided) backprop Deconvolution-based approach: Feed image into net Pick a layer, set the gradient there to be all zero except for one 1 for some neuron of interest Backprop to image(use guided backprop to pass the positive influence; activation with positive values, using modified relu or deconvnet) Visualizing CNN features: Gradient Ascent (Guided) backprop: Find the part of an image that a neuron responds to. Gradient ascent: Generate a synthetic image that maximally activates a neuron. \(I^{\ast} = \mbox{argmax}_I f(I) + R(I)\); (neuron value + regularizer) Optimization-based approach: freeze/fix the weights and run “image update” to find images that maximize the score of chosen class. Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, “Deep Inside Convolutional Networks: Visualizing Image Classification Models and Saliency Maps”, Workshop at ICLR, 2014. Find images that maximize some class score: \(\mbox{argmax}_I S_c(I) - \lambda {\lVert I \rVert}^2_2\) initialize image to zeros forward image to compute current scores set the gradient of the scores vector to be I, then backprop to image make a small update to the image Instead of using L2 norm, we can use better regularizer(Gaussian blur image, Clip pixels with small values/gradients to 0, …) Optimize in FC6 latent space instead of pixel space: Adversarial perturbations Fooling Images / Adversarial Examples Start from an arbitrary image Pick an arbitrary class Modify the image to maximize the class Repeat until network is fooled Ian Goodfellow, “Explaining and Harnessing Adversarial Examples”, 2014 Classifier is vulnerable to adversarical perturbation because of its linear nature. Check Ian Goodfellow’s lecture from 2017 Moosavi-Dezfooli, Seyed-Mohsen, et al. “Universal adversarial perturbations”, IEEE, 2017. Style Transfer DeepDream: Amplify existing features Rather than synthesizing an image to maximize a specific neuron, instead try to amplify the neuron activations at some layer in the network. Choose an image and a layer in a CNN; repeat: Forward: compute activations at chosen layer Set gradient of chosen layer equal to its activation Backward: Compute gradient on image Update image Feature Inversion Mahendran and Vedaldi, “Understanding Deep Image Representations by Inverting Them”, CVPR 2015 Given a CNN feature vector for an image, find a new image that: Matches the given feature vector “looks natural” (image prior regularization) \(\begin{align*} x^{\ast} &amp;amp;= \underset{x\in\mathbb{R}^{H \times W \times C}}{\mbox{argmin}} l(\Phi(x), \Phi_0) + \lambda \mathcal{R}(x) \\ &amp;amp; \mbox{where loss } l = {\lVert \Phi(x) - \Phi_0 \rVert}^2 \end{align*}\) Texture Synthesis: Nearest Neighbor Given a sample patch of some texture, generate a bigger image of the same texture Generate pixels one at a time in scanline order; form neighborhood of already generated pixels and copy nearest neighbor from input Neural Texture Synthesis: Gram Matrix a pair-wise statistics; interpret given CxHxW features as HxW grid of C-dimensional vectors. By computing outer product and sum up for all spacial locations($G=V^T V$), it gives CxC matrix measuring co-occurrence. Pretrain a CNN on ImageNet (VGG-19) Run input texture forward through CNN, record activations on every layer At each layer compute the Gram matrix Initialize generated image from random noise Pass generated image through CNN, compute Gram matrix on each layer Compute loss: weighted sum of L2 distance between Gram matrices Backprop to get gradient on image Make gradient step on image GOTO 5 Neural Style Transfer: Feature + Gram Reconstruction Extract content targets (ConvNet activations of all layers for the given image) Extract style targets (Gram matrices of ConvNet activations of all layers) Optimize over image to have: The content of the content image(activations match content) The style of the stlye image(Gram matrices of activations match style) Problem: requires many forward, backward passes; VGG is very slow Solution: Train another neural network; Fast Style Transfer Fast Style Transfer</summary></entry><entry><title type="html">Unsupervised Representation Learning by Predicting Image Rotations</title><link href="http://0.0.0.0:4000/Rotation" rel="alternate" type="text/html" title="Unsupervised Representation Learning by Predicting Image Rotations" /><published>2022-01-24T15:00:00+00:00</published><updated>2022-01-24T15:00:00+00:00</updated><id>http://0.0.0.0:4000/Rotation</id><content type="html" xml:base="http://0.0.0.0:4000/Rotation">&lt;h2 id=&quot;unsupervised-representation-learning-by-predicting-image-rotations&quot;&gt;Unsupervised Representation Learning by Predicting Image Rotations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Gidaris et al. 2018&lt;/em&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/gidariss/FeatureLearningRotNet&quot;&gt;https://github.com/gidariss/FeatureLearningRotNet&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ConvNet:&lt;br /&gt;
  (+) Unparalleled capacity to learn high level semantic image features&lt;br /&gt;
  (-) Require massive amounts of manually labeled data, expensive and impractical to scale&lt;br /&gt;
  $\rightarrow$ &lt;em&gt;Unsupervised Learning&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Unsupervised semantic feature learning:&lt;br /&gt;
  Learn image features by training ConvNets to recognize the 2d rotated images as input. With apparently simple task, provides a very powerful supervisory signal for semantic feature learning(Conv). Evaluated in various unsupervised feature learning benchmarks, exceeds SotA performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;featurelearningrotnet&quot;&gt;FeatureLearningRotNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;How To:&lt;br /&gt;
  First define a small set of discrete geometric transformations, then each of those transformations are applied to each image on the dataset and produced transformed images are fed to ConvNet model that is trained to recognize the transformation of each image.
    &lt;ul&gt;
      &lt;li&gt;Set of geometric transformations define the classification pretext task that the ConvNet has to learn; to achieve unsupervised semantic feature learning, it is important to properly choose those geometric transformations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Purpose: to define the geometric transformations as rotations of 4 different degrees, ConvNet trained on the 4-way image classification task of recognizing one of the four Maximizing prob. $F^y(x^{y^{*}})$, probability of transformation &lt;em&gt;y&lt;/em&gt; predicted by &lt;em&gt;F&lt;/em&gt;, when given X is transformed by the transformation $y^{*}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;With idea: In order a ConvNet model to be able recognize the rotation transformations, it will require to understand the concept of the objects depicted in the image	such as their location, type, and pose.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;define a set of &lt;em&gt;K&lt;/em&gt; discrete geometric transformations \(G = \{g(\cdot\vert y)\}_{y=1}^K\), where $g(.\vert y)$ applies to input X, transformed image $X^y = g(X\vert y)$&lt;/li&gt;
  &lt;li&gt;ConvNet model F(.) gets as input an image $X^{y^{\ast}}$, to recognize unknown $y^{\ast}$ yields as output a probability distribution over all possible transformations	\(F(X^{y^{\ast}}\vert\theta) = \{ F^y(X^{y^{\ast}}\vert\theta) \}_{y=1}^K\), output &lt;em&gt;F&lt;/em&gt; returns probs for all classes $y$.&lt;/li&gt;
  &lt;li&gt;Therefore, &lt;em&gt;N&lt;/em&gt; training images \(D = \{ X_i \}_{i=0}^N\), the self-supervised training objective that ConvNet must learn to solve is:&lt;br /&gt;
 \(\mbox{min}_{\theta}\frac{1}{N}\sum_{i=1}^N \mbox{loss}(X_i,\theta)\),&lt;br /&gt;
 where the loss function is defined as:&lt;br /&gt;
 \(\mbox{loss}(X_i,\theta) = -\frac{1}{K}\sum_{y=1}^K \log(F^y(g(X_i|y)|\theta))\)&lt;br /&gt;
 (negative sum of log probs &lt;em&gt;F&lt;/em&gt; for all classes &lt;em&gt;y&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;2d image rotations:&lt;br /&gt;
  $Rot(X, \phi)$, operator that rotates image X by $\phi$ degrees&lt;br /&gt;
  In this case 0, 90, 180, 270; K=4 for G, where $g(X|y)=Rot(X,(y-1)90)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;forcing-the-learning-of-semantic-features&quot;&gt;Forcing the learning of semantic features&lt;/h3&gt;
&lt;p&gt;Fact that it is essentially impossible for a ConvNet model to effectively perform the above rotation recognition task, unless it has first learnt to recognize and detect classes of objects as well as their &lt;strong&gt;semantic parts&lt;/strong&gt; in images.&lt;br /&gt;
$\rightarrow$ &lt;strong&gt;ATTENTION MAPS&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/papers/papers_rotation_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;By comparing the attention maps from two models trained on supervised and unsupervised way, we observe that both models seem to focus on roughly &lt;strong&gt;the same image regions&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/papers/papers_rotation_1.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Also, trained on the proposed rotation recognition task, &lt;strong&gt;visualized layer filters&lt;/strong&gt; learnt appear to have a big variety of edge filters on multiple orientations and multiple frequencies, then the filters learnt by the supervised task.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Absence of low-level visual artifacts&lt;/strong&gt;:&lt;br /&gt;
  An additional important advantage of using image rotations over other geometric transformations, is that they do not leave e any easily detectable low-level visual artifacts that will lead the ConvNet to learn trivial features with no practical value for the vision perception tasks.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Well-posedness&lt;/strong&gt;:&lt;br /&gt;
  Human captured images tend to depict objects in an “up-standing” position. When defining the rotation recognition task, there is usually no ambiguity of what is the rotation transformation.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Implementing image rotations&lt;/strong&gt;:&lt;br /&gt;
  Flip and transpose.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$*$ Activation-based Attention Maps from &lt;em&gt;“Paying More Attention to Attention”, Zagoruyko et al., 2017&lt;/em&gt; - &lt;a href=&quot;https://arxiv.org/abs/1612.03928&quot;&gt;https://arxiv.org/abs/1612.03928&lt;/a&gt;&lt;br /&gt;
  Activation tensor of a conv. layer: $A\in R^{C\times H\times W}$ consists of &lt;em&gt;C&lt;/em&gt; feautre planes with spatial dimensions &lt;em&gt;H&lt;/em&gt;x&lt;em&gt;W&lt;/em&gt;&lt;br /&gt;
  Activation-based mapping function &lt;em&gt;F&lt;/em&gt; w.r.t that layer: $\mathcal{F}: R^{C\times H\times W} \rightarrow R^{H\times W}$&lt;br /&gt;
  With implicit assumption: Absolute value of a hidden neuron activation(that results when the network is evaluated on given input) can be used as an indication about the importance of that neuron w.r.t. the specific input.&lt;br /&gt;
  By considering, therefore, the absolute values of the elements of tensor A,	we construct a spatial attention map by computing statistics of these values	across the channel dimension(&lt;em&gt;C&lt;/em&gt;)
    &lt;ul&gt;
      &lt;li&gt;sum of abs: $F_{sum}(A)=\sum_{i=1}^C\vert A_i\vert$&lt;/li&gt;
      &lt;li&gt;sum of abs, raised to the power of p(&amp;gt;1): $F_{sum}^p(A) = \sum_{i=1}^C\vert A_i\vert^p$&lt;/li&gt;
      &lt;li&gt;max of abs, raised to the pwoer of p(&amp;gt;1): $F_{max}^p(A) = \mbox{max}_{i=1,C}\vert A_i\vert^p$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;transfer-learning&quot;&gt;Transfer Learning&lt;/h3&gt;
&lt;p&gt;With a model trained on proposed rotation recognition task with unlabeled data, freeze its early conv. layers and attach the layers from a supervised model, evaluate on a supervised task with a subset of labeled data.&lt;/p&gt;

&lt;h3 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h3&gt;

&lt;h4 id=&quot;cifar-10-experiments&quot;&gt;CIFAR-10 Experiments&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;RotNet&lt;/em&gt; implementation details&lt;/strong&gt;:&lt;br /&gt;
  Network-In-Network (NIN) architectures (&lt;em&gt;Lin et al., 2013&lt;/em&gt;)&lt;br /&gt;
  Pretask train: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;optimizer = SGD, batch_size = 128, momentum = 0.9, weight_decay = 5e−4, lr = 0.1, lr_decay = 0.2 (after 30, 60, 80 epochs), num_epochs = 100&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evaluation of the learned feature hierarchies&lt;/strong&gt;:&lt;br /&gt;
  Using the CIFAR-10 training images, train three &lt;em&gt;RotNet&lt;/em&gt; models which have 3, 4, and 5 conv. blocks respectively. Afterwards, on top of the feature maps generated by each conv. block of each &lt;em&gt;RotNet&lt;/em&gt; model, add classifiers trained in a supervised way on the object recognition task of CIFAR-10; consists of 3 FC layers. The accuracy results of CIFAR-10 test set:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_2.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;
    &lt;ol&gt;
      &lt;li&gt;In all cases the feature maps generated by the 2nd conv. block achieve the highest accuracy, i.e., between 88.26% and 89.06%. Then the accuracy gradually degrades, which we assume is because they start becoming more and more specific on the self-supervised task of rotation prediction.&lt;/li&gt;
      &lt;li&gt;Observe that increasing the total depth of the RotNet models leads to increased object recognition performance by the feature maps generated by earlier layers. We assume that this is because increasing the depth of the model and thus the complexity of its head (i.e., top ConvNet layers) allows the features of earlier layers to be less specific to the rotation prediction task.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Exploring the quality of the learned features w.r.t. the number of recognized rotations&lt;/strong&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_3.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;Observed that 4 discrete rotations as proposed achieved better performance over other cases.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Comparison against supervised and other unsupervised methods&lt;/strong&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_4.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_5.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;Model using the feature maps generated by the 2nd conv. block of a RotNet model with 4 conv. blocks in total.&lt;br /&gt;
  (a) &lt;em&gt;RotNet + non-linear&lt;/em&gt;: a non-linear classifier with 3 fully connected layers&lt;br /&gt;
  (b) &lt;em&gt;RotNet +conv.&lt;/em&gt;: three conv. layers + a linear prediction layer&lt;/li&gt;
      &lt;li&gt;Achieved best result among the unsupervised approaches&lt;/li&gt;
      &lt;li&gt;Very close to the fully supervised NIN model&lt;/li&gt;
      &lt;li&gt;Observed that fine-tuning the unsupervised learned features further improves the classification performance&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Correlation between object classification task and rotation prediction task&lt;/strong&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_6.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;As the ability of the RotNet features for solving the rotation prediction task improves(as the rotation prediction accuracy increases), their ability to help solving the object recognition task improves as well(the object recognition accuracy also increases).&lt;/li&gt;
      &lt;li&gt;Object recognition accuracy converges fast w.r.t. the number of training epochs used for solving the pretext task of rotation prediction.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Semi-supervised setting&lt;/strong&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_7.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  Train a 4 block RotNet model on the rotation prediction task using the entire image dataset of CIFAR-10, then train on top of its feature maps object classifiers using only a subset of the available images and their corresponding labels. As feature maps we use those from 2nd conv. block of the RotNet model. As a classifier we use a set of convolutional layers of the same e architecture as the 3rd conv. block of a NIN model plus a linear classifier, all randomly initialized.  For training the object classifier we use for each category 20, 100, 400, 1000, or 5000 image examples. Comapred with a supervised model that is trained only on the available examples each time:
    &lt;ul&gt;
      &lt;li&gt;Observed that our unsupervised trained model exceeds in this semi-supervised setting the supervised model when the number of examples per category drops below 1000; can be useful when there are only small subset of labeled data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;evaluation-of-self-supervised-features-trained-in-imagenet&quot;&gt;Evaluation of self-supervised features trained in ImageNet&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Train a &lt;em&gt;RotNet&lt;/em&gt; model on the training images of the ImageNet dataset and evaluate the performance of the self-self-supervised features on the image classification tasks of ImageNet, Places, and PASCAL VOC datasets and on the object detection and object segmentation tasks of PASCAL VOC.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Implementation details&lt;/strong&gt;:&lt;br /&gt;
  Based on an AlexNet architecture, without local response normalization units, dropout units, or groups in the colvolutional layers, while it includes batch normalization units after each linear layer (either convolutional or fully connected).&lt;br /&gt;
  Train with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SGD, batch_size=192, momentum=0.9, weight_decay=5e-4, lr=0.01, lr_decay=0.1(after 10, 20 epochs), num_epochs=30&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ImageNet classification task&lt;/strong&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_8.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;Observed that &lt;em&gt;our approach surpasses all the other methods by a significant margin&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transfer learning evaluation on PASCAL VOC&lt;/strong&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_9.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Places classification task:&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_10.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="papers" /><summary type="html">Unsupervised Representation Learning by Predicting Image Rotations Gidaris et al. 2018 https://github.com/gidariss/FeatureLearningRotNet ConvNet: (+) Unparalleled capacity to learn high level semantic image features (-) Require massive amounts of manually labeled data, expensive and impractical to scale $\rightarrow$ Unsupervised Learning Unsupervised semantic feature learning: Learn image features by training ConvNets to recognize the 2d rotated images as input. With apparently simple task, provides a very powerful supervisory signal for semantic feature learning(Conv). Evaluated in various unsupervised feature learning benchmarks, exceeds SotA performance. FeatureLearningRotNet How To: First define a small set of discrete geometric transformations, then each of those transformations are applied to each image on the dataset and produced transformed images are fed to ConvNet model that is trained to recognize the transformation of each image. Set of geometric transformations define the classification pretext task that the ConvNet has to learn; to achieve unsupervised semantic feature learning, it is important to properly choose those geometric transformations. Purpose: to define the geometric transformations as rotations of 4 different degrees, ConvNet trained on the 4-way image classification task of recognizing one of the four Maximizing prob. $F^y(x^{y^{*}})$, probability of transformation y predicted by F, when given X is transformed by the transformation $y^{*}$. With idea: In order a ConvNet model to be able recognize the rotation transformations, it will require to understand the concept of the objects depicted in the image such as their location, type, and pose. Overview define a set of K discrete geometric transformations \(G = \{g(\cdot\vert y)\}_{y=1}^K\), where $g(.\vert y)$ applies to input X, transformed image $X^y = g(X\vert y)$ ConvNet model F(.) gets as input an image $X^{y^{\ast}}$, to recognize unknown $y^{\ast}$ yields as output a probability distribution over all possible transformations \(F(X^{y^{\ast}}\vert\theta) = \{ F^y(X^{y^{\ast}}\vert\theta) \}_{y=1}^K\), output F returns probs for all classes $y$. Therefore, N training images \(D = \{ X_i \}_{i=0}^N\), the self-supervised training objective that ConvNet must learn to solve is: \(\mbox{min}_{\theta}\frac{1}{N}\sum_{i=1}^N \mbox{loss}(X_i,\theta)\), where the loss function is defined as: \(\mbox{loss}(X_i,\theta) = -\frac{1}{K}\sum_{y=1}^K \log(F^y(g(X_i|y)|\theta))\) (negative sum of log probs F for all classes y) 2d image rotations: $Rot(X, \phi)$, operator that rotates image X by $\phi$ degrees In this case 0, 90, 180, 270; K=4 for G, where $g(X|y)=Rot(X,(y-1)90)$ Forcing the learning of semantic features Fact that it is essentially impossible for a ConvNet model to effectively perform the above rotation recognition task, unless it has first learnt to recognize and detect classes of objects as well as their semantic parts in images. $\rightarrow$ ATTENTION MAPS By comparing the attention maps from two models trained on supervised and unsupervised way, we observe that both models seem to focus on roughly the same image regions. Also, trained on the proposed rotation recognition task, visualized layer filters learnt appear to have a big variety of edge filters on multiple orientations and multiple frequencies, then the filters learnt by the supervised task. Absence of low-level visual artifacts: An additional important advantage of using image rotations over other geometric transformations, is that they do not leave e any easily detectable low-level visual artifacts that will lead the ConvNet to learn trivial features with no practical value for the vision perception tasks. Well-posedness: Human captured images tend to depict objects in an “up-standing” position. When defining the rotation recognition task, there is usually no ambiguity of what is the rotation transformation. Implementing image rotations: Flip and transpose. $*$ Activation-based Attention Maps from “Paying More Attention to Attention”, Zagoruyko et al., 2017 - https://arxiv.org/abs/1612.03928 Activation tensor of a conv. layer: $A\in R^{C\times H\times W}$ consists of C feautre planes with spatial dimensions HxW Activation-based mapping function F w.r.t that layer: $\mathcal{F}: R^{C\times H\times W} \rightarrow R^{H\times W}$ With implicit assumption: Absolute value of a hidden neuron activation(that results when the network is evaluated on given input) can be used as an indication about the importance of that neuron w.r.t. the specific input. By considering, therefore, the absolute values of the elements of tensor A, we construct a spatial attention map by computing statistics of these values across the channel dimension(C) sum of abs: $F_{sum}(A)=\sum_{i=1}^C\vert A_i\vert$ sum of abs, raised to the power of p(&amp;gt;1): $F_{sum}^p(A) = \sum_{i=1}^C\vert A_i\vert^p$ max of abs, raised to the pwoer of p(&amp;gt;1): $F_{max}^p(A) = \mbox{max}_{i=1,C}\vert A_i\vert^p$ Transfer Learning With a model trained on proposed rotation recognition task with unlabeled data, freeze its early conv. layers and attach the layers from a supervised model, evaluate on a supervised task with a subset of labeled data. Experimental Results CIFAR-10 Experiments RotNet implementation details: Network-In-Network (NIN) architectures (Lin et al., 2013) Pretask train: optimizer = SGD, batch_size = 128, momentum = 0.9, weight_decay = 5e−4, lr = 0.1, lr_decay = 0.2 (after 30, 60, 80 epochs), num_epochs = 100 Evaluation of the learned feature hierarchies: Using the CIFAR-10 training images, train three RotNet models which have 3, 4, and 5 conv. blocks respectively. Afterwards, on top of the feature maps generated by each conv. block of each RotNet model, add classifiers trained in a supervised way on the object recognition task of CIFAR-10; consists of 3 FC layers. The accuracy results of CIFAR-10 test set: In all cases the feature maps generated by the 2nd conv. block achieve the highest accuracy, i.e., between 88.26% and 89.06%. Then the accuracy gradually degrades, which we assume is because they start becoming more and more specific on the self-supervised task of rotation prediction. Observe that increasing the total depth of the RotNet models leads to increased object recognition performance by the feature maps generated by earlier layers. We assume that this is because increasing the depth of the model and thus the complexity of its head (i.e., top ConvNet layers) allows the features of earlier layers to be less specific to the rotation prediction task. Exploring the quality of the learned features w.r.t. the number of recognized rotations: Observed that 4 discrete rotations as proposed achieved better performance over other cases. Comparison against supervised and other unsupervised methods: Model using the feature maps generated by the 2nd conv. block of a RotNet model with 4 conv. blocks in total. (a) RotNet + non-linear: a non-linear classifier with 3 fully connected layers (b) RotNet +conv.: three conv. layers + a linear prediction layer Achieved best result among the unsupervised approaches Very close to the fully supervised NIN model Observed that fine-tuning the unsupervised learned features further improves the classification performance Correlation between object classification task and rotation prediction task: As the ability of the RotNet features for solving the rotation prediction task improves(as the rotation prediction accuracy increases), their ability to help solving the object recognition task improves as well(the object recognition accuracy also increases). Object recognition accuracy converges fast w.r.t. the number of training epochs used for solving the pretext task of rotation prediction. Semi-supervised setting: Train a 4 block RotNet model on the rotation prediction task using the entire image dataset of CIFAR-10, then train on top of its feature maps object classifiers using only a subset of the available images and their corresponding labels. As feature maps we use those from 2nd conv. block of the RotNet model. As a classifier we use a set of convolutional layers of the same e architecture as the 3rd conv. block of a NIN model plus a linear classifier, all randomly initialized. For training the object classifier we use for each category 20, 100, 400, 1000, or 5000 image examples. Comapred with a supervised model that is trained only on the available examples each time: Observed that our unsupervised trained model exceeds in this semi-supervised setting the supervised model when the number of examples per category drops below 1000; can be useful when there are only small subset of labeled data. Evaluation of self-supervised features trained in ImageNet Train a RotNet model on the training images of the ImageNet dataset and evaluate the performance of the self-self-supervised features on the image classification tasks of ImageNet, Places, and PASCAL VOC datasets and on the object detection and object segmentation tasks of PASCAL VOC. Implementation details: Based on an AlexNet architecture, without local response normalization units, dropout units, or groups in the colvolutional layers, while it includes batch normalization units after each linear layer (either convolutional or fully connected). Train with SGD, batch_size=192, momentum=0.9, weight_decay=5e-4, lr=0.01, lr_decay=0.1(after 10, 20 epochs), num_epochs=30 ImageNet classification task: Observed that our approach surpasses all the other methods by a significant margin Transfer learning evaluation on PASCAL VOC: Places classification task:</summary></entry></feed>