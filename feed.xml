<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2022-01-12T19:19:53+09:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Darron’s Devlog</title><entry><title type="html">cs231n - Lecture 11. Attention and Transformers</title><link href="http://0.0.0.0:4000/cs231n_lec11" rel="alternate" type="text/html" title="cs231n - Lecture 11. Attention and Transformers" /><published>2022-01-05T00:00:00+09:00</published><updated>2022-01-05T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec11</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec11">&lt;h2 id=&quot;attention-with-rnns&quot;&gt;Attention with RNNs&lt;/h2&gt;

&lt;h3 id=&quot;image-captioning-using-spatial-features&quot;&gt;Image Captioning using spatial features&lt;/h3&gt;
&lt;p&gt;Input: Image &lt;em&gt;I&lt;/em&gt;&lt;br /&gt;
Output: Sequence &lt;strong&gt;y&lt;/strong&gt; $= y_1, y_2, \ldots, y_T$&lt;br /&gt;
Encoder: $h_0 = f_W(z)$, where &lt;em&gt;z&lt;/em&gt; is spatial CNN features, $f_W(\cdot)$ is an MLP&lt;br /&gt;
Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector &lt;em&gt;c&lt;/em&gt; is often $c=h_0$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
Problem: Input is “bottlenecked” through &lt;em&gt;c&lt;/em&gt;; especially in a long descriptions.  Model needs to encode everything it wants to say within &lt;em&gt;c&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Attention idea: New context vector &lt;em&gt;c_t&lt;/em&gt; at every time step&lt;br /&gt;
Each context vector will attend to different image regions&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alignment scores(scalars): $H \times W$ matrix &lt;strong&gt;&lt;em&gt;e&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
  $e_{t,i,j} = f_{\mbox{att}}(h_{t-1}, z_{i,j})$&lt;br /&gt;
  where $f_{\mbox{att}}(\cdot)$ is an MLP&lt;/li&gt;
  &lt;li&gt;Normalize to get attention weights:&lt;br /&gt;
  $a_{t,:,:} = \mbox{softmax}(e_{t,:,:})$,&lt;br /&gt;
  $0&amp;lt;a_{t,i,j}&amp;lt;1$, attention values sum to &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Compute context vector &lt;em&gt;c&lt;/em&gt;: multiply &lt;em&gt;CNN features&lt;/em&gt; and &lt;em&gt;Attention weights&lt;/em&gt;&lt;br /&gt;
  $c_t = \sum_{i,j} a_{t,i,j} z_{t,i,j}$&lt;/li&gt;
  &lt;li&gt;Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, \color{red}{c_t})$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_1.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;50%&quot; /&gt;&lt;br /&gt;
Each timestep of decode uses a different context vector that looks(attend) at different parts of the input image. This entire process is differentiable; model chooses its own attention weights. No attention supervision is required.&lt;/p&gt;

&lt;h3 id=&quot;similar-tasks-in-nlp---language-translation-example&quot;&gt;Similar tasks in NLP - Language translation example&lt;/h3&gt;
&lt;p&gt;Vanilla Encoder-Decoder setting:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Input: sequence &lt;strong&gt;x&lt;/strong&gt; $= x_1, x_2, \ldots, x_T$&lt;/li&gt;
  &lt;li&gt;Output: sequence &lt;strong&gt;y&lt;/strong&gt; $= y_1, y_2, \ldots, y_T$&lt;/li&gt;
  &lt;li&gt;Encoder: $h_0 = f_W(z)$, where $z_t = \mbox{RNN}(x_t, u_{t-1})$, $f_W(\cdot)$ is MLP, &lt;em&gt;u&lt;/em&gt; is the hidden RNN state&lt;/li&gt;
  &lt;li&gt;Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector &lt;em&gt;c&lt;/em&gt; is often $c=h_0$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
Attention in NLP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alignment scores(scalars):&lt;br /&gt;
  $e_{t,i} = f_{\mbox{att}}(h_{t-1}, z_t)$, where $f_{\mbox{att}}(\cdot)$ is an MLP&lt;/li&gt;
  &lt;li&gt;Normalize to get attention weights:&lt;br /&gt;
  $a_{t,:} = \mbox{softmax}(e_{t,:})$,&lt;br /&gt;
  $0&amp;lt;a_{t,i}&amp;lt;1$, attention values sum to &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Compute context vector &lt;em&gt;c&lt;/em&gt;:&lt;br /&gt;
  $c_t = \sum_i a_{t,i} z_{t,i}$&lt;/li&gt;
  &lt;li&gt;Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, \color{red}c_t)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Heatmap: visualization of attention weights; without any attention supervision, model learns different word orderings for different languages&lt;/p&gt;

&lt;h2 id=&quot;general-attention-layer&quot;&gt;General Attention Layer&lt;/h2&gt;
&lt;p&gt;Attention in image captioning before
&lt;img src=&quot;/assets/images/cs231n_lec11_4.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;single-query-setting&quot;&gt;Single query setting&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_5.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
&lt;strong&gt;Inputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;input vectors: &lt;strong&gt;x&lt;/strong&gt;(shape: $N\times D$)&lt;br /&gt;
  Attention operation is permutation invariant; produces the same output regardless of the order of elements(features) in the input vector. Stretch $H\times W = N$ into &lt;em&gt;N&lt;/em&gt; vectors, transform $H\times W\times D$ features into $N\times D$ input vectors &lt;strong&gt;x&lt;/strong&gt;(similar to attention in NLP).&lt;/li&gt;
  &lt;li&gt;Query: &lt;strong&gt;h&lt;/strong&gt;(shape: D)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alignment
    &lt;ul&gt;
      &lt;li&gt;Change $f_{\mbox{att}}(\cdot)$ to a simple dot product:&lt;br /&gt;
  $e_i = h\cdot x_i$; only works well with key &amp;amp; value transformation trick&lt;/li&gt;
      &lt;li&gt;Change $f_{\mbox{att}}(\cdot)$ to a &lt;strong&gt;scaled&lt;/strong&gt; dot product:&lt;br /&gt;
  $e_i = h\cdot x_i / \sqrt{D}$;&lt;br /&gt;
  Larger dimensions means more terms in the dot product sum. So, the variance of the logits is higher. Large magnitude(length) vectors will produce much higher logits. Then, the post-softmax distribution(&lt;em&gt;e&lt;/em&gt;) has lower-entropy(high uncertainty) assuming logits are &lt;em&gt;I.I.D&lt;/em&gt;. Ultimately, these large magnitude vectors will cause softmax to peak and assign very little weight to all others. To reduce this effect, divide by $sqrt{D}$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$&lt;/li&gt;
  &lt;li&gt;Output: $\mathbf{c} = \sum_i a_i x_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;br /&gt;
	- context vector: &lt;strong&gt;c&lt;/strong&gt;(shape: D)&lt;/p&gt;

&lt;h3 id=&quot;multiple-query-setting&quot;&gt;Multiple query setting&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_6.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
&lt;strong&gt;Inputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;input vectors: &lt;strong&gt;x&lt;/strong&gt;(shape: $N\times D$)&lt;/li&gt;
  &lt;li&gt;Queries: &lt;strong&gt;q&lt;/strong&gt;(shape: $M\times D$); multiple query vectors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alignment: $e_{i,j} = q_j\cdot x_i / \sqrt{D}$&lt;/li&gt;
  &lt;li&gt;Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$&lt;/li&gt;
  &lt;li&gt;Output: $y_j = \sum_i a_{i,j} x_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;context vectors: &lt;strong&gt;y&lt;/strong&gt;(shape: D);&lt;br /&gt;
  each query creates a new output context vector&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;weight-layers-added&quot;&gt;Weight layers added&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_7.png&quot; alt=&quot;png&quot; width=&quot;45%&amp;quot;, height=&amp;quot;45%&quot; /&gt;&lt;br /&gt;
Notice that the input vectors &lt;strong&gt;x&lt;/strong&gt; are used for both the alignment(&lt;strong&gt;e&lt;/strong&gt;) and attention calculations(&lt;strong&gt;y&lt;/strong&gt;); We can add more expressivity to the layer by adding a different FC layer before each of the two steps. The input and output dimensions can now change depending on the key and value FC layers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;input vectors: &lt;strong&gt;x&lt;/strong&gt;(shape: $N\times D$)&lt;/li&gt;
  &lt;li&gt;Queries: &lt;strong&gt;q&lt;/strong&gt;(shape: $M\times D_k$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Key vectors: $\mathbf{k} = \mathbf{x}W_k$&lt;/li&gt;
  &lt;li&gt;Value vectors: $\mathbf{v} = \mathbf{x}W_v$&lt;/li&gt;
  &lt;li&gt;Alignment: $e_{i,j} = q_j\cdot k_i / \sqrt{D}$&lt;/li&gt;
  &lt;li&gt;Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$&lt;/li&gt;
  &lt;li&gt;Output: $y_j = \sum_i a_{i,j} v_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;context vectors: &lt;strong&gt;y&lt;/strong&gt;(shape: $D_v$);&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;self-attention-layer&quot;&gt;Self attention layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_8.png&quot; alt=&quot;png&quot; width=&quot;45%&amp;quot;, height=&amp;quot;45%&quot; /&gt;&lt;br /&gt;
Recall that the query vector was a function of the input vectors; Encoder $h_0=f_W(z)$, where &lt;em&gt;z&lt;/em&gt; is spatial CNN features, $f_W(\cdot)$ is an MLP. We can calculate the query vectors from the input vectors, defining a “self-attention” layer. No input query vectors anymore, instead query vectors are calculated using a FC layer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;input vectors: &lt;strong&gt;x&lt;/strong&gt;(shape: $N\times D$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Key vectors: $\mathbf{k} = \mathbf{x}W_k$&lt;/li&gt;
  &lt;li&gt;Value vectors: $\mathbf{v} = \mathbf{x}W_v$&lt;/li&gt;
  &lt;li&gt;Query vectors: $\mathbf{q} = \mathbf{x}W_q$&lt;/li&gt;
  &lt;li&gt;Alignment: $e_{i,j} = q_j\cdot k_i / \sqrt{D}$&lt;/li&gt;
  &lt;li&gt;Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$&lt;/li&gt;
  &lt;li&gt;Output: $y_j = \sum_i a_{i,j} v_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;context vectors: &lt;strong&gt;y&lt;/strong&gt;(shape: $D_v$)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;positional-encoding&quot;&gt;&lt;em&gt;Positional encoding&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_9.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;br /&gt;
Self attention attends over sets of inputs; is permutation invariant. To encode the ordered sequences(e.g. language, image), concatenate special positional encoding $p_j$ to each input vector $x_j$.&lt;/p&gt;

&lt;p&gt;$\mathit{pos}: N\rightarrow R^d$ to process the position &lt;em&gt;j&lt;/em&gt; of the vector into a &lt;em&gt;d&lt;/em&gt;-dimensional vector; $p_j = \mathit{pos}(j)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Desiderata&lt;/strong&gt; of $\mathit{pos}(\cdot)$:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Should output a &lt;strong&gt;unique&lt;/strong&gt; encoding for each time-step(word’s position in a sentence).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Distance&lt;/strong&gt; between any two time-steps should be consistent across sentences with different lengths(variable inputs).&lt;/li&gt;
  &lt;li&gt;Model should generalize to &lt;strong&gt;longer&lt;/strong&gt; sentences without any efforts. Its values should be bounded.&lt;/li&gt;
  &lt;li&gt;Must be &lt;strong&gt;deterministic&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Options&lt;/strong&gt; for $\mathit{pos}(\cdot)$:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Learn a lookup table:
    &lt;ul&gt;
      &lt;li&gt;Learn parameters to use for $\mathit{pos}(t)$ for $t \in [0,T)$&lt;/li&gt;
      &lt;li&gt;Lookup table contains $T\times d$ parameters&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Design a fixed function with the desiderata&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec11_10.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;masked-self-attention-layer&quot;&gt;Masked self-attention layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_11.png&quot; alt=&quot;png&quot; width=&quot;45%&amp;quot;, height=&amp;quot;45%&quot; /&gt;&lt;br /&gt;
Manually set alignment scores to $-\infty$, prevent vectors from looking at future vectors.&lt;/p&gt;

&lt;h3 id=&quot;multi-head-self-attention-layer&quot;&gt;Multi-head self attention layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_12.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
Multiple self-attention heads in parallel; similar to ensemble&lt;/p&gt;

&lt;h3 id=&quot;comparing-rnns-to-transformers&quot;&gt;Comparing RNNs to Transformers&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;RNNs&lt;/strong&gt;&lt;br /&gt;
&lt;span style=&quot;color:green&quot;&gt;(+) LSTMs work reasonably well for long sequences.&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:red&quot;&gt;(-) Expects an ordered sequences of inputs&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:red&quot;&gt;(-) Sequential computation: subsequent hidden states can only be computed after the previous ones are done.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Transformers&lt;/strong&gt;&lt;br /&gt;
&lt;span style=&quot;color:green&quot;&gt;(+) Good at long sequences. Each attention calculation looks at all inputs.&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:green&quot;&gt;(+) Can operate over unordered sets or ordered sequences with positional encodings.&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:green&quot;&gt;(+) Parallel computation: All alignment and attention scores for all inputs can be done in parallel.&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:red&quot;&gt;(-) Requires a lot of memory: N x M alignment and attention scalers need to be calculated and stored for a single self-attention head.&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;transformers&quot;&gt;Transformers&lt;/h2&gt;
&lt;h3 id=&quot;image-captioning-using-transformers&quot;&gt;Image Captioning using transformers&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_13.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;No recurrence at all&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Input: Image &lt;strong&gt;I&lt;/strong&gt;&lt;br /&gt;
Output: Sequence &lt;strong&gt;y&lt;/strong&gt; $= y_1, y_2, \ldots, y_T$&lt;br /&gt;
Encoder: $c = T_W(z)$, where &lt;em&gt;z&lt;/em&gt; is spatial CNN features, $T_W(\cdot)$ is the transformer encoder&lt;br /&gt;
Decoder: $y_t = T_D(y_{0:t-1}, c)$, where $T_D(\cdot)$ is the transformer decoder&lt;/p&gt;

&lt;h4 id=&quot;the-transformer-encoder-block&quot;&gt;The Transformer encoder block&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_14.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
Inputs: Set of vectors &lt;strong&gt;x&lt;/strong&gt;&lt;br /&gt;
Outputs: Set of vectors &lt;strong&gt;y&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Self-attention is the only interaction between vectors; Layer norm and MLP operate independently per vector. Highly scalable, highly parallelizable, but high memory usage.&lt;/p&gt;

&lt;h4 id=&quot;the-transformer-decoder-block&quot;&gt;The Transformer Decoder block&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_15.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
Inputs: Set of vectors &lt;strong&gt;x&lt;/strong&gt; and Set of context vector &lt;strong&gt;c&lt;/strong&gt;&lt;br /&gt;
Outputs: Set of vectors &lt;strong&gt;y&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Masked Self-attention only interacts with past inputs(&lt;em&gt;x&lt;/em&gt;, or previous output $y_{t-1}$). Multi-head attention block is NOT self-attention; it attends over the transformer encoder outputs. In this phase, we inject image features into the decoder. Highly scalable, highly parallelizable, but high memory usage.&lt;/p&gt;

&lt;h3 id=&quot;image-captioning-using-only-transformers&quot;&gt;Image Captioning using ONLY transformers&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Transformers from pixels to language&lt;br /&gt;
  &lt;em&gt;Dosovitskiy et al, “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”, ArXiv 2020&lt;/em&gt;  &lt;a href=&quot;https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax.ipynb&quot; target=&quot;_blank&quot;&gt;colab notebook link&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note: in Google Colab - TPU runtime setting&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# TPU initialization
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolver&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distribute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cluster_resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TPUClusterResolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'grpc://'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'COLAB_TPU_ADDR'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experimental_connect_to_cluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experimental&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initialize_tpu_system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;strategy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distribute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TPUStrategy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# compile in strategy.scope
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strategy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'adam'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SparseCategoricalCrossentropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_logits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sparse_categorical_accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Adding &lt;strong&gt;attention&lt;/strong&gt; to RNNs allows them to “attend” to different parts of the input at every time step&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;general attention layer&lt;/strong&gt; is a new type of layer that can be used to design new neural network architectures&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transformers&lt;/strong&gt; are a type of layer that uses self-attention and layer norm.
    &lt;ul&gt;
      &lt;li&gt;It is highly scalable and highly parallelizable&lt;/li&gt;
      &lt;li&gt;Faster training, larger models, better performance across vision and language tasks&lt;/li&gt;
      &lt;li&gt;They are quickly replacing RNNs, LSTMs, and may even replace convolutions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Attention with RNNs Image Captioning using spatial features Input: Image I Output: Sequence y $= y_1, y_2, \ldots, y_T$ Encoder: $h_0 = f_W(z)$, where z is spatial CNN features, $f_W(\cdot)$ is an MLP Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector c is often $c=h_0$ Problem: Input is “bottlenecked” through c; especially in a long descriptions. Model needs to encode everything it wants to say within c Attention idea: New context vector c_t at every time step Each context vector will attend to different image regions Alignment scores(scalars): $H \times W$ matrix e $e_{t,i,j} = f_{\mbox{att}}(h_{t-1}, z_{i,j})$ where $f_{\mbox{att}}(\cdot)$ is an MLP Normalize to get attention weights: $a_{t,:,:} = \mbox{softmax}(e_{t,:,:})$, $0&amp;lt;a_{t,i,j}&amp;lt;1$, attention values sum to 1 Compute context vector c: multiply CNN features and Attention weights $c_t = \sum_{i,j} a_{t,i,j} z_{t,i,j}$ Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, \color{red}{c_t})$ Each timestep of decode uses a different context vector that looks(attend) at different parts of the input image. This entire process is differentiable; model chooses its own attention weights. No attention supervision is required. Similar tasks in NLP - Language translation example Vanilla Encoder-Decoder setting: Input: sequence x $= x_1, x_2, \ldots, x_T$ Output: sequence y $= y_1, y_2, \ldots, y_T$ Encoder: $h_0 = f_W(z)$, where $z_t = \mbox{RNN}(x_t, u_{t-1})$, $f_W(\cdot)$ is MLP, u is the hidden RNN state Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector c is often $c=h_0$ Attention in NLP Alignment scores(scalars): $e_{t,i} = f_{\mbox{att}}(h_{t-1}, z_t)$, where $f_{\mbox{att}}(\cdot)$ is an MLP Normalize to get attention weights: $a_{t,:} = \mbox{softmax}(e_{t,:})$, $0&amp;lt;a_{t,i}&amp;lt;1$, attention values sum to 1 Compute context vector c: $c_t = \sum_i a_{t,i} z_{t,i}$ Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, \color{red}c_t)$ Heatmap: visualization of attention weights; without any attention supervision, model learns different word orderings for different languages General Attention Layer Attention in image captioning before Single query setting Inputs input vectors: x(shape: $N\times D$) Attention operation is permutation invariant; produces the same output regardless of the order of elements(features) in the input vector. Stretch $H\times W = N$ into N vectors, transform $H\times W\times D$ features into $N\times D$ input vectors x(similar to attention in NLP). Query: h(shape: D) Operations Alignment Change $f_{\mbox{att}}(\cdot)$ to a simple dot product: $e_i = h\cdot x_i$; only works well with key &amp;amp; value transformation trick Change $f_{\mbox{att}}(\cdot)$ to a scaled dot product: $e_i = h\cdot x_i / \sqrt{D}$; Larger dimensions means more terms in the dot product sum. So, the variance of the logits is higher. Large magnitude(length) vectors will produce much higher logits. Then, the post-softmax distribution(e) has lower-entropy(high uncertainty) assuming logits are I.I.D. Ultimately, these large magnitude vectors will cause softmax to peak and assign very little weight to all others. To reduce this effect, divide by $sqrt{D}$. Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$ Output: $\mathbf{c} = \sum_i a_i x_i$ Outputs - context vector: c(shape: D) Multiple query setting Inputs input vectors: x(shape: $N\times D$) Queries: q(shape: $M\times D$); multiple query vectors Operations Alignment: $e_{i,j} = q_j\cdot x_i / \sqrt{D}$ Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$ Output: $y_j = \sum_i a_{i,j} x_i$ Outputs context vectors: y(shape: D); each query creates a new output context vector Weight layers added Notice that the input vectors x are used for both the alignment(e) and attention calculations(y); We can add more expressivity to the layer by adding a different FC layer before each of the two steps. The input and output dimensions can now change depending on the key and value FC layers. Inputs input vectors: x(shape: $N\times D$) Queries: q(shape: $M\times D_k$) Operations Key vectors: $\mathbf{k} = \mathbf{x}W_k$ Value vectors: $\mathbf{v} = \mathbf{x}W_v$ Alignment: $e_{i,j} = q_j\cdot k_i / \sqrt{D}$ Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$ Output: $y_j = \sum_i a_{i,j} v_i$ Outputs context vectors: y(shape: $D_v$); Self attention layer Recall that the query vector was a function of the input vectors; Encoder $h_0=f_W(z)$, where z is spatial CNN features, $f_W(\cdot)$ is an MLP. We can calculate the query vectors from the input vectors, defining a “self-attention” layer. No input query vectors anymore, instead query vectors are calculated using a FC layer. Inputs input vectors: x(shape: $N\times D$) Operations Key vectors: $\mathbf{k} = \mathbf{x}W_k$ Value vectors: $\mathbf{v} = \mathbf{x}W_v$ Query vectors: $\mathbf{q} = \mathbf{x}W_q$ Alignment: $e_{i,j} = q_j\cdot k_i / \sqrt{D}$ Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$ Output: $y_j = \sum_i a_{i,j} v_i$ Outputs context vectors: y(shape: $D_v$) Positional encoding Self attention attends over sets of inputs; is permutation invariant. To encode the ordered sequences(e.g. language, image), concatenate special positional encoding $p_j$ to each input vector $x_j$. $\mathit{pos}: N\rightarrow R^d$ to process the position j of the vector into a d-dimensional vector; $p_j = \mathit{pos}(j)$ Desiderata of $\mathit{pos}(\cdot)$: Should output a unique encoding for each time-step(word’s position in a sentence). Distance between any two time-steps should be consistent across sentences with different lengths(variable inputs). Model should generalize to longer sentences without any efforts. Its values should be bounded. Must be deterministic. Options for $\mathit{pos}(\cdot)$: Learn a lookup table: Learn parameters to use for $\mathit{pos}(t)$ for $t \in [0,T)$ Lookup table contains $T\times d$ parameters Design a fixed function with the desiderata Masked self-attention layer Manually set alignment scores to $-\infty$, prevent vectors from looking at future vectors. Multi-head self attention layer Multiple self-attention heads in parallel; similar to ensemble Comparing RNNs to Transformers RNNs (+) LSTMs work reasonably well for long sequences. (-) Expects an ordered sequences of inputs (-) Sequential computation: subsequent hidden states can only be computed after the previous ones are done. Transformers (+) Good at long sequences. Each attention calculation looks at all inputs. (+) Can operate over unordered sets or ordered sequences with positional encodings. (+) Parallel computation: All alignment and attention scores for all inputs can be done in parallel. (-) Requires a lot of memory: N x M alignment and attention scalers need to be calculated and stored for a single self-attention head. Transformers Image Captioning using transformers No recurrence at all Input: Image I Output: Sequence y $= y_1, y_2, \ldots, y_T$ Encoder: $c = T_W(z)$, where z is spatial CNN features, $T_W(\cdot)$ is the transformer encoder Decoder: $y_t = T_D(y_{0:t-1}, c)$, where $T_D(\cdot)$ is the transformer decoder The Transformer encoder block Inputs: Set of vectors x Outputs: Set of vectors y Self-attention is the only interaction between vectors; Layer norm and MLP operate independently per vector. Highly scalable, highly parallelizable, but high memory usage. The Transformer Decoder block Inputs: Set of vectors x and Set of context vector c Outputs: Set of vectors y Masked Self-attention only interacts with past inputs(x, or previous output $y_{t-1}$). Multi-head attention block is NOT self-attention; it attends over the transformer encoder outputs. In this phase, we inject image features into the decoder. Highly scalable, highly parallelizable, but high memory usage. Image Captioning using ONLY transformers Transformers from pixels to language Dosovitskiy et al, “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”, ArXiv 2020 colab notebook link Note: in Google Colab - TPU runtime setting import tensorflow as tf import os # TPU initialization resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR']) tf.config.experimental_connect_to_cluster(resolver) tf.tpu.experimental.initialize_tpu_system(resolver) strategy = tf.distribute.TPUStrategy(resolver) # compile in strategy.scope def create_model(): return tf.keras.Sequential( [tf.keras.layers.Conv2D(256, 3, activation='relu', input_shape=(28, 28, 1)), tf.keras.layers.Conv2D(256, 3, activation='relu'), tf.keras.layers.Flatten(), tf.keras.layers.Dense(256, activation='relu'), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dense(10)]) with strategy.scope(): model = create_model() model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['sparse_categorical_accuracy']) Summary Adding attention to RNNs allows them to “attend” to different parts of the input at every time step The general attention layer is a new type of layer that can be used to design new neural network architectures Transformers are a type of layer that uses self-attention and layer norm. It is highly scalable and highly parallelizable Faster training, larger models, better performance across vision and language tasks They are quickly replacing RNNs, LSTMs, and may even replace convolutions.</summary></entry><entry><title type="html">cs231n - Lecture 10. Recurrent Neural Networks</title><link href="http://0.0.0.0:4000/cs231n_lec10" rel="alternate" type="text/html" title="cs231n - Lecture 10. Recurrent Neural Networks" /><published>2022-01-04T00:00:00+09:00</published><updated>2022-01-04T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec10</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec10">&lt;h2 id=&quot;rnn-process-sequences&quot;&gt;RNN: Process Sequences&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;one to one; vanilla neural networks&lt;/li&gt;
  &lt;li&gt;one to many; e.g. Image Captioning(image to sequence of words)&lt;/li&gt;
  &lt;li&gt;many to one; e.g. Action Prediction(video sequence to action class)&lt;/li&gt;
  &lt;li&gt;many to many(1); e.g. Video Captioning(video sequence to caption)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;many to many(2); e.g. Video Classification on frame level&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Why existing convnets are insufficient?:&lt;br /&gt;
  Variable sequence length inputs and outputs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_1.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;70%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Key idea: RNNs have an “internal state” that is updated as a sequence is processed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN hidden state update:&lt;br /&gt;
  \(h_t = f_W(h_{t-1}, x_t)\)&lt;br /&gt;
  The same function and the same set of parameters are used at every time step.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RNN output generation: \(y_t = f_{W_hy}(h_t)\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Simple(Vanilla) RNN: The state consists of a single hidden vector &lt;em&gt;h&lt;/em&gt;&lt;br /&gt;
  $h_t = \mbox{tanh}(W_hh h_{t-1} + W_{xh}x_t)$&lt;br /&gt;
  $y_t = W_{hy}h_t$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sequence-to-sequenceseq2seq-many-to-one--one-to-many&quot;&gt;Sequence to Sequence(Seq2Seq): Many-to-One + One-to-Many&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Many-to-One: Encode input sequence in a single vector&lt;br /&gt;
  One-to-Many: Produce output sequence from single input vector&lt;br /&gt;
  Encoder produces the last hidden state $h_T$ and decoder uses it as a default $h_0$. Weights($W_1, W_2$) are re-used for each procedure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Example: Character-level Language Model Sampling&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_2.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;backpropagation&quot;&gt;Backpropagation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Backpropagation through time: Computationally Expensive&lt;br /&gt;
  Forward through entire sequence to compute loss, then backward through entire sequence to compute gradient.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Truncated&lt;/strong&gt; Backpropagation through time:&lt;br /&gt;
  Run forward and backward through &lt;strong&gt;chunks of the sequence&lt;/strong&gt; instead of whole sequence. Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rnn-tradeoffs&quot;&gt;RNN tradeoffs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RNN Advantages:
    &lt;ul&gt;
      &lt;li&gt;Can process any length input&lt;/li&gt;
      &lt;li&gt;Computation for step t can (in theory) use information from many steps back&lt;/li&gt;
      &lt;li&gt;Model size doesn’t increase for longer input&lt;/li&gt;
      &lt;li&gt;Same weights applied on every timestep, so there is symmetry in how inputs are processed.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN Disadvantages:
    &lt;ul&gt;
      &lt;li&gt;Recurrent computation is slow&lt;/li&gt;
      &lt;li&gt;In practice, difficult to access information from many steps back&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;image-captioning-cnn--rnn&quot;&gt;Image Captioning: CNN + RNN&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Instead of the final FC layer and the classifier in CNN, use FC output &lt;em&gt;v&lt;/em&gt;(say 4096 length vector) to formulate the default hidden state $h_0$ in RNN.
    &lt;ul&gt;
      &lt;li&gt;before: $h = \mbox{tanh}(W_{xh}\ast x+W_{hh}\ast h)$&lt;/li&gt;
      &lt;li&gt;now: $h=\mbox{tanh}(W_{xh}\ast x + W_{hh}\ast h + W_{ih}\ast v)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN for Image Captioning&lt;br /&gt;
  Re-sample the previous output $y_{t-1}$ as the next input $x_t$, iterate untill $y_t$ sample takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;END&amp;gt;&lt;/code&gt; token.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;visual-question-answering-rnns-with-attention&quot;&gt;Visual Question Answering: RNNs with Attention&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;other-tasks&quot;&gt;Other tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Visual Dialog: Conversations about images&lt;/li&gt;
  &lt;li&gt;Visual Language Navigation: Go to the living room&lt;br /&gt;
  Agent encodes instructions in language and uses an RNN to generate a series of movements as the visual input changes after each move.&lt;/li&gt;
  &lt;li&gt;Visual Question Answering: Dataset Bias&lt;br /&gt;
  With different types(Image + Question + Answer) of data used, model performances are better.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;long-short-term-memory-lstm&quot;&gt;Long Short Term Memory (LSTM)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Vanilla RNN&lt;br /&gt;
  \(h_t = \mbox{tanh}(W_{hh}h_{t-1} + W_{xh}x_t) \\
      = \mbox{tanh}\left(
          (W_{hh} \ W_{hx}) {\begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}}
                  \right) \\
      = \mbox{tanh}\left(
          W {\begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}}
                  \right)\)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(\frac{\partial h_t}{\partial h_{t-1}} = \mbox{tanh}' (W_{hh}h_{t-1} + W_{xh}x_t)W_{hh}\)&lt;br /&gt;
  $\frac{\partial L}{\partial W} = \sum_{t=1}^T \frac{\partial L_t}{\partial W}$&lt;/p&gt;

\[\begin{align*}
  \frac{\partial L_T}{\partial W} &amp;amp;= \frac{\partial L_T}{\partial h_T}
                                      \frac{\partial h_t}{\partial h_{t-1}}\cdots
                                      \frac{\partial h_1}{\partial W} \\
                                   &amp;amp;= \frac{\partial L_T}{\partial h_T}(\prod_{t=2}^T \frac{\partial h_t}{\partial h_{t-1}})\frac{\partial h_1}{\partial W} \\
                                  &amp;amp;= \frac{\partial L_T}{\partial h_T}(\prod_{t=2}^T \mbox{tanh}'(W_{hh}h_{t-1} + W_{xh}x_t))W_{hh}^{T-1} \frac{\partial h_1}{\partial W}
  \end{align*}\]
  &lt;/li&gt;
  &lt;li&gt;Problem&lt;br /&gt;
  As the output of &lt;em&gt;tanh&lt;/em&gt; function are in range of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[-1,1]&lt;/code&gt; and almost smaller than 1, vanilla RNN has &lt;strong&gt;&lt;em&gt;vanishing gradients&lt;/em&gt;&lt;/strong&gt;. If we assume no non-linearity, the gradient will be \(\frac{\partial L_T}{\partial W} = \frac{\partial L_T}{\partial h_T}W_{hh}^{T-1}\frac{\partial h_1}{\partial W}\). In this case, when the largest singular value is greater than 1, we have exploding gradients, while the value is smaller than 1, we have vanishing gradients.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# dimensionality of hidden state
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# number of time steps
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Whh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# forward pass of an RNN (ignoring inputs x)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Whh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	
&lt;span class=&quot;c1&quot;&gt;# backward pass of the RNN
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dhs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dhs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#start off the chain with random gradient
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reversed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dhs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# backprop through the nonlinearity
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;dhs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Whh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# backprop into previous hidden state
&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# &quot;Whh.T&quot; multiplied by &quot;T&quot; times!
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For exploding gradients: control with gradient clipping.&lt;br /&gt;
  For vanishing gradients: change the architecture, LSTM introduced.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LSTM:&lt;br /&gt;
  \(\begin{pmatrix} i \\ f \\ o \\ g \end{pmatrix} =
  \begin{pmatrix} \sigma \\ \sigma \\ \sigma \\ \mbox{tanh}\end{pmatrix} W \begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}\)&lt;br /&gt;
  \(c_t = f \odot c_{t-1} + i \odot g\), &lt;em&gt;memory cell update&lt;/em&gt;&lt;br /&gt;
  \(h_t = o \odot \mbox{tanh}(c_t)\), &lt;em&gt;hidden state update&lt;/em&gt;&lt;br /&gt;
  where &lt;em&gt;W&lt;/em&gt; is a stack of $W_h$ and $W_x$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_4.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
i: Input gate, whether to write to cell&lt;br /&gt;
f: Forget gate, Whether to erase cell&lt;br /&gt;
o: Output gate, How much to reveal cell&lt;br /&gt;
g: Gate gate, How much to write to cell&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_5.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Backpropagation from $c_t$ to $c_{t-1}$ only elementwise multiplication by &lt;em&gt;f&lt;/em&gt;, no matrix multiply by &lt;em&gt;W&lt;/em&gt;. Notice that the gradient contains the &lt;em&gt;f&lt;/em&gt; gate’s vector of activations; it allows better control of gradients values, using suitable parameter updates of the forget gate. Also notice that are added through the &lt;em&gt;f, i, g,&lt;/em&gt; and &lt;em&gt;o&lt;/em&gt; gates, we can have better balancing of gradient values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Recall: “PlainNets” vs. ResNets&lt;br /&gt;
  ResNet is to PlainNet what LSTM is to RNN, kind of.&lt;br /&gt;
  &lt;em&gt;Additive skip connections&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Do LSTMs solve the vanishing gradient problem?:&lt;br /&gt;
  The LSTM architecture makes it easier for the RNN to preserve information over many timesteps. e.g. If $f=1$ and $i=0$, then the information of that cell is preserved indefinitely. By contrast, it’s harder for vanilla RNN to learn a recurrent weight matrix $W_h$ that preserves information in hidden state.&lt;br /&gt;
  LSTM doesn’t guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;in between: Highway Networks, &lt;em&gt;Srivastava et al, 2015, [arXiv:1505.00387v2]&lt;/em&gt;&lt;br /&gt;
  A new architecture designed to ease gradient-based training of very deep networks. To regulate the flow of information and enlarge the possibility of studying extremely deep and efficient architectures.&lt;br /&gt;
  $g = T(x, W_T)$, $y = g \odot H(x, W_H) + (1-g)\odot x$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-rnn-variants&quot;&gt;Other RNN Variants&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Neural Architecture Search(NAS) with Reinforcement Learning, &lt;em&gt;Zoph et Le, 2017&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;RNN to design model; idea that we can represent the model architecture with a variable-length string.&lt;/li&gt;
      &lt;li&gt;Apply reinforcement learning on a neural network to maximize the accuracy(as a reward) on validation set, find a good architecture.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GRU; smaller LSTM, &lt;em&gt;“Learning phrase representations using rnn encoder-decoder for statistical machine translation”, Cho et al., 2014&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“An Empirical Exploration of Recurrent Network Architectures”, Jozefowicz et al., 2015&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;LSTM: A Search Space Odyssey, Greff et al., 2015&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;recurrence-for-vision&quot;&gt;Recurrence for Vision&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;LSTM wer a good default choice until this year&lt;/li&gt;
  &lt;li&gt;Use variants like GRU if you want faster compute and less parameters&lt;/li&gt;
  &lt;li&gt;Use transformers (next lecture) as they are dominating NLP models&lt;/li&gt;
  &lt;li&gt;almost everyday there is a new vision transformer model&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RNNs allow a lot of flexibility in architecture design&lt;/li&gt;
  &lt;li&gt;Vanilla RNNs are simple but don’t work very well&lt;/li&gt;
  &lt;li&gt;Common to use LSTM or GRU: their additive interactions improve gradient flow&lt;/li&gt;
  &lt;li&gt;Backward flow of gradients in RNN can explode or vanish. Exploding is controlled with gradient clipping. Vanishing is controlled with additive interactions (LSTM)&lt;/li&gt;
  &lt;li&gt;Better/simpler architectures are a hot topic of current research, as well as new paradigms for reasoning over sequences&lt;/li&gt;
  &lt;li&gt;Better understanding (both theoretical and empirical) is needed.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">RNN: Process Sequences one to one; vanilla neural networks one to many; e.g. Image Captioning(image to sequence of words) many to one; e.g. Action Prediction(video sequence to action class) many to many(1); e.g. Video Captioning(video sequence to caption) many to many(2); e.g. Video Classification on frame level Why existing convnets are insufficient?: Variable sequence length inputs and outputs Key idea: RNNs have an “internal state” that is updated as a sequence is processed. RNN hidden state update: \(h_t = f_W(h_{t-1}, x_t)\) The same function and the same set of parameters are used at every time step. RNN output generation: \(y_t = f_{W_hy}(h_t)\) Simple(Vanilla) RNN: The state consists of a single hidden vector h $h_t = \mbox{tanh}(W_hh h_{t-1} + W_{xh}x_t)$ $y_t = W_{hy}h_t$ Sequence to Sequence(Seq2Seq): Many-to-One + One-to-Many Many-to-One: Encode input sequence in a single vector One-to-Many: Produce output sequence from single input vector Encoder produces the last hidden state $h_T$ and decoder uses it as a default $h_0$. Weights($W_1, W_2$) are re-used for each procedure. Example: Character-level Language Model Sampling Backpropagation Backpropagation through time: Computationally Expensive Forward through entire sequence to compute loss, then backward through entire sequence to compute gradient. Truncated Backpropagation through time: Run forward and backward through chunks of the sequence instead of whole sequence. Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps. RNN tradeoffs RNN Advantages: Can process any length input Computation for step t can (in theory) use information from many steps back Model size doesn’t increase for longer input Same weights applied on every timestep, so there is symmetry in how inputs are processed. RNN Disadvantages: Recurrent computation is slow In practice, difficult to access information from many steps back Image Captioning: CNN + RNN Instead of the final FC layer and the classifier in CNN, use FC output v(say 4096 length vector) to formulate the default hidden state $h_0$ in RNN. before: $h = \mbox{tanh}(W_{xh}\ast x+W_{hh}\ast h)$ now: $h=\mbox{tanh}(W_{xh}\ast x + W_{hh}\ast h + W_{ih}\ast v)$ RNN for Image Captioning Re-sample the previous output $y_{t-1}$ as the next input $x_t$, iterate untill $y_t$ sample takes &amp;lt;END&amp;gt; token. Visual Question Answering: RNNs with Attention Other tasks Visual Dialog: Conversations about images Visual Language Navigation: Go to the living room Agent encodes instructions in language and uses an RNN to generate a series of movements as the visual input changes after each move. Visual Question Answering: Dataset Bias With different types(Image + Question + Answer) of data used, model performances are better. Long Short Term Memory (LSTM) Vanilla RNN \(h_t = \mbox{tanh}(W_{hh}h_{t-1} + W_{xh}x_t) \\ = \mbox{tanh}\left( (W_{hh} \ W_{hx}) {\begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}} \right) \\ = \mbox{tanh}\left( W {\begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}} \right)\) \(\frac{\partial h_t}{\partial h_{t-1}} = \mbox{tanh}' (W_{hh}h_{t-1} + W_{xh}x_t)W_{hh}\) $\frac{\partial L}{\partial W} = \sum_{t=1}^T \frac{\partial L_t}{\partial W}$ \[\begin{align*} \frac{\partial L_T}{\partial W} &amp;amp;= \frac{\partial L_T}{\partial h_T} \frac{\partial h_t}{\partial h_{t-1}}\cdots \frac{\partial h_1}{\partial W} \\ &amp;amp;= \frac{\partial L_T}{\partial h_T}(\prod_{t=2}^T \frac{\partial h_t}{\partial h_{t-1}})\frac{\partial h_1}{\partial W} \\ &amp;amp;= \frac{\partial L_T}{\partial h_T}(\prod_{t=2}^T \mbox{tanh}'(W_{hh}h_{t-1} + W_{xh}x_t))W_{hh}^{T-1} \frac{\partial h_1}{\partial W} \end{align*}\] Problem As the output of tanh function are in range of [-1,1] and almost smaller than 1, vanilla RNN has vanishing gradients. If we assume no non-linearity, the gradient will be \(\frac{\partial L_T}{\partial W} = \frac{\partial L_T}{\partial h_T}W_{hh}^{T-1}\frac{\partial h_1}{\partial W}\). In this case, when the largest singular value is greater than 1, we have exploding gradients, while the value is smaller than 1, we have vanishing gradients. H = 5 # dimensionality of hidden state T = 50 # number of time steps Whh = np.random.randn(H, H) # forward pass of an RNN (ignoring inputs x) hs = {} ss = {} hs[-1] = np.random.randn(H) for t in xrange(T): ss[t] = np.dot(Whh, hs[t-1]) hs[t] = np.maximum(0, ss[t]) # backward pass of the RNN dhs = {} dss = {} dhs[T-1] = np.random.randn(H) #start off the chain with random gradient for t in reversed(xrange(T)): dss[t] = (hs[t] &amp;gt; 0) * dhs[t] # backprop through the nonlinearity dhs[t-1] = np.dot(Whh.T, dss[t]) # backprop into previous hidden state # &quot;Whh.T&quot; multiplied by &quot;T&quot; times! For exploding gradients: control with gradient clipping. For vanishing gradients: change the architecture, LSTM introduced. LSTM: \(\begin{pmatrix} i \\ f \\ o \\ g \end{pmatrix} = \begin{pmatrix} \sigma \\ \sigma \\ \sigma \\ \mbox{tanh}\end{pmatrix} W \begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}\) \(c_t = f \odot c_{t-1} + i \odot g\), memory cell update \(h_t = o \odot \mbox{tanh}(c_t)\), hidden state update where W is a stack of $W_h$ and $W_x$ i: Input gate, whether to write to cell f: Forget gate, Whether to erase cell o: Output gate, How much to reveal cell g: Gate gate, How much to write to cell Backpropagation from $c_t$ to $c_{t-1}$ only elementwise multiplication by f, no matrix multiply by W. Notice that the gradient contains the f gate’s vector of activations; it allows better control of gradients values, using suitable parameter updates of the forget gate. Also notice that are added through the f, i, g, and o gates, we can have better balancing of gradient values. Recall: “PlainNets” vs. ResNets ResNet is to PlainNet what LSTM is to RNN, kind of. Additive skip connections Do LSTMs solve the vanishing gradient problem?: The LSTM architecture makes it easier for the RNN to preserve information over many timesteps. e.g. If $f=1$ and $i=0$, then the information of that cell is preserved indefinitely. By contrast, it’s harder for vanilla RNN to learn a recurrent weight matrix $W_h$ that preserves information in hidden state. LSTM doesn’t guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies. in between: Highway Networks, Srivastava et al, 2015, [arXiv:1505.00387v2] A new architecture designed to ease gradient-based training of very deep networks. To regulate the flow of information and enlarge the possibility of studying extremely deep and efficient architectures. $g = T(x, W_T)$, $y = g \odot H(x, W_H) + (1-g)\odot x$ Other RNN Variants Neural Architecture Search(NAS) with Reinforcement Learning, Zoph et Le, 2017 RNN to design model; idea that we can represent the model architecture with a variable-length string. Apply reinforcement learning on a neural network to maximize the accuracy(as a reward) on validation set, find a good architecture. GRU; smaller LSTM, “Learning phrase representations using rnn encoder-decoder for statistical machine translation”, Cho et al., 2014 “An Empirical Exploration of Recurrent Network Architectures”, Jozefowicz et al., 2015 LSTM: A Search Space Odyssey, Greff et al., 2015 Recurrence for Vision LSTM wer a good default choice until this year Use variants like GRU if you want faster compute and less parameters Use transformers (next lecture) as they are dominating NLP models almost everyday there is a new vision transformer model Summary RNNs allow a lot of flexibility in architecture design Vanilla RNNs are simple but don’t work very well Common to use LSTM or GRU: their additive interactions improve gradient flow Backward flow of gradients in RNN can explode or vanish. Exploding is controlled with gradient clipping. Vanishing is controlled with additive interactions (LSTM) Better/simpler architectures are a hot topic of current research, as well as new paradigms for reasoning over sequences Better understanding (both theoretical and empirical) is needed.</summary></entry><entry><title type="html">cs231n - Lecture 9. CNN Architectures</title><link href="http://0.0.0.0:4000/cs231n_lec9" rel="alternate" type="text/html" title="cs231n - Lecture 9. CNN Architectures" /><published>2022-01-03T00:00:00+09:00</published><updated>2022-01-03T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec9</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec9">&lt;h2 id=&quot;review&quot;&gt;Review&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;LeCun et al., 1998&lt;/em&gt;&lt;br /&gt;
  $5\times 5$ Conv filters applied at stride &lt;em&gt;1&lt;/em&gt;&lt;br /&gt;
  $2\times 2$ Subsampling (Pooling) layers applied at stride &lt;em&gt;2&lt;/em&gt;&lt;br /&gt;
  i.e. architecture is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[CONV-POOL-CONV-POOL-FC-FC]&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stride: Downsample output activations&lt;br /&gt;
  Padding: Preserve input spatial dimensions in output activations&lt;br /&gt;
  Filter: Each conv filter outputs a “slice” in the activation&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;case-studies&quot;&gt;Case Studies&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec9_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;alexnet-first-cnn-based-winner&quot;&gt;AlexNet: First CNN-based winner&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Architecture: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MaxPOOL3-FC6-FC7-FC8]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;Input: $227\times 227\times 3$ images&lt;/li&gt;
      &lt;li&gt;First layer(CONV1):&lt;br /&gt;
  &lt;em&gt;96&lt;/em&gt; $11\times 11$ filters applied at stride &lt;em&gt;4&lt;/em&gt;, pad &lt;em&gt;0&lt;/em&gt;&lt;br /&gt;
  Output volume: $W’ = (W-F+2P)/S + 1 \rightarrow$ $55\times 55\times 96$&lt;br /&gt;
  Parameters: $(11* 11* 3 +1)* 96 =$ &lt;strong&gt;36K&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Second layer(POOL1):&lt;br /&gt;
  $3\times 3\times$ filters applied at stride &lt;em&gt;2&lt;/em&gt;&lt;br /&gt;
  Output volume: $27\times 27\times 96$&lt;br /&gt;
  Parameters: 0&lt;br /&gt;
  $\vdots$&lt;/li&gt;
      &lt;li&gt;CONV2($27\times 27\times 256$):&lt;br /&gt;
  &lt;em&gt;256&lt;/em&gt; $5\times 5$ filters applied at stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;2&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;MAX POOL2($13\times 13\times 256):&lt;br /&gt;
  $3\times 3\times$ filters applied at stride &lt;em&gt;2&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;CONV3($13\times 13\times 384$):&lt;br /&gt;
  &lt;em&gt;384&lt;/em&gt; $3\times 3$ filters applied at stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;CONV4($13\times 13\times 384$):&lt;br /&gt;
  &lt;em&gt;384&lt;/em&gt; $3\times 3$ filters applied at stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;CONV5($13\times 13\times 256$):&lt;br /&gt;
  &lt;em&gt;256&lt;/em&gt; $3\times 3$ filters applied at stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;MAX POOL3($6\times 6\times 256$):&lt;br /&gt;
  $3\times 3\times$ filters applied at stride &lt;em&gt;2&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;FC6(4096): &lt;em&gt;4096&lt;/em&gt; neurons&lt;/li&gt;
      &lt;li&gt;FC7(4096): &lt;em&gt;4096&lt;/em&gt; neurons&lt;/li&gt;
      &lt;li&gt;FC8(1000): &lt;em&gt;1000&lt;/em&gt; neurons (class scores)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Historical note:
    &lt;ul&gt;
      &lt;li&gt;Network spread across &lt;em&gt;2&lt;/em&gt; GPUs, half the neurons (feature maps) on each GPU.&lt;/li&gt;
      &lt;li&gt;CONV1, CONV2, CONV4, CONV5: Connections only with feature maps on same GPU&lt;/li&gt;
      &lt;li&gt;CONV3, FC6, FC7, FC8: Connections with all feature maps in preceding layer, communication across GPUs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Details/Retrospectives:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Krizhevsky et al. 2012&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;first use of ReLU&lt;/li&gt;
      &lt;li&gt;used Norm layers (not common anymore)&lt;/li&gt;
      &lt;li&gt;heavy data augmentation&lt;/li&gt;
      &lt;li&gt;dropout &lt;em&gt;0.5&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;batch size &lt;em&gt;128&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;SGD Momentum &lt;em&gt;0.9&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Learning rate &lt;em&gt;1e-2&lt;/em&gt;, reduced by &lt;em&gt;10&lt;/em&gt; manually when val accuracy plateaus&lt;/li&gt;
      &lt;li&gt;L2 weight decay &lt;em&gt;5e-4&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;7 CNN ensemble: &lt;em&gt;18.2%&lt;/em&gt; $\rightarrow$ &lt;em&gt;15.4%&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;zfnet-improved-hyperparameters-over-alexnet&quot;&gt;ZFNet: Improved hyperparameters over AlexNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;AlexNet but:
    &lt;ul&gt;
      &lt;li&gt;CONV1: change from ($11\times 11$ stride &lt;em&gt;4&lt;/em&gt;) to ($7\times 7$ stride &lt;em&gt;2&lt;/em&gt;)&lt;/li&gt;
      &lt;li&gt;CONV3,4,5: instead of &lt;em&gt;384&lt;/em&gt;, &lt;em&gt;384&lt;/em&gt;, &lt;em&gt;256&lt;/em&gt; filters use &lt;em&gt;512&lt;/em&gt;, &lt;em&gt;1024&lt;/em&gt;, &lt;em&gt;512&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;ImageNet top 5 error: &lt;em&gt;16.4%&lt;/em&gt; -&amp;gt; &lt;em&gt;11.7%&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Zeiler and Fergus, 2013&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vggnet-deeper-networks&quot;&gt;VGGNet: Deeper Networks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Small filters, Deeper networks
    &lt;ul&gt;
      &lt;li&gt;8 layers (AlexNet) $\rightarrow$ 16 - 19 layers (VGG16Net)&lt;/li&gt;
      &lt;li&gt;Only $3\times 3$ CONV stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;1&lt;/em&gt; and $2\times 2$ MAX POOL with stride &lt;em&gt;2&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;11.7%&lt;/em&gt; top 5 error(ZFNet) $\rightarrow$ &lt;em&gt;7.3%&lt;/em&gt; top 5 error in ILSVRC’14&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Why use smaller filters?&lt;br /&gt;
  :Stack of three $3\times 3$ conv (stride &lt;em&gt;1&lt;/em&gt;) layers has same effective receptive field as one $7\times 7$ conv layer, but with deeper, more non-linearities and fewer parameters&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec9_1.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TOTAL memory: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;24M * 4 bytes ~= 96MB&lt;/code&gt; / image (for a forward pass)&lt;br /&gt;
  TOTAL params: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;138M&lt;/code&gt; parameters&lt;br /&gt;
  Most memory is in early CONV, Most params are in late FC&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Details:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Simonyan and Zisserman, 2014&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;ILSVRC’14 2nd in classification, 1st in localization&lt;/li&gt;
      &lt;li&gt;Similar training procedure as &lt;em&gt;Krizhevsky 2012&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;No Local Response Normalisation (LRN)&lt;/li&gt;
      &lt;li&gt;Use VGG16 or VGG19 (VGG19 only slightly better, more memory)&lt;/li&gt;
      &lt;li&gt;Use ensembles for best results&lt;/li&gt;
      &lt;li&gt;FC7 features generalize well to other	tasks&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;googlenet&quot;&gt;GoogLeNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Inception module&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;design a good local network topology(network within a network) and then stack these modules on top of each other&lt;/li&gt;
      &lt;li&gt;Apply parallel filter operations on the input from previous layer: Multiple receptive field sizes for convolution(1x1, 3x3, 5x5), Pooling(3x3)&lt;/li&gt;
      &lt;li&gt;Concatenate all filter outputs together channel-wise&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“Bottlenect”&lt;/em&gt; layers to reduce computational complexity of inception:
    &lt;ul&gt;
      &lt;li&gt;use 1x1 conv to reduce feature channel size; alternatively, interpret it as applying the same FC layer on each input pixel&lt;/li&gt;
      &lt;li&gt;preserves spatial dimensions, reduces depth&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec9_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Full GoogLeNet Architecture:
    &lt;ul&gt;
      &lt;li&gt;Stem Network: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[Conv-POOL-2x CONV-POOL]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Stack Inception modules: with dimension reduction on top of each other&lt;/li&gt;
      &lt;li&gt;Classifier output: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[(H*W*c)-Avg POOL-(1*1*c)-FC-Softmax]&lt;/code&gt;&lt;br /&gt;
  Global average pooling layer before final FC layer, avoids expensive FC layers&lt;/li&gt;
      &lt;li&gt;Auxiliary classification layers: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[AvgPool-1x1 Conv-FC-FC-Softmax]&lt;/code&gt;&lt;br /&gt;
  to inject additional gradient at lower layers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Details:
    &lt;ul&gt;
      &lt;li&gt;Deeper networks, with computational efficiency&lt;/li&gt;
      &lt;li&gt;ILSVRC’14 classification winner (&lt;em&gt;6.7%&lt;/em&gt; top 5 error)&lt;/li&gt;
      &lt;li&gt;22 layers&lt;/li&gt;
      &lt;li&gt;Only 5 million parameters(12x less than AlexNet, 27x less than VGG-16)&lt;/li&gt;
      &lt;li&gt;Efficient “Inception” module&lt;/li&gt;
      &lt;li&gt;No FC layers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;resnet&quot;&gt;ResNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;From 2015, “Revolution of Depth”; more than 100 layers&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Stacking deeper layers on a “plain” convolutional neural network results in lower both test and training error. The deeper model performs worse, but it’s &lt;strong&gt;not caused by overfitting&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;Fact: Deep models have more representation power (more parameters) than shallower models.&lt;/li&gt;
      &lt;li&gt;Hypothesis: the problem is an optimization problem, &lt;strong&gt;deeper models are harder to optimize&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Solution: copying the learned layers from the shallower model and setting additional layers to identity mapping.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“Residual block”&lt;/em&gt;:
    &lt;ul&gt;
      &lt;li&gt;Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec9_3.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Full ResNet Architecture:
    &lt;ul&gt;
      &lt;li&gt;Stack residual blocks&lt;/li&gt;
      &lt;li&gt;Every residual block has two $3\times 3$ conv layers&lt;/li&gt;
      &lt;li&gt;Periodically, double number of filters and downsample spatially using stride &lt;em&gt;2&lt;/em&gt; (/2 in each dimension). Reduce the activation volume by half.&lt;/li&gt;
      &lt;li&gt;Additional conv layer at the beginning (7x7 conv in stem)&lt;/li&gt;
      &lt;li&gt;No FC layers at the end (only FC 1000 to output classes)&lt;/li&gt;
      &lt;li&gt;(In theory, you can train a ResNet with input image of variable sizes)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For deeper networks(ResNet-50+):
  use bottleneck layer to improve efficiency (similar to GoogLeNet)&lt;br /&gt;
  e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[(28x28x256 INPUT)-(1x1 CONV, 64)-(3x3 CONV, 64)-(1x1 CONV, 256)-(28x28x256 OUTPUT)]&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Training ResNet in practice:
    &lt;ul&gt;
      &lt;li&gt;Batch Normalization after every CONV layer&lt;/li&gt;
      &lt;li&gt;Xavier initialization from &lt;em&gt;He et al.&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;SGD + Momentum (&lt;em&gt;0.9&lt;/em&gt;)&lt;/li&gt;
      &lt;li&gt;Learning rate: &lt;em&gt;0.1&lt;/em&gt;, divided by &lt;em&gt;10&lt;/em&gt; when validation error plateaus&lt;/li&gt;
      &lt;li&gt;Mini-batch size &lt;em&gt;256&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Weight decay of &lt;em&gt;1e-5&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;No dropout used&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Experimental Results:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;He et al., 2015&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Able to train very deep networks without degrading (152 layers on ImageNet, 1202 on Cifar)&lt;/li&gt;
      &lt;li&gt;Deeper networks now achieve lower training error as expected&lt;/li&gt;
      &lt;li&gt;Swept 1st place in all ILSVRC and COCO 2015 competitions&lt;/li&gt;
      &lt;li&gt;ILSVRC 2015 classification winner (&lt;em&gt;3.6%&lt;/em&gt; top 5 error); better than human performance!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Details:
    &lt;ul&gt;
      &lt;li&gt;Very deep networks using residual connections&lt;/li&gt;
      &lt;li&gt;152-layer model for ImageNet&lt;/li&gt;
      &lt;li&gt;ILSVRC’15 classification winner(&lt;em&gt;3.57%&lt;/em&gt; top 5 error)&lt;/li&gt;
      &lt;li&gt;Swept all classification and detection competitions in ILSVRC’15 and COCO’15&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Review LeCun et al., 1998 $5\times 5$ Conv filters applied at stride 1 $2\times 2$ Subsampling (Pooling) layers applied at stride 2 i.e. architecture is [CONV-POOL-CONV-POOL-FC-FC] Stride: Downsample output activations Padding: Preserve input spatial dimensions in output activations Filter: Each conv filter outputs a “slice” in the activation Case Studies AlexNet: First CNN-based winner Architecture: [CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MaxPOOL3-FC6-FC7-FC8] Input: $227\times 227\times 3$ images First layer(CONV1): 96 $11\times 11$ filters applied at stride 4, pad 0 Output volume: $W’ = (W-F+2P)/S + 1 \rightarrow$ $55\times 55\times 96$ Parameters: $(11* 11* 3 +1)* 96 =$ 36K Second layer(POOL1): $3\times 3\times$ filters applied at stride 2 Output volume: $27\times 27\times 96$ Parameters: 0 $\vdots$ CONV2($27\times 27\times 256$): 256 $5\times 5$ filters applied at stride 1, pad 2 MAX POOL2($13\times 13\times 256): $3\times 3\times$ filters applied at stride 2 CONV3($13\times 13\times 384$): 384 $3\times 3$ filters applied at stride 1, pad 1 CONV4($13\times 13\times 384$): 384 $3\times 3$ filters applied at stride 1, pad 1 CONV5($13\times 13\times 256$): 256 $3\times 3$ filters applied at stride 1, pad 1 MAX POOL3($6\times 6\times 256$): $3\times 3\times$ filters applied at stride 2 FC6(4096): 4096 neurons FC7(4096): 4096 neurons FC8(1000): 1000 neurons (class scores) Historical note: Network spread across 2 GPUs, half the neurons (feature maps) on each GPU. CONV1, CONV2, CONV4, CONV5: Connections only with feature maps on same GPU CONV3, FC6, FC7, FC8: Connections with all feature maps in preceding layer, communication across GPUs Details/Retrospectives: Krizhevsky et al. 2012 first use of ReLU used Norm layers (not common anymore) heavy data augmentation dropout 0.5 batch size 128 SGD Momentum 0.9 Learning rate 1e-2, reduced by 10 manually when val accuracy plateaus L2 weight decay 5e-4 7 CNN ensemble: 18.2% $\rightarrow$ 15.4% ZFNet: Improved hyperparameters over AlexNet AlexNet but: CONV1: change from ($11\times 11$ stride 4) to ($7\times 7$ stride 2) CONV3,4,5: instead of 384, 384, 256 filters use 512, 1024, 512 ImageNet top 5 error: 16.4% -&amp;gt; 11.7% Zeiler and Fergus, 2013 VGGNet: Deeper Networks Small filters, Deeper networks 8 layers (AlexNet) $\rightarrow$ 16 - 19 layers (VGG16Net) Only $3\times 3$ CONV stride 1, pad 1 and $2\times 2$ MAX POOL with stride 2 11.7% top 5 error(ZFNet) $\rightarrow$ 7.3% top 5 error in ILSVRC’14 Why use smaller filters? :Stack of three $3\times 3$ conv (stride 1) layers has same effective receptive field as one $7\times 7$ conv layer, but with deeper, more non-linearities and fewer parameters TOTAL memory: 24M * 4 bytes ~= 96MB / image (for a forward pass) TOTAL params: 138M parameters Most memory is in early CONV, Most params are in late FC Details: Simonyan and Zisserman, 2014 ILSVRC’14 2nd in classification, 1st in localization Similar training procedure as Krizhevsky 2012 No Local Response Normalisation (LRN) Use VGG16 or VGG19 (VGG19 only slightly better, more memory) Use ensembles for best results FC7 features generalize well to other tasks GoogLeNet Inception module: design a good local network topology(network within a network) and then stack these modules on top of each other Apply parallel filter operations on the input from previous layer: Multiple receptive field sizes for convolution(1x1, 3x3, 5x5), Pooling(3x3) Concatenate all filter outputs together channel-wise “Bottlenect” layers to reduce computational complexity of inception: use 1x1 conv to reduce feature channel size; alternatively, interpret it as applying the same FC layer on each input pixel preserves spatial dimensions, reduces depth Full GoogLeNet Architecture: Stem Network: [Conv-POOL-2x CONV-POOL] Stack Inception modules: with dimension reduction on top of each other Classifier output: [(H*W*c)-Avg POOL-(1*1*c)-FC-Softmax] Global average pooling layer before final FC layer, avoids expensive FC layers Auxiliary classification layers: [AvgPool-1x1 Conv-FC-FC-Softmax] to inject additional gradient at lower layers Details: Deeper networks, with computational efficiency ILSVRC’14 classification winner (6.7% top 5 error) 22 layers Only 5 million parameters(12x less than AlexNet, 27x less than VGG-16) Efficient “Inception” module No FC layers ResNet From 2015, “Revolution of Depth”; more than 100 layers Stacking deeper layers on a “plain” convolutional neural network results in lower both test and training error. The deeper model performs worse, but it’s not caused by overfitting. Fact: Deep models have more representation power (more parameters) than shallower models. Hypothesis: the problem is an optimization problem, deeper models are harder to optimize Solution: copying the learned layers from the shallower model and setting additional layers to identity mapping. “Residual block”: Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping Full ResNet Architecture: Stack residual blocks Every residual block has two $3\times 3$ conv layers Periodically, double number of filters and downsample spatially using stride 2 (/2 in each dimension). Reduce the activation volume by half. Additional conv layer at the beginning (7x7 conv in stem) No FC layers at the end (only FC 1000 to output classes) (In theory, you can train a ResNet with input image of variable sizes) For deeper networks(ResNet-50+): use bottleneck layer to improve efficiency (similar to GoogLeNet) e.g. [(28x28x256 INPUT)-(1x1 CONV, 64)-(3x3 CONV, 64)-(1x1 CONV, 256)-(28x28x256 OUTPUT)] Training ResNet in practice: Batch Normalization after every CONV layer Xavier initialization from He et al. SGD + Momentum (0.9) Learning rate: 0.1, divided by 10 when validation error plateaus Mini-batch size 256 Weight decay of 1e-5 No dropout used Experimental Results: He et al., 2015 Able to train very deep networks without degrading (152 layers on ImageNet, 1202 on Cifar) Deeper networks now achieve lower training error as expected Swept 1st place in all ILSVRC and COCO 2015 competitions ILSVRC 2015 classification winner (3.6% top 5 error); better than human performance! Details: Very deep networks using residual connections 152-layer model for ImageNet ILSVRC’15 classification winner(3.57% top 5 error) Swept all classification and detection competitions in ILSVRC’15 and COCO’15</summary></entry><entry><title type="html">cs231n - Lecture 8. Training Neural Networks II</title><link href="http://0.0.0.0:4000/cs231n_lec8" rel="alternate" type="text/html" title="cs231n - Lecture 8. Training Neural Networks II" /><published>2021-12-28T00:00:00+09:00</published><updated>2021-12-28T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec8</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec8">&lt;h2 id=&quot;optimization&quot;&gt;Optimization&lt;/h2&gt;

&lt;h3 id=&quot;problems-with-sgd&quot;&gt;Problems with SGD&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;What if loss changes quickly in one direction and slowly in another? What does gradient descent do?
 Very slow progress along shallow dimension, jitter along steep direction&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What if the loss function has a local minima or saddle point?&lt;br /&gt;
 Zero gradient, gradient descent gets stuck(more common in high dimension)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradients come from minibatches can be noisy&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;sgd--momentum&quot;&gt;SGD + Momentum&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;To avoid local minima, combine gradient at current point with &lt;em&gt;velocity&lt;/em&gt; to get step used to update weights; continue moving in the general direction as the previous iterations&lt;br /&gt;
  \(v_{t+1}=\rho v_t + \nabla f(x_t)\)&lt;br /&gt;
  \(x_{t+1}=x_t - \alpha v_{t+1}\)&lt;br /&gt;
  with &lt;em&gt;rho&lt;/em&gt; giving “friction”; typically &lt;em&gt;0.9&lt;/em&gt; or &lt;em&gt;0.99&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rho&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;nesterov-momentum&quot;&gt;Nesterov Momentum&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;“Look ahead” to the point where updating using velocity would take us; compute gradient there and mix it with velocity to get actual update direction&lt;br /&gt;
  \(v_{t+1}=\rho v_t - \alpha\nabla f(x_t + \rho v_t)\)&lt;br /&gt;
  \(x_{t+1}=x_t + v_{t+1}\)&lt;br /&gt;
  rearrange with \(\tilde{x}_t = x_t + \rho v_t\),&lt;br /&gt;
  \(v_{t+1}=\rho v_t - \alpha\nabla f(\tilde{x}_t)\)&lt;br /&gt;
  \(\begin{align*}
  \tilde{x}_{t+1} &amp;amp;= \tilde{x}_t - \rho v_t + (1+\rho)v_{t+1}
                  &amp;amp;= \tilde{x}_t + v_{t+1} + \rho(v_{t+1}-v_t)
  \end{align*}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adagrad&quot;&gt;AdaGrad&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Added element-wise scaling of the gradient based on the historical sum of squares in each dimension&lt;br /&gt;
  “Per-parameter learning rates” or “adaptive learning rates”&lt;br /&gt;
  Progress along “steep” directions is damped and “flat” directions is accelerated&lt;br /&gt;
  Step size decays to zero over time&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rmsprop-leaky-adagrad&quot;&gt;RMSProp: “Leaky AdaGrad”&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decay_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decay_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;adam&quot;&gt;Adam&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# Momentum
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;first_unbias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;			&lt;span class=&quot;c1&quot;&gt;# Bias correction
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;second_unbias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first_unbias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second_unbias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# AdaGrad/ RMSProp
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Sort of like RMSProp with momentum
  Bias correction for the fact that first and second moment estimates start at zero&lt;br /&gt;
  Adam with &lt;em&gt;beta1 = 0.9, beta2 = 0.999, and learning_rate = 1e-3 or 5e-4&lt;/em&gt; is a great starting point&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learning-rate-schedules&quot;&gt;Learning rate schedules&lt;/h2&gt;

&lt;h3 id=&quot;learning-rate-decays-over-time&quot;&gt;Learning rate decays over time&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Reduce learning rate by a certain value at a few fixed points(after some epochs)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;learning-rate-decay&quot;&gt;Learning Rate Decay&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Reduce learning rate gradually, e.g.&lt;br /&gt;
  Cosine:  $\alpha_t = \frac{1}{2}\alpha_0(1+\mbox{cos}(t\pi / T))$&lt;br /&gt;
  Linear: $\alpha_t = \alpha_0(1-t/T)$&lt;br /&gt;
  Inverse sqrt: $\alpha_t = \alpha_0 / \sqrt{t}$&lt;br /&gt;
  while $\alpha_0$ is the initial learning rate, $\alpha_t$ is one at epoch &lt;em&gt;t&lt;/em&gt;, and &lt;em&gt;T&lt;/em&gt; is the total number of epochs&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Linear Warmup&lt;br /&gt;
  High initial learning rates can make loss explode; linearly increasing learning rate from &lt;em&gt;0&lt;/em&gt; over the first &lt;em&gt;~5000&lt;/em&gt; iterations can prevent this&lt;br /&gt;
  Empirical rule of thumb: If you increase the batch size by &lt;em&gt;N&lt;/em&gt;, also scale the initial learning rate by &lt;em&gt;N&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;first-order-optimization&quot;&gt;First-Order Optimization&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Use gradient from linear approximation&lt;/li&gt;
  &lt;li&gt;Step to minimize the approximation&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;second-order-optimization&quot;&gt;Second-Order Optimization&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Use gradient and &lt;strong&gt;Hessian&lt;/strong&gt; to form &lt;strong&gt;quadratic&lt;/strong&gt; approximation&lt;/li&gt;
  &lt;li&gt;Step to the &lt;strong&gt;minima&lt;/strong&gt; of the (quadratic) approximation&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;But Hessian has &lt;em&gt;O(N^2)&lt;/em&gt; elements and inverting takes &lt;em&gt;O(N^3)&lt;/em&gt;, &lt;em&gt;N&lt;/em&gt; is extremely large
    &lt;ul&gt;
      &lt;li&gt;Quasi-Newton methods (BGFS most popular):&lt;br /&gt;
  instead of inverting the Hessian, approximate inverse Hessian with rank 1 updates over time&lt;/li&gt;
      &lt;li&gt;L-BFGS (Limited memory BFGS):&lt;br /&gt;
  Does not form/store the full inverse Hessian. Usually works very well in full batch, deterministic mode, but does not transfer very well to mini-batch setting. Large-scale, stochastic setting is an active area of research.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Adam is a good default choice in many cases; even with constant learning rate&lt;/li&gt;
  &lt;li&gt;SGD+Momentum can outperform Adam but may equire more tuning of LR and schedule. Cosine schedule preferred, since it has very few hyperparameters.&lt;/li&gt;
  &lt;li&gt;L-BFGS is good if you can afford to do full batch updates.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;improve-test-error&quot;&gt;Improve test error&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Better optimization algorithms help reduce &lt;strong&gt;training&lt;/strong&gt; loss, but what we really care is about error on new data - how to reduce the gap?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;early-stopping-always-do-this&quot;&gt;Early Stopping: Always do this&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Stop training the model when accuracy on the validation set decreases. Or train for a long time, but always keep track of the model snapshot that worked best on val.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-ensembles&quot;&gt;Model Ensembles&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Train multiple independent models&lt;/li&gt;
  &lt;li&gt;At test time average their results&lt;br /&gt;
 (Take average of predicted probability distributions, then choose argmax)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;regularization&quot;&gt;Regularization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;To improve single-model performance, add terms to loss&lt;br /&gt;
  e.g. L1, L2, Elastic net.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;or, use Dropout:&lt;br /&gt;
  In each forward pass, randomly set some neurons to zero. Probability of dropping is a hyperparameter; 0.5 is common.&lt;br /&gt;
  It forces the network to have a redundant representation; Prevents co-adaptation of features. Dropout can be interpreted as training a large ensemble of models (that share parameters).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# dropout rate
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# drop in train time
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt;

	&lt;span class=&quot;c1&quot;&gt;# backward pass: compute gradients...
&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# perform parameter update...
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;			&lt;span class=&quot;c1&quot;&gt;# scale at test time
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;more common: “Inverted dropout”&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;U1 = (np.random.rand(*H1.shape) &amp;lt; p) / p&lt;/code&gt; in train time and no scaling in test time&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A common pattern of regularization&lt;br /&gt;
  Training: Add some kind of randomness&lt;br /&gt;
  $y = fw(x,z)$&lt;br /&gt;
  Testing: Average out randomness (sometimes approximate)&lt;br /&gt;
  \(y = f(x) = E_z[f(x,z)] = \int p(z)f(x,z)\, dz\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data Augmentation:&lt;br /&gt;
  Addes &lt;em&gt;transformed&lt;/em&gt; data to train model&lt;br /&gt;
  e.g. translation, rotation, stretching, shearing, lens distortions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DropConnect:&lt;br /&gt;
  Training: Drop connections between neurons (set weights to 0)&lt;br /&gt;
  Testing: Use all the connections&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fractional Pooling:&lt;br /&gt;
  Training: Use randomized pooling regions&lt;br /&gt;
  Testing: Average predictions from several regions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stochastic Depth:&lt;br /&gt;
  Training: Skip some layers in the network&lt;br /&gt;
  Testing: Use all the layer&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cutout:&lt;br /&gt;
  Training: Set random image regions to zero&lt;br /&gt;
  Testing: Use full image&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mixup:&lt;br /&gt;
  Training: Train on random blends of images&lt;br /&gt;
  Testing: Use original images&lt;br /&gt;
  e.g. Randomly blend the pixels of pairs of training images, say 40% cat and 60% dog, and set the target label as cat:0.4 and dog:0.6.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Summary
  Consider dropout for large fully-connected layers&lt;br /&gt;
  Batch normalization and data augmentation almost always a good idea&lt;br /&gt;
  Try cutout and mixup especially for small classification datasets&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;choosing-hyperparameters&quot;&gt;Choosing Hyperparameters&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Step 1: Check initial loss&lt;br /&gt;
  Turn off weight decay, sanity check loss at initialization&lt;br /&gt;
  e.g. &lt;em&gt;log(C)&lt;/em&gt; for softmax with &lt;em&gt;C&lt;/em&gt; classes&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 2: Overfit a small sample&lt;br /&gt;
  Try to train to 100% training accuracy on a small sample of training data (~5-10 minibatches); fiddle with architecture, learning rate, weight initialization&lt;br /&gt;
  If loss is not going down, LR too low or bad initialization. If loss explodes, then LR is too high or bad initialization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 3: Find LR that makes loss go down&lt;br /&gt;
  Use the architecture from the previous step, use all training data, turn on small weight decay, find a learning rate that makes the loss drop significantly within ~100 iterations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 4: Coarse grid, train for ~1-5 epochs&lt;br /&gt;
  Choose a few values of learning rate and weight decay around what worked from Step 3, train a few models for ~1-5 epochs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 5: Refine grid, train longer&lt;br /&gt;
  Pick best models from Step 4, train them for longer (~10-20 epochs) without learning rate decay&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 6: Look at loss and accuracy curves&lt;br /&gt;
  If a ccuracy still going up, you need to train longer. If it goes down, huge train / val gap means overfitting. You need to increase regularization or get more data. If there’s no gap between train / val, it means underfitting. Train longer or use a bigger model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Look at learning curves&lt;br /&gt;
  Losses may be noisy, use a scatter plot and also plot moving average to see trends better. Cross-validation is useful too.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 7: &lt;strong&gt;GO TO Step 5&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperparameters to play with:&lt;br /&gt;
  network architecture,&lt;br /&gt;
  learning rate, its decay schedule, update type,&lt;br /&gt;
  regularization (L2/Dropout strength)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;for Hyper-Parameter Optimization, consider both Random Search and Grid Search&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-1&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Improve your training error:
    &lt;ul&gt;
      &lt;li&gt;Optimizers&lt;/li&gt;
      &lt;li&gt;Learning rate schedules&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Improve your test error:
    &lt;ul&gt;
      &lt;li&gt;Regularization&lt;/li&gt;
      &lt;li&gt;Choosing Hyperparameters&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Optimization Problems with SGD What if loss changes quickly in one direction and slowly in another? What does gradient descent do? Very slow progress along shallow dimension, jitter along steep direction What if the loss function has a local minima or saddle point? Zero gradient, gradient descent gets stuck(more common in high dimension) Gradients come from minibatches can be noisy SGD + Momentum To avoid local minima, combine gradient at current point with velocity to get step used to update weights; continue moving in the general direction as the previous iterations \(v_{t+1}=\rho v_t + \nabla f(x_t)\) \(x_{t+1}=x_t - \alpha v_{t+1}\) with rho giving “friction”; typically 0.9 or 0.99 vx = 0 while True: dx = compute_gradient(x) vx = rho * vx + dx x -= learning_rate * vx Nesterov Momentum “Look ahead” to the point where updating using velocity would take us; compute gradient there and mix it with velocity to get actual update direction \(v_{t+1}=\rho v_t - \alpha\nabla f(x_t + \rho v_t)\) \(x_{t+1}=x_t + v_{t+1}\) rearrange with \(\tilde{x}_t = x_t + \rho v_t\), \(v_{t+1}=\rho v_t - \alpha\nabla f(\tilde{x}_t)\) \(\begin{align*} \tilde{x}_{t+1} &amp;amp;= \tilde{x}_t - \rho v_t + (1+\rho)v_{t+1} &amp;amp;= \tilde{x}_t + v_{t+1} + \rho(v_{t+1}-v_t) \end{align*}\) AdaGrad grad_squared = 0 while True: dx = compute_gradient(x) grad_squared += dx * dx x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7) Added element-wise scaling of the gradient based on the historical sum of squares in each dimension “Per-parameter learning rates” or “adaptive learning rates” Progress along “steep” directions is damped and “flat” directions is accelerated Step size decays to zero over time RMSProp: “Leaky AdaGrad” grad_squared = 0 while True: dx = compute_gradient(x) grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dx * dx x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7) Adam first_moment = 0 second_moment = 0 for t in range(1, num_iterations): dx = compute_gradient(x) first_moment = beta1 * first_moment + (1 - beta1) * dx # Momentum second_moment = beta2 * second_moment + (1 - beta2) * dx * dx first_unbias = first_moment / (1 - beta1 ** t) # Bias correction second_unbias = second_moment / (1 - beta2 ** t) x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + 1e-7) # AdaGrad/ RMSProp Sort of like RMSProp with momentum Bias correction for the fact that first and second moment estimates start at zero Adam with beta1 = 0.9, beta2 = 0.999, and learning_rate = 1e-3 or 5e-4 is a great starting point Learning rate schedules Learning rate decays over time Reduce learning rate by a certain value at a few fixed points(after some epochs) Learning Rate Decay Reduce learning rate gradually, e.g. Cosine: $\alpha_t = \frac{1}{2}\alpha_0(1+\mbox{cos}(t\pi / T))$ Linear: $\alpha_t = \alpha_0(1-t/T)$ Inverse sqrt: $\alpha_t = \alpha_0 / \sqrt{t}$ while $\alpha_0$ is the initial learning rate, $\alpha_t$ is one at epoch t, and T is the total number of epochs Linear Warmup High initial learning rates can make loss explode; linearly increasing learning rate from 0 over the first ~5000 iterations can prevent this Empirical rule of thumb: If you increase the batch size by N, also scale the initial learning rate by N First-Order Optimization Use gradient from linear approximation Step to minimize the approximation Second-Order Optimization Use gradient and Hessian to form quadratic approximation Step to the minima of the (quadratic) approximation But Hessian has O(N^2) elements and inverting takes O(N^3), N is extremely large Quasi-Newton methods (BGFS most popular): instead of inverting the Hessian, approximate inverse Hessian with rank 1 updates over time L-BFGS (Limited memory BFGS): Does not form/store the full inverse Hessian. Usually works very well in full batch, deterministic mode, but does not transfer very well to mini-batch setting. Large-scale, stochastic setting is an active area of research. Summary Adam is a good default choice in many cases; even with constant learning rate SGD+Momentum can outperform Adam but may equire more tuning of LR and schedule. Cosine schedule preferred, since it has very few hyperparameters. L-BFGS is good if you can afford to do full batch updates. Improve test error Better optimization algorithms help reduce training loss, but what we really care is about error on new data - how to reduce the gap? Early Stopping: Always do this Stop training the model when accuracy on the validation set decreases. Or train for a long time, but always keep track of the model snapshot that worked best on val. Model Ensembles Train multiple independent models At test time average their results (Take average of predicted probability distributions, then choose argmax) Regularization To improve single-model performance, add terms to loss e.g. L1, L2, Elastic net. or, use Dropout: In each forward pass, randomly set some neurons to zero. Probability of dropping is a hyperparameter; 0.5 is common. It forces the network to have a redundant representation; Prevents co-adaptation of features. Dropout can be interpreted as training a large ensemble of models (that share parameters). p = 0.5 # dropout rate def train_step(X): # drop in train time H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = np.random.rand(*H1.shape) &amp;lt; p H1 *= U1 H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = np.random.rand(*H2.shape) &amp;lt; p H2 *= U2 out = np.dot(W3, H2) + b3 # backward pass: compute gradients... # perform parameter update... def predict(X): # scale at test time H1 = np.maximum(0, np.dot(W1, X) + b1) * p H2 = np.maximum(0, np.dot(W2, H1) + b2) * p out = np.dot(W3, H2) + b3 more common: “Inverted dropout” U1 = (np.random.rand(*H1.shape) &amp;lt; p) / p in train time and no scaling in test time A common pattern of regularization Training: Add some kind of randomness $y = fw(x,z)$ Testing: Average out randomness (sometimes approximate) \(y = f(x) = E_z[f(x,z)] = \int p(z)f(x,z)\, dz\) Data Augmentation: Addes transformed data to train model e.g. translation, rotation, stretching, shearing, lens distortions. DropConnect: Training: Drop connections between neurons (set weights to 0) Testing: Use all the connections Fractional Pooling: Training: Use randomized pooling regions Testing: Average predictions from several regions Stochastic Depth: Training: Skip some layers in the network Testing: Use all the layer Cutout: Training: Set random image regions to zero Testing: Use full image Mixup: Training: Train on random blends of images Testing: Use original images e.g. Randomly blend the pixels of pairs of training images, say 40% cat and 60% dog, and set the target label as cat:0.4 and dog:0.6. Summary Consider dropout for large fully-connected layers Batch normalization and data augmentation almost always a good idea Try cutout and mixup especially for small classification datasets Choosing Hyperparameters Step 1: Check initial loss Turn off weight decay, sanity check loss at initialization e.g. log(C) for softmax with C classes Step 2: Overfit a small sample Try to train to 100% training accuracy on a small sample of training data (~5-10 minibatches); fiddle with architecture, learning rate, weight initialization If loss is not going down, LR too low or bad initialization. If loss explodes, then LR is too high or bad initialization. Step 3: Find LR that makes loss go down Use the architecture from the previous step, use all training data, turn on small weight decay, find a learning rate that makes the loss drop significantly within ~100 iterations. Step 4: Coarse grid, train for ~1-5 epochs Choose a few values of learning rate and weight decay around what worked from Step 3, train a few models for ~1-5 epochs. Step 5: Refine grid, train longer Pick best models from Step 4, train them for longer (~10-20 epochs) without learning rate decay Step 6: Look at loss and accuracy curves If a ccuracy still going up, you need to train longer. If it goes down, huge train / val gap means overfitting. You need to increase regularization or get more data. If there’s no gap between train / val, it means underfitting. Train longer or use a bigger model. Look at learning curves Losses may be noisy, use a scatter plot and also plot moving average to see trends better. Cross-validation is useful too. Step 7: GO TO Step 5 Hyperparameters to play with: network architecture, learning rate, its decay schedule, update type, regularization (L2/Dropout strength) for Hyper-Parameter Optimization, consider both Random Search and Grid Search Summary Improve your training error: Optimizers Learning rate schedules Improve your test error: Regularization Choosing Hyperparameters</summary></entry><entry><title type="html">cs231n - Lecture 7. Training Neural Networks I</title><link href="http://0.0.0.0:4000/cs231n_lec7" rel="alternate" type="text/html" title="cs231n - Lecture 7. Training Neural Networks I" /><published>2021-12-27T00:00:00+09:00</published><updated>2021-12-27T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec7</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec7">&lt;h2 id=&quot;activation-functions&quot;&gt;Activation Functions&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec7_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;sigmoid&quot;&gt;Sigmoid&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;$\sigma(x)=1/(1+e^{-x})$
    &lt;ul&gt;
      &lt;li&gt;Squashes numbers to range [0,1]&lt;/li&gt;
      &lt;li&gt;Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Problem:
    &lt;ul&gt;
      &lt;li&gt;Gradient Vanishing: Saturated neurons “kill” the gradients; If all the gradients flowing back will be zero and weights will never change.&lt;/li&gt;
      &lt;li&gt;Sigmoid outputs are not zero-centered and always positive, so the gradients will be always all positive or all negative. Then the gradient update would follow a zig-zag path, resulting in bad efficiency.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;exp()&lt;/em&gt; is a bit compute expensive.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tanhx&quot;&gt;tanh(x)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Squashes numbers to range [-1,1]&lt;br /&gt;
  zero centered&lt;br /&gt;
  &lt;strong&gt;but still kills gradients when saturated&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;relurectified-linear-unit&quot;&gt;ReLU(Rectified Linear Unit)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(x) = \mbox{max}(0,x)\)&lt;br /&gt;
  Does not saturate (in &lt;em&gt;+&lt;/em&gt; region)&lt;br /&gt;
  Very computationally efficient&lt;br /&gt;
  Converges much faster than sigmoid/tanh&lt;br /&gt;
  &lt;strong&gt;but has not zero-centered output and weights will never be updated for negative &lt;em&gt;x&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;leaky-relu&quot;&gt;Leaky ReLU&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(x) = \mbox{max}(0.01x,x)\)&lt;br /&gt;
  (or &lt;em&gt;parametric&lt;/em&gt;, PReLU: \(f(x) = \mbox{max}(\alpha x, x)\))&lt;br /&gt;
  Not saturate&lt;br /&gt;
  Computationally efficient&lt;br /&gt;
  Converges much faster&lt;br /&gt;
  will not “die”&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;eluexponential-linear-units&quot;&gt;ELU(Exponential Linear Units)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(n)= \begin{cases} x &amp;amp; \mbox{if }x&amp;gt;0 \\
                      \alpha(\mbox{exp}(x)-1) &amp;amp; \mbox{if }x\le 0\end{cases}\)&lt;br /&gt;
  ($\scriptstyle{\alpha = 1}$)&lt;br /&gt;
  All benefits of ReLU&lt;br /&gt;
  Closer to zero mean outputs&lt;br /&gt;
  Negative saturation regime compared with Leaky ReLU adds some robustness to noise&lt;br /&gt;
  &lt;strong&gt;Computation requires exp()&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;selu-scaled-exponential-linear-units&quot;&gt;SELU (Scaled Exponential Linear Units)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(n)= \begin{cases} \lambda x &amp;amp; \mbox{if }x&amp;gt;0 \\
                      \lambda\alpha(e^x -1) &amp;amp; \mbox{otherwise}\end{cases}\)&lt;br /&gt;
  ($\scriptstyle{\alpha=1.6733, \lambda=1.0507}$)&lt;br /&gt;
  Scaled versionof ELU that works better for deep networks&lt;br /&gt;
  “Self-normalizing” property;&lt;br /&gt;
  Can train deep SELU networks without BatchNorm&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;maxout-neuron&quot;&gt;Maxout “Neuron”&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(\mbox{max}(w_1^T x + b_1, w_2^T x + b_2)\)&lt;br /&gt;
  Nonlinearity; does not have the basic form of dot product&lt;br /&gt;
  Generalizes ReLU and Leaky ReLU&lt;br /&gt;
  Linear Regime; does not saturate or die&lt;br /&gt;
  &lt;strong&gt;Complexity; Doubles the number of parameters/neuron&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;swish&quot;&gt;Swish&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(x)=x\sigma(\beta x)\)&lt;br /&gt;
  train a neural network to generate and test out different non-linearities&lt;br /&gt;
  outperformed all other options for &lt;em&gt;CIFAR-10&lt;/em&gt; accuracy&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Use &lt;em&gt;ReLU&lt;/em&gt; and be careful with learning rates&lt;br /&gt;
  Try out &lt;em&gt;Leaky ReLU / Maxout / ELU / SELU&lt;/em&gt; to squeeze out some marginal gains&lt;br /&gt;
  Don’t use sigmoid or tanh&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-preprocessing&quot;&gt;Data Preprocessing&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;We may have zero-centered, normalized, decorrelated(PCA) or whitened data&lt;/li&gt;
  &lt;li&gt;After normalization, it will be less sensitive to small changes in weights and easier to optimize&lt;/li&gt;
  &lt;li&gt;In practice for images, centering only used.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;weight-initialization&quot;&gt;Weight Initialization&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;First idea: Small random numbers&lt;br /&gt;
  (gaussian with zero mean and 1e-2 standard deviation)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;It works okay for small networks, but problems with deeper networks&lt;br /&gt;
  All activations and gradients tend to zero and no learning proceeded.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;xavier-initialization&quot;&gt;“Xavier” Initialization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;std = 1/sqrt(D_in)&lt;/em&gt;&lt;br /&gt;
  For conv layers, $\mbox{D_in}$ is $\mbox{filter_size}^2\times \mbox{input_channels}$&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Activations are nicely scaled for deeper layers&lt;br /&gt;
  works well especially in non-linear activation functions like sigmoid, tanh&lt;br /&gt;
  &lt;strong&gt;but cannot used in ReLU activation function; activations collapse to zero and no learning&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;kaiming--msra-initialization&quot;&gt;Kaiming / MSRA Initialization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ReLU correction: &lt;em&gt;std = sqrt(2/D_in)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;To make each dimension zero-mean unit-variance, apply:&lt;br /&gt;
  \(\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{\mbox{Var}[x^{(k)}]}}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Usually inserted after Fully Connected or Convolutional layers, and before nonlinearity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Makes deep networks much easier to train&lt;br /&gt;
  Improves gradient flow&lt;br /&gt;
  Allows higher learning rates, faster convergence&lt;br /&gt;
  Networks become more robust to initialization&lt;br /&gt;
  Acts as regularization during training&lt;br /&gt;
  Zero overhead at test-time: can be fused with conv&lt;br /&gt;
  &lt;strong&gt;Behaves differently during training and testing: can have bugs&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comparison of Normalization Layers&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec7_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;transfer-learning&quot;&gt;Transfer Learning&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Deep learning models are trained to capture characteristics of data, from general features at the first layer to specific features at the last layer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In transfer learning, we import pre-trained model and fine-tune to our cases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Strategies&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec7_2.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;50%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;E.g.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec7_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Transfer learning with CNNs is pervasive,&lt;br /&gt;
  for Object Detection(Fast R-CNN), Image Captioning(CNN + RNN), etc.&lt;br /&gt;
  but not always be necessary&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Activation Functions Sigmoid $\sigma(x)=1/(1+e^{-x})$ Squashes numbers to range [0,1] Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron. Problem: Gradient Vanishing: Saturated neurons “kill” the gradients; If all the gradients flowing back will be zero and weights will never change. Sigmoid outputs are not zero-centered and always positive, so the gradients will be always all positive or all negative. Then the gradient update would follow a zig-zag path, resulting in bad efficiency. exp() is a bit compute expensive. tanh(x) Squashes numbers to range [-1,1] zero centered but still kills gradients when saturated ReLU(Rectified Linear Unit) \(f(x) = \mbox{max}(0,x)\) Does not saturate (in + region) Very computationally efficient Converges much faster than sigmoid/tanh but has not zero-centered output and weights will never be updated for negative x Leaky ReLU \(f(x) = \mbox{max}(0.01x,x)\) (or parametric, PReLU: \(f(x) = \mbox{max}(\alpha x, x)\)) Not saturate Computationally efficient Converges much faster will not “die” ELU(Exponential Linear Units) \(f(n)= \begin{cases} x &amp;amp; \mbox{if }x&amp;gt;0 \\ \alpha(\mbox{exp}(x)-1) &amp;amp; \mbox{if }x\le 0\end{cases}\) ($\scriptstyle{\alpha = 1}$) All benefits of ReLU Closer to zero mean outputs Negative saturation regime compared with Leaky ReLU adds some robustness to noise Computation requires exp() SELU (Scaled Exponential Linear Units) \(f(n)= \begin{cases} \lambda x &amp;amp; \mbox{if }x&amp;gt;0 \\ \lambda\alpha(e^x -1) &amp;amp; \mbox{otherwise}\end{cases}\) ($\scriptstyle{\alpha=1.6733, \lambda=1.0507}$) Scaled versionof ELU that works better for deep networks “Self-normalizing” property; Can train deep SELU networks without BatchNorm Maxout “Neuron” \(\mbox{max}(w_1^T x + b_1, w_2^T x + b_2)\) Nonlinearity; does not have the basic form of dot product Generalizes ReLU and Leaky ReLU Linear Regime; does not saturate or die Complexity; Doubles the number of parameters/neuron Swish \(f(x)=x\sigma(\beta x)\) train a neural network to generate and test out different non-linearities outperformed all other options for CIFAR-10 accuracy Summary Use ReLU and be careful with learning rates Try out Leaky ReLU / Maxout / ELU / SELU to squeeze out some marginal gains Don’t use sigmoid or tanh Data Preprocessing We may have zero-centered, normalized, decorrelated(PCA) or whitened data After normalization, it will be less sensitive to small changes in weights and easier to optimize In practice for images, centering only used. Weight Initialization First idea: Small random numbers (gaussian with zero mean and 1e-2 standard deviation) W = 0.01 * np.random.randn(D_in, D_out) It works okay for small networks, but problems with deeper networks All activations and gradients tend to zero and no learning proceeded. “Xavier” Initialization std = 1/sqrt(D_in) For conv layers, $\mbox{D_in}$ is $\mbox{filter_size}^2\times \mbox{input_channels}$ W = np.random.randn(D_in, D_out) / np.sqrt(D_in) x = np.tanh(x.dot(W)) Activations are nicely scaled for deeper layers works well especially in non-linear activation functions like sigmoid, tanh but cannot used in ReLU activation function; activations collapse to zero and no learning Kaiming / MSRA Initialization ReLU correction: std = sqrt(2/D_in) W = np.random.randn(D_in, D_out) * np.sqrt(2/D_in) x = np.maximum(0, x.dot(W)) Batch Normalization To make each dimension zero-mean unit-variance, apply: \(\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{\mbox{Var}[x^{(k)}]}}\) Usually inserted after Fully Connected or Convolutional layers, and before nonlinearity. Makes deep networks much easier to train Improves gradient flow Allows higher learning rates, faster convergence Networks become more robust to initialization Acts as regularization during training Zero overhead at test-time: can be fused with conv Behaves differently during training and testing: can have bugs Comparison of Normalization Layers Transfer Learning Deep learning models are trained to capture characteristics of data, from general features at the first layer to specific features at the last layer. In transfer learning, we import pre-trained model and fine-tune to our cases. Strategies E.g. Transfer learning with CNNs is pervasive, for Object Detection(Fast R-CNN), Image Captioning(CNN + RNN), etc. but not always be necessary</summary></entry><entry><title type="html">cs231n - Lecture 5. Convolutional Neural Networks</title><link href="http://0.0.0.0:4000/cs231n_lec5" rel="alternate" type="text/html" title="cs231n - Lecture 5. Convolutional Neural Networks" /><published>2021-12-19T00:00:00+09:00</published><updated>2021-12-19T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec5</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec5">&lt;h3 id=&quot;convolutional-neural-networks&quot;&gt;Convolutional Neural Networks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ConvNets are everywhere&lt;br /&gt;
  Classification, Retrieval, Detection, Segmentation, Image Captioning, etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Recap: Fully Connected Layer&lt;br /&gt;
  $32\times 32\times 3$ image $\rightarrow$ stretch to $3072\times 1$&lt;br /&gt;
  Then a dot product of $3072\times 1$ input &lt;em&gt;x&lt;/em&gt; and scoring weights &lt;em&gt;W&lt;/em&gt;, &lt;em&gt;Wx&lt;/em&gt; is in $10 \times 3072$. With some activation function, we can make classification scores in &lt;em&gt;10&lt;/em&gt; classes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;convolution-layer-preserve-spatial-structure&quot;&gt;Convolution Layer: preserve spatial structure&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Convolve the filter with the image, slide over the image spatially, computing dot products. Filters always extend the full depth of the input volume.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec5_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
Convolve(slide) over all spatial locations, we can make an activation map of size $28\times 28\times 1$ for each convolution filter. For example, if we had &lt;em&gt;6&lt;/em&gt; $5\times 5$ filters, we’ll get &lt;em&gt;6&lt;/em&gt; separate activation maps. We stack these up to get a “new image” of size $28\times 28\times 6$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ConvNet is a sequence of Convolution Layers, interspersed with activation functions.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec5_1.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;70%&quot; /&gt;&lt;br /&gt;
Input convolved repeatedly with filters shrinks volumes spatially. By each sequence, an image is processed from low-level features to high-level features. Shrinking too fast is not good, doesn’t work well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We call the layer convolutional because it is related to convolution of two signals: \(f[x,y]*g[x,y]=\sum_{n_1=-\infty}^\infty \sum_{n_2=-\infty}^\infty f[n_1,n_2]\cdot g[x-n_1,y-n_2]\); elementwise multiplication and sum of a filter and the signal (image)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Zero pad the border:&lt;br /&gt;
  The data on the border of an image will be convolved only once with each filter, while the others on the center of an image will be treated several times. Zero padding is introduced to solve this problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;General CONV layers:&lt;br /&gt;
  with $N\times N$ input, $F\times F$ filter, applied with stride &lt;em&gt;s&lt;/em&gt;, pad with &lt;em&gt;p&lt;/em&gt; pixel border, the output is &lt;strong&gt;$(N+2P-F)/s + 1$&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Example:&lt;br /&gt;
  Input volume &lt;strong&gt;$32\times 32\times 3$&lt;/strong&gt;&lt;br /&gt;
  &lt;strong&gt;&lt;em&gt;10&lt;/em&gt; $5\times 5$&lt;/strong&gt; filters with stride &lt;strong&gt;&lt;em&gt;1&lt;/em&gt;&lt;/strong&gt;, pad &lt;strong&gt;&lt;em&gt;2&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;$\rightarrow$ Output volume size: $(32+2*2-5)/1+1=32$ spatially, so $32\times 32\times 10$.&lt;/p&gt;

    &lt;p&gt;$\rightarrow$ Number of parameters in this layer: each filter has $5\times 5\times 3+1=76$ parameters(&lt;em&gt;+1&lt;/em&gt; for bias), thus for all &lt;em&gt;760&lt;/em&gt; params.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec5_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$1\times 1$ convolution layers used:&lt;br /&gt;
  To reduce the number of channels, so the number of parameters,&lt;br /&gt;
  Then we can perform a deeper layers(Bottleneck architecture).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pooling-layer&quot;&gt;Pooling layer&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Summarize the data in a partial space, into some representations&lt;br /&gt;
  Reducing output dimensions and the number of parameters&lt;br /&gt;
  Make it smaller and more manageable&lt;br /&gt;
  Operate over each activation map independently(downsampling)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;e.g. max pool with $2\times 2$ filters and stride &lt;em&gt;2&lt;/em&gt;, $4\times 4$ input reduced to $2\times 2$ output consisted of regional maximums.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fully-connected-layer-fc-layer&quot;&gt;Fully Connected Layer (FC layer)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Contains neurons that connect to the entire input volume, as in ordinary Neural
Networks. Stacked and followed by some activations, finally we make predictions or classifications.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ConvNets stack CONV,POOL,FC layers&lt;/li&gt;
  &lt;li&gt;Trend towards smaller filters and deeper architectures&lt;/li&gt;
  &lt;li&gt;Trend towards getting rid of POOL/FC layers (just CONV)&lt;/li&gt;
  &lt;li&gt;Historically architectures looked like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[(CONV-RELU)*N-POOL?]*M-(FC-RELU)*K,SOFTMAX&lt;/code&gt; where &lt;em&gt;N&lt;/em&gt; is usually up to &lt;em&gt;~5&lt;/em&gt;, &lt;em&gt;M&lt;/em&gt; is large, $0\le K \le 2$.&lt;/li&gt;
  &lt;li&gt;but recent advances such as ResNet/GoogLeNet have challenged this paradigm&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Convolutional Neural Networks ConvNets are everywhere Classification, Retrieval, Detection, Segmentation, Image Captioning, etc. Recap: Fully Connected Layer $32\times 32\times 3$ image $\rightarrow$ stretch to $3072\times 1$ Then a dot product of $3072\times 1$ input x and scoring weights W, Wx is in $10 \times 3072$. With some activation function, we can make classification scores in 10 classes. Convolution Layer: preserve spatial structure Convolve the filter with the image, slide over the image spatially, computing dot products. Filters always extend the full depth of the input volume. Convolve(slide) over all spatial locations, we can make an activation map of size $28\times 28\times 1$ for each convolution filter. For example, if we had 6 $5\times 5$ filters, we’ll get 6 separate activation maps. We stack these up to get a “new image” of size $28\times 28\times 6$. ConvNet is a sequence of Convolution Layers, interspersed with activation functions. Input convolved repeatedly with filters shrinks volumes spatially. By each sequence, an image is processed from low-level features to high-level features. Shrinking too fast is not good, doesn’t work well. We call the layer convolutional because it is related to convolution of two signals: \(f[x,y]*g[x,y]=\sum_{n_1=-\infty}^\infty \sum_{n_2=-\infty}^\infty f[n_1,n_2]\cdot g[x-n_1,y-n_2]\); elementwise multiplication and sum of a filter and the signal (image) Zero pad the border: The data on the border of an image will be convolved only once with each filter, while the others on the center of an image will be treated several times. Zero padding is introduced to solve this problem. General CONV layers: with $N\times N$ input, $F\times F$ filter, applied with stride s, pad with p pixel border, the output is $(N+2P-F)/s + 1$ Example: Input volume $32\times 32\times 3$ 10 $5\times 5$ filters with stride 1, pad 2 $\rightarrow$ Output volume size: $(32+2*2-5)/1+1=32$ spatially, so $32\times 32\times 10$. $\rightarrow$ Number of parameters in this layer: each filter has $5\times 5\times 3+1=76$ parameters(+1 for bias), thus for all 760 params. $1\times 1$ convolution layers used: To reduce the number of channels, so the number of parameters, Then we can perform a deeper layers(Bottleneck architecture). Pooling layer Summarize the data in a partial space, into some representations Reducing output dimensions and the number of parameters Make it smaller and more manageable Operate over each activation map independently(downsampling) e.g. max pool with $2\times 2$ filters and stride 2, $4\times 4$ input reduced to $2\times 2$ output consisted of regional maximums. Fully Connected Layer (FC layer) Contains neurons that connect to the entire input volume, as in ordinary Neural Networks. Stacked and followed by some activations, finally we make predictions or classifications. Summary ConvNets stack CONV,POOL,FC layers Trend towards smaller filters and deeper architectures Trend towards getting rid of POOL/FC layers (just CONV) Historically architectures looked like [(CONV-RELU)*N-POOL?]*M-(FC-RELU)*K,SOFTMAX where N is usually up to ~5, M is large, $0\le K \le 2$. but recent advances such as ResNet/GoogLeNet have challenged this paradigm</summary></entry><entry><title type="html">cs231n - Lecture 6. Hardware and Software</title><link href="http://0.0.0.0:4000/cs231n_lec6" rel="alternate" type="text/html" title="cs231n - Lecture 6. Hardware and Software" /><published>2021-12-19T00:00:00+09:00</published><updated>2021-12-19T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec6</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec6">&lt;h3 id=&quot;deeplearning-software&quot;&gt;Deeplearning Software&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The point of deep learning frameworks&lt;br /&gt;
  (1) Quick to develop and test new ideas&lt;br /&gt;
  (2) Automatically compute gradients&lt;br /&gt;
  (3) Run it all efficiently on GPU (wrap cuDNN, cuBLAS, OpenCL, etc)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;computational-graph-example&quot;&gt;Computational graph example&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;grad_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grad_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grad_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grad_z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grad_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grad_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;in Numpy&lt;br /&gt;
  Good: Clean API, easy to write numeric code&lt;br /&gt;
  Bad: Have to compute our own gradients and can’t run on GPU&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'cuda:0'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;in PyTorch&lt;br /&gt;
  PyTorch handles gradients for us&lt;br /&gt;
  Can run on GPU&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-fundamental-concepts&quot;&gt;PyTorch: Fundamental Concepts&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.Tensor&lt;/code&gt;: Like a numpy array, but can run on GPU&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.autograd&lt;/code&gt;: Package for building computational graphs out of Tensors, and automatically computing gradients&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.Module&lt;/code&gt;: A neural network layer; may store state or learnable weights&lt;/li&gt;
  &lt;li&gt;we are using PyTorch version &lt;em&gt;1.7&lt;/em&gt; here&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-autograd&quot;&gt;PyTorch: Autograd&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Running example: Train a two-layer ReLU network on random data with L2 loss
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# enables autograd
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;							&lt;span class=&quot;c1&quot;&gt;# Forward pass
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;					&lt;span class=&quot;c1&quot;&gt;# no need to track intermediate values
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;					&lt;span class=&quot;c1&quot;&gt;# = x.mm(w1).clamp(min=0).mm(w2)
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;loss_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;							&lt;span class=&quot;c1&quot;&gt;# Compute gradient of loss
&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;					&lt;span class=&quot;c1&quot;&gt;# Gradient descent
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;			
		&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;or-you-can-define-your-own&quot;&gt;Or you can define your own&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;staticmethod&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;#Use ctx object to “cache” values for the backward pass
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_for_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;staticmethod&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;saved_tensors&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_input&lt;/span&gt;
		
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;my_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# a helper function to make it easy to use the new function
&lt;/span&gt;	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Now we can replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_pred = x.mm(w1).clamp(min=0).mm(w2)&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_pred = my_relu(x.mm(w1)).mm(w2)&lt;/code&gt;. In practice, do it only when you need custom backward.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-nn&quot;&gt;PyTorch: nn&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Higher-level wrapper for working with neural nets
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-2&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	
	&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;pytorch-optim&quot;&gt;PyTorch: optim&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# different update rules
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;pytorch-define-new-modules&quot;&gt;PyTorch: Define new Modules&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TwoLayerNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# init sets up two children
&lt;/span&gt;		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TwoLayerNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TwoLayerNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;pytorch-pretrained-models&quot;&gt;PyTorch: Pretrained Models&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;alexnet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alexnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vgg16&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vgg16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;resnet101&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resnet101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;tensorflow-24&quot;&gt;TensorFlow 2.4&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Default dynamic graph, optionally static.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# weights
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# weights
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientTape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;					&lt;span class=&quot;c1&quot;&gt;# build dynamic graph
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;			&lt;span class=&quot;c1&quot;&gt;# forward pass
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# backward pass
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# gradient descent
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;keras-high-level-wrapper&quot;&gt;Keras: High-level Wrapper&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientTape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MeanSquaredError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainable_variables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainable_variables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We can make use of different update rules with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.optimizers.{}&lt;/code&gt; and predefined loss functions as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Keras can handle the training loop;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MeanSquaredError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
			 &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;tensorflow-compile-static-graph&quot;&gt;TensorFlow: compile static graph&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;model_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MeanSquaredError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@tf.function&lt;/code&gt; decorator (implicitly) compiles python functions to static graph for better performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dynamic-vs-static&quot;&gt;Dynamic vs. Static&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Dynamic Computation Graphs: 
  Building the graph and computing the graph happen at the same time.&lt;br /&gt;
  Graph building and execution are intertwined, so always need to keep code around&lt;br /&gt;
  Inefficient, especially if we are building the same graph over and over again.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Static Computation Graphs:&lt;br /&gt;
  Build computational graph describing our computation(including finding paths for backprop)&lt;br /&gt;
  Reuse the same graph on every iteration&lt;br /&gt;
  Once graph is built, can serialize it and run it without the code that built the graph&lt;br /&gt;
  Framework can optimize the graph before it runs&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-vs-tensorflow&quot;&gt;PyTorch vs. TensorFlow&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;PyTorch&lt;br /&gt;
  Dynamic Graphs as default set&lt;br /&gt;
  Static: ONNX, TorchScript&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TensorFlow&lt;br /&gt;
  Dynamic: Eager set&lt;br /&gt;
  Static: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@tf.function&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-parallel-vs-data-parallel&quot;&gt;Model Parallel vs. Data Parallel&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Model parallelism:&lt;br /&gt;
  split computation graph into parts and distribute to GPUs/nodes&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data parallelism:&lt;br /&gt;
  split minibatch into chunks and distribute to GPUs/ nodes&lt;br /&gt;
  PyTorch: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.DataParallel&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.DistributedDataParallel&lt;/code&gt;&lt;br /&gt;
  TensorFlow: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.distributed.Strategy&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Deeplearning Software The point of deep learning frameworks (1) Quick to develop and test new ideas (2) Automatically compute gradients (3) Run it all efficiently on GPU (wrap cuDNN, cuBLAS, OpenCL, etc) Computational graph example import numpy as np np.random.seed(0) N, D = 3, 4 x = np.random.randn(N, D) y = np.random.randn(N, D) z = np.random.randn(N, D) a = x * y b = a + z c = np.sum(b) grad_c = 1.0 grad_b = grad_c * np.ones((N, D)) grad_a = grad_b.copy() grad_z = grad_b.copy() grad_x = grad_a * y grad_y = grad_a * x in Numpy Good: Clean API, easy to write numeric code Bad: Have to compute our own gradients and can’t run on GPU import torch device = 'cuda:0' N, D = 3, 4 x = torch.randn(N, D, requires_grad=True, device=device) y = torch.randn(N, D) z = torch.randn(N, D) a = x * y b = a + z c = torch.sum(b) c.backward() print(x.grad) in PyTorch PyTorch handles gradients for us Can run on GPU PyTorch: Fundamental Concepts torch.Tensor: Like a numpy array, but can run on GPU torch.autograd: Package for building computational graphs out of Tensors, and automatically computing gradients torch.nn.Module: A neural network layer; may store state or learnable weights we are using PyTorch version 1.7 here PyTorch: Autograd # Running example: Train a two-layer ReLU network on random data with L2 loss import torch N, D_in, H, D_out = 64, 1000, 100, 10 x = torch.randn(N, D_in) y = torch.randn(N, D_out) w1 = torch.randn(D_in, H, requires_grad=True) # enables autograd w2 = torch.randn(H, D_out, requires_grad=True) learning_rate = 1e-6 for t in range(500): h = x.mm(w1) # Forward pass h_relu = h.clamp(min=0) # no need to track intermediate values y_pred = h_relu.mm(w2) # = x.mm(w1).clamp(min=0).mm(w2) loss = (y_pred - y).pow(2).sum() loss_backward() # Compute gradient of loss with torch.no_grad(): # Gradient descent w1 -= learning_rate * w1.grad w2 -= learning_rate * w2.grad w1.grad.zero_() w2.grad.zero_() Or you can define your own class MyReLU(torch.autograd.Function): @staticmethod def forward(ctx, x): #Use ctx object to “cache” values for the backward pass ctx.save_for_backward(x) return x.clamp(min=0) @staticmethod def backward(ctx, grad_y): x, = ctx.saved_tensors grad_input = grad_y.clone() grad_input[x &amp;lt; 0] = 0 return grad_input def my_relu(x): # a helper function to make it easy to use the new function return MyReLU.apply(x) Now we can replace y_pred = x.mm(w1).clamp(min=0).mm(w2) with y_pred = my_relu(x.mm(w1)).mm(w2). In practice, do it only when you need custom backward. PyTorch: nn # Higher-level wrapper for working with neural nets import torch N, D_in, H, D_out = 64, 1000, 100, 10 x = torch.randn(N, D_in) y = torch.randn(N, D_out) model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out)) learning_rate = 1e-2 for t in range(500): y_pred = model(x) loss = torch.nn.functional.mse_loss(y_pred, y) loss.backward() with torch.no_grad(): for param in model.parameters(): param -= learning_rate * param.grad model.zero_grad() PyTorch: optim import torch N, D_in, H, D_out = 64, 1000, 100, 10 x = torch.randn(N, D_in) y = torch.randn(N, D_out) model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out)) learning_rate = 1e-4 optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # different update rules for t in range(500): y_pred = model(x) loss = torch.nn.functional.mse_loss(y_pred, y) loss.backward() optimizer.step() optimizer.zero_grad() PyTorch: Define new Modules import torch class TwoLayerNet(torch.nn.Module): def __init__(self, D_in, H, D_out): # init sets up two children super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(D_in, H) self.linear2 = torch.nn.Linear(H, D_out) def forward(self, x): h_relu = self.linear1(x).clamp(min=0) y_pred = self.linear2(h_relu) return y_pred N, D_in, H, D_out = 64, 1000, 100, 10 x = torch.randn(N, D_in) y = torch.randn(N, D_out) model = TwoLayerNet(D_in, H, D_out) optimizer = torch.optim.SGD(model.parameters(), lr=1e-4) for t in range(500): y_pred = model(x) loss = torch.nn.functional.mse_loss(y_pred, y) loss.backward() optimizer.step() optimizer.zero_grad() PyTorch: Pretrained Models import torch import torchvision alexnet = torchvision.models.alexnet(pretrained=True) vgg16 = torchvision.models.vgg16(pretrained=True) resnet101 = torchvision.models.resnet101(pretrained=True) TensorFlow 2.4 Default dynamic graph, optionally static. import numpy as np import tensorflow as tf N, D, H = 64, 1000, 100 x = tf.convert_to_tensor(np.random.randn(N, D), np.float32) y = tf.convert_to_tensor(np.random.randn(N, D), np.float32) w1 = tf.Variable(tf.random.uniform((D, H))) # weights w2 = tf.Variable(tf.random.uniform((H, D))) # weights learning_rate = 1e-6 for t in range(50): with tf.GradientTape() as tape: # build dynamic graph h = tf.maximum(tf.matmul(x, w1), 0) # forward pass y_pred = tf.matmul(h, w2) diff = y_pred - y loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1)) gradients = tape.gradient(loss, [w1, w2]) # backward pass w1.assign(w1 - learning_rate * gradients[0]) # gradient descent w2.assign(w2 - learning_rate * gradients[1]) Keras: High-level Wrapper import numpy as np import tensorflow as tf N, D, H = 64, 1000, 100 x = tf.convert_to_tensor(np.random.randn(N, D), np.float32) y = tf.convert_to_tensor(np.random.randn(N, D), np.float32) model = tf.keras.Sequential() model.add(tf.keras.layers.Dense(H, input_shape=(D,), activation=tf.nn.relu)) model.add(tf.keras.layers.Dense(D)) optimizer = tf.optimizers.SGD(1e-1) losses = [] for t in range(50): with tf.GradientTape() as tape: y_pred = model(x) loss = tf.losses.MeanSquaredError()(y_pred, y) gradients = tape.gradient( loss, model.trainable_variables) optimizer.apply_gradients( zip(gradients, model.trainable_variables)) We can make use of different update rules with tf.optimizers.{} and predefined loss functions as well. Keras can handle the training loop; N, D, H = 64, 1000, 100 x = tf.convert_to_tensor(np.random.randn(N, D), np.float32) y = tf.convert_to_tensor(np.random.randn(N, D), np.float32) model = tf.keras.Sequential() model.add(tf.keras.layers.Dense(H, input_shape=(D,), activation=tf.nn.relu)) model.add(tf.keras.layers.Dense(D)) optimizer = tf.optimizers.SGD(1e-1) model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=optimizer) history = model.fit(x, y, epochs=50, batch_size=N) TensorFlow: compile static graph ... model.add(tf.keras.layers.Dense(D)) optimizer = tf.optimizers.SGD(1e-1) @tf.function def model_func(x, y): y_pred = model(x) loss = tf.losses.MeanSquaredError()(y_pred, y) return y_pred, loss for t in range(50): ... @tf.function decorator (implicitly) compiles python functions to static graph for better performance. Dynamic vs. Static Dynamic Computation Graphs: Building the graph and computing the graph happen at the same time. Graph building and execution are intertwined, so always need to keep code around Inefficient, especially if we are building the same graph over and over again. Static Computation Graphs: Build computational graph describing our computation(including finding paths for backprop) Reuse the same graph on every iteration Once graph is built, can serialize it and run it without the code that built the graph Framework can optimize the graph before it runs PyTorch vs. TensorFlow PyTorch Dynamic Graphs as default set Static: ONNX, TorchScript TensorFlow Dynamic: Eager set Static: @tf.function Model Parallel vs. Data Parallel Model parallelism: split computation graph into parts and distribute to GPUs/nodes Data parallelism: split minibatch into chunks and distribute to GPUs/ nodes PyTorch: nn.DataParallel, nn.DistributedDataParallel TensorFlow: tf.distributed.Strategy</summary></entry><entry><title type="html">cs231n - Lecture 4. Neural Networks and Backpropagation</title><link href="http://0.0.0.0:4000/cs231n_lec4" rel="alternate" type="text/html" title="cs231n - Lecture 4. Neural Networks and Backpropagation" /><published>2021-12-16T00:00:00+09:00</published><updated>2021-12-16T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec4</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec4">&lt;h3 id=&quot;image-features&quot;&gt;Image Features&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Problem: Linear Classifiers are not very powerful
    &lt;ul&gt;
      &lt;li&gt;Visual Viewpoint: Linear classifiers learn one template per class&lt;/li&gt;
      &lt;li&gt;Geometric Viewpoint: Linear classifiers can only draw linear decision boundaries&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Image Features: Motivation&lt;br /&gt;
After applying feature transform, points can be separated by linear classifier&lt;br /&gt;
$f(x,y) = (r(x,y), \theta(x,y))$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Image Features vs. ConvNets&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec4_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;neural-networks&quot;&gt;Neural Networks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Neural networks, also called &lt;em&gt;Fully connected networks&lt;/em&gt;(FCN) or sometimes &lt;em&gt;multi-layer perceptrons&lt;/em&gt;(MLP)&lt;br /&gt;
(Before) Linear score function:&lt;br /&gt;
\(\begin{align*}&amp;amp; f=Wx \\
&amp;amp; x\in\mathbb{R}^D, W\in\mathbb{R}^{C\times D}
\end{align*}\)&lt;br /&gt;
$\rightarrow$ &lt;em&gt;2&lt;/em&gt;-layer Neural Network:&lt;br /&gt;
\(\begin{align*}&amp;amp; f=W_2 \mbox{max}(0,W_1 x) \\
&amp;amp; x\in\mathbb{R}^D, W_1\in\mathbb{R}^{H\times D}, 
W_2\in\mathbb{R}^{C\times H}
\end{align*}\)&lt;br /&gt;
$\rightarrow$ or &lt;em&gt;3&lt;/em&gt;-layer Neural Network:&lt;br /&gt;
\(f=W_3\mbox{max}(0,W_2 \mbox{max}(0,W_1 x)) \\ 
\vdots\)&lt;br /&gt;
(In practice we will usually add a learnable bias at each layer as well)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Neural networks: hierarchical computation&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec4_1.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;50%&quot; /&gt;&lt;br /&gt;
Learning 100s of templates instead of 10 and share templates between classes&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Why is &lt;em&gt;max&lt;/em&gt; operator important?&lt;br /&gt;
The function $\mbox{max}(0,z)$ is called the activation function.&lt;br /&gt;
Q: What if we try to build a neural network without one?&lt;br /&gt;
A: We end up with a linear classifier again!&lt;br /&gt;
$f=W_2 W_1 x, W_3=W_1 W_2, f = W_3 x$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Activation functions&lt;br /&gt;
  ReLU($\mbox{max}(0,z)$) is a good default choice for most problems&lt;br /&gt;
  Others: Sigmoid, tanh, Leaky ReLU, Maxout, ELU, etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Neural networks: Architectures&lt;br /&gt;
Example feed-forward computation of a neural network&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec4_2.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;50%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# forward-pass of a 3-layer neural network:
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# activation function (use sigmoid)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# random input vector of three numbers (3x1)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# calculate first hidden layer activations (4x1)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#calculate second hidden layer activations (4x1)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#output neuron (1x1)
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Full implementation of training a &lt;em&gt;2&lt;/em&gt;-layer Neural Network:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy.random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# Define the network
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;					&lt;span class=&quot;c1&quot;&gt;# Forward pass
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# Calculate the analytical gradients
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;grad_w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grad_h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grad_w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_w1&lt;/span&gt;				&lt;span class=&quot;c1&quot;&gt;# Gradient descent
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_w2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Plugging in neural networks with loss functions&lt;br /&gt;
$s = f(x;W_1,W_2) = W_2\mbox{max}(0,W_1 x)$ Nonlinear score function&lt;br /&gt;
$L_i = \sum_{j\ne y_i}\mbox{max}(0,s_j-s_{y_i}+1)$ SVM Loss on predictions&lt;br /&gt;
$R(W)=\sum_k W_k^2$ Regularization&lt;br /&gt;
$L=\frac{1}{N}\sum_{i=1}^N L_i + \lambda R(W_1) + \lambda R(W_2)$ Total loss: data loss + regularization&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Problem: How to compute gradients?&lt;br /&gt;
If we can compute partial derivaties, then we can learn $W_1$ and $W_2$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;backpropagation&quot;&gt;Backpropagation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Chain rule:&lt;br /&gt;
\(\begin{align*}
\frac{\partial f}{\partial y}=\frac{\partial f}{\partial q} \frac{\partial q}{\partial y} \mbox{Upstream gradient} \times \mbox{Local gradient}
\end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Patterns in gradient flow&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec4_3.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Image Features Problem: Linear Classifiers are not very powerful Visual Viewpoint: Linear classifiers learn one template per class Geometric Viewpoint: Linear classifiers can only draw linear decision boundaries Image Features: Motivation After applying feature transform, points can be separated by linear classifier $f(x,y) = (r(x,y), \theta(x,y))$ Image Features vs. ConvNets Neural Networks Neural networks, also called Fully connected networks(FCN) or sometimes multi-layer perceptrons(MLP) (Before) Linear score function: \(\begin{align*}&amp;amp; f=Wx \\ &amp;amp; x\in\mathbb{R}^D, W\in\mathbb{R}^{C\times D} \end{align*}\) $\rightarrow$ 2-layer Neural Network: \(\begin{align*}&amp;amp; f=W_2 \mbox{max}(0,W_1 x) \\ &amp;amp; x\in\mathbb{R}^D, W_1\in\mathbb{R}^{H\times D}, W_2\in\mathbb{R}^{C\times H} \end{align*}\) $\rightarrow$ or 3-layer Neural Network: \(f=W_3\mbox{max}(0,W_2 \mbox{max}(0,W_1 x)) \\ \vdots\) (In practice we will usually add a learnable bias at each layer as well) Neural networks: hierarchical computation Learning 100s of templates instead of 10 and share templates between classes Why is max operator important? The function $\mbox{max}(0,z)$ is called the activation function. Q: What if we try to build a neural network without one? A: We end up with a linear classifier again! $f=W_2 W_1 x, W_3=W_1 W_2, f = W_3 x$ Activation functions ReLU($\mbox{max}(0,z)$) is a good default choice for most problems Others: Sigmoid, tanh, Leaky ReLU, Maxout, ELU, etc. Neural networks: Architectures Example feed-forward computation of a neural network # forward-pass of a 3-layer neural network: f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid) x = np.random.randn(3,1) # random input vector of three numbers (3x1) h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1) h2 = f(np.dot(W2, h1) + b2) #calculate second hidden layer activations (4x1) out = np.dot(W3, h2) + b3 #output neuron (1x1) Full implementation of training a 2-layer Neural Network: import numpy as np from numpy.random import randn N, D_in, H, D_out = 64, 1000, 100, 10 # Define the network x, y = randn(N, D_in), randn(N, D_out) w1, w2 = randn(D_in, H), randn(H, D_out) for t in range(2000): # Forward pass h = 1 / (1 + np.exp(-x.dot(w1))) y_pred = h.dot(w2) loss = np.square(y_pred - y).sum() print(t, loss) grad_y_pred = 2.0 * (y_pred - y) # Calculate the analytical gradients grad_w2 = h.T.dot(grad_y_pred) grad_h = grad_y_pred.dot(w2.T) grad_w1 = x.T.dot(grad_h * h * (1-h)) w1 -= 1e-4 * grad_w1 # Gradient descent w2 -= 1e-4 * grad_w2 Plugging in neural networks with loss functions $s = f(x;W_1,W_2) = W_2\mbox{max}(0,W_1 x)$ Nonlinear score function $L_i = \sum_{j\ne y_i}\mbox{max}(0,s_j-s_{y_i}+1)$ SVM Loss on predictions $R(W)=\sum_k W_k^2$ Regularization $L=\frac{1}{N}\sum_{i=1}^N L_i + \lambda R(W_1) + \lambda R(W_2)$ Total loss: data loss + regularization Problem: How to compute gradients? If we can compute partial derivaties, then we can learn $W_1$ and $W_2$. Backpropagation Chain rule: \(\begin{align*} \frac{\partial f}{\partial y}=\frac{\partial f}{\partial q} \frac{\partial q}{\partial y} \mbox{Upstream gradient} \times \mbox{Local gradient} \end{align*}\) Patterns in gradient flow</summary></entry><entry><title type="html">cs231n - Lecture 3. Loss Functions and Optimization</title><link href="http://0.0.0.0:4000/cs231n_lec3" rel="alternate" type="text/html" title="cs231n - Lecture 3. Loss Functions and Optimization" /><published>2021-12-15T00:00:00+09:00</published><updated>2021-12-15T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec3</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec3">&lt;h3 id=&quot;linear-classifier-cont&quot;&gt;Linear Classifier (cont.)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Todo:
    &lt;ol&gt;
      &lt;li&gt;Define a loss function: how good the classifier is&lt;/li&gt;
      &lt;li&gt;Optimization: efficient way of finding the parameters that minimize the loss function&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loss function&lt;br /&gt;
given a dataset of examples \(\left\{ (x_i,y_i) \right\}_{n=1}^N\)&lt;br /&gt;
where $x_i$ is image and $y_i$ is (integer) label&lt;br /&gt;
Average of loss over examples: \(L=\frac{1}{N}\sum_i L_i(f(x_i,W),y_i)\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Multiclass SVM loss:&lt;br /&gt;
using the shorthand for the scores vector: $s = f(x_i,W)$&lt;br /&gt;
\(L_i=\sum_{j\ne y_i}\mbox{max}(0,s_j-s_{y_i}+1)\)&lt;br /&gt;
where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+1&lt;/code&gt; is a safety margin&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec3_0.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Q1: What happens to loss if car scores decrease by &lt;em&gt;0.5&lt;/em&gt; for this training example?&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L = max(0,0.8-4.4+1) + max(0,1.5-4.4+1) = 0&lt;/code&gt;&lt;br /&gt;
Result will not be changed; SVM hinge loss is robust to small change of scores.&lt;/p&gt;

&lt;p&gt;Q2: what is the min/max possible SVM loss $L_i$?&lt;br /&gt;
The possible minimum value of SVM loss is &lt;em&gt;0&lt;/em&gt; and maximum value is $\infty$.&lt;/p&gt;

&lt;p&gt;Q3: At initialization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W&lt;/code&gt; is small so all &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s ≈ 0&lt;/code&gt;. What is the loss $L_i$, assuming &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt; examples and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; classes?&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C-1&lt;/code&gt; the number of classes minus &lt;em&gt;1&lt;/em&gt;.&lt;br /&gt;
(&lt;em&gt;Sanity check for weight initialization&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;Q4: What if the sum was over all classes? (including $j = y_i$)&lt;br /&gt;
All losses increased by &lt;em&gt;1&lt;/em&gt;, so as the average loss over full dataset. In this case, the minimum value of loss will be &lt;em&gt;1&lt;/em&gt;. What we want is to minimize the loss function and that’s why we remove the class $s_j=s_{y_i}$ to set it &lt;em&gt;0&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Q5: What if we used mean instead of sum?&lt;br /&gt;
Because a loss function is to find optimizing parameters, rescaling loss has no effect to the result.&lt;/p&gt;

&lt;p&gt;Q6: What if we used squared function of
\(L_i=\sum_{j\ne y_i}\mbox{max}(0,s_j-s_{y_i}+1)^2\)&lt;br /&gt;
A squared hinge loss can be better at finding optimizing parameters &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W&lt;/code&gt;. If the observation is close to the answer, it will reflect the loss much smaller than original hinge loss.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;L_i_vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
	&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
	&lt;span class=&quot;n&quot;&gt;margins&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;margins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;margins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_i&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;$f(x,W) = Wx$&lt;br /&gt;
\(L=\frac{1}{N}\sum_{i=1}^N \mbox{max}(0,f(x_i;W)_j -f(x_i;W)y_i +1)\)&lt;/p&gt;

&lt;p&gt;Q7: Suppose that we found a W such that $L = 0$. Is this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W&lt;/code&gt; unique?&lt;br /&gt;
No. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2W&lt;/code&gt; is also has $L=0$. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W&lt;/code&gt; not a unique solution.&lt;br /&gt;
$\rightarrow$ How do we choose between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2W&lt;/code&gt;?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Regularization: introduce penalty term $\lambda R(W)$ on L(W)&lt;br /&gt;
L2 regularization: $R(W)=\sum_k\sum_l W_{k,l}^2$; likes to spread out the weights&lt;br /&gt;
L1 regularization: $R(W)=\sum_k\sum_l |W_{k,l}|$&lt;br /&gt;
Elastic net (L1 + L2): $R(W) = \sum_k\sum_l\beta W_{k,l}^2 + |W_{k,l}|$&lt;br /&gt;
Dropout, Batch normalization, Stochastic depth, fractional pooling, etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Why regularize?&lt;br /&gt;
Express preferences over weights&lt;br /&gt;
Make the model simple so it works on test data&lt;br /&gt;
Improve optimization by adding curvature&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;softmax-classifiermultinomial-logistic-regression&quot;&gt;Softmax classifier(Multinomial Logistic Regression)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec3_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
where Kullback–Leibler divergence $D_{KL}(P||Q)=\sum_y P(y)\log\frac{P(y)}{Q(y)}$&lt;/p&gt;

&lt;p&gt;Q1: What is the min/max possible softmax loss $L_i$?&lt;br /&gt;
min &lt;em&gt;0&lt;/em&gt; to max $\infty$&lt;/p&gt;

&lt;p&gt;Q2: At initialization all $s_j$ will be approximately equal; what is the softmax loss $L_i$, assuming &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; classes?&lt;br /&gt;
$-\log(1/C)=\log C$&lt;/p&gt;

&lt;h3 id=&quot;recap&quot;&gt;Recap&lt;/h3&gt;
&lt;p&gt;For some dataset of $(x,y)$, a score function $s = f(x;W)$, a loss function $L_i$,&lt;br /&gt;
Full loss \(L=\frac{1}{N}\sum_{i=1}^N L_i + R(W)\)&lt;/p&gt;

&lt;h3 id=&quot;optimization&quot;&gt;Optimization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Strategy: Follow the slope&lt;br /&gt;
In &lt;em&gt;1&lt;/em&gt;-dimension, the derivative of a function:&lt;br /&gt;
\(\frac{df(x)}{dx}=\lim_{n\to 0}\frac{f(x+h)-f(x)}{h}\)&lt;br /&gt;
In multiple dimensions, the gradient is the vector of (partial derivatives) along each dimension.&lt;br /&gt;
The slope in any direction is the dot product of the direction with the gradient. The direction of steepest descent is the negative gradient.&lt;br /&gt;
The loss is just a function of &lt;em&gt;W&lt;/em&gt;, what we want is $\nabla_w L$&lt;br /&gt;
In practice: Always use analytic gradient, but check implementation with numerical gradient. This is called a gradient check.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stochastic Gradient Descent(SGD)&lt;br /&gt;
Full sum expensive when &lt;em&gt;N&lt;/em&gt; is large. Approximate su using a minibatch of examples
(32 / 64 / 128 common)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Vanilla Minibatch Gradient Descent
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;data_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
	&lt;span class=&quot;n&quot;&gt;weights_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_fun&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights_grad&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Linear Classifier (cont.) Todo: Define a loss function: how good the classifier is Optimization: efficient way of finding the parameters that minimize the loss function Loss function given a dataset of examples \(\left\{ (x_i,y_i) \right\}_{n=1}^N\) where $x_i$ is image and $y_i$ is (integer) label Average of loss over examples: \(L=\frac{1}{N}\sum_i L_i(f(x_i,W),y_i)\) Multiclass SVM loss: using the shorthand for the scores vector: $s = f(x_i,W)$ \(L_i=\sum_{j\ne y_i}\mbox{max}(0,s_j-s_{y_i}+1)\) where +1 is a safety margin Q1: What happens to loss if car scores decrease by 0.5 for this training example? L = max(0,0.8-4.4+1) + max(0,1.5-4.4+1) = 0 Result will not be changed; SVM hinge loss is robust to small change of scores. Q2: what is the min/max possible SVM loss $L_i$? The possible minimum value of SVM loss is 0 and maximum value is $\infty$. Q3: At initialization W is small so all s ≈ 0. What is the loss $L_i$, assuming N examples and C classes? C-1 the number of classes minus 1. (Sanity check for weight initialization) Q4: What if the sum was over all classes? (including $j = y_i$) All losses increased by 1, so as the average loss over full dataset. In this case, the minimum value of loss will be 1. What we want is to minimize the loss function and that’s why we remove the class $s_j=s_{y_i}$ to set it 0. Q5: What if we used mean instead of sum? Because a loss function is to find optimizing parameters, rescaling loss has no effect to the result. Q6: What if we used squared function of \(L_i=\sum_{j\ne y_i}\mbox{max}(0,s_j-s_{y_i}+1)^2\) A squared hinge loss can be better at finding optimizing parameters W. If the observation is close to the answer, it will reflect the loss much smaller than original hinge loss. def L_i_vectorized(x,y,W): scores = W.dot(x) margins = np.maximum(0, scores -scores[y] +1) margins[y]=0 loss_i = np.sum(margins) return loss_i $f(x,W) = Wx$ \(L=\frac{1}{N}\sum_{i=1}^N \mbox{max}(0,f(x_i;W)_j -f(x_i;W)y_i +1)\) Q7: Suppose that we found a W such that $L = 0$. Is this W unique? No. 2W is also has $L=0$. W not a unique solution. $\rightarrow$ How do we choose between W and 2W? Regularization: introduce penalty term $\lambda R(W)$ on L(W) L2 regularization: $R(W)=\sum_k\sum_l W_{k,l}^2$; likes to spread out the weights L1 regularization: $R(W)=\sum_k\sum_l |W_{k,l}|$ Elastic net (L1 + L2): $R(W) = \sum_k\sum_l\beta W_{k,l}^2 + |W_{k,l}|$ Dropout, Batch normalization, Stochastic depth, fractional pooling, etc. Why regularize? Express preferences over weights Make the model simple so it works on test data Improve optimization by adding curvature Softmax classifier(Multinomial Logistic Regression) where Kullback–Leibler divergence $D_{KL}(P||Q)=\sum_y P(y)\log\frac{P(y)}{Q(y)}$ Q1: What is the min/max possible softmax loss $L_i$? min 0 to max $\infty$ Q2: At initialization all $s_j$ will be approximately equal; what is the softmax loss $L_i$, assuming C classes? $-\log(1/C)=\log C$ Recap For some dataset of $(x,y)$, a score function $s = f(x;W)$, a loss function $L_i$, Full loss \(L=\frac{1}{N}\sum_{i=1}^N L_i + R(W)\) Optimization Strategy: Follow the slope In 1-dimension, the derivative of a function: \(\frac{df(x)}{dx}=\lim_{n\to 0}\frac{f(x+h)-f(x)}{h}\) In multiple dimensions, the gradient is the vector of (partial derivatives) along each dimension. The slope in any direction is the dot product of the direction with the gradient. The direction of steepest descent is the negative gradient. The loss is just a function of W, what we want is $\nabla_w L$ In practice: Always use analytic gradient, but check implementation with numerical gradient. This is called a gradient check. Stochastic Gradient Descent(SGD) Full sum expensive when N is large. Approximate su using a minibatch of examples (32 / 64 / 128 common) # Vanilla Minibatch Gradient Descent while True: data_batch = sample_training_data(data,256) weights_grad = evaluate_gradient(loss_fun, data_batch, weights) weights += - step_size * weights_grad</summary></entry><entry><title type="html">cs231n - Lecture 2. Image Classification</title><link href="http://0.0.0.0:4000/cs231n_lec2" rel="alternate" type="text/html" title="cs231n - Lecture 2. Image Classification" /><published>2021-12-14T00:00:00+09:00</published><updated>2021-12-14T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec2</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec2">&lt;h3 id=&quot;image-classification-a-core-task-in-computer-vision&quot;&gt;Image Classification: A Core Task in Computer Vision&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The Problem: Semantic Gap&lt;br /&gt;
considering image as a tensor of integers between [0,255] with 3 channels RGB&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Challenges:&lt;br /&gt;
  Viewpoint variation&lt;br /&gt;
  Background Clutter
  Illumination&lt;br /&gt;
  Occlusion&lt;br /&gt;
  Deformation&lt;br /&gt;
  Intraclass variation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An image classifier&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;classify_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
&lt;span class=&quot;c1&quot;&gt;# Some magic here?  
&lt;/span&gt;	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_label&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;ML: Data-Driven Approach
    &lt;ol&gt;
      &lt;li&gt;Collect a dataset of images and labels&lt;/li&gt;
      &lt;li&gt;Use ML algorithms to train a classifier&lt;/li&gt;
      &lt;li&gt;Evaluate the classifier on new images&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
	&lt;span class=&quot;c1&quot;&gt;# Machine Learning!  
&lt;/span&gt;	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;  

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
	&lt;span class=&quot;c1&quot;&gt;# Use model to predict labels  
&lt;/span&gt;	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_labels&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Nearest Neighbor Classifier&lt;br /&gt;
Predict the label of the most similar training imgae&lt;br /&gt;
Training data with labels x $\leftrightarrow$ query data \(x^*\)&lt;br /&gt;
distance metric \(|x,x^*| \rightarrow R\)&lt;br /&gt;
&lt;em&gt;L1&lt;/em&gt; distance \(d_1(I_1,I_2) = \sum_p |I_1^p-I_2^p|\)&lt;br /&gt;
pixel-wise absolute value differences $\rightarrow$ sum for scoring&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NearestNeighbor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
		&lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;### Memorize training data
&lt;/span&gt;		&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; X is N x D for n example. Y is 1-dim of size N&quot;&quot;&quot;&lt;/span&gt;  
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xtr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xf&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ytr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;  

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
		&lt;span class=&quot;n&quot;&gt;num_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  
		&lt;span class=&quot;n&quot;&gt;Ypred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ytr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;c1&quot;&gt;### find closest train image for each test image, predict label of its
&lt;/span&gt;			&lt;span class=&quot;n&quot;&gt;distances&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xtr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;min_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
			&lt;span class=&quot;n&quot;&gt;Ypred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ytr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ypred&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Q: With &lt;em&gt;N&lt;/em&gt; examples, how fast are training and prediction?&lt;br /&gt;
Answer: Train &lt;strong&gt;O(1)&lt;/strong&gt;, predict &lt;strong&gt;O(N)&lt;/strong&gt;&lt;br /&gt;
$\rightarrow$ Bad: we want fast at prediction; slow for training is ok.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;KNN with majority vote&lt;br /&gt;
Distance metric: &lt;em&gt;L1(Manhattan), L2(Euclidean)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperparameters&lt;br /&gt;
To find best value of &lt;em&gt;k&lt;/em&gt; and best distance(metric) to use, use train-val-test approach&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, pixel distances are not informative for KNN&lt;br /&gt;
very slow at test time &lt;em&gt;&amp;amp;&lt;/em&gt; curse of dimensionality&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;linear-classifier&quot;&gt;Linear Classifier&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Parametric Approach&lt;br /&gt;
$f(x,W)=Wx+b$; &lt;em&gt;W&lt;/em&gt; for parameters or weights&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interpreting a linear classifier: Geometric Viewpoint&lt;br /&gt;
hard cases in non-linearity&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Image Classification: A Core Task in Computer Vision The Problem: Semantic Gap considering image as a tensor of integers between [0,255] with 3 channels RGB Challenges: Viewpoint variation Background Clutter Illumination Occlusion Deformation Intraclass variation An image classifier def classify_image(image): # Some magic here? return class_label ML: Data-Driven Approach Collect a dataset of images and labels Use ML algorithms to train a classifier Evaluate the classifier on new images def train(images,labels): # Machine Learning! return model def predict(model, test_images): # Use model to predict labels return test_labels Nearest Neighbor Classifier Predict the label of the most similar training imgae Training data with labels x $\leftrightarrow$ query data \(x^*\) distance metric \(|x,x^*| \rightarrow R\) L1 distance \(d_1(I_1,I_2) = \sum_p |I_1^p-I_2^p|\) pixel-wise absolute value differences $\rightarrow$ sum for scoring import numpy as np class NearestNeighbor: def __init__(self): pass def train(self,X,y): ### Memorize training data &quot;&quot;&quot; X is N x D for n example. Y is 1-dim of size N&quot;&quot;&quot; self.Xtr = Xf self.ytr = y def predict(self,X): num_test = X.shape[0] Ypred = np.zeros(num_test, dtype = self.ytr.dtype) for i in xrange(num_test): ### find closest train image for each test image, predict label of its distances = np.sum(np.abs(self.Xtr - X[i,:]), axis=1) min_index = np.argmin(distances) Ypred[i] = self.ytr[min_index] return Ypred Q: With N examples, how fast are training and prediction? Answer: Train O(1), predict O(N) $\rightarrow$ Bad: we want fast at prediction; slow for training is ok. KNN with majority vote Distance metric: L1(Manhattan), L2(Euclidean) Hyperparameters To find best value of k and best distance(metric) to use, use train-val-test approach However, pixel distances are not informative for KNN very slow at test time &amp;amp; curse of dimensionality Linear Classifier Parametric Approach $f(x,W)=Wx+b$; W for parameters or weights Interpreting a linear classifier: Geometric Viewpoint hard cases in non-linearity</summary></entry></feed>