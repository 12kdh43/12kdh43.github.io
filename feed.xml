<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2022-03-30T11:50:57+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Darron’s Devlog</title><entry><title type="html">cs224n - Lecture 9. Self-Attention and Transformers</title><link href="http://0.0.0.0:4000/cs224n_lec9" rel="alternate" type="text/html" title="cs224n - Lecture 9. Self-Attention and Transformers" /><published>2022-03-26T00:00:00+00:00</published><updated>2022-03-26T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec9</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec9">&lt;h2 id=&quot;so-far-recurrent-models-for-most-nlp&quot;&gt;So far: recurrent models for (most) NLP&lt;/h2&gt;

&lt;p&gt;
	&lt;img src=&quot;/assets/images/cs224n/lec9_0.png&quot; alt=&quot;png&quot; width=&quot;40%&quot; style=&quot;float: left&quot; /&gt;
	&lt;br /&gt;
	- Circa 2016, the de facto strategy in NLP is to &lt;strong&gt;encode&lt;/strong&gt;
	sentences with a bidirectional LSTM. &lt;br /&gt;
	(for example, the source sentence in a translation) &lt;br /&gt;  
	&lt;br /&gt;
	&lt;br /&gt;
	&lt;br /&gt;
	- Define your output (parse, sentence, summary) as a sequence, and use an (unidirectional) LSTM to generate it. &lt;br /&gt;   
	&lt;br /&gt;
	&lt;br /&gt;
	&lt;br /&gt;
	&lt;br /&gt;
	- Use attention to allow flexible access to memory.  
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Seq2Seq models need to process variable-length inputs into fixed-length representations&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To deal with seq2seq problems, we learned end-to-end differentiable system using an encoder-decoder architecture.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of entirely new ways of looking at problems, we’re trying to find the best &lt;strong&gt;building blocks&lt;/strong&gt; to plug into our models and enable broad progress.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec9_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;issues-with-recurrent-models-linear-interaction-distance&quot;&gt;Issues with recurrent models: Linear interaction distance&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RNNs are unrolled “left-to-right”:&lt;br /&gt;
  This encodes linear locality: a useful heuristic
    &lt;ul&gt;
      &lt;li&gt;Nearby words often affect each other’s meanings&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: RNNs take &lt;strong&gt;O(sequence length)&lt;/strong&gt; steps for distant word pairs to interact.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec9_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;Hard to learn long-distance dependencies (because gradient problems)&lt;/li&gt;
      &lt;li&gt;Linear order of words is “baked in”; linear order isn’t the right way to think about sentences&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;issues-with-recurrent-models-lack-of-parallelizability&quot;&gt;Issues with recurrent models: Lack of parallelizability&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Forward and backward passes have &lt;strong&gt;O(sequence length)&lt;/strong&gt; unparallelizable operations
    &lt;ul&gt;
      &lt;li&gt;Future RNN hidden states can’t be computed in full before past RNN hidden states have been computed; not GPU friendly, inhibits training on very large datasets.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;alternatives-word-windows&quot;&gt;Alternatives: Word windows&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Word window models aggregate local contexts (Also known as 1D convolution)
    &lt;ul&gt;
      &lt;li&gt;Number of unparallelizable operations does not increase sequence length&lt;br /&gt;
  (&lt;strong&gt;O(1)&lt;/strong&gt; dependence in time)&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec9_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;What about in long-distance dependencies?
    &lt;ul&gt;
      &lt;li&gt;Stacking word window layers allows interaction between farther words&lt;/li&gt;
      &lt;li&gt;Maximum Interaction distance = &lt;strong&gt;sequence length / window size&lt;/strong&gt;&lt;br /&gt;
  But if your sequences are too long, you’ll just ignore long-distance context
  &lt;img src=&quot;/assets/images/cs224n/lec9_4.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;alternatives-attention&quot;&gt;Alternatives: Attention&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Attention&lt;/strong&gt; treats each word’s representation as a &lt;strong&gt;query&lt;/strong&gt; to access and
incorporate information from &lt;strong&gt;a set of values&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;Out of the encoder-decoder structure; think about attention &lt;strong&gt;within a single sentence&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Number of unparallelizable operations does not increase sequence length; not parallelizable in depth but parallelizable in time.&lt;/li&gt;
      &lt;li&gt;Maximum interaction distance: &lt;strong&gt;O(1)&lt;/strong&gt;, since all words interact at every layer&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec9_5.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;self-attention&quot;&gt;Self-Attention&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Recall: Attention operates on &lt;strong&gt;queries&lt;/strong&gt;, &lt;strong&gt;keys&lt;/strong&gt;, and &lt;strong&gt;values&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;queries: $q_1, q_2, \ldots, q_T \in \mathbb{R}^d$&lt;/li&gt;
      &lt;li&gt;keys: $k_1, k_2, \ldots, k_T \in \mathbb{R}d$&lt;/li&gt;
      &lt;li&gt;values: $v_1, v_2, \ldots, v_T \in \mathbb{R}d$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In &lt;strong&gt;self-attention&lt;/strong&gt;, the queries, keys, and values are drawn from the &lt;strong&gt;same source&lt;/strong&gt;.&lt;br /&gt;
  For example, if the output of the previous layer is $x_1, \ldots, x_T$ (one vec per word), we could let $v_i = k_i = q_i = x_i$ (use the same vectors for all of them).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The (dot product) self-attention operation is as follows:
    &lt;ul&gt;
      &lt;li&gt;Compute &lt;strong&gt;key-query&lt;/strong&gt; affinities: $e_{ij} = q_i^T k_j $&lt;/li&gt;
      &lt;li&gt;Compute attention weights from affinities(softmax):&lt;br /&gt;
  \(\begin{align*}
  \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{j^\prime} \exp(e_{ij^\prime})}
  \end{align*}\)&lt;/li&gt;
      &lt;li&gt;Compute outputs as weighted sum of &lt;strong&gt;values&lt;/strong&gt;&lt;br /&gt;
  \(\begin{align*}
  \text{output}_i = \sum_j \alpha_{ij}v_j
  \end{align*}\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Q: FCN vs Self-Attention?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;self-attention-as-an-nlp-building-block&quot;&gt;Self-attention as an NLP building block&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec9_6.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Can self-attention replace the recurrence? &lt;strong&gt;NO&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sequence-order&quot;&gt;Sequence order&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Self-attention is an operation on &lt;strong&gt;sets&lt;/strong&gt;; it has &lt;strong&gt;no inherent notion of order&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;To fix this, encode the order of the sentence.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Consider representing each &lt;strong&gt;sequence index&lt;/strong&gt; as a &lt;strong&gt;vector&lt;/strong&gt;;&lt;br /&gt;
  $p_i \in \mathbb{R}^d$, for \(i \in \left\{ 1,2,\ldots, T \right\}\) are position vectors&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Let $\tilde{v}_i, \tilde{k}_i, \tilde{q}_i$ be our old inputs, then add $p_i$ to inputs;&lt;br /&gt;
  $v_i = \tilde{v}_i + p_i$&lt;br /&gt;
  $q_i = \tilde{q}_i + p_i$&lt;br /&gt;
  $k_i = \tilde{k}_i + p_i$
    &lt;ul&gt;
      &lt;li&gt;In deep self-attention networks, we do this at the first layer; you could concatenate them as well, but mostly, just add.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Position representation vectors through sinusoids
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Sinusoidal position representations&lt;/strong&gt;: concatenate sinusoidal functions of varying periods&lt;br /&gt;
 &lt;img src=&quot;/assets/images/cs224n/lec9_7.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;Pros:
        &lt;ul&gt;
          &lt;li&gt;Periodicity indicates that maybe “absolute position” isn’t as important&lt;/li&gt;
          &lt;li&gt;Maybe can extrapolate to longer sequences as periods restart!&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Cons:
        &lt;ul&gt;
          &lt;li&gt;Not learnable; also the extrapolation doesn’t really work!&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Position representation vectors learned from scratch
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Learned absolute position representations&lt;/strong&gt;: Let a matrix $p \in \mathbb{R}^{d \times T}$ and each learnable parameters $p_i$ be a column of that matrix; Most systems use this!&lt;/li&gt;
      &lt;li&gt;Pros:
        &lt;ul&gt;
          &lt;li&gt;Flexibility: each position gets to be learned to fit the data&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Cons:
        &lt;ul&gt;
          &lt;li&gt;Definitely can’t extrapolate to indices outside $1, \ldots, T$.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Some more flexible representations of position:
        &lt;ul&gt;
          &lt;li&gt;Relative linear position attention (&lt;em&gt;Shaw et al., 2018&lt;/em&gt;)&lt;/li&gt;
          &lt;li&gt;Dependency syntax-based position (&lt;em&gt;Wang et al., 2019&lt;/em&gt;)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;nonlinearities&quot;&gt;Nonlinearities&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There are no elementwise nonlinearities in self-attention; stacking more self-attention layers just re-averages &lt;strong&gt;value&lt;/strong&gt; vectors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Easy fix: add a &lt;strong&gt;feed-forward network&lt;/strong&gt; to post-process each output vector.&lt;br /&gt;
  (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Self-Att. - FF - Self-Att. - FF - ...&lt;/code&gt;)&lt;br /&gt;
  \(\begin{align*}
  m_i &amp;amp;= MLP(\text{output}_i) \\
      &amp;amp;= W_2 \ast \text{ReLU}(W_1 \times \text{output}_i + b_1) + b_2
  \end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;masking-the-future&quot;&gt;Masking the future&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;To use self-attention in &lt;strong&gt;decoders&lt;/strong&gt;, we need to ensure we don’t “look at the future” when predicting a sequence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Easily: at every timestep, we could change the set of &lt;strong&gt;keys and queries&lt;/strong&gt; to include only past words. But it’s inefficient dealing with tensors, not parallelizable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead, to enable parallelization, &lt;strong&gt;mask out attention&lt;/strong&gt; to future words by setting attention scores to $-\infty$ (attention weights to 0).&lt;br /&gt;
  $e_{ij} = \begin{cases} q_i^T k_j, &amp;amp; j &amp;lt; i &lt;br /&gt;
                          -\infty, &amp;amp; j \ge i \end{cases}$&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec9_8.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;recap-necessities-for-a-self-attention-building-block&quot;&gt;Recap: Necessities for a self-attention building block&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Self-attention&lt;/strong&gt;: the basis of the method&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Position representations&lt;/strong&gt;:&lt;br /&gt;
  Specify the sequence order, since self-attention is an unordered function of its inputs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Nonlinearities&lt;/strong&gt;:&lt;br /&gt;
  At the output of the self-attention block, frequently implemented as a simple feed-forward network.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Masking&lt;/strong&gt;:&lt;br /&gt;
  In order to parallelize operations while not looking at the future, keeps information about the future from “leaking” to the past.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;transformer&quot;&gt;Transformer&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The Transformer Encoder-Decodr (&lt;em&gt;Vaswani et al., 2017&lt;/em&gt;)&lt;br /&gt;
  At a high level look;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec9_9.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What’s left in a Transformer Encoder Block:
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;Key-query-value attention&lt;/strong&gt;: How do we get input vectors from a single word embedding?&lt;/li&gt;
      &lt;li&gt;__Multi-headed attention: Attend to multiple places in a single layer&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Tricks to help with training&lt;/strong&gt;:
        &lt;ul&gt;
          &lt;li&gt;Residual connections&lt;/li&gt;
          &lt;li&gt;Layer normalization&lt;/li&gt;
          &lt;li&gt;Scaling the dot product
  These tricks &lt;strong&gt;don’t improve&lt;/strong&gt; what the model is able to do; they help improve the training process. Both of these types of modeling improvements are very important.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-transformer-encoder-key-query-value-attention&quot;&gt;The Transformer Encoder: Key-Query-Value Attention&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Let $x_1, \ldots, x_T \in \mathbb{R}^d$ be input vectors to the Transformer encoder. Then keys, queries, values are:
    &lt;ul&gt;
      &lt;li&gt;$k_i = Kx_i$, where $K \in \mathbb{R}^{d \times d}$ is the key matrix.&lt;/li&gt;
      &lt;li&gt;$q_i = Qx_i$, where $Q \in \mathbb{R}^{d \times d}$ is the query matrix.&lt;/li&gt;
      &lt;li&gt;$v_i = Vx_i$, where $V \in \mathbb{R}^{d \times d}$ is the value matrix.&lt;br /&gt;
These matrices (of learnable parameters) allow &lt;em&gt;different aspects&lt;/em&gt; of the $x$ vectors to be used/emphasized in each of the three roles.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Computed in matrices,
    &lt;ul&gt;
      &lt;li&gt;Let $X = \left[ x_1; \ldots; x_T \right] \in \mathbb{R}^{T\times d}$ be the concatenation of input vectors.&lt;/li&gt;
      &lt;li&gt;First, note that $XK \in \mathbb{R}^{T\times d}$, $XQ \in \mathbb{R}^{T\times d}$, $XV \in \mathbb{R}^{T\times d}$.&lt;/li&gt;
      &lt;li&gt;The output tensor is defined as $\text{output} = \text{softmax}(XQ(XK)^T) \times XV$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Take the query-key dot products in one matrix multiplication: $XQ(XK)^T = XQK^T X^T \in \mathbb{T}^{T\times T} $ (All pairs of attention scores)&lt;/li&gt;
  &lt;li&gt;Take softmax, and compute the weighted average with another matrix multiplication: $\text{output} \in \mathbb{T}^{T\times d}$&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;the-transformer-encoder-multi-headed-attention&quot;&gt;The Transformer Encoder: Multi-headed attention&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;What if we want to look in multiple places in the sentence at once?
    &lt;ul&gt;
      &lt;li&gt;For word $i$, self-attention “looks” where $x_i^T Q^T K x_j$ is high, but maybe we want to focus on different $j$ for different reasons?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Define &lt;strong&gt;multiple attention “heads”&lt;/strong&gt; through multiple $Q, K, V$ matrices:
    &lt;ul&gt;
      &lt;li&gt;Let $Q_l, K_l, V_l \in \mathbb{R}^{d\times \frac{d}{h}}$, where $h$ is the number of attention heads, and $l$ ranges from $1$ to $h$.&lt;/li&gt;
      &lt;li&gt;Each attention head performs attention independently:&lt;br /&gt;
  $\text{output}_l = \text{softmax}(XQ_l K_l^T X^T) \ast X V_l \in \mathbb{R}^{d/h}$&lt;/li&gt;
      &lt;li&gt;Then combine the all outputs from the heads:&lt;br /&gt;
  $\text{output} = Y\left[ \text{output}_1; \ldots, \text{output}_h \right]$, where $Y \in \mathbb{R}^{d\times d}$.&lt;/li&gt;
      &lt;li&gt;Each head gets to “look” at different things and construct value vectors differently.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec9_10.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-transformer-encoder-residual-connections-he-et-al-2016&quot;&gt;The Transformer Encoder: Residual connections [He et al., 2016]&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Residual connections&lt;/strong&gt; are a trick to help models train better.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of $X^{(i)} = \text{Layer}(X^{(i-1)})$ (where $i$ represents the layer)&lt;br /&gt;
  We let $X^{(i)} = X^{(i-1)} + \text{Layer}(X^{(i-1)})$ (so we only have to learn “the residual” from the previous layer)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Solves vanishing gradient problem&lt;/li&gt;
  &lt;li&gt;Residual connections are thought to make the &lt;em&gt;loss landscape&lt;/em&gt; considerably smoother (thus easier training!)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec9_11.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-transformer-encoder-layer-normalization-ba-et-al-2016&quot;&gt;The Transformer Encoder: Layer normalization [Ba et al., 2016]&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Layer normalization&lt;/strong&gt; is a trick to help models train faster.
    &lt;ul&gt;
      &lt;li&gt;Idea: cut down on uninformative variation in hidden vector values by normalizing to unit mean and standard deviation &lt;strong&gt;within each layer&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;LayerNorm’s success may be due to its normalizing gradients (&lt;em&gt;Xu et al., 2019&lt;/em&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Let $x\in \mathbb{R}^d$ be an individual (word) vector in the model.
    &lt;ul&gt;
      &lt;li&gt;Let the mean $\mu = \sum_{j=1}^d x_j \in \mathbb{R}$&lt;/li&gt;
      &lt;li&gt;Let the standard deviation $\sigma = \sqrt{\frac{1}{d}\sum_{j=1}^d (x_j-\mu)^2} \in \mathbb{R}$&lt;/li&gt;
      &lt;li&gt;Let $\gamma \in \mathbb{R}^d$ and $\beta \in \mathbb{R}$ be learned “gain” and “bias” parameters (Can omit)&lt;/li&gt;
      &lt;li&gt;Then layer normalization computes:&lt;br /&gt;
  \(\begin{align*}\text{output} = \frac{x-\mu}{\sigma + \epsilon}\ast\gamma + \beta
  \end{align*}\);&lt;br /&gt;
  (Normalize by scalar mean and variance, and modulate by learned elementwise gain and bias)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-transformer-encoder-scaled-dot-product-vaswani-et-al-2017&quot;&gt;The Transformer Encoder: Scaled Dot Product [Vaswani et al., 2017]&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When dimensionality $d$ becomes large, dot products between vectors tend to become large.&lt;br /&gt;
  Because of this, inputs to the softmax function can be large, making the gradients small (leading to saturate region)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of the self-attention functiopn we’ve seen:&lt;br /&gt;
  $\text{output}_l = \text{softmax}(XQ_l K_l^T X^T) \ast X V_l$&lt;br /&gt;
  We divide the attention scores by $\sqrt{d/h}$, to stop the scores from becoming large just as a function of $d/h$ (The dimensionality divided by the number of heads.):&lt;br /&gt;
  $\text{output}_l = \text{softmax}(\frac{XQ_l K_l^T X^T}{\sqrt{d/h}}) \ast X V_l$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;now-look-at-the-decoder-blocks&quot;&gt;Now look at the Decoder Blocks&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec9_12.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-transformer-decoder-cross-attention-details&quot;&gt;The Transformer Decoder: Cross-attention (details)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Let $h_1, \ldots, h_T$ be &lt;strong&gt;output&lt;/strong&gt; vecotrs &lt;strong&gt;from&lt;/strong&gt; the Transformer &lt;strong&gt;encoder&lt;/strong&gt; (the last block); $x_i \in \mathbb{R}^d$&lt;br /&gt;
  Let $z_1, \ldots, z_T$ be input vectors from the Transformer &lt;strong&gt;decoder&lt;/strong&gt;, $z_i\in \mathbb{R}^d$&lt;br /&gt;
  Then keys and values are drawn from the &lt;strong&gt;encoder&lt;/strong&gt; (like a memory); $k_i = Kh_i, v_i = Vh_i$&lt;br /&gt;
  The queries are drawn from the &lt;strong&gt;decoder&lt;/strong&gt;; $q_i = Qz_i$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In matrices:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Let $H = \left[ h_1; \ldots, h_T \right] \in \mathbb{R}^{T\times d}$ be the concatenation of encoder vecotrs.&lt;/li&gt;
      &lt;li&gt;Let $Z = \left[ z_1; \ldots, z_T \right] \in \mathbb{R}^{T\times d}$ be the concatenation of decoder vectors.&lt;/li&gt;
      &lt;li&gt;The output is defined as $\text{output} = \text{softmax}(ZQ(HK)^T)\times HV$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-would-we-like-to-fix-about-the-transformer&quot;&gt;What would we like to fix about the Transformer?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Quadratic compute in self-attention&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Computing all pairs of interactions means our computation grows quadratically with the sequence length.&lt;/li&gt;
      &lt;li&gt;For recurrent models, it only grew linearly.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Position representations&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Are simple absolute indices the best we can do to represent position?&lt;/li&gt;
      &lt;li&gt;Relative linear position attention (&lt;em&gt;Shaw et al., 2018&lt;/em&gt;)&lt;/li&gt;
      &lt;li&gt;Dependency syntax-based position (&lt;em&gt;Wang et al., 2019&lt;/em&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;quadratic-computation-as-a-function-of-sequence-length&quot;&gt;Quadratic computation as a function of sequence length&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;One of the benefits of self-attention over recurrence was that it’s highly parallelizable.&lt;br /&gt;
  However, its total number of operations grows as $O(T^2 d)$, where $T$ is the sequence length, and $d$ is the dimensionality.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec9_13.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Think of $d$ as around &lt;strong&gt;1,000&lt;/strong&gt;.&lt;br /&gt;
  So, for a single (shortish) sentence, $T \le 30; T^2 \le$ &lt;strong&gt;900&lt;/strong&gt;.&lt;br /&gt;
  In practice, we set a bound like $T = 512$.&lt;br /&gt;
  But what if we’d like &lt;strong&gt;$T\ge 10,000$&lt;/strong&gt;? to work on long documents?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;recent-work-on-improving-on-quadratic-self-attention-cost&quot;&gt;Recent work on improving on quadratic self-attention cost&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Considerable recent work has gone into the question,&lt;br /&gt;
  &lt;em&gt;Can we build models like Transformers without paying the $O(T^2)$ all-pairs self-attention cost?&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Linformer&lt;/strong&gt; (&lt;em&gt;Wang et al., 2020&lt;/em&gt;)&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec9_14.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;Key idea: map the sequence length dimension to a lower-dimensional space for values, keys.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;BigBird&lt;/strong&gt; (&lt;em&gt;Zaheer et al., 2021&lt;/em&gt;)&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec9_15.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;Key idea: replace all-pairs interactions with a family of other interactions, like &lt;strong&gt;local windows&lt;/strong&gt;, &lt;strong&gt;looking at everything&lt;/strong&gt;, and &lt;strong&gt;random interactions&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">So far: recurrent models for (most) NLP - Circa 2016, the de facto strategy in NLP is to encode sentences with a bidirectional LSTM. (for example, the source sentence in a translation) - Define your output (parse, sentence, summary) as a sequence, and use an (unidirectional) LSTM to generate it. - Use attention to allow flexible access to memory. Seq2Seq models need to process variable-length inputs into fixed-length representations To deal with seq2seq problems, we learned end-to-end differentiable system using an encoder-decoder architecture. Instead of entirely new ways of looking at problems, we’re trying to find the best building blocks to plug into our models and enable broad progress. Issues with recurrent models: Linear interaction distance RNNs are unrolled “left-to-right”: This encodes linear locality: a useful heuristic Nearby words often affect each other’s meanings Problem: RNNs take O(sequence length) steps for distant word pairs to interact. Hard to learn long-distance dependencies (because gradient problems) Linear order of words is “baked in”; linear order isn’t the right way to think about sentences Issues with recurrent models: Lack of parallelizability Forward and backward passes have O(sequence length) unparallelizable operations Future RNN hidden states can’t be computed in full before past RNN hidden states have been computed; not GPU friendly, inhibits training on very large datasets. Alternatives: Word windows Word window models aggregate local contexts (Also known as 1D convolution) Number of unparallelizable operations does not increase sequence length (O(1) dependence in time) What about in long-distance dependencies? Stacking word window layers allows interaction between farther words Maximum Interaction distance = sequence length / window size But if your sequences are too long, you’ll just ignore long-distance context Alternatives: Attention Attention treats each word’s representation as a query to access and incorporate information from a set of values. Out of the encoder-decoder structure; think about attention within a single sentence. Number of unparallelizable operations does not increase sequence length; not parallelizable in depth but parallelizable in time. Maximum interaction distance: O(1), since all words interact at every layer Self-Attention Recall: Attention operates on queries, keys, and values. queries: $q_1, q_2, \ldots, q_T \in \mathbb{R}^d$ keys: $k_1, k_2, \ldots, k_T \in \mathbb{R}d$ values: $v_1, v_2, \ldots, v_T \in \mathbb{R}d$ In self-attention, the queries, keys, and values are drawn from the same source. For example, if the output of the previous layer is $x_1, \ldots, x_T$ (one vec per word), we could let $v_i = k_i = q_i = x_i$ (use the same vectors for all of them). The (dot product) self-attention operation is as follows: Compute key-query affinities: $e_{ij} = q_i^T k_j $ Compute attention weights from affinities(softmax): \(\begin{align*} \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{j^\prime} \exp(e_{ij^\prime})} \end{align*}\) Compute outputs as weighted sum of values \(\begin{align*} \text{output}_i = \sum_j \alpha_{ij}v_j \end{align*}\) Q: FCN vs Self-Attention? Self-attention as an NLP building block Can self-attention replace the recurrence? NO. Sequence order Self-attention is an operation on sets; it has no inherent notion of order. To fix this, encode the order of the sentence. Consider representing each sequence index as a vector; $p_i \in \mathbb{R}^d$, for \(i \in \left\{ 1,2,\ldots, T \right\}\) are position vectors Let $\tilde{v}_i, \tilde{k}_i, \tilde{q}_i$ be our old inputs, then add $p_i$ to inputs; $v_i = \tilde{v}_i + p_i$ $q_i = \tilde{q}_i + p_i$ $k_i = \tilde{k}_i + p_i$ In deep self-attention networks, we do this at the first layer; you could concatenate them as well, but mostly, just add. Position representation vectors through sinusoids Sinusoidal position representations: concatenate sinusoidal functions of varying periods Pros: Periodicity indicates that maybe “absolute position” isn’t as important Maybe can extrapolate to longer sequences as periods restart! Cons: Not learnable; also the extrapolation doesn’t really work! Position representation vectors learned from scratch Learned absolute position representations: Let a matrix $p \in \mathbb{R}^{d \times T}$ and each learnable parameters $p_i$ be a column of that matrix; Most systems use this! Pros: Flexibility: each position gets to be learned to fit the data Cons: Definitely can’t extrapolate to indices outside $1, \ldots, T$. Some more flexible representations of position: Relative linear position attention (Shaw et al., 2018) Dependency syntax-based position (Wang et al., 2019) Nonlinearities There are no elementwise nonlinearities in self-attention; stacking more self-attention layers just re-averages value vectors. Easy fix: add a feed-forward network to post-process each output vector. (Self-Att. - FF - Self-Att. - FF - ...) \(\begin{align*} m_i &amp;amp;= MLP(\text{output}_i) \\ &amp;amp;= W_2 \ast \text{ReLU}(W_1 \times \text{output}_i + b_1) + b_2 \end{align*}\) Masking the future To use self-attention in decoders, we need to ensure we don’t “look at the future” when predicting a sequence. Easily: at every timestep, we could change the set of keys and queries to include only past words. But it’s inefficient dealing with tensors, not parallelizable. Instead, to enable parallelization, mask out attention to future words by setting attention scores to $-\infty$ (attention weights to 0). $e_{ij} = \begin{cases} q_i^T k_j, &amp;amp; j &amp;lt; i -\infty, &amp;amp; j \ge i \end{cases}$ Recap: Necessities for a self-attention building block Self-attention: the basis of the method Position representations: Specify the sequence order, since self-attention is an unordered function of its inputs. Nonlinearities: At the output of the self-attention block, frequently implemented as a simple feed-forward network. Masking: In order to parallelize operations while not looking at the future, keeps information about the future from “leaking” to the past. Transformer The Transformer Encoder-Decodr (Vaswani et al., 2017) At a high level look; What’s left in a Transformer Encoder Block: Key-query-value attention: How do we get input vectors from a single word embedding? __Multi-headed attention: Attend to multiple places in a single layer Tricks to help with training: Residual connections Layer normalization Scaling the dot product These tricks don’t improve what the model is able to do; they help improve the training process. Both of these types of modeling improvements are very important. The Transformer Encoder: Key-Query-Value Attention Let $x_1, \ldots, x_T \in \mathbb{R}^d$ be input vectors to the Transformer encoder. Then keys, queries, values are: $k_i = Kx_i$, where $K \in \mathbb{R}^{d \times d}$ is the key matrix. $q_i = Qx_i$, where $Q \in \mathbb{R}^{d \times d}$ is the query matrix. $v_i = Vx_i$, where $V \in \mathbb{R}^{d \times d}$ is the value matrix. These matrices (of learnable parameters) allow different aspects of the $x$ vectors to be used/emphasized in each of the three roles. Computed in matrices, Let $X = \left[ x_1; \ldots; x_T \right] \in \mathbb{R}^{T\times d}$ be the concatenation of input vectors. First, note that $XK \in \mathbb{R}^{T\times d}$, $XQ \in \mathbb{R}^{T\times d}$, $XV \in \mathbb{R}^{T\times d}$. The output tensor is defined as $\text{output} = \text{softmax}(XQ(XK)^T) \times XV$. Take the query-key dot products in one matrix multiplication: $XQ(XK)^T = XQK^T X^T \in \mathbb{T}^{T\times T} $ (All pairs of attention scores) Take softmax, and compute the weighted average with another matrix multiplication: $\text{output} \in \mathbb{T}^{T\times d}$ The Transformer Encoder: Multi-headed attention What if we want to look in multiple places in the sentence at once? For word $i$, self-attention “looks” where $x_i^T Q^T K x_j$ is high, but maybe we want to focus on different $j$ for different reasons? Define multiple attention “heads” through multiple $Q, K, V$ matrices: Let $Q_l, K_l, V_l \in \mathbb{R}^{d\times \frac{d}{h}}$, where $h$ is the number of attention heads, and $l$ ranges from $1$ to $h$. Each attention head performs attention independently: $\text{output}_l = \text{softmax}(XQ_l K_l^T X^T) \ast X V_l \in \mathbb{R}^{d/h}$ Then combine the all outputs from the heads: $\text{output} = Y\left[ \text{output}_1; \ldots, \text{output}_h \right]$, where $Y \in \mathbb{R}^{d\times d}$. Each head gets to “look” at different things and construct value vectors differently. The Transformer Encoder: Residual connections [He et al., 2016] Residual connections are a trick to help models train better. Instead of $X^{(i)} = \text{Layer}(X^{(i-1)})$ (where $i$ represents the layer) We let $X^{(i)} = X^{(i-1)} + \text{Layer}(X^{(i-1)})$ (so we only have to learn “the residual” from the previous layer) Solves vanishing gradient problem Residual connections are thought to make the loss landscape considerably smoother (thus easier training!) The Transformer Encoder: Layer normalization [Ba et al., 2016] Layer normalization is a trick to help models train faster. Idea: cut down on uninformative variation in hidden vector values by normalizing to unit mean and standard deviation within each layer. LayerNorm’s success may be due to its normalizing gradients (Xu et al., 2019) Let $x\in \mathbb{R}^d$ be an individual (word) vector in the model. Let the mean $\mu = \sum_{j=1}^d x_j \in \mathbb{R}$ Let the standard deviation $\sigma = \sqrt{\frac{1}{d}\sum_{j=1}^d (x_j-\mu)^2} \in \mathbb{R}$ Let $\gamma \in \mathbb{R}^d$ and $\beta \in \mathbb{R}$ be learned “gain” and “bias” parameters (Can omit) Then layer normalization computes: \(\begin{align*}\text{output} = \frac{x-\mu}{\sigma + \epsilon}\ast\gamma + \beta \end{align*}\); (Normalize by scalar mean and variance, and modulate by learned elementwise gain and bias) The Transformer Encoder: Scaled Dot Product [Vaswani et al., 2017] When dimensionality $d$ becomes large, dot products between vectors tend to become large. Because of this, inputs to the softmax function can be large, making the gradients small (leading to saturate region) Instead of the self-attention functiopn we’ve seen: $\text{output}_l = \text{softmax}(XQ_l K_l^T X^T) \ast X V_l$ We divide the attention scores by $\sqrt{d/h}$, to stop the scores from becoming large just as a function of $d/h$ (The dimensionality divided by the number of heads.): $\text{output}_l = \text{softmax}(\frac{XQ_l K_l^T X^T}{\sqrt{d/h}}) \ast X V_l$ Now look at the Decoder Blocks The Transformer Decoder: Cross-attention (details) Let $h_1, \ldots, h_T$ be output vecotrs from the Transformer encoder (the last block); $x_i \in \mathbb{R}^d$ Let $z_1, \ldots, z_T$ be input vectors from the Transformer decoder, $z_i\in \mathbb{R}^d$ Then keys and values are drawn from the encoder (like a memory); $k_i = Kh_i, v_i = Vh_i$ The queries are drawn from the decoder; $q_i = Qz_i$ In matrices: Let $H = \left[ h_1; \ldots, h_T \right] \in \mathbb{R}^{T\times d}$ be the concatenation of encoder vecotrs. Let $Z = \left[ z_1; \ldots, z_T \right] \in \mathbb{R}^{T\times d}$ be the concatenation of decoder vectors. The output is defined as $\text{output} = \text{softmax}(ZQ(HK)^T)\times HV$. What would we like to fix about the Transformer? Quadratic compute in self-attention: Computing all pairs of interactions means our computation grows quadratically with the sequence length. For recurrent models, it only grew linearly. Position representations: Are simple absolute indices the best we can do to represent position? Relative linear position attention (Shaw et al., 2018) Dependency syntax-based position (Wang et al., 2019) Quadratic computation as a function of sequence length One of the benefits of self-attention over recurrence was that it’s highly parallelizable. However, its total number of operations grows as $O(T^2 d)$, where $T$ is the sequence length, and $d$ is the dimensionality. Think of $d$ as around 1,000. So, for a single (shortish) sentence, $T \le 30; T^2 \le$ 900. In practice, we set a bound like $T = 512$. But what if we’d like $T\ge 10,000$? to work on long documents? Recent work on improving on quadratic self-attention cost Considerable recent work has gone into the question, Can we build models like Transformers without paying the $O(T^2)$ all-pairs self-attention cost? Linformer (Wang et al., 2020) Key idea: map the sequence length dimension to a lower-dimensional space for values, keys. BigBird (Zaheer et al., 2021) Key idea: replace all-pairs interactions with a family of other interactions, like local windows, looking at everything, and random interactions.</summary></entry><entry><title type="html">cs224n - Lecture 7. Translation, Seq2Seq, Attention</title><link href="http://0.0.0.0:4000/cs224n_lec7" rel="alternate" type="text/html" title="cs224n - Lecture 7. Translation, Seq2Seq, Attention" /><published>2022-03-23T00:00:00+00:00</published><updated>2022-03-23T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec7</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec7">&lt;h2 id=&quot;new-task-machine-translation&quot;&gt;New Task: Machine Translation&lt;/h2&gt;

&lt;h2 id=&quot;pre-neural-machine-translation&quot;&gt;Pre-Neural Machine Translation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Machine Translation (MT)&lt;/strong&gt; is the task of translating a sentence &lt;em&gt;x&lt;/em&gt; from one language (the &lt;strong&gt;source language&lt;/strong&gt;) to a sentence &lt;em&gt;y&lt;/em&gt; in another language (the &lt;strong&gt;target language&lt;/strong&gt;).&lt;/p&gt;

&lt;h3 id=&quot;1990s-2010s-statistical-machine-translation&quot;&gt;1990s-2010s: Statistical Machine Translation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Core idea: Learn a &lt;strong&gt;probabilistic model&lt;/strong&gt; from &lt;strong&gt;data&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Find best translated sentence &lt;em&gt;y&lt;/em&gt;, given sentence &lt;em&gt;x&lt;/em&gt;&lt;br /&gt;
  $\text{argmax}_y P(y|x)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Use &lt;strong&gt;Bayes Rule&lt;/strong&gt; to break this down into &lt;strong&gt;two components&lt;/strong&gt; to be learned separately:&lt;br /&gt;
  $= \text{argmax}_y P(x|y)P(y)$
    &lt;ul&gt;
      &lt;li&gt;$P(x\vert y)$: &lt;strong&gt;Translation model&lt;/strong&gt; that models how words and phrases should be translated (&lt;em&gt;fidelity&lt;/em&gt;); Learnt from parallel data.&lt;/li&gt;
      &lt;li&gt;$P(y)$: &lt;strong&gt;Language Model&lt;/strong&gt; that models how to write good English (&lt;em&gt;fluency&lt;/em&gt;). Learnt from monolingual data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Question: How to learn translation model $P(x\vert y)$?
    &lt;ul&gt;
      &lt;li&gt;First, need large amount of &lt;strong&gt;parallel data&lt;/strong&gt;&lt;br /&gt;
  (e.g., pairs of human-translated French/English sentences)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;learning-alignment-for-smt&quot;&gt;Learning alignment for SMT&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Question: How to learn translation model $P(x\vert y)$ from the parallel corpus?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Break it down further: Introduce latent &lt;em&gt;a&lt;/em&gt; variable into the model:&lt;br /&gt;
  $P(x, a|y)$&lt;br /&gt;
  where &lt;em&gt;a&lt;/em&gt; is the &lt;strong&gt;alignment&lt;/strong&gt;, i.e. word-level correspondence between source sentence &lt;em&gt;x&lt;/em&gt; and target sentence &lt;em&gt;y&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec7_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Alignment is the &lt;strong&gt;correspondence between particular words&lt;/strong&gt; in the translated sentence pair, capturing the grammatical differences between languages. &lt;strong&gt;Typological differences&lt;/strong&gt; lead to complicated alignments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Alignment is complex:&lt;br /&gt;
  Some words have &lt;strong&gt;no counterpart&lt;/strong&gt;&lt;br /&gt;
  Alignment can be &lt;strong&gt;many-to-one&lt;/strong&gt;, &lt;strong&gt;one-to-many&lt;/strong&gt;, or &lt;strong&gt;many-to-many&lt;/strong&gt;(phrase-level)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;We learn $P(x, a\vert y)$ as a combination of many factors, including:
    &lt;ul&gt;
      &lt;li&gt;Probability of particular words aligning (also depends on position in sent)&lt;/li&gt;
      &lt;li&gt;Probability of particular words having a particular fertility (number of corresponding words)&lt;/li&gt;
      &lt;li&gt;etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Alignments &lt;em&gt;a&lt;/em&gt; are &lt;strong&gt;latent variables&lt;/strong&gt;: They aren’t explicitly specified in the data
    &lt;ul&gt;
      &lt;li&gt;Require the use of special learning algorithms (like Expectation-Maximization) for learning the parameters of distributions with latent variables&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;decoding-for-smt&quot;&gt;Decoding for SMT&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;How to compute $\text{argmax}_y$?&lt;/li&gt;
  &lt;li&gt;Translation Model $P(x\vert y)$&lt;/li&gt;
  &lt;li&gt;Language Model $P(y)$&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Enumerating every possible &lt;em&gt;y&lt;/em&gt; and calculate the probability is too expensive;&lt;br /&gt;
  $\rightarrow$ Impose strong &lt;strong&gt;independence assumptions&lt;/strong&gt; in model, use dynamic programming for globally optimal solutions (e.g. Viterbi algorithm). This process is called &lt;strong&gt;&lt;em&gt;decoding&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec7_1.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;about-smt&quot;&gt;about SMT&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;SMT was a huge research field&lt;/li&gt;
  &lt;li&gt;The best systems were extremely complex
    &lt;ul&gt;
      &lt;li&gt;Hundreds of important details we haven’t mentioned here&lt;/li&gt;
      &lt;li&gt;Systems had many separately-designed subcomponents&lt;/li&gt;
      &lt;li&gt;Lots of feature engineering; need to design features to capture particular language phenomena&lt;/li&gt;
      &lt;li&gt;Require compiling and maintaining extra resources (like tables of equivalent phrases)&lt;/li&gt;
      &lt;li&gt;Lots of human effort to maintain; repeated effort for each language pair&lt;/li&gt;
      &lt;li&gt;Fairly successful, &lt;em&gt;Google Translate&lt;/em&gt; launched in mid 2000s.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;neural-machine-translation-2014&quot;&gt;Neural Machine Translation (2014~)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Neural Machine Translation (NMT)&lt;/strong&gt; is a way to do Machine Translation with a &lt;em&gt;single end-to-end neural network&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The neural network architecture is called a &lt;strong&gt;sequence-to-sequence&lt;/strong&gt; model (aka &lt;strong&gt;seq2seq&lt;/strong&gt;) and it involves &lt;strong&gt;two RNNs&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sequence-to-sequence-model&quot;&gt;Sequence-to-sequence Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec7_2.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
(Note: This diagram shows &lt;strong&gt;test time&lt;/strong&gt; behavior: decoder output is fed in as next step’s input)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sequence-to-sequence is useful for &lt;strong&gt;more than just MT&lt;/strong&gt;&lt;br /&gt;
  Many NLP tasks can be phrased as sequence-to-sequence:
    &lt;ul&gt;
      &lt;li&gt;Summarization (long text $\rightarrow$ short text)&lt;/li&gt;
      &lt;li&gt;Dialogue (previous utterances $\rightarrow$ next utterance)&lt;/li&gt;
      &lt;li&gt;Parsing (input text $\rightarrow$ output parse as sequence)&lt;/li&gt;
      &lt;li&gt;Code generation (natural language $\rightarrow$ Python code)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;An example of a &lt;strong&gt;Conditional Language Model&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Language Model&lt;/strong&gt; because the decoder is predicting the next word of the target sentence &lt;em&gt;y&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Conditional&lt;/strong&gt; because its prediction are &lt;em&gt;also&lt;/em&gt; conditioned on the source sentence &lt;em&gt;x&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;NMT directly calculates $P(y\vert x)$:&lt;br /&gt;
  $P(y|x) = P(y_1|x)P(y_2|y_1,x)P(y_3|y_1,y_2,x)\ldots P(y_T|y_1,\ldots, y_{T-1}, x)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Question: How to &lt;strong&gt;train&lt;/strong&gt; a NMT system?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;training-a-neural-machine-translation-system&quot;&gt;Training a Neural Machine Translation system&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec7_3.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;End-to-End: update all of the parameters of the both encoder and decoder.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multi-layer-rnns&quot;&gt;Multi-layer RNNs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RNNs are already “deep” on one dimension (they unroll horizontally over many timesteps), but shallow; a single layer of recurrent structure about the sentences.&lt;/li&gt;
  &lt;li&gt;We can also make them “deep” in another dimension by &lt;strong&gt;applying multiple RNNs&lt;/strong&gt; -  a multi-layer RNN(or &lt;strong&gt;stacked RNNs&lt;/strong&gt;). This allows the network to compute &lt;strong&gt;more complex representations&lt;/strong&gt;; The lower RNNs should compute lower-level features and the higher RNNs should compute higher-level features.&lt;/li&gt;
  &lt;li&gt;It’s not same: four LSTMs with a hidden state 500 each are more powerful than one 2000 layer LSTM (though we have the same number of parameters roughly).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multi-layer-deep-encoder-decoder-machine-translation-net&quot;&gt;Multi-layer deep encoder-decoder machine translation net&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec7_4.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;multi-layer-rnns-in-practice&quot;&gt;Multi-layer RNNs in practice&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High-performing RNNs are usually multi-layer&lt;/strong&gt; (but aren’t as deep as convolutional or feed-forward networks)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;For example: In a 2017 paper, (&lt;em&gt;“Massive Exploration of Neural Machine Translation Architecutres”, Britz et al&lt;/em&gt;.) find that for Neural Machine Translation, &lt;strong&gt;2 to 4 layers&lt;/strong&gt; is best for the encoder RNN, and &lt;strong&gt;4 layers&lt;/strong&gt; is best for the decoder RNN
    &lt;ul&gt;
      &lt;li&gt;Often 2 layers is a lot better than 1, and 3 might be a little better than 2 (rules of thumb)&lt;/li&gt;
      &lt;li&gt;Usually, &lt;strong&gt;skip-connections/dense-connections&lt;/strong&gt; are needed to train deeper RNNs (e.g., 8 layers)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transformer&lt;/strong&gt;-based networks (e.g., BERT) are usually deeper, like &lt;strong&gt;12 or 24 layers&lt;/strong&gt; having a lot of skipping-like connections.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;greedy-decoding&quot;&gt;Greedy Decoding&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Greedy decoding&lt;/strong&gt;(take most probable word on each step) is what we saw so far; taking argmax on each step of the decoder
    &lt;ul&gt;
      &lt;li&gt;Problem: has no way to undo decisions&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;exhaustive-search-decoding&quot;&gt;Exhaustive search decoding&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ideally, we want to find a (length &lt;em&gt;T&lt;/em&gt;) translation &lt;em&gt;y&lt;/em&gt; that maximizes&lt;br /&gt;
  \(\begin{align*}
  P(y|x) &amp;amp;= P(y_1|x)P(y_2|y_1,x)P(y_3|y_1,y_2,x)\ldots P(y_T|y_1,\ldots, y_{T-1}, x) \\
  &amp;amp;= \prod_{t=1}^T P(y_t|y_1,\ldots,y_{t-1},x)
  \end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We could try computing &lt;strong&gt;all possible sequences&lt;/strong&gt; &lt;em&gt;y&lt;/em&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;This means that on each step &lt;em&gt;t&lt;/em&gt; of the decoder, we’re tracking $V^t$ possible partial translations, where &lt;em&gt;V&lt;/em&gt; is vocab size&lt;/li&gt;
      &lt;li&gt;This $O(V^T)$ complexity is &lt;strong&gt;far too expensive&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;beam-search-decoding&quot;&gt;Beam search decoding&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Core idea: On each step of decoder, keep track of the &lt;em&gt;k most probable&lt;/em&gt; partial translations (which we call &lt;strong&gt;&lt;em&gt;hypotheses&lt;/em&gt;&lt;/strong&gt;)
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;k&lt;/em&gt; is the &lt;strong&gt;beam size&lt;/strong&gt; (in practice around 5 to 10)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A hypothesis $y_1, \ldots, y_t$ has a &lt;strong&gt;score&lt;/strong&gt; which is its log probability:&lt;br /&gt;
  $\text{score}(y_1, \ldots, y_t) = \log P_{LM}(y_1, \ldots, y_t|x) = \sum_{i=1}^t \log P_{LM}(y_i|y_1, \ldots, y_{i-1},x)$
    &lt;ul&gt;
      &lt;li&gt;Scores are all negative, and higher is better&lt;/li&gt;
      &lt;li&gt;Search for high-scoring hypotheses, tracking top &lt;em&gt;k&lt;/em&gt; on each step&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Not guaranteed&lt;/strong&gt; to find optimal solution, but much more efficient&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Stopping criterion
    &lt;ul&gt;
      &lt;li&gt;In &lt;strong&gt;greedy decoding&lt;/strong&gt;, usually we decode until the model produces an &lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;END&amp;gt;&lt;/code&gt; token&lt;/strong&gt; (“&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;START&amp;gt;&lt;/code&gt; &lt;em&gt;he hit me with a pie&lt;/em&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;END&amp;gt;&lt;/code&gt;”)&lt;/li&gt;
      &lt;li&gt;In &lt;strong&gt;beam search decoding&lt;/strong&gt;, different hypotheses may produce &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;END&amp;gt;&lt;/code&gt; tokens on &lt;strong&gt;different timesteps&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;When a hypothesis produces &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;END&amp;gt;&lt;/code&gt;, that hypothesis is &lt;strong&gt;complete&lt;/strong&gt;.&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Place it aside&lt;/strong&gt; and continue exploring other hypotheses via beam search.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Usually continue beam search until:
        &lt;ul&gt;
          &lt;li&gt;We reach timestep &lt;em&gt;T&lt;/em&gt; or we have at least &lt;em&gt;n&lt;/em&gt; completed hypotheses (where &lt;em&gt;T&lt;/em&gt; and &lt;em&gt;n&lt;/em&gt; are some pre-defined cutoff)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Finishing up
    &lt;ul&gt;
      &lt;li&gt;We have our list of completed hypotheses. Then how to select top one with highest score?&lt;/li&gt;
      &lt;li&gt;Each hypothesis $y_1,\ldots, y_t$ on our list has a score:&lt;br /&gt;
  \(\text{score}(y_1, \ldots, y_t) = \log P_{LM}(y_1, \ldots, y_t|x) = \sum_{i=1}^t \log P_{LM}(y_i|y_1, \ldots, y_{i-1},x)\)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Problem: longer hypotheses have lower scores&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Fix: Normalize by length.&lt;br /&gt;
  \(\frac{1}{t}\sum_{i=1}^t \log P_{LM}(y_i|y_1, \ldots, y_{i-1},x)\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;advantages-of-nmt&quot;&gt;Advantages of NMT&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Better &lt;strong&gt;performance&lt;/strong&gt;:&lt;br /&gt;
  More &lt;strong&gt;fluent&lt;/strong&gt;&lt;br /&gt;
  Better use of &lt;strong&gt;context&lt;/strong&gt;&lt;br /&gt;
  Better use of &lt;strong&gt;phrase similarities&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;strong&gt;single neural network&lt;/strong&gt; to be optimized end-to-end&lt;br /&gt;
  No subcomponents to be individually optimized&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Requires much less &lt;strong&gt;human engineering effort&lt;/strong&gt;&lt;br /&gt;
  No feature engineering&lt;br /&gt;
  Same method for all language pairs&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;disadvantages-of-nmt&quot;&gt;Disadvantages of NMT&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Less interpretable&lt;/strong&gt;&lt;br /&gt;
  Hard to debug&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Difficult to control&lt;/strong&gt;&lt;br /&gt;
  For example, can’t easily specify rules or guidelines for translation&lt;br /&gt;
  Safety concerns&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-do-we-evaluate-machine-translation&quot;&gt;How do we evaluate Machine Translation?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;BLEU&lt;/strong&gt; (Bilingual Evaluation Understudy)&lt;br /&gt;
  Compares the machine-written translation to one or several human-written translation(s), and computes a &lt;strong&gt;similarity score&lt;/strong&gt; based on:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;n&lt;/em&gt;-gram precision (usually 1 to 4)&lt;/li&gt;
      &lt;li&gt;Plus a penalty for too-short system translations&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Useful but imperfect&lt;br /&gt;
  There are many valid ways to translate a sentence&lt;br /&gt;
  So a good translation can get a poor BLEU score because it has low &lt;em&gt;n&lt;/em&gt;-gram overlap with the human translation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mt-progress-over-time&quot;&gt;MT progress over time&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec7_5.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NMT: perhaps the biggest success story of NLP Deep Learning?&lt;br /&gt;
  Neural Machine Translation went from a &lt;strong&gt;fringe research attempt&lt;/strong&gt; in 2014 to the &lt;strong&gt;leading standard method&lt;/strong&gt; in 2016
    &lt;ul&gt;
      &lt;li&gt;2014: First seq2seq paper published&lt;/li&gt;
      &lt;li&gt;2016: Google Translate switches from SMT to NMT – and by 2018 everyone has&lt;/li&gt;
      &lt;li&gt;SMT systems, built by hundreds of engineers over many years, outperformed by NMT systems trained by a &lt;strong&gt;small group&lt;/strong&gt; of engineers in a few &lt;strong&gt;months&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Many difficulties remain:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Out-of-vocabulary&lt;/strong&gt; words&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Domain mismatch&lt;/strong&gt; between train and test data&lt;/li&gt;
      &lt;li&gt;Maintaining &lt;strong&gt;context&lt;/strong&gt; over longer text&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Low-resource&lt;/strong&gt; language pairs&lt;/li&gt;
      &lt;li&gt;Failures to accurately capture &lt;strong&gt;sentence meaning&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Pronoun&lt;/strong&gt; (or &lt;strong&gt;zero pronoun&lt;/strong&gt;) &lt;strong&gt;resolution&lt;/strong&gt; errors&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Morphological agreement&lt;/strong&gt; errors&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Bias&lt;/strong&gt; in training data&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;attention&quot;&gt;ATTENTION&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec7_6.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Information bottlenect: the last hidden state of encoder RNN needs to capture &lt;em&gt;all information&lt;/em&gt; about the source sentence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Core idea: on each step of the decoder, &lt;strong&gt;use direct connection to the encoder&lt;/strong&gt; to &lt;strong&gt;focus on a particular part&lt;/strong&gt; of the source sequence&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Each step, dot products a decoder hidden state to all encoder hidden states and get &lt;em&gt;Attention scores&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Take softmax to turn scores into &lt;em&gt;Attention distribution&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Using this distribution, take a &lt;strong&gt;weighted sum&lt;/strong&gt; of the encoder hidden states to calculate &lt;em&gt;Attention output&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Concatenate with decoder hidden state, predict $\hat{y_i}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec7_7.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">New Task: Machine Translation Pre-Neural Machine Translation Machine Translation (MT) is the task of translating a sentence x from one language (the source language) to a sentence y in another language (the target language). 1990s-2010s: Statistical Machine Translation Core idea: Learn a probabilistic model from data Find best translated sentence y, given sentence x $\text{argmax}_y P(y|x)$ Use Bayes Rule to break this down into two components to be learned separately: $= \text{argmax}_y P(x|y)P(y)$ $P(x\vert y)$: Translation model that models how words and phrases should be translated (fidelity); Learnt from parallel data. $P(y)$: Language Model that models how to write good English (fluency). Learnt from monolingual data. Question: How to learn translation model $P(x\vert y)$? First, need large amount of parallel data (e.g., pairs of human-translated French/English sentences) Learning alignment for SMT Question: How to learn translation model $P(x\vert y)$ from the parallel corpus? Break it down further: Introduce latent a variable into the model: $P(x, a|y)$ where a is the alignment, i.e. word-level correspondence between source sentence x and target sentence y Alignment is the correspondence between particular words in the translated sentence pair, capturing the grammatical differences between languages. Typological differences lead to complicated alignments. Alignment is complex: Some words have no counterpart Alignment can be many-to-one, one-to-many, or many-to-many(phrase-level) We learn $P(x, a\vert y)$ as a combination of many factors, including: Probability of particular words aligning (also depends on position in sent) Probability of particular words having a particular fertility (number of corresponding words) etc. Alignments a are latent variables: They aren’t explicitly specified in the data Require the use of special learning algorithms (like Expectation-Maximization) for learning the parameters of distributions with latent variables Decoding for SMT How to compute $\text{argmax}_y$? Translation Model $P(x\vert y)$ Language Model $P(y)$ Enumerating every possible y and calculate the probability is too expensive; $\rightarrow$ Impose strong independence assumptions in model, use dynamic programming for globally optimal solutions (e.g. Viterbi algorithm). This process is called decoding. about SMT SMT was a huge research field The best systems were extremely complex Hundreds of important details we haven’t mentioned here Systems had many separately-designed subcomponents Lots of feature engineering; need to design features to capture particular language phenomena Require compiling and maintaining extra resources (like tables of equivalent phrases) Lots of human effort to maintain; repeated effort for each language pair Fairly successful, Google Translate launched in mid 2000s. Neural Machine Translation (2014~) Neural Machine Translation (NMT) is a way to do Machine Translation with a single end-to-end neural network The neural network architecture is called a sequence-to-sequence model (aka seq2seq) and it involves two RNNs Sequence-to-sequence Model (Note: This diagram shows test time behavior: decoder output is fed in as next step’s input) Sequence-to-sequence is useful for more than just MT Many NLP tasks can be phrased as sequence-to-sequence: Summarization (long text $\rightarrow$ short text) Dialogue (previous utterances $\rightarrow$ next utterance) Parsing (input text $\rightarrow$ output parse as sequence) Code generation (natural language $\rightarrow$ Python code) An example of a Conditional Language Model Language Model because the decoder is predicting the next word of the target sentence y Conditional because its prediction are also conditioned on the source sentence x NMT directly calculates $P(y\vert x)$: $P(y|x) = P(y_1|x)P(y_2|y_1,x)P(y_3|y_1,y_2,x)\ldots P(y_T|y_1,\ldots, y_{T-1}, x)$ Question: How to train a NMT system? Training a Neural Machine Translation system End-to-End: update all of the parameters of the both encoder and decoder. Multi-layer RNNs RNNs are already “deep” on one dimension (they unroll horizontally over many timesteps), but shallow; a single layer of recurrent structure about the sentences. We can also make them “deep” in another dimension by applying multiple RNNs - a multi-layer RNN(or stacked RNNs). This allows the network to compute more complex representations; The lower RNNs should compute lower-level features and the higher RNNs should compute higher-level features. It’s not same: four LSTMs with a hidden state 500 each are more powerful than one 2000 layer LSTM (though we have the same number of parameters roughly). Multi-layer deep encoder-decoder machine translation net Multi-layer RNNs in practice High-performing RNNs are usually multi-layer (but aren’t as deep as convolutional or feed-forward networks) For example: In a 2017 paper, (“Massive Exploration of Neural Machine Translation Architecutres”, Britz et al.) find that for Neural Machine Translation, 2 to 4 layers is best for the encoder RNN, and 4 layers is best for the decoder RNN Often 2 layers is a lot better than 1, and 3 might be a little better than 2 (rules of thumb) Usually, skip-connections/dense-connections are needed to train deeper RNNs (e.g., 8 layers) Transformer-based networks (e.g., BERT) are usually deeper, like 12 or 24 layers having a lot of skipping-like connections. Greedy Decoding Greedy decoding(take most probable word on each step) is what we saw so far; taking argmax on each step of the decoder Problem: has no way to undo decisions Exhaustive search decoding Ideally, we want to find a (length T) translation y that maximizes \(\begin{align*} P(y|x) &amp;amp;= P(y_1|x)P(y_2|y_1,x)P(y_3|y_1,y_2,x)\ldots P(y_T|y_1,\ldots, y_{T-1}, x) \\ &amp;amp;= \prod_{t=1}^T P(y_t|y_1,\ldots,y_{t-1},x) \end{align*}\) We could try computing all possible sequences y This means that on each step t of the decoder, we’re tracking $V^t$ possible partial translations, where V is vocab size This $O(V^T)$ complexity is far too expensive Beam search decoding Core idea: On each step of decoder, keep track of the k most probable partial translations (which we call hypotheses) k is the beam size (in practice around 5 to 10) A hypothesis $y_1, \ldots, y_t$ has a score which is its log probability: $\text{score}(y_1, \ldots, y_t) = \log P_{LM}(y_1, \ldots, y_t|x) = \sum_{i=1}^t \log P_{LM}(y_i|y_1, \ldots, y_{i-1},x)$ Scores are all negative, and higher is better Search for high-scoring hypotheses, tracking top k on each step Not guaranteed to find optimal solution, but much more efficient Stopping criterion In greedy decoding, usually we decode until the model produces an &amp;lt;END&amp;gt; token (“&amp;lt;START&amp;gt; he hit me with a pie &amp;lt;END&amp;gt;”) In beam search decoding, different hypotheses may produce &amp;lt;END&amp;gt; tokens on different timesteps When a hypothesis produces &amp;lt;END&amp;gt;, that hypothesis is complete. Place it aside and continue exploring other hypotheses via beam search. Usually continue beam search until: We reach timestep T or we have at least n completed hypotheses (where T and n are some pre-defined cutoff) Finishing up We have our list of completed hypotheses. Then how to select top one with highest score? Each hypothesis $y_1,\ldots, y_t$ on our list has a score: \(\text{score}(y_1, \ldots, y_t) = \log P_{LM}(y_1, \ldots, y_t|x) = \sum_{i=1}^t \log P_{LM}(y_i|y_1, \ldots, y_{i-1},x)\) Problem: longer hypotheses have lower scores Fix: Normalize by length. \(\frac{1}{t}\sum_{i=1}^t \log P_{LM}(y_i|y_1, \ldots, y_{i-1},x)\) Advantages of NMT Better performance: More fluent Better use of context Better use of phrase similarities A single neural network to be optimized end-to-end No subcomponents to be individually optimized Requires much less human engineering effort No feature engineering Same method for all language pairs Disadvantages of NMT Less interpretable Hard to debug Difficult to control For example, can’t easily specify rules or guidelines for translation Safety concerns How do we evaluate Machine Translation? BLEU (Bilingual Evaluation Understudy) Compares the machine-written translation to one or several human-written translation(s), and computes a similarity score based on: n-gram precision (usually 1 to 4) Plus a penalty for too-short system translations Useful but imperfect There are many valid ways to translate a sentence So a good translation can get a poor BLEU score because it has low n-gram overlap with the human translation MT progress over time NMT: perhaps the biggest success story of NLP Deep Learning? Neural Machine Translation went from a fringe research attempt in 2014 to the leading standard method in 2016 2014: First seq2seq paper published 2016: Google Translate switches from SMT to NMT – and by 2018 everyone has SMT systems, built by hundreds of engineers over many years, outperformed by NMT systems trained by a small group of engineers in a few months Many difficulties remain: Out-of-vocabulary words Domain mismatch between train and test data Maintaining context over longer text Low-resource language pairs Failures to accurately capture sentence meaning Pronoun (or zero pronoun) resolution errors Morphological agreement errors Bias in training data ATTENTION Information bottlenect: the last hidden state of encoder RNN needs to capture all information about the source sentence. Core idea: on each step of the decoder, use direct connection to the encoder to focus on a particular part of the source sequence Each step, dot products a decoder hidden state to all encoder hidden states and get Attention scores Take softmax to turn scores into Attention distribution Using this distribution, take a weighted sum of the encoder hidden states to calculate Attention output Concatenate with decoder hidden state, predict $\hat{y_i}$</summary></entry><entry><title type="html">cs224n - Lecture 8. Attention (Cont.)</title><link href="http://0.0.0.0:4000/cs224n_lec8" rel="alternate" type="text/html" title="cs224n - Lecture 8. Attention (Cont.)" /><published>2022-03-23T00:00:00+00:00</published><updated>2022-03-23T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec8</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec8">&lt;h2 id=&quot;attention&quot;&gt;Attention&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec7_7.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Encoder hidden states&lt;/strong&gt; $\mathbf{h}_1, \ldots, \mathbf{h}_N \in \mathbb{R}^h$&lt;/li&gt;
  &lt;li&gt;On timestep $t$, we have &lt;strong&gt;Decoder hidden state&lt;/strong&gt; $\mathbf{s}_t \in \mathbb{R}^h$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Attention score&lt;/strong&gt; $\mathbf{e}^t$ for this step:&lt;br /&gt;
  \(\mathbf{e}^t = \left[ \mathbf{s}_t^T \mathbf{h}_1, \ldots, \mathbf{s}_t^T \mathbf{h}_N \right] \in \mathbb{R}^N\)&lt;/li&gt;
  &lt;li&gt;Take softmax to get the &lt;strong&gt;Attention distribution&lt;/strong&gt;:&lt;br /&gt;
  \(\alpha^t = \text{softmax}(\mathbf{e}^t) \in \mathbb{R}^N\)&lt;/li&gt;
  &lt;li&gt;Use $\alpha^t$ to take a weighted sum of the encoder hidden states to get the &lt;strong&gt;Attention output&lt;/strong&gt;:&lt;br /&gt;
  \(\mathbf{a}_t = \sum_{i=1}^N \alpha_i^t \mathbf{h}_i \in \mathbb{R}^h\)&lt;/li&gt;
  &lt;li&gt;Finally, concatenate the attention output $\mathbf{a}_t$ with the decoder hidden state $s_t$ and proceed as in the non-attention seq2seq model&lt;br /&gt;
  \(\left[ \mathbf{a}_t ; \mathbf{s}_t \right] \in \mathbb{R}^{2h}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;attention-is-great&quot;&gt;Attention is great&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Attention significantly improves NMT performance&lt;br /&gt;
  Allow decoder to focus on certain parts of the source&lt;/li&gt;
  &lt;li&gt;Attention provides more “human-like” model of the MT process&lt;br /&gt;
  You can look back at the source sentence while translating, rather than needing to remember it all&lt;/li&gt;
  &lt;li&gt;Attention solves the &lt;strong&gt;bottleneck problem&lt;/strong&gt;&lt;br /&gt;
  Attention allows decoder to look directly at source; bypass bottleneck&lt;/li&gt;
  &lt;li&gt;Attention helps with the &lt;strong&gt;vanishing gradient problem&lt;/strong&gt;&lt;br /&gt;
  Provides shortcut to faraway states&lt;/li&gt;
  &lt;li&gt;Attention provides &lt;strong&gt;some interpretability&lt;/strong&gt;&lt;br /&gt;
  By inspecting attention distribution, we can see what the decoder was focusing on; we can get (soft) &lt;strong&gt;alignment&lt;/strong&gt; for free. Without explicitly trained an alignment system, the network just learned alignment by itself&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;attention-variants&quot;&gt;Attention variants&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;With some &lt;strong&gt;values&lt;/strong&gt; \(\mathbf{h}_1, \ldots, \mathbf{h}_N \in \mathbb{R}^{d_1}\) and a &lt;strong&gt;query&lt;/strong&gt; \(\mathbf{s} \in \mathbb{R}^{d_2}\),&lt;br /&gt;
  Attention always involves:&lt;br /&gt;
      1. Computing the &lt;strong&gt;attention scores&lt;/strong&gt; $\mathbf{e}\in\mathbb{R}^N$&lt;br /&gt;
      2. Taking softmax to get &lt;strong&gt;attention distribution&lt;/strong&gt;:&lt;br /&gt;
          $\alpha = \text{softmax}(\mathbf{e})\in\mathbb{R}^N$&lt;br /&gt;
      3. Take weighted sum of values to get &lt;strong&gt;attention output&lt;/strong&gt;:&lt;br /&gt;
          $\mathbf{a} = \sum_{i=1}^N \alpha_i \mathbf{h}_i \in \mathbb{R}^{d_1}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$\rightarrow$ There are multiple ways to compute &lt;em&gt;attention scores&lt;/em&gt; $\mathbf{e}\in \mathbb{R}^N$ from $\mathbf{h}_1, \ldots, \mathbf{h}_N \in \mathbb{R}^{d_1}$ and $\mathbf{s} \in \mathbb{R}^{d_2}$:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Basic dot-product attention: $\mathbf{e}_i = \mathbf{s}^T \mathbf{h}_i \in \mathbb{R}$
    &lt;ul&gt;
      &lt;li&gt;What we saw earlier, assume that $d_1 = d_2$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Multiplicative attention: $\mathbf{e}_i = \mathbf{s}^T \mathbf{W} \mathbf{h}_i \in \mathbb{R}$
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Luong, Pham, and Manning 2015&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Where $\mathbf{W} \in \mathbb{R}^{d_2 \times d_1}$ is a weight matrix of learnable parameters(but too many!)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reduced rank multiplicative attention: $e_i = \mathbf{s}^T(\mathbf{U}^T \mathbf{V})h_i = (\mathbf{U}s)^T (\mathbf{V}h_i)$
    &lt;ul&gt;
      &lt;li&gt;low rank matrices $\mathbf{U} \in \mathbb{R}^{k\times d_2}$, $\mathbf{V} \in \mathbb{R}^{k\times d_1}$, $k \ll d_1, d_2$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Additive attention: $\mathbf{e}_i = \mathbf{v}^T \tanh (\mathbf{W}_1 \mathbf{h}_i + \mathbf{W}_2 \mathbf{s}) \in \mathbb{R}$
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Bahdanau, Cho, and Bengio 2014&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Where  $\mathbf{W}_1 \in \mathbb{R}^{d_3 \times d_1}$, $\mathbf{W}_2 \in \mathbb{R}^{d_3 \times d_2}$ are weighted matrices and $\mathbf{v} \in \mathbb{R}^{d_3}$ is a weight vector.&lt;/li&gt;
      &lt;li&gt;$d_3$ (the attention dimensionality) is a hyperparameter&lt;/li&gt;
      &lt;li&gt;“&lt;em&gt;Additive&lt;/em&gt;” is a weird name; it’s really using a neural net layer.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;attention-is-a-general-deep-learning-technique&quot;&gt;Attention is a general Deep Learning technique&lt;/h3&gt;
&lt;p&gt;Attention is a great way to improve the sequence-to-sequence model for Machine Translation. However, you can use attention in many architectures (not just seq2seq) and many tasks (not just MT)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;More general definition of attention:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Given a set of vector &lt;strong&gt;values&lt;/strong&gt;, and a vector &lt;strong&gt;query&lt;/strong&gt;, &lt;strong&gt;attention&lt;/strong&gt; is a technique to compute a weighted sum of the values, dependent on the query.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We sometimes say that the &lt;em&gt;query attends to the values&lt;/em&gt;. A kind of memory access mechanism.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Intuition&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;The weighted sum is a &lt;strong&gt;selective summary&lt;/strong&gt; of the information contained in the values, where the query determines which values to focus on.&lt;/li&gt;
      &lt;li&gt;Attention is a way to obtain a &lt;strong&gt;fixed-size representation of an arbitrary set of representations&lt;/strong&gt; (the values), dependent on some other representation (the query).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Upshot&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Attention has become the powerful, flexible, general way pointer and memory manipulation in deep learning models. A new idea from after 2010!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">Attention Encoder hidden states $\mathbf{h}_1, \ldots, \mathbf{h}_N \in \mathbb{R}^h$ On timestep $t$, we have Decoder hidden state $\mathbf{s}_t \in \mathbb{R}^h$ Attention score $\mathbf{e}^t$ for this step: \(\mathbf{e}^t = \left[ \mathbf{s}_t^T \mathbf{h}_1, \ldots, \mathbf{s}_t^T \mathbf{h}_N \right] \in \mathbb{R}^N\) Take softmax to get the Attention distribution: \(\alpha^t = \text{softmax}(\mathbf{e}^t) \in \mathbb{R}^N\) Use $\alpha^t$ to take a weighted sum of the encoder hidden states to get the Attention output: \(\mathbf{a}_t = \sum_{i=1}^N \alpha_i^t \mathbf{h}_i \in \mathbb{R}^h\) Finally, concatenate the attention output $\mathbf{a}_t$ with the decoder hidden state $s_t$ and proceed as in the non-attention seq2seq model \(\left[ \mathbf{a}_t ; \mathbf{s}_t \right] \in \mathbb{R}^{2h}\) Attention is great Attention significantly improves NMT performance Allow decoder to focus on certain parts of the source Attention provides more “human-like” model of the MT process You can look back at the source sentence while translating, rather than needing to remember it all Attention solves the bottleneck problem Attention allows decoder to look directly at source; bypass bottleneck Attention helps with the vanishing gradient problem Provides shortcut to faraway states Attention provides some interpretability By inspecting attention distribution, we can see what the decoder was focusing on; we can get (soft) alignment for free. Without explicitly trained an alignment system, the network just learned alignment by itself Attention variants With some values \(\mathbf{h}_1, \ldots, \mathbf{h}_N \in \mathbb{R}^{d_1}\) and a query \(\mathbf{s} \in \mathbb{R}^{d_2}\), Attention always involves: 1. Computing the attention scores $\mathbf{e}\in\mathbb{R}^N$ 2. Taking softmax to get attention distribution: $\alpha = \text{softmax}(\mathbf{e})\in\mathbb{R}^N$ 3. Take weighted sum of values to get attention output: $\mathbf{a} = \sum_{i=1}^N \alpha_i \mathbf{h}_i \in \mathbb{R}^{d_1}$ $\rightarrow$ There are multiple ways to compute attention scores $\mathbf{e}\in \mathbb{R}^N$ from $\mathbf{h}_1, \ldots, \mathbf{h}_N \in \mathbb{R}^{d_1}$ and $\mathbf{s} \in \mathbb{R}^{d_2}$: Basic dot-product attention: $\mathbf{e}_i = \mathbf{s}^T \mathbf{h}_i \in \mathbb{R}$ What we saw earlier, assume that $d_1 = d_2$. Multiplicative attention: $\mathbf{e}_i = \mathbf{s}^T \mathbf{W} \mathbf{h}_i \in \mathbb{R}$ Luong, Pham, and Manning 2015 Where $\mathbf{W} \in \mathbb{R}^{d_2 \times d_1}$ is a weight matrix of learnable parameters(but too many!) Reduced rank multiplicative attention: $e_i = \mathbf{s}^T(\mathbf{U}^T \mathbf{V})h_i = (\mathbf{U}s)^T (\mathbf{V}h_i)$ low rank matrices $\mathbf{U} \in \mathbb{R}^{k\times d_2}$, $\mathbf{V} \in \mathbb{R}^{k\times d_1}$, $k \ll d_1, d_2$ Additive attention: $\mathbf{e}_i = \mathbf{v}^T \tanh (\mathbf{W}_1 \mathbf{h}_i + \mathbf{W}_2 \mathbf{s}) \in \mathbb{R}$ Bahdanau, Cho, and Bengio 2014 Where $\mathbf{W}_1 \in \mathbb{R}^{d_3 \times d_1}$, $\mathbf{W}_2 \in \mathbb{R}^{d_3 \times d_2}$ are weighted matrices and $\mathbf{v} \in \mathbb{R}^{d_3}$ is a weight vector. $d_3$ (the attention dimensionality) is a hyperparameter “Additive” is a weird name; it’s really using a neural net layer. Attention is a general Deep Learning technique Attention is a great way to improve the sequence-to-sequence model for Machine Translation. However, you can use attention in many architectures (not just seq2seq) and many tasks (not just MT) More general definition of attention: Given a set of vector values, and a vector query, attention is a technique to compute a weighted sum of the values, dependent on the query. We sometimes say that the query attends to the values. A kind of memory access mechanism. Intuition: The weighted sum is a selective summary of the information contained in the values, where the query determines which values to focus on. Attention is a way to obtain a fixed-size representation of an arbitrary set of representations (the values), dependent on some other representation (the query). Upshot: Attention has become the powerful, flexible, general way pointer and memory manipulation in deep learning models. A new idea from after 2010!</summary></entry><entry><title type="html">cs224n - Lecture 6. Simple and LSTM RNNs</title><link href="http://0.0.0.0:4000/cs224n_lec6" rel="alternate" type="text/html" title="cs224n - Lecture 6. Simple and LSTM RNNs" /><published>2022-03-20T00:00:00+00:00</published><updated>2022-03-20T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec6</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec6">&lt;h3 id=&quot;the-simple-rnn-language-model&quot;&gt;The Simple RNN Language Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec5_13.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-an-rnn-language-model&quot;&gt;Training an RNN Language Model&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Get a &lt;strong&gt;big corpus of text&lt;/strong&gt; which is a sequence of words  $x^{(1)}, \ldots, x^{(T)}$&lt;/li&gt;
  &lt;li&gt;Feed into RNN-LM; compute output distribution $\hat{y}^{(t)}$ &lt;strong&gt;for every step $t$&lt;/strong&gt;.&lt;br /&gt;
  i.e., predict probability distribution of &lt;em&gt;every word&lt;/em&gt;, given words so far&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Loss function&lt;/strong&gt; on step is &lt;strong&gt;cross-entropy&lt;/strong&gt;(negative log-likelihood) between predicted probability distribution $\hat{y}^{(t)}$, and the true next word $y^{(t)}$(one-hot for $x^{(t+1)}$):&lt;br /&gt;
  \(\begin{align*}
  J^{(t)}(\theta) = CE(y^{(t)}, \hat{y}^{(t)}) = -\sum_{w\in V} y_w^{(t)} \log \hat{y}_w^{(t)} = -\log \hat{y}_{x_{t+1}}^{(t)}
  \end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Average this to get &lt;strong&gt;overall loss&lt;/strong&gt; for entire training set:&lt;br /&gt;
  \(\begin{align*}
  J(\theta) = \frac{1}{T}\sum_{t=1}^T J^{(t)}(\theta) = \frac{1}{T}\sum_{t=1}^T -\log \hat{y}_{x_{t+1}}^{(t)}
  \end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;“Teacher forcing”&lt;/em&gt; algorithm:&lt;br /&gt;
  At each step, reset to what was actually in the corpus and &lt;strong&gt;not reuse&lt;/strong&gt; what the model have suggested.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However: Computing loss and gradients across &lt;strong&gt;entire corpus&lt;/strong&gt; $x^{(1)}, \ldots, x^{(T)}$ is &lt;strong&gt;too expensive&lt;/strong&gt;&lt;br /&gt;
  In pratice, consider $x^{(1)}, \ldots, x^{(T)}$ as a &lt;em&gt;sentence&lt;/em&gt; or a &lt;em&gt;document&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Recall: &lt;strong&gt;Stochastic Gradient Descent&lt;/strong&gt; allows us to compute loss and gradients for small chunk of data, and update.&lt;br /&gt;
  Compute loss $J(\theta)$ for a sentence (actually, a batch of sentences), compute gradients and update weights. Repeat.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;training-the-parameters-rnns-backpropagation-for-rnns&quot;&gt;Training the parameters RNNs: Backpropagation for RNNs&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec6_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Question&lt;/strong&gt;: What’s the derivative of $J^{(t)}(\theta)$ w.r.t. the &lt;strong&gt;repeated&lt;/strong&gt; weight matrix $W_h$?&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;: sum of the gradient w.r.t. each time it appears&lt;br /&gt;
  \(\begin{align*}
  \frac{\partial J^{(t)}}{\partial W_h} = \sum_{i=1}^t \left. \frac{\partial J^{(t)}}{\partial W_h} \right|_{(i)}
  \end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Backpropagation for RNNs: Proof sketch&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec6_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How to calculate:&lt;br /&gt;
  Backpropagate over timesteps $i=t,\ldots,0$, summing gradients as you go.&lt;br /&gt;
  &lt;em&gt;“Backpropagation through time, [Werbos, P.G., 1988, Neural Networks 1, and others]”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;generating-text-with-a-rnn-language-model&quot;&gt;Generating text with a RNN Language Model&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Like a n-gram Language Model, use an RNN Language Model to &lt;strong&gt;generate text&lt;/strong&gt; by &lt;strong&gt;repeated sampling&lt;/strong&gt;. Sampled output becomes next step’s input.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You can train an RNN-LM on any kind of text, then generate text in that style.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;evaluating-language-models&quot;&gt;Evaluating Language Models&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The standard &lt;strong&gt;evaluation metric&lt;/strong&gt; for Language Model is &lt;strong&gt;perlexity&lt;/strong&gt;; a &lt;em&gt;geometric mean of the inverse probabilities&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec6_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This is equal to &lt;strong&gt;the exponential&lt;/strong&gt; of the cross-entropy loss $J(\theta)$:&lt;br /&gt;
  \(\begin{align*}
  = \prod_{t=1}^T \left( \frac{1}{\hat{y}_{x_{t+1}}^{(t)}}\right)^{1/T} = \exp \left( \frac{1}{T}\sum_{t=1}^T -\log \hat{y}_{x_{t+1}}^{(t)} \right) = \exp(J(\theta))
  \end{align*}\)&lt;br /&gt;
  $\rightarrow$ Lower perplexity is better&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RNNs have greatly improved perplexity&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec6_3.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;why-should-we-care-about-language-modeling&quot;&gt;Why should we care about Language Modeling?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Language Modeling is a &lt;strong&gt;benchmark task&lt;/strong&gt; that helps us &lt;strong&gt;measure our progress&lt;/strong&gt; on understanding language&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Language Modeling is a &lt;strong&gt;subcomponent&lt;/strong&gt; of &lt;strong&gt;many&lt;/strong&gt; NLP tasks, especially those involving &lt;strong&gt;generating text&lt;/strong&gt; or &lt;strong&gt;estimating the probability of text&lt;/strong&gt;:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Predictive typing&lt;/li&gt;
      &lt;li&gt;Speech recognition&lt;/li&gt;
      &lt;li&gt;Handwriting recognition&lt;/li&gt;
      &lt;li&gt;Spelling/grammar correction&lt;/li&gt;
      &lt;li&gt;Authorship identification&lt;/li&gt;
      &lt;li&gt;Machine translation&lt;/li&gt;
      &lt;li&gt;Summarization&lt;/li&gt;
      &lt;li&gt;Dialogue&lt;/li&gt;
      &lt;li&gt;etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;recap&quot;&gt;Recap&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Language Model: A system that predicts the next word&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Recurrent Neural Network: A family of neural networks that:
    &lt;ul&gt;
      &lt;li&gt;Take sequential input of any length&lt;/li&gt;
      &lt;li&gt;Apply the same weights on each step&lt;/li&gt;
      &lt;li&gt;Can optionally produce output on each step&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Recurrent Neural Network $\ne$ Language Model&lt;br /&gt;
  shown that RNNs are a great way to build a LM, but RNNs are useful for much more&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-rnn-uses&quot;&gt;Other RNN uses&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Sequence tagging: e.g., part-of-speech tagging, named entity recognition&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec6_4.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Sentence classification: e.g., sentiment classification&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec6_5.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;
    &lt;ol&gt;
      &lt;li&gt;Basic way: 
 Use the final hidden state. After running RNN(or LSTM), the final hidden state was encoded the whole sentence and treat it as the whole meaning of the sentence. Then put an extra classifier layer.&lt;/li&gt;
      &lt;li&gt;Usually better:&lt;br /&gt;
 Take element-wise max or mean of all hidden states to more symmetrically encode the hidden state over each time step.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Language encoder module: e.g., question answering, machine translation, many other tasks&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec6_6.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Generate text: e.g., speech recognition, machine translation, summarization&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec6_7.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;problems-with-vanishing-and-exploding-gradients&quot;&gt;Problems with Vanishing and Exploding Gradients&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec6_8.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;vanishing-gradient-proof-sketch-linear-case&quot;&gt;Vanishing gradient: Proof sketch (linear case)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Recall:&lt;br /&gt;
  $\hat{h}^{(t)} = \sigma(W_h h^{(t-1)} + W_x x^{(t)} + b_1)$&lt;/li&gt;
  &lt;li&gt;What if $\sigma(x) = x$?&lt;br /&gt;
  \(\begin{align*}
  \frac{\partial h^{(t)}}{\partial h^{(t-1)}} 
  &amp;amp;= \text{diag} \left( \sigma^\prime \left( W_h h^{(t-1)} + W_x x^{(t)} + b_1 \right) \right) W_h &amp;amp;&amp;amp;(\text{chain rule}) \\
  &amp;amp;= I\ W_h = W_h
  \end{align*}\)&lt;/li&gt;
  &lt;li&gt;Consider the gradient of the loss $J^{(i)}(\theta)$ on step $i$, w.r.t. the hidden state $h^{(j)}$ one some previous step $j$. Let $l = i - j$&lt;br /&gt;
  \(\begin{align*}
  \frac{\partial J^{(i)}(\theta)}{\partial h^{(j)}}
  &amp;amp;= \frac{\partial J^{(i)}(\theta)}{\partial h^{(i)}} \prod_{j &amp;lt; t \le i}\frac{\partial h^{(t)}}{\partial h^{(t-1)}} &amp;amp;&amp;amp;(\text{chain rule}) \\
  &amp;amp;= \frac{\partial J^{(i)}(\theta)}{\partial h^{(i)}} \prod_{j &amp;lt; t \le i} W_h = \frac{\partial J^{(i)}(\theta)}{\partial h^{(i)}} W_h^l &amp;amp;&amp;amp;(\text{value of } \frac{\partial h^{(t)}}{\partial h^{(t-1)}})
  \end{align*}\)&lt;br /&gt;
  $\rightarrow$ If $W_h$ is “small”, then $W_h^l$ gets exponentially problematic as $l$ becomes large&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Consider if the eigenvalues of $W_h$ are all less than 1(sufficient but not necessary):&lt;br /&gt;
  $\lambda_1, \lambda_2, \ldots, \lambda_n &amp;lt; 1$&lt;br /&gt;
  $q_1, q_2, \ldots, q_n$ (eigenvectors)&lt;br /&gt;
  Rewrite using the eigenvectors of $W_h$ as a basis:&lt;br /&gt;
  \(\begin{align*}
  \frac{\partial J^{(i)}(\theta)}{\partial h^{(i)}} = \sum_{i=1}^n c_i \lambda_i^l q_i \approx 0
  \end{align*}\) (for large $l$)&lt;br /&gt;
  $\therefore \lambda_i^l$ approaches 0 as $l$ grows, gradient vanishes&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Choosing nonlinear activations $\sigma$&lt;br /&gt;
  Pretty much the same thing, except the proof requires $\lambda_i &amp;lt; \gamma$ for some $\gamma$ dependdent on dimensionality and $\sigma$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;why-is-vanishing-gradient-a-problem&quot;&gt;Why is vanishing gradient a problem?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Gradient signal from far away is lost because it’s much smaller than gradient signal from close-by. So, model weights are updated only with respect to near effects, not &lt;strong&gt;long-term effects&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;effect-of-vanishing-gradient-on-rnn-lm&quot;&gt;Effect of vanishing gradient on RNN-LM&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;LM task:
    &lt;blockquote&gt;
      &lt;p&gt;“When she tried to print her tickets, she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer, she finally printed her [target]”&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;To learn from this training example, the RNN-LM needs to model the dependency between “tickets” on the 7th step and the target word “tickets” at the end&lt;br /&gt;
  But if gradient is small, the model can’t learn this dependency and  the model is unable to predict similar long-distance dependencies at test time&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;why-is-exploding-gradient-a-problem&quot;&gt;Why is exploding gradient a problem?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;If the gradient becomes too big, then the SGD update step becomes too big:&lt;br /&gt;
  This can cause &lt;strong&gt;bad updates&lt;/strong&gt;: we take too large a step and reach a weird and bad parameter configuration (with large loss)&lt;/li&gt;
  &lt;li&gt;In the worst case, this will result in &lt;em&gt;Inf&lt;/em&gt; or &lt;em&gt;NaN&lt;/em&gt; in your network&lt;br /&gt;
  (then you have to restart training from an earlier checkpoint)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gradient-clipping-solution-for-exploding-gradient&quot;&gt;Gradient clipping: solution for exploding gradient&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;If the norm of the gradient is greater than some threshold, scale it down before applying SGD update&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec6_9.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Intuition: take a step in the same direction, but a smaller step&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, remembering to clip gradients is important, but exploding gradients are an easy problem to solve&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-fix-the-vanishing-gradient-problem&quot;&gt;How to fix the vanishing gradient problem?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The main problem is that it’s too difficult for the RNN to learn to preserve information &lt;em&gt;over many timesteps&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In a vanilla RNN, the hidden state is constantly being &lt;strong&gt;rewritten&lt;/strong&gt;&lt;br /&gt;
  $h^{(t)} = \sigma \left( W_h h^{(t-1)} + W_x x^{(t)} + b \right)$&lt;br /&gt;
  $\rightarrow$ How about a RNN with separate &lt;strong&gt;memory&lt;/strong&gt;?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;long-short-term-memory-rnns-lstms&quot;&gt;Long Short-Term Memory RNNs (LSTMs)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Proposed by &lt;em&gt;“Long short-term memory”, Hochreiter and Schmidhuber, 1997&lt;/em&gt;, but really a crucial part of the modern LSTM is from &lt;em&gt;“Learning to Forgt: Continual Prediction with LSTM”, Gers et al., 2000&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;On step $t$, there is a hidden state $h^{(t)}$ and a cell state $c^{(t)}$
    &lt;ul&gt;
      &lt;li&gt;Both are vectors length $n$&lt;/li&gt;
      &lt;li&gt;The cell stores &lt;strong&gt;long-term information&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;The LSTM can &lt;em&gt;read, erase, and write&lt;/em&gt; information from the cell&lt;br /&gt;
  The cell becomes conceptually rather like RAM in a computer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The selection of which information is erased/written/read is controlled by three corresponding &lt;strong&gt;gates&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The gates are also vectors length $n$&lt;/li&gt;
      &lt;li&gt;On each timestep, each element of the gates can be &lt;strong&gt;open&lt;/strong&gt; (1), &lt;strong&gt;closed&lt;/strong&gt; (0), or somewhere in-between&lt;/li&gt;
      &lt;li&gt;The gates are &lt;strong&gt;dynamic&lt;/strong&gt;: their value is computed based on the current context&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;With a sequence of inputs $x^{(t)}$, compute a sequence of hidden states $h^{(t)}$ and cell states $c^{(t)}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec6_10.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec6_11.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;how-does-lstm-solve-vanishing-gradients&quot;&gt;How does LSTM solve vanishing gradients?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The LSTM architecture makes it easier for the RNN to preserve information over many timesteps
    &lt;ul&gt;
      &lt;li&gt;e.g., if the forget gate is set to 1 for a cell dimension and the input gate set to 0, then the information of that cell is preserved indefinitely.&lt;/li&gt;
      &lt;li&gt;In practice, you get about 100 timesteps rather than about 7 of effective memory.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LSTM doesn’t &lt;em&gt;guarantee&lt;/em&gt; that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies
    &lt;h3 id=&quot;lstm-real-world-success&quot;&gt;LSTM: real-world success&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;In 2013–2015, LSTMs started achieving state-of-the-art results
    &lt;ul&gt;
      &lt;li&gt;Successful tasks include handwriting recognition, speech recognition, machine translation, parsing, and image captioning, as well as language models&lt;/li&gt;
      &lt;li&gt;LSTMs became the &lt;strong&gt;dominant approach&lt;/strong&gt; for most NLP tasks&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Now (2021), other approaches (e.g., Transformers) have become dominant for many tasks
    &lt;ul&gt;
      &lt;li&gt;For example, in WMT (a Machine Translation conference + competition):&lt;br /&gt;
  In WMT 2016, the summary report contains “RNN” 44 times&lt;br /&gt;
  In WMT 2019, “RNN” 7 times, “Transformer” 105 times&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;is-vanishingexploding-gradient-just-a-rnn-problem&quot;&gt;Is vanishing/exploding gradient just a RNN problem?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;It can be a problem for all neural architectures (including &lt;strong&gt;feed-forward&lt;/strong&gt; and &lt;strong&gt;convolutional&lt;/strong&gt;), especially &lt;strong&gt;very deep&lt;/strong&gt; ones.&lt;br /&gt;
  Due to chain rule / choice of nonlinearity function, gradient can become vanishingly small as it backpropagates. Thus, lower layers are learned very slowly (hard to train)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Solution: lots of new deep feedforward/convolutional architectures that &lt;strong&gt;add more direct connections&lt;/strong&gt; (thus allowing the gradient to flow)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;For example:
    &lt;ul&gt;
      &lt;li&gt;Resnet, &lt;em&gt;“Deep Residual Learning for Image Recognition”, He et al, 2015.&lt;/em&gt;	 
  &lt;img src=&quot;/assets/images/cs224n/lec6_12.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  Also known as skip-connects, the identity connection preserves information by default and makes deep networks much easier to train.&lt;/li&gt;
      &lt;li&gt;Densenet, &lt;em&gt;“Densely Connected Convolutional Networks”, Huang et al, 2017.&lt;/em&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec6_13.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  Dense connections that directly connect each layer to all future layers&lt;/li&gt;
      &lt;li&gt;HighwayNet, &lt;em&gt;“Highway Networks”, Srivastava et al, 2015.&lt;/em&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec6_14.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  Similar to residual connections, but controlled by a dynamic gate.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Conclusion: Though vanishing/exploding gradients are a general problem, &lt;strong&gt;RNNs are particularly unstable&lt;/strong&gt; due to the repeated multiplication by the &lt;strong&gt;same&lt;/strong&gt; weight matrix (&lt;em&gt;Bengio et al, 1994&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bidirectional-and-multi-layer-rnns-motivation&quot;&gt;Bidirectional and Multi-layer RNNs: motivation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Example: Sentiment Classification task&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec6_15.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec6_16.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec6_17.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Note: bidirectional RNNs are only applicable if you have access to the &lt;strong&gt;entire input sequence&lt;/strong&gt;&lt;br /&gt;
  They are &lt;strong&gt;not&lt;/strong&gt; applicable to Language Modeling, because in LM you &lt;em&gt;only&lt;/em&gt; have left context available.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you do have entire input sequence(e.g., any kind of encoding), &lt;strong&gt;bidirectionality is powerful&lt;/strong&gt; (you should use it by default).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For example, &lt;strong&gt;BERT&lt;/strong&gt; (&lt;strong&gt;Bidirectional&lt;/strong&gt; Encoder Representations from Transformers) is a powerful pretrained contextual representation system &lt;strong&gt;built on bidirectionality&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">The Simple RNN Language Model Training an RNN Language Model Get a big corpus of text which is a sequence of words $x^{(1)}, \ldots, x^{(T)}$ Feed into RNN-LM; compute output distribution $\hat{y}^{(t)}$ for every step $t$. i.e., predict probability distribution of every word, given words so far Loss function on step is cross-entropy(negative log-likelihood) between predicted probability distribution $\hat{y}^{(t)}$, and the true next word $y^{(t)}$(one-hot for $x^{(t+1)}$): \(\begin{align*} J^{(t)}(\theta) = CE(y^{(t)}, \hat{y}^{(t)}) = -\sum_{w\in V} y_w^{(t)} \log \hat{y}_w^{(t)} = -\log \hat{y}_{x_{t+1}}^{(t)} \end{align*}\) Average this to get overall loss for entire training set: \(\begin{align*} J(\theta) = \frac{1}{T}\sum_{t=1}^T J^{(t)}(\theta) = \frac{1}{T}\sum_{t=1}^T -\log \hat{y}_{x_{t+1}}^{(t)} \end{align*}\) “Teacher forcing” algorithm: At each step, reset to what was actually in the corpus and not reuse what the model have suggested. However: Computing loss and gradients across entire corpus $x^{(1)}, \ldots, x^{(T)}$ is too expensive In pratice, consider $x^{(1)}, \ldots, x^{(T)}$ as a sentence or a document Recall: Stochastic Gradient Descent allows us to compute loss and gradients for small chunk of data, and update. Compute loss $J(\theta)$ for a sentence (actually, a batch of sentences), compute gradients and update weights. Repeat. Training the parameters RNNs: Backpropagation for RNNs Question: What’s the derivative of $J^{(t)}(\theta)$ w.r.t. the repeated weight matrix $W_h$? Answer: sum of the gradient w.r.t. each time it appears \(\begin{align*} \frac{\partial J^{(t)}}{\partial W_h} = \sum_{i=1}^t \left. \frac{\partial J^{(t)}}{\partial W_h} \right|_{(i)} \end{align*}\) Backpropagation for RNNs: Proof sketch How to calculate: Backpropagate over timesteps $i=t,\ldots,0$, summing gradients as you go. “Backpropagation through time, [Werbos, P.G., 1988, Neural Networks 1, and others]” Generating text with a RNN Language Model Like a n-gram Language Model, use an RNN Language Model to generate text by repeated sampling. Sampled output becomes next step’s input. You can train an RNN-LM on any kind of text, then generate text in that style. Evaluating Language Models The standard evaluation metric for Language Model is perlexity; a geometric mean of the inverse probabilities. This is equal to the exponential of the cross-entropy loss $J(\theta)$: \(\begin{align*} = \prod_{t=1}^T \left( \frac{1}{\hat{y}_{x_{t+1}}^{(t)}}\right)^{1/T} = \exp \left( \frac{1}{T}\sum_{t=1}^T -\log \hat{y}_{x_{t+1}}^{(t)} \right) = \exp(J(\theta)) \end{align*}\) $\rightarrow$ Lower perplexity is better RNNs have greatly improved perplexity Why should we care about Language Modeling? Language Modeling is a benchmark task that helps us measure our progress on understanding language Language Modeling is a subcomponent of many NLP tasks, especially those involving generating text or estimating the probability of text: Predictive typing Speech recognition Handwriting recognition Spelling/grammar correction Authorship identification Machine translation Summarization Dialogue etc. Recap Language Model: A system that predicts the next word Recurrent Neural Network: A family of neural networks that: Take sequential input of any length Apply the same weights on each step Can optionally produce output on each step Recurrent Neural Network $\ne$ Language Model shown that RNNs are a great way to build a LM, but RNNs are useful for much more Other RNN uses Sequence tagging: e.g., part-of-speech tagging, named entity recognition Sentence classification: e.g., sentiment classification Basic way: Use the final hidden state. After running RNN(or LSTM), the final hidden state was encoded the whole sentence and treat it as the whole meaning of the sentence. Then put an extra classifier layer. Usually better: Take element-wise max or mean of all hidden states to more symmetrically encode the hidden state over each time step. Language encoder module: e.g., question answering, machine translation, many other tasks Generate text: e.g., speech recognition, machine translation, summarization Problems with Vanishing and Exploding Gradients Vanishing gradient: Proof sketch (linear case) Recall: $\hat{h}^{(t)} = \sigma(W_h h^{(t-1)} + W_x x^{(t)} + b_1)$ What if $\sigma(x) = x$? \(\begin{align*} \frac{\partial h^{(t)}}{\partial h^{(t-1)}} &amp;amp;= \text{diag} \left( \sigma^\prime \left( W_h h^{(t-1)} + W_x x^{(t)} + b_1 \right) \right) W_h &amp;amp;&amp;amp;(\text{chain rule}) \\ &amp;amp;= I\ W_h = W_h \end{align*}\) Consider the gradient of the loss $J^{(i)}(\theta)$ on step $i$, w.r.t. the hidden state $h^{(j)}$ one some previous step $j$. Let $l = i - j$ \(\begin{align*} \frac{\partial J^{(i)}(\theta)}{\partial h^{(j)}} &amp;amp;= \frac{\partial J^{(i)}(\theta)}{\partial h^{(i)}} \prod_{j &amp;lt; t \le i}\frac{\partial h^{(t)}}{\partial h^{(t-1)}} &amp;amp;&amp;amp;(\text{chain rule}) \\ &amp;amp;= \frac{\partial J^{(i)}(\theta)}{\partial h^{(i)}} \prod_{j &amp;lt; t \le i} W_h = \frac{\partial J^{(i)}(\theta)}{\partial h^{(i)}} W_h^l &amp;amp;&amp;amp;(\text{value of } \frac{\partial h^{(t)}}{\partial h^{(t-1)}}) \end{align*}\) $\rightarrow$ If $W_h$ is “small”, then $W_h^l$ gets exponentially problematic as $l$ becomes large Consider if the eigenvalues of $W_h$ are all less than 1(sufficient but not necessary): $\lambda_1, \lambda_2, \ldots, \lambda_n &amp;lt; 1$ $q_1, q_2, \ldots, q_n$ (eigenvectors) Rewrite using the eigenvectors of $W_h$ as a basis: \(\begin{align*} \frac{\partial J^{(i)}(\theta)}{\partial h^{(i)}} = \sum_{i=1}^n c_i \lambda_i^l q_i \approx 0 \end{align*}\) (for large $l$) $\therefore \lambda_i^l$ approaches 0 as $l$ grows, gradient vanishes Choosing nonlinear activations $\sigma$ Pretty much the same thing, except the proof requires $\lambda_i &amp;lt; \gamma$ for some $\gamma$ dependdent on dimensionality and $\sigma$ Why is vanishing gradient a problem? Gradient signal from far away is lost because it’s much smaller than gradient signal from close-by. So, model weights are updated only with respect to near effects, not long-term effects. Effect of vanishing gradient on RNN-LM LM task: “When she tried to print her tickets, she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer, she finally printed her [target]” To learn from this training example, the RNN-LM needs to model the dependency between “tickets” on the 7th step and the target word “tickets” at the end But if gradient is small, the model can’t learn this dependency and the model is unable to predict similar long-distance dependencies at test time Why is exploding gradient a problem? If the gradient becomes too big, then the SGD update step becomes too big: This can cause bad updates: we take too large a step and reach a weird and bad parameter configuration (with large loss) In the worst case, this will result in Inf or NaN in your network (then you have to restart training from an earlier checkpoint) Gradient clipping: solution for exploding gradient If the norm of the gradient is greater than some threshold, scale it down before applying SGD update Intuition: take a step in the same direction, but a smaller step In practice, remembering to clip gradients is important, but exploding gradients are an easy problem to solve How to fix the vanishing gradient problem? The main problem is that it’s too difficult for the RNN to learn to preserve information over many timesteps In a vanilla RNN, the hidden state is constantly being rewritten $h^{(t)} = \sigma \left( W_h h^{(t-1)} + W_x x^{(t)} + b \right)$ $\rightarrow$ How about a RNN with separate memory? Long Short-Term Memory RNNs (LSTMs) Proposed by “Long short-term memory”, Hochreiter and Schmidhuber, 1997, but really a crucial part of the modern LSTM is from “Learning to Forgt: Continual Prediction with LSTM”, Gers et al., 2000. On step $t$, there is a hidden state $h^{(t)}$ and a cell state $c^{(t)}$ Both are vectors length $n$ The cell stores long-term information The LSTM can read, erase, and write information from the cell The cell becomes conceptually rather like RAM in a computer The selection of which information is erased/written/read is controlled by three corresponding gates The gates are also vectors length $n$ On each timestep, each element of the gates can be open (1), closed (0), or somewhere in-between The gates are dynamic: their value is computed based on the current context With a sequence of inputs $x^{(t)}$, compute a sequence of hidden states $h^{(t)}$ and cell states $c^{(t)}$. How does LSTM solve vanishing gradients? The LSTM architecture makes it easier for the RNN to preserve information over many timesteps e.g., if the forget gate is set to 1 for a cell dimension and the input gate set to 0, then the information of that cell is preserved indefinitely. In practice, you get about 100 timesteps rather than about 7 of effective memory. LSTM doesn’t guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies LSTM: real-world success In 2013–2015, LSTMs started achieving state-of-the-art results Successful tasks include handwriting recognition, speech recognition, machine translation, parsing, and image captioning, as well as language models LSTMs became the dominant approach for most NLP tasks Now (2021), other approaches (e.g., Transformers) have become dominant for many tasks For example, in WMT (a Machine Translation conference + competition): In WMT 2016, the summary report contains “RNN” 44 times In WMT 2019, “RNN” 7 times, “Transformer” 105 times Is vanishing/exploding gradient just a RNN problem? It can be a problem for all neural architectures (including feed-forward and convolutional), especially very deep ones. Due to chain rule / choice of nonlinearity function, gradient can become vanishingly small as it backpropagates. Thus, lower layers are learned very slowly (hard to train) Solution: lots of new deep feedforward/convolutional architectures that add more direct connections (thus allowing the gradient to flow) For example: Resnet, “Deep Residual Learning for Image Recognition”, He et al, 2015. Also known as skip-connects, the identity connection preserves information by default and makes deep networks much easier to train. Densenet, “Densely Connected Convolutional Networks”, Huang et al, 2017. Dense connections that directly connect each layer to all future layers HighwayNet, “Highway Networks”, Srivastava et al, 2015. Similar to residual connections, but controlled by a dynamic gate. Conclusion: Though vanishing/exploding gradients are a general problem, RNNs are particularly unstable due to the repeated multiplication by the same weight matrix (Bengio et al, 1994) Bidirectional and Multi-layer RNNs: motivation Example: Sentiment Classification task Note: bidirectional RNNs are only applicable if you have access to the entire input sequence They are not applicable to Language Modeling, because in LM you only have left context available. If you do have entire input sequence(e.g., any kind of encoding), bidirectionality is powerful (you should use it by default). For example, BERT (Bidirectional Encoder Representations from Transformers) is a powerful pretrained contextual representation system built on bidirectionality.</summary></entry><entry><title type="html">cs224n - Lecture 5. Language Models and RNNs</title><link href="http://0.0.0.0:4000/cs224n_lec5" rel="alternate" type="text/html" title="cs224n - Lecture 5. Language Models and RNNs" /><published>2022-03-15T00:00:00+00:00</published><updated>2022-03-15T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec5</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec5">&lt;h3 id=&quot;how-do-we-gain-from-a-neural-dependency-parser&quot;&gt;How do we gain from a neural dependency parser?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;So far…&lt;br /&gt;
  Transition based dependency parsers were an efficient linear time method for giving the syntactic structure of natural language text.&lt;br /&gt;
  Worked pretty well before neural nets came along.&lt;br /&gt;
  $\color{red}{(-)}$ They worked with indicator features, specifying some condition and then checking whether it was true of a configuration. Problems of those features are:
    &lt;ul&gt;
      &lt;li&gt;Problem #1: sparse&lt;/li&gt;
      &lt;li&gt;Problem #2: incomplete&lt;/li&gt;
      &lt;li&gt;Problem #3: &lt;strong&gt;expensive computation&lt;/strong&gt;&lt;br /&gt;
  More than 95% of parsing time is consumed by feature computation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Neural Approach:&lt;br /&gt;
  Start with the same configuration of a stack and a buffer, run exactly the same transition sequence but with a dense vector.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;a-neural-dependency-parserchen-and-manning-2014&quot;&gt;A neural dependency parser(&lt;em&gt;Chen and Manning, 2014&lt;/em&gt;)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec5_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Results on English parsing to Stanford Dependencies:
    &lt;ul&gt;
      &lt;li&gt;Unlabeled attachment score (UAS) = head&lt;/li&gt;
      &lt;li&gt;Labeled attachment score (LAS) = head and label&lt;/li&gt;
      &lt;li&gt;2% more accurate than the symbolic dependency parser&lt;/li&gt;
      &lt;li&gt;noticeably faster&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;first-win-distributed-representations&quot;&gt;First win: Distributed Representations&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Represent each word as a &lt;em&gt;d&lt;/em&gt;-dimensional dense vector (i.e., word embedding)
    &lt;ul&gt;
      &lt;li&gt;Similar words are expected to have close vectors.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Meanwhile, &lt;strong&gt;part-of-speech tags&lt;/strong&gt;(POS) and &lt;strong&gt;dependency labels&lt;/strong&gt; are also represented as &lt;em&gt;d&lt;/em&gt;-dimensional vectors.
    &lt;ul&gt;
      &lt;li&gt;The smaller discrete sets also exhibit many semantical similarities.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Extracting Tokens &amp;amp; vector representations from configuration
    &lt;ul&gt;
      &lt;li&gt;Extract a set of tokens based on the stack / buffer positions:&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec5_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;second-win-deep-learning-classifiers-are-non-linear-classifiers&quot;&gt;Second win: Deep Learning classifiers are non-linear classifiers&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;strong&gt;softmax classifier&lt;/strong&gt; assigns classes $y\in C$ based on inputs $x \in \mathbb{R}^d$ via the probability:&lt;br /&gt;
  \(\begin{align*}
  p(y\mid x) = \frac{\exp(W_y . x)}{\sum_{c=1}^C \exp(W_c . x)}
  \end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;We train the weight matrix $W \in \mathbb{R}^{C\times d}$ to minimize the neg. log loss: $\sum_i - \log p(y_i\mid x_i)$ (a.k.a. “&lt;em&gt;cross entropy loss&lt;/em&gt;”)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Traditional ML classifiers&lt;/strong&gt; (including Naive Bayes, SVMs, logistic regression and softmax classifier) are not very powerful classifiers: they only &lt;strong&gt;give linear decision boundaries&lt;/strong&gt;; limiting, unhelpful when a problem is complex.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Neural networks can learn much more complex functions with nonlinear decision boundaries.
    &lt;ul&gt;
      &lt;li&gt;Non-linear in the original space, linear for the softmax at the top of the neural network&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec5_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;simple-feed-forward-neural-network-multi-class-classifier&quot;&gt;Simple feed-forward neural network multi-class classifier&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec5_3.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;neural-dependency-parser-model-architecture&quot;&gt;Neural Dependency Parser Model Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec5_4.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;dependency-parsing-for-sentence-structure&quot;&gt;Dependency parsing for sentence structure&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;C &amp;amp; M 2014 showed that neural networks can accurately determine the structure of sentences, supporting meaning interpretation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec5_5.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It was the first simple, successful neural dependency parser&lt;/li&gt;
  &lt;li&gt;The dense representations (and non-linear classifier) let it outperform other greedy parsers in both accuracy and speed&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;further-developments-in-transition-based-neural-dependency-parsing&quot;&gt;Further developments in transition-based neural dependency parsing&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Improvements
  Bigger, deeper networks with better tuned hyperparameters&lt;br /&gt;
  Beam search&lt;br /&gt;
  Global, conditional random field (CRF)-style inference over the decision sequence&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Leading to SyntaxNet and the Parsey McParseFace model(2016):&lt;br /&gt;
  “The World’s Most Accurate Parser”&lt;br /&gt;
  &lt;a href=&quot;https://ai.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html&quot;&gt;https://ai.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;graph-based-dependency-parsers&quot;&gt;Graph-based dependency parsers&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Compute a score for every possible pair; dependency (choice of head) for each word
    &lt;ul&gt;
      &lt;li&gt;Doing this well requires more than just knowing the two words&lt;/li&gt;
      &lt;li&gt;We need good “contextual” representations of each word token, which we will develop in the coming lectures&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Repeat the same process for each other word; find the best parse (MST algorithm)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec5_7.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;a-neural-graph-based-dependency-parser&quot;&gt;A Neural graph-based dependency parser&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Dozat and Manning 2017; Dozat, Qi, and Manning 2017&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;This paper revived interest in graph-based dependency parsing in a neural world
    &lt;ul&gt;
      &lt;li&gt;Designed a biaffine scoring model for neural dependency parsing&lt;/li&gt;
      &lt;li&gt;Also crucially uses a neural sequence model&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Great results&lt;/strong&gt;, but slower than the simple neural transition-based parsers
    &lt;ul&gt;
      &lt;li&gt;There are $n^2$ possible dependencies in a sentence of length $n$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec5_6.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-bit-more-about-neural-networks&quot;&gt;A bit more about neural networks&lt;/h2&gt;

&lt;h3 id=&quot;regularization&quot;&gt;Regularization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A full loss function includes &lt;strong&gt;regularization&lt;/strong&gt; over all parameters $\theta$, e.g., L2 regularization:&lt;br /&gt;
  \(\begin{align*}
  J(\theta) = \frac{1}{N}\sum_{i=1}^N -\log \left( \frac{e^{f_{y_i}}}{\sum_{c=1}^C e^{f_c}} \right) + \lambda\sum_k \theta_k^2
  \end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Classic view: Regularization works to prevent &lt;strong&gt;overfitting&lt;/strong&gt; when we have a lot of features (or later a very powerful/deep model, etc.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now: Regularization &lt;strong&gt;produces models that generalize well&lt;/strong&gt; when we have a “big” model&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;We &lt;strong&gt;do not care that our models overfit on the training data&lt;/strong&gt;, even though they are hugely overfit&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dropout&quot;&gt;Dropout&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;[Srivastava, Hinton, Krizhevsky, Sutskever, &amp;amp; Salakhutdinov 2012/JMLR 2014]&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Preventing Feature Co-adaptation = Good Regularization Method&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Training time: at each instance(or batch) of evaluation (in online SGD-training), randomly set 50% of the inputs to each neuron to 0&lt;/li&gt;
      &lt;li&gt;Test time: halve the model weights (because we now keep twice as many active neurons)&lt;br /&gt;
  (Except usually only drop first layer inputs a little (~15%) or not at all)&lt;/li&gt;
      &lt;li&gt;Prevents feature co-adaptation: A feature cannot only be useful in the presence of particular other features&lt;/li&gt;
      &lt;li&gt;In a single layer: A kind of middle-ground between Naive Bayes (where all feature weights are set independently) and logistic regression models (where weights are set in the context of all others)&lt;/li&gt;
      &lt;li&gt;Can be thought of as a form of model bagging (i.e., like an ensemble model)&lt;/li&gt;
      &lt;li&gt;Nowadays usually thought of as &lt;strong&gt;strong, feature-dependent regularizer&lt;/strong&gt;&lt;br /&gt;
  &lt;em&gt;[Wager, Wang, &amp;amp; Liang 2013]&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vectorization&quot;&gt;“Vectorization”&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;E.g., looping over word vectors versus concatenating them all into one large matrix and then multiplying the softmax weights with that matrix:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# number of windows to classify
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# dimensionality of each window
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# number of classes
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wordvectors_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wordvectors_one_matrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeit&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wordvectors_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeit&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wordvectors_one_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Always try to use vectors and matrices rather than for loops; the speed gain goes from 1 to 2 orders of magnitude with GPUs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;non-linearities-old-and-new&quot;&gt;Non-linearities, old and new&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;logistic(“sigmoid”)&lt;br /&gt;
  \(\begin{align*} f(z) = \frac{1}{1+\exp(-z)}\end{align*}\)&lt;/li&gt;
  &lt;li&gt;tanh&lt;br /&gt;
  \(\begin{align*} f(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\end{align*}\)&lt;br /&gt;
  tanh is just a rescaled and shifted sigmoid(2x as steep, [-1, 1]):&lt;br /&gt;
  $tanh(z) = 2 \text{logistic}(2z) - 1$&lt;/li&gt;
  &lt;li&gt;hard tanh&lt;br /&gt;
  \(\begin{align*} \text{PHardTanh}(x) = \begin{cases}
  -1 &amp;amp; \mbox{ if } x &amp;lt; -1 \\
  x &amp;amp; \mbox{ if } -1 \le x \le 1 \\
  1 &amp;amp; \mbox{ if } x &amp;gt; 1 \end{cases}\end{align*}\)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ReLU(Rectrified Linear Unit)&lt;br /&gt;
  $\text{rect}(z) = \text{max}(z, 0)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Others&lt;br /&gt;
  Leaky ReLU / Parametric ReLU, Swish(&lt;em&gt;Ramachandran, Zoph &amp;amp; Le 2017&lt;/em&gt;)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Both logistic and tanh are still used in various places(e.g., to get a probability), but are no longer the defaults for making deep networks&lt;br /&gt;
  For building a deep network, first try ReLU - it trains quickly and performs well due to good gradient backflow&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;parameter-initialization&quot;&gt;Parameter Initialization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize weights to small random values (i.e., not zero matrices)&lt;br /&gt;
  To avoid symmetries that prevent learning/specialization&lt;/li&gt;
  &lt;li&gt;Initialize hidden layer biases to 0 and output (or reconstruction) biases to optimal value if weights were 0 (e.g., mean target or inverse sigmoid of mean target)&lt;/li&gt;
  &lt;li&gt;Initialize &lt;strong&gt;all other weights&lt;/strong&gt; $~ \text{Uniform}(–r, r)$, with $r$ chosen so numbers get neither too big or too small (later the need for this is removed with use of &lt;em&gt;layer normalization&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;Xavier initialization has variance inversely proportional to &lt;em&gt;fan-in&lt;/em&gt; $n_{in}$ (previous layer size) and &lt;em&gt;fan-out&lt;/em&gt; $n_{out}$ (next layer size):&lt;br /&gt;
  \(\begin{align*}
  Var(W_i) = \frac{2}{n_{in} + n_{out}}
  \end{align*}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimizers&quot;&gt;Optimizers&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;SGD will work just fine, but getting good results will often require hand-tuning the learning rate&lt;/li&gt;
  &lt;li&gt;Sophisticated “adaptive” optimizers that scale the parameter adjustment by an accumulated gradient; &lt;em&gt;Adam&lt;/em&gt; is fairly good, safe place to start in many cases&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;learning-rates&quot;&gt;Learning Rates&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A constant learning rate. Start around $lr = 0.001$?
    &lt;ul&gt;
      &lt;li&gt;It must be order of magnitude right – try powers of 10&lt;br /&gt;
  Too big: model may diverge or not converge&lt;br /&gt;
  Too small: model may not have trained by the assignment deadline&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Better try &lt;em&gt;learning rate decay&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;By hand: : halve the learning rate every &lt;em&gt;k&lt;/em&gt; epochs&lt;/li&gt;
      &lt;li&gt;By a formula: $lr = lr_0 e^{-kt}$, for epoch $t$&lt;/li&gt;
      &lt;li&gt;There are fancier methods like cyclic learning rates (q.v.)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Fancier optimizers still use a learning rate but it may be an initial rate that the optimizer shrinks – so you may want to start with a higher learning rate&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;language-modeling&quot;&gt;Language Modeling&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Language Modeling&lt;/strong&gt; is the task of predicting what word comes next&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More formally: given a sequence of words $x^{(1)}, x^{(2)}, \ldots, x^{(t)}$, compute the probability distribution of the next words $x^{(t+1)}$:&lt;br /&gt;
  $P(x^{(t+1)} \mid x^{(t)}, \ldots, x^{(1)})$&lt;br /&gt;
  where $x^{(t+1)}$ can be any word in the vocabulary \(V = \left\{ w_1, \ldots, w_{\lvert V \rvert} \right\}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You can also think of a Language Model as a system that &lt;strong&gt;assigns probability to a piece of text&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;For example, if we have some text $x^{(1)}, x^{(2)}, \ldots, x^{(T)}$, then the probability of this text (according to the Language Model) is:&lt;br /&gt;
  \(\begin{align*}
  P(x^{(1)}, \ldots, x^{(T)}) &amp;amp;= P(x^{(1)}) \times P(x^{(2)}\mid x^{(1)}) \times \cdots \times P(x^{(T)}\mid x^{(T-1)}, \ldots, x^{(1)}) \\
  &amp;amp;= \prod_{t=1}^T \underbrace{P(x^{(t)}\mid x^{(t-1)}, \ldots, x^{(1)})}_{\text{This is what our LM provides}}
  \end{align*}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;n-gram-language-models&quot;&gt;n-gram Language Models&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Question: How to learn a Language Model?&lt;/li&gt;
  &lt;li&gt;Answer(traditional, pre- Deep Learning): learn an &lt;strong&gt;n-gram Language Model&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Definition: A _n-gram__ is a chunk of &lt;em&gt;n&lt;/em&gt; consecutive words.&lt;/li&gt;
  &lt;li&gt;Idea: Collect statistics about how frequent different n-grams are and use these to predict next word.&lt;/li&gt;
  &lt;li&gt;First we make a &lt;strong&gt;Markov assumption&lt;/strong&gt;: $x^{(t+1)}$ depends only on the preceding &lt;em&gt;n-1&lt;/em&gt; words;&lt;/li&gt;
&lt;/ul&gt;

\[\begin{align*}
P(x^{(t+1)}\mid x^{(t)}, \ldots, x^{(1)}) &amp;amp;= \overbrace{P(x^{(t+1)}\mid x^{(t)}, \ldots, x^{(t-n+2)})}^{n-1 \text{words}} &amp;amp;\text{(assumption)} \\
&amp;amp;= \frac{\overbrace{P(x^{(t+1)}, x^{(t)}, \ldots, x^{(t-n+2)})}^{\text{prob of a n-gram}}}{\underbrace{P(x^{(t)}, \ldots, x^{(t-n+2)})}_{\text{prob of a (n-1)-gram}}} &amp;amp;\text{(definition of conditional prob)}
\end{align*}\]

&lt;ul&gt;
  &lt;li&gt;Question: How do we get these &lt;em&gt;n&lt;/em&gt;-gram and &lt;em&gt;(n-1)&lt;/em&gt;-gram probabilities?&lt;/li&gt;
  &lt;li&gt;Answer: By counting them in some large corpus of text&lt;/li&gt;
&lt;/ul&gt;

\[\begin{align*}
\approx \frac{\text{count}(x^{(t+1)}, x^{(t)}, \ldots, x^{(t-n+2)})}{\text{count}(x^{(t)}, \ldots, x^{(t-n+2)})} &amp;amp;&amp;amp;\text{(statistical approxtimation)}  
\end{align*}\]

&lt;h3 id=&quot;n-gram-language-models-example&quot;&gt;n-gram Language Models: Example&lt;/h3&gt;
&lt;p&gt;Suppose we are learning a &lt;strong&gt;4-gram&lt;/strong&gt; Language Model.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;del&gt;as the proctor started the clock&lt;/del&gt;, the students opened their &lt;strong&gt;w&lt;/strong&gt;(target)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;\(\begin{align*}
P(\mathbf{w}|\text{students opened their}) = \frac{\text{count}(\text{students opened their } \mathbf{w})}{\text{students opened their}}
\end{align*}\)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For example, suppose that in the corpus:
    &lt;ul&gt;
      &lt;li&gt;“students opened their” occurred 1000 times&lt;/li&gt;
      &lt;li&gt;“students opened their &lt;strong&gt;books&lt;/strong&gt;” occurred 400 times&lt;br /&gt;
  $\rightarrow P(\text{books}|\text{students opened their}) = 0.4$&lt;/li&gt;
      &lt;li&gt;“students opened their &lt;strong&gt;exams&lt;/strong&gt;” occurred 100 times&lt;br /&gt;
  $\rightarrow P(\text{exams}|\text{students opened their}) = 0.1$&lt;br /&gt;
  Then, Should we have discarded the “proctor” context?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Naive Bayes: a class specific unigram language model, counting individual words&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;problems-with-n-gram-language-models&quot;&gt;Problems with n-gram Language Models&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Sparsity:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec5_8.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Storage: Need to store count for all n-grams you saw in the corpus&lt;br /&gt;
  Increasing &lt;em&gt;n&lt;/em&gt; or increasing corpus increases model size&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;n-gram-language-models-in-practice&quot;&gt;n-gram Language Models in practice&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;You can build a simple trigram Language Model over a 1.7 million word corpus (Reuters) in a few &lt;strong&gt;seconds&lt;/strong&gt; on your laptop&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec5_9.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You can also use a Language Model to generate text
    &lt;ul&gt;
      &lt;li&gt;Get probability distribution, sample one. move forward and iterate.&lt;/li&gt;
      &lt;li&gt;Results are &lt;em&gt;incoherent&lt;/em&gt;. Need to consider more &lt;em&gt;n&lt;/em&gt; words but increasing &lt;em&gt;n&lt;/em&gt; worsens sparsity problem, and increases model size&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-build-a-neural-language-model&quot;&gt;How to build a neural Language Model?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Recall the Language Modeling task:
    &lt;ul&gt;
      &lt;li&gt;Input: sequence of words $x^{(1)}, x^{(2)}, \ldots, x^{(t)}$&lt;/li&gt;
      &lt;li&gt;Output: prob dist of the next word $P(x^{(t+1)} \mid x^{(t)}, \ldots, x^{(1)})$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Window-based neural model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec5_10.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;a-fixed-window-neural-language-model&quot;&gt;A fixed-window neural Language Model&lt;/h3&gt;
&lt;p&gt;Approximately: &lt;em&gt;Y. Bengio, et al. (2000/2003): A Neural Probabilistic Language Model&lt;/em&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec5_11.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:green&quot;&gt;Improvements&lt;/span&gt; over &lt;em&gt;n&lt;/em&gt;-gram LM:
    &lt;ul&gt;
      &lt;li&gt;No sparsity problem&lt;/li&gt;
      &lt;li&gt;Don’t need to store all observed &lt;em&gt;n&lt;/em&gt;-grams&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Remaining &lt;span style=&quot;color:red&quot;&gt;problems&lt;/span&gt;:
    &lt;ul&gt;
      &lt;li&gt;Fixed window is &lt;strong&gt;too small&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Enlarging window enlarges $W$&lt;/li&gt;
      &lt;li&gt;Window can never be large enough&lt;/li&gt;
      &lt;li&gt;$x^{(1)}$ and $x^{(2)}$ are multiplied by completely different weights in $W$. &lt;strong&gt;No symmetry&lt;/strong&gt; in how the inputs are processed.&lt;br /&gt;
$\rightarrow$ We need a neural architecture that can process &lt;em&gt;any length input&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;recurrent-neural-networks-rnn&quot;&gt;Recurrent Neural Networks (RNN)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Core idea: Apply the same weights $W$ repeatedly&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec5_12.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec5_13.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RNN &lt;span style=&quot;color:green&quot;&gt;Advantages&lt;/span&gt;:
    &lt;ul&gt;
      &lt;li&gt;Can process &lt;strong&gt;any length&lt;/strong&gt; input&lt;/li&gt;
      &lt;li&gt;Computation for step &lt;em&gt;t&lt;/em&gt; can (in theory) use information from &lt;strong&gt;many steps back&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Model size doesn’t increase&lt;/strong&gt; for longer input context&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Symmetry&lt;/strong&gt; input process; same weights applied on every timestep&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN &lt;span style=&quot;color:red&quot;&gt;Disadvantages&lt;/span&gt;:
    &lt;ul&gt;
      &lt;li&gt;Recurrent computation is &lt;strong&gt;slow&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;In practice, difficult to access information from &lt;strong&gt;many steps back&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">How do we gain from a neural dependency parser? So far… Transition based dependency parsers were an efficient linear time method for giving the syntactic structure of natural language text. Worked pretty well before neural nets came along. $\color{red}{(-)}$ They worked with indicator features, specifying some condition and then checking whether it was true of a configuration. Problems of those features are: Problem #1: sparse Problem #2: incomplete Problem #3: expensive computation More than 95% of parsing time is consumed by feature computation Neural Approach: Start with the same configuration of a stack and a buffer, run exactly the same transition sequence but with a dense vector. A neural dependency parser(Chen and Manning, 2014) Results on English parsing to Stanford Dependencies: Unlabeled attachment score (UAS) = head Labeled attachment score (LAS) = head and label 2% more accurate than the symbolic dependency parser noticeably faster First win: Distributed Representations Represent each word as a d-dimensional dense vector (i.e., word embedding) Similar words are expected to have close vectors. Meanwhile, part-of-speech tags(POS) and dependency labels are also represented as d-dimensional vectors. The smaller discrete sets also exhibit many semantical similarities. Extracting Tokens &amp;amp; vector representations from configuration Extract a set of tokens based on the stack / buffer positions: Second win: Deep Learning classifiers are non-linear classifiers A softmax classifier assigns classes $y\in C$ based on inputs $x \in \mathbb{R}^d$ via the probability: \(\begin{align*} p(y\mid x) = \frac{\exp(W_y . x)}{\sum_{c=1}^C \exp(W_c . x)} \end{align*}\) We train the weight matrix $W \in \mathbb{R}^{C\times d}$ to minimize the neg. log loss: $\sum_i - \log p(y_i\mid x_i)$ (a.k.a. “cross entropy loss”) Traditional ML classifiers (including Naive Bayes, SVMs, logistic regression and softmax classifier) are not very powerful classifiers: they only give linear decision boundaries; limiting, unhelpful when a problem is complex. Neural networks can learn much more complex functions with nonlinear decision boundaries. Non-linear in the original space, linear for the softmax at the top of the neural network Simple feed-forward neural network multi-class classifier Neural Dependency Parser Model Architecture Dependency parsing for sentence structure C &amp;amp; M 2014 showed that neural networks can accurately determine the structure of sentences, supporting meaning interpretation It was the first simple, successful neural dependency parser The dense representations (and non-linear classifier) let it outperform other greedy parsers in both accuracy and speed Further developments in transition-based neural dependency parsing Improvements Bigger, deeper networks with better tuned hyperparameters Beam search Global, conditional random field (CRF)-style inference over the decision sequence Leading to SyntaxNet and the Parsey McParseFace model(2016): “The World’s Most Accurate Parser” https://ai.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html Graph-based dependency parsers Compute a score for every possible pair; dependency (choice of head) for each word Doing this well requires more than just knowing the two words We need good “contextual” representations of each word token, which we will develop in the coming lectures Repeat the same process for each other word; find the best parse (MST algorithm) A Neural graph-based dependency parser Dozat and Manning 2017; Dozat, Qi, and Manning 2017 This paper revived interest in graph-based dependency parsing in a neural world Designed a biaffine scoring model for neural dependency parsing Also crucially uses a neural sequence model Great results, but slower than the simple neural transition-based parsers There are $n^2$ possible dependencies in a sentence of length $n$ A bit more about neural networks Regularization A full loss function includes regularization over all parameters $\theta$, e.g., L2 regularization: \(\begin{align*} J(\theta) = \frac{1}{N}\sum_{i=1}^N -\log \left( \frac{e^{f_{y_i}}}{\sum_{c=1}^C e^{f_c}} \right) + \lambda\sum_k \theta_k^2 \end{align*}\) Classic view: Regularization works to prevent overfitting when we have a lot of features (or later a very powerful/deep model, etc.) Now: Regularization produces models that generalize well when we have a “big” model We do not care that our models overfit on the training data, even though they are hugely overfit Dropout [Srivastava, Hinton, Krizhevsky, Sutskever, &amp;amp; Salakhutdinov 2012/JMLR 2014] Preventing Feature Co-adaptation = Good Regularization Method Training time: at each instance(or batch) of evaluation (in online SGD-training), randomly set 50% of the inputs to each neuron to 0 Test time: halve the model weights (because we now keep twice as many active neurons) (Except usually only drop first layer inputs a little (~15%) or not at all) Prevents feature co-adaptation: A feature cannot only be useful in the presence of particular other features In a single layer: A kind of middle-ground between Naive Bayes (where all feature weights are set independently) and logistic regression models (where weights are set in the context of all others) Can be thought of as a form of model bagging (i.e., like an ensemble model) Nowadays usually thought of as strong, feature-dependent regularizer [Wager, Wang, &amp;amp; Liang 2013] “Vectorization” E.g., looping over word vectors versus concatenating them all into one large matrix and then multiplying the softmax weights with that matrix: from numpy import random N = 500 # number of windows to classify d = 300 # dimensionality of each window C = 5 # number of classes W = random.rand(C, d) wordvectors_list = [random.rand(d, 1) for i in range(N)] wordvectors_one_matrix = random.rand(d, N) %timeit [W.dot(wordvectors_list[i]) for i in range(N)] %timeit W.dot(wordvectors_one_matrix) Always try to use vectors and matrices rather than for loops; the speed gain goes from 1 to 2 orders of magnitude with GPUs. Non-linearities, old and new logistic(“sigmoid”) \(\begin{align*} f(z) = \frac{1}{1+\exp(-z)}\end{align*}\) tanh \(\begin{align*} f(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\end{align*}\) tanh is just a rescaled and shifted sigmoid(2x as steep, [-1, 1]): $tanh(z) = 2 \text{logistic}(2z) - 1$ hard tanh \(\begin{align*} \text{PHardTanh}(x) = \begin{cases} -1 &amp;amp; \mbox{ if } x &amp;lt; -1 \\ x &amp;amp; \mbox{ if } -1 \le x \le 1 \\ 1 &amp;amp; \mbox{ if } x &amp;gt; 1 \end{cases}\end{align*}\) ReLU(Rectrified Linear Unit) $\text{rect}(z) = \text{max}(z, 0)$ Others Leaky ReLU / Parametric ReLU, Swish(Ramachandran, Zoph &amp;amp; Le 2017) Both logistic and tanh are still used in various places(e.g., to get a probability), but are no longer the defaults for making deep networks For building a deep network, first try ReLU - it trains quickly and performs well due to good gradient backflow Parameter Initialization Initialize weights to small random values (i.e., not zero matrices) To avoid symmetries that prevent learning/specialization Initialize hidden layer biases to 0 and output (or reconstruction) biases to optimal value if weights were 0 (e.g., mean target or inverse sigmoid of mean target) Initialize all other weights $~ \text{Uniform}(–r, r)$, with $r$ chosen so numbers get neither too big or too small (later the need for this is removed with use of layer normalization) Xavier initialization has variance inversely proportional to fan-in $n_{in}$ (previous layer size) and fan-out $n_{out}$ (next layer size): \(\begin{align*} Var(W_i) = \frac{2}{n_{in} + n_{out}} \end{align*}\) Optimizers SGD will work just fine, but getting good results will often require hand-tuning the learning rate Sophisticated “adaptive” optimizers that scale the parameter adjustment by an accumulated gradient; Adam is fairly good, safe place to start in many cases Learning Rates A constant learning rate. Start around $lr = 0.001$? It must be order of magnitude right – try powers of 10 Too big: model may diverge or not converge Too small: model may not have trained by the assignment deadline Better try learning rate decay By hand: : halve the learning rate every k epochs By a formula: $lr = lr_0 e^{-kt}$, for epoch $t$ There are fancier methods like cyclic learning rates (q.v.) Fancier optimizers still use a learning rate but it may be an initial rate that the optimizer shrinks – so you may want to start with a higher learning rate Language Modeling Language Modeling is the task of predicting what word comes next More formally: given a sequence of words $x^{(1)}, x^{(2)}, \ldots, x^{(t)}$, compute the probability distribution of the next words $x^{(t+1)}$: $P(x^{(t+1)} \mid x^{(t)}, \ldots, x^{(1)})$ where $x^{(t+1)}$ can be any word in the vocabulary \(V = \left\{ w_1, \ldots, w_{\lvert V \rvert} \right\}\) You can also think of a Language Model as a system that assigns probability to a piece of text For example, if we have some text $x^{(1)}, x^{(2)}, \ldots, x^{(T)}$, then the probability of this text (according to the Language Model) is: \(\begin{align*} P(x^{(1)}, \ldots, x^{(T)}) &amp;amp;= P(x^{(1)}) \times P(x^{(2)}\mid x^{(1)}) \times \cdots \times P(x^{(T)}\mid x^{(T-1)}, \ldots, x^{(1)}) \\ &amp;amp;= \prod_{t=1}^T \underbrace{P(x^{(t)}\mid x^{(t-1)}, \ldots, x^{(1)})}_{\text{This is what our LM provides}} \end{align*}\) n-gram Language Models Question: How to learn a Language Model? Answer(traditional, pre- Deep Learning): learn an n-gram Language Model Definition: A _n-gram__ is a chunk of n consecutive words. Idea: Collect statistics about how frequent different n-grams are and use these to predict next word. First we make a Markov assumption: $x^{(t+1)}$ depends only on the preceding n-1 words; \[\begin{align*} P(x^{(t+1)}\mid x^{(t)}, \ldots, x^{(1)}) &amp;amp;= \overbrace{P(x^{(t+1)}\mid x^{(t)}, \ldots, x^{(t-n+2)})}^{n-1 \text{words}} &amp;amp;\text{(assumption)} \\ &amp;amp;= \frac{\overbrace{P(x^{(t+1)}, x^{(t)}, \ldots, x^{(t-n+2)})}^{\text{prob of a n-gram}}}{\underbrace{P(x^{(t)}, \ldots, x^{(t-n+2)})}_{\text{prob of a (n-1)-gram}}} &amp;amp;\text{(definition of conditional prob)} \end{align*}\] Question: How do we get these n-gram and (n-1)-gram probabilities? Answer: By counting them in some large corpus of text \[\begin{align*} \approx \frac{\text{count}(x^{(t+1)}, x^{(t)}, \ldots, x^{(t-n+2)})}{\text{count}(x^{(t)}, \ldots, x^{(t-n+2)})} &amp;amp;&amp;amp;\text{(statistical approxtimation)} \end{align*}\] n-gram Language Models: Example Suppose we are learning a 4-gram Language Model. as the proctor started the clock, the students opened their w(target) \(\begin{align*} P(\mathbf{w}|\text{students opened their}) = \frac{\text{count}(\text{students opened their } \mathbf{w})}{\text{students opened their}} \end{align*}\) For example, suppose that in the corpus: “students opened their” occurred 1000 times “students opened their books” occurred 400 times $\rightarrow P(\text{books}|\text{students opened their}) = 0.4$ “students opened their exams” occurred 100 times $\rightarrow P(\text{exams}|\text{students opened their}) = 0.1$ Then, Should we have discarded the “proctor” context? Naive Bayes: a class specific unigram language model, counting individual words Problems with n-gram Language Models Sparsity: Storage: Need to store count for all n-grams you saw in the corpus Increasing n or increasing corpus increases model size n-gram Language Models in practice You can build a simple trigram Language Model over a 1.7 million word corpus (Reuters) in a few seconds on your laptop You can also use a Language Model to generate text Get probability distribution, sample one. move forward and iterate. Results are incoherent. Need to consider more n words but increasing n worsens sparsity problem, and increases model size How to build a neural Language Model? Recall the Language Modeling task: Input: sequence of words $x^{(1)}, x^{(2)}, \ldots, x^{(t)}$ Output: prob dist of the next word $P(x^{(t+1)} \mid x^{(t)}, \ldots, x^{(1)})$ Window-based neural model A fixed-window neural Language Model Approximately: Y. Bengio, et al. (2000/2003): A Neural Probabilistic Language Model Improvements over n-gram LM: No sparsity problem Don’t need to store all observed n-grams Remaining problems: Fixed window is too small Enlarging window enlarges $W$ Window can never be large enough $x^{(1)}$ and $x^{(2)}$ are multiplied by completely different weights in $W$. No symmetry in how the inputs are processed. $\rightarrow$ We need a neural architecture that can process any length input Recurrent Neural Networks (RNN) Core idea: Apply the same weights $W$ repeatedly RNN Advantages: Can process any length input Computation for step t can (in theory) use information from many steps back Model size doesn’t increase for longer input context Symmetry input process; same weights applied on every timestep RNN Disadvantages: Recurrent computation is slow In practice, difficult to access information from many steps back</summary></entry><entry><title type="html">cs224n - Lecture 4. Dependency Parsing</title><link href="http://0.0.0.0:4000/cs224n_lec4" rel="alternate" type="text/html" title="cs224n - Lecture 4. Dependency Parsing" /><published>2022-03-11T00:00:00+00:00</published><updated>2022-03-11T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec4</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec4">&lt;h3 id=&quot;two-views-of-linguistic-structure-phrase-structure&quot;&gt;Two views of linguistic structure: Phrase structure&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Constituency = phrase structure grammar = context-free grammers(CFGs)&lt;br /&gt;
  &lt;strong&gt;Phrase structure&lt;/strong&gt; organizes words into nested constituents&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Starting unit: &lt;strong&gt;words&lt;/strong&gt; (noun, preposition, adjective, determiner, …)&lt;br /&gt;
  the, $\ $ cat, $\ $ cuddly, $\ $ by, $\ $ door&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Words combine into &lt;strong&gt;phrases&lt;/strong&gt;&lt;br /&gt;
  the cuddly cat(noun phrase),&lt;br /&gt;
  by the door(prepositional phrase; preposition(by) + noun phrase)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Phrases can combine into bigger phrases&lt;br /&gt;
  the cuddly cat by the door(noun phrase)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Lexicon:&lt;br /&gt;
  $\text{N} \rightarrow \text{cat}$&lt;br /&gt;
  $\text{N} \rightarrow \text{door}$&lt;br /&gt;
  $\text{Det} \rightarrow \text{the}$&lt;br /&gt;
  $P \rightarrow \text{by}$&lt;br /&gt;
  $\text{Adj} \rightarrow \text{cuddly}$&lt;br /&gt;
  $\vdots$&lt;/li&gt;
  &lt;li&gt;Grammar:&lt;br /&gt;
  $\text{NP} \rightarrow \text{Det } \  \text{ (Adj)}^{\ast} \  \text{ N } \  \text{ (PP)}$&lt;br /&gt;
  $\text{PP} \rightarrow \text{P } \  \text{ NP}$&lt;br /&gt;
  $\vdots$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;two-views-of-linguistic-structure-dependency-structure&quot;&gt;Two views of linguistic structure: Dependency structure&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Dependency structure shows which words depend on (modify, attach to, or are arguments of) which other words.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;why-do-we-need-sentence-structure&quot;&gt;Why do we need sentence structure?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Humans communicate complex ideas by composing words together into bigger units to convey complex meanings&lt;/li&gt;
  &lt;li&gt;Listeners need to work out what modifies &lt;em&gt;attaches to&lt;/em&gt; what&lt;/li&gt;
  &lt;li&gt;A model needs to understand sentence structure in order to be able to interpret language correctly&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ambiguities&quot;&gt;Ambiguities&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Prepositional phrase ambiguity
    &lt;ul&gt;
      &lt;li&gt;A key parsing decision is how we ‘attach’ various constituents
        &lt;ul&gt;
          &lt;li&gt;&lt;em&gt;PP&lt;/em&gt;s, adverbial or participial phrases, infinitives, coordinations&lt;br /&gt;
  e.g.&lt;br /&gt;
  \(\begin{align*}
  \text{The board approved [its acquisition]} &amp;amp; \text{[by Royal Trustco Ltd.]} \\
                                      &amp;amp; \text{[of Toronto]} \\
                                      &amp;amp; \text{[for \$27 a share]} \\
                                      &amp;amp; \text{[at its monthly meeting].}
  \end{align*}\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;With a sentence of &lt;em&gt;k&lt;/em&gt; prepositional phrases at the end of it, the number of parses is given by the Catalan numbers; $C_n = (2n)!/[(n+1)!n!]$, an exponential series growing as the number of prepositional phrases.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Coordination scopre ambiguity&lt;br /&gt;
  e.g. &lt;strong&gt;Shuttle veteran&lt;/strong&gt; and longtime NASA executive__ Fred Gregory appointed to board&lt;br /&gt;
      $\rightarrow$ 1 or 2 person?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adjectival/Adverbial Modifier ambiguity&lt;br /&gt;
  e.g. Students get &lt;strong&gt;first hand job experience&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Verb Phrase(VP) attachment ambiguity&lt;br /&gt;
  e.g. Mutilated body washes up on Rio beach &lt;strong&gt;to be used for Olympics beach volleyball&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dependency-paths-help-extract-semantic-interpretation&quot;&gt;Dependency paths help extract semantic interpretation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;simple practical example: extracting protein-protein interaction&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec4_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;dependency-grammar-and-dependency-structure&quot;&gt;Dependency Grammar and Dependency Structure&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Dependency syntax postulates that syntactic structure consists of relations between lexical items, normally binary asymmetric relations (“arrows”) called &lt;strong&gt;dependencies&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec4_1.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The arrows are commonly &lt;strong&gt;typed&lt;/strong&gt; with the name of grammatical relations (subject, prepositional object, apposition, etc.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;An arrow connects a &lt;strong&gt;head&lt;/strong&gt;(governor, superior, regent) with a &lt;strong&gt;dependent&lt;/strong&gt;(modifier, inferior, subordinate)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Usually, dependencies form a tree(a connected, acyclic, single-root graph)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Check: some people draw the arrows one way; some the other way&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Usually add a fake ROOT so every word is a dependent of precisely 1 other node&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-rise-of-annotated-data--universal-dependencies-treebanks&quot;&gt;The rise of annotated data &amp;amp; Universal Dependencies treebanks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Advantages of treebank
    &lt;ul&gt;
      &lt;li&gt;Reusability of the labor
        &lt;ul&gt;
          &lt;li&gt;Many parsers, part-of-speech taggers, etc. can be built on it&lt;/li&gt;
          &lt;li&gt;Valuable resource for linguistic&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Broad coverage, not just a few intuitions&lt;/li&gt;
      &lt;li&gt;Frequencies and distributional information(statistics)&lt;/li&gt;
      &lt;li&gt;A way to evaluate NLP systems&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dependency-conditioning-preferences&quot;&gt;Dependency Conditioning Preferences&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec4_2.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The sources of information for dependency parsing
    &lt;ol&gt;
      &lt;li&gt;Bilexical affinities: The dependency $\text{discussion}\rightarrow\text{issues}$ is plausible&lt;/li&gt;
      &lt;li&gt;Dependency distance: Most dependencies are between nearby words&lt;/li&gt;
      &lt;li&gt;Intervening material: Dependencies rarely span intervening verbs or punctuation&lt;/li&gt;
      &lt;li&gt;Valency of heads: How many dependents on which side are usual for a head?&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dependency-parsing&quot;&gt;Dependency Parsing&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A sentence is parsed by choosing for each word what other word (including ROOT) it is a dependent of&lt;/li&gt;
  &lt;li&gt;Usually some constraints:
    &lt;ul&gt;
      &lt;li&gt;Only one word is a dependent of ROOT&lt;/li&gt;
      &lt;li&gt;Don’t want cycles $A\rightarrow B$, $B\rightarrow A$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;This makes the dependencies a tree&lt;/li&gt;
  &lt;li&gt;Final issue is whether arrows can cross(be &lt;strong&gt;non-projective&lt;/strong&gt;) or not&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;projectivity&quot;&gt;Projectivity&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Definition of a &lt;strong&gt;projective parse&lt;/strong&gt;: There are no crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words&lt;/li&gt;
  &lt;li&gt;Dependencies corresponding to a CFG tree must be &lt;strong&gt;projective&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;i.e., by forming dependencies by taking 1 child of each category as head&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Most syntactic structure is projective like this, but dependency theory normally does allow non-projective structures to account for displaced constituents
    &lt;ul&gt;
      &lt;li&gt;You can’t easily get the semantics of certain constructions right without these nonprojective dependencies&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;e.g.&lt;br /&gt;
  &lt;strong&gt;From&lt;/strong&gt; who did Bill buy the coffee yesterday?&lt;br /&gt;
  Who did Bill buy the coffee &lt;strong&gt;from&lt;/strong&gt; yesterday&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;methods-of-dependency-parsing&quot;&gt;Methods of Dependency Parsing&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Dynamic programming, &lt;em&gt;Eisner(1996)&lt;/em&gt;: $O(n^3)$ complexity&lt;/li&gt;
  &lt;li&gt;Graph algorithms, &lt;em&gt;McDonald et al.(2005)&lt;/em&gt;: creating a Minimun Spanning Tree&lt;/li&gt;
  &lt;li&gt;Constraint Satisfaction, &lt;em&gt;Karlsson(1990)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;“Transition-based parsing” or “deterministic dependency parsing”&lt;br /&gt;
 Greedy choice of attachments guided by good machine learning classifiers&lt;br /&gt;
 E.g., MaltParser, &lt;em&gt;Nivre et al.(2008)&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;greedy-transition-based-parsing-nivre-2003&quot;&gt;Greedy transition-based parsing, Nivre 2003&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A simple form of greedy discriminative dependency parser&lt;/li&gt;
  &lt;li&gt;The parser does a sequence of bottom-up actions
    &lt;ul&gt;
      &lt;li&gt;Roughly like “shift” or “reduce” in a shift-reduce parser, but the “reduce” actions are specialized to create dependencies with head on left or right&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The parser has:
    &lt;ul&gt;
      &lt;li&gt;a stack $\sigma$, written with top to the right
        &lt;ul&gt;
          &lt;li&gt;which starts with the ROOT symbol&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;a buffer $\beta$, written with top to the left
        &lt;ul&gt;
          &lt;li&gt;which starts with the input sentence&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;a set of dependency arcs A
        &lt;ul&gt;
          &lt;li&gt;which starts off empty&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;a set of actions&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;basic-transition-based-dependency-parser&quot;&gt;Basic transition-based dependency parser&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec4_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Arc-standard transition-based parser&lt;br /&gt;
  E.g., Analysis of “I ate fish”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec4_4.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec4_5.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;maltparser-nivre-and-hall-2005&quot;&gt;MaltParser, Nivre and Hall 2005&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;How we choose the next action?&lt;br /&gt;
  Answer: Machine Learning!&lt;/li&gt;
  &lt;li&gt;Each action is predicted by a discriminative classifier (e.g., softmax classifier) over legal move
    &lt;ul&gt;
      &lt;li&gt;Max of 3 untyped choices; max of $\lvert R \rvert \times 2 + 1 $ when typed&lt;/li&gt;
      &lt;li&gt;Features: top of stack word, POS; first in buffer word, POS; etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;There is NO search(in the simplest form)
    &lt;ul&gt;
      &lt;li&gt;But you can profitably do a beam search if you wish(slower but better): You keep &lt;em&gt;k&lt;/em&gt; good parse prefixes at each time step&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The model’s accuracy is &lt;em&gt;fractionally&lt;/em&gt; below the state of the art in dependency parsing, but it provides &lt;strong&gt;very fast linear time parsing&lt;/strong&gt;, with high accuracy, great for parsing the web&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conventional-feature-representation&quot;&gt;Conventional Feature Representation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec4_6.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;evaluation-of-dependency-parsing-labeled-dependency-accuracy&quot;&gt;Evaluation of Dependency Parsing: (labeled) dependency accuracy&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec4_7.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">Two views of linguistic structure: Phrase structure Constituency = phrase structure grammar = context-free grammers(CFGs) Phrase structure organizes words into nested constituents Starting unit: words (noun, preposition, adjective, determiner, …) the, $\ $ cat, $\ $ cuddly, $\ $ by, $\ $ door Words combine into phrases the cuddly cat(noun phrase), by the door(prepositional phrase; preposition(by) + noun phrase) Phrases can combine into bigger phrases the cuddly cat by the door(noun phrase) Lexicon: $\text{N} \rightarrow \text{cat}$ $\text{N} \rightarrow \text{door}$ $\text{Det} \rightarrow \text{the}$ $P \rightarrow \text{by}$ $\text{Adj} \rightarrow \text{cuddly}$ $\vdots$ Grammar: $\text{NP} \rightarrow \text{Det } \ \text{ (Adj)}^{\ast} \ \text{ N } \ \text{ (PP)}$ $\text{PP} \rightarrow \text{P } \ \text{ NP}$ $\vdots$ Two views of linguistic structure: Dependency structure Dependency structure shows which words depend on (modify, attach to, or are arguments of) which other words. Why do we need sentence structure? Humans communicate complex ideas by composing words together into bigger units to convey complex meanings Listeners need to work out what modifies attaches to what A model needs to understand sentence structure in order to be able to interpret language correctly Ambiguities Prepositional phrase ambiguity A key parsing decision is how we ‘attach’ various constituents PPs, adverbial or participial phrases, infinitives, coordinations e.g. \(\begin{align*} \text{The board approved [its acquisition]} &amp;amp; \text{[by Royal Trustco Ltd.]} \\ &amp;amp; \text{[of Toronto]} \\ &amp;amp; \text{[for \$27 a share]} \\ &amp;amp; \text{[at its monthly meeting].} \end{align*}\) With a sentence of k prepositional phrases at the end of it, the number of parses is given by the Catalan numbers; $C_n = (2n)!/[(n+1)!n!]$, an exponential series growing as the number of prepositional phrases. Coordination scopre ambiguity e.g. Shuttle veteran and longtime NASA executive__ Fred Gregory appointed to board $\rightarrow$ 1 or 2 person? Adjectival/Adverbial Modifier ambiguity e.g. Students get first hand job experience Verb Phrase(VP) attachment ambiguity e.g. Mutilated body washes up on Rio beach to be used for Olympics beach volleyball Dependency paths help extract semantic interpretation simple practical example: extracting protein-protein interaction Dependency Grammar and Dependency Structure Dependency syntax postulates that syntactic structure consists of relations between lexical items, normally binary asymmetric relations (“arrows”) called dependencies The arrows are commonly typed with the name of grammatical relations (subject, prepositional object, apposition, etc.) An arrow connects a head(governor, superior, regent) with a dependent(modifier, inferior, subordinate) Usually, dependencies form a tree(a connected, acyclic, single-root graph) Check: some people draw the arrows one way; some the other way Usually add a fake ROOT so every word is a dependent of precisely 1 other node The rise of annotated data &amp;amp; Universal Dependencies treebanks Advantages of treebank Reusability of the labor Many parsers, part-of-speech taggers, etc. can be built on it Valuable resource for linguistic Broad coverage, not just a few intuitions Frequencies and distributional information(statistics) A way to evaluate NLP systems Dependency Conditioning Preferences The sources of information for dependency parsing Bilexical affinities: The dependency $\text{discussion}\rightarrow\text{issues}$ is plausible Dependency distance: Most dependencies are between nearby words Intervening material: Dependencies rarely span intervening verbs or punctuation Valency of heads: How many dependents on which side are usual for a head? Dependency Parsing A sentence is parsed by choosing for each word what other word (including ROOT) it is a dependent of Usually some constraints: Only one word is a dependent of ROOT Don’t want cycles $A\rightarrow B$, $B\rightarrow A$ This makes the dependencies a tree Final issue is whether arrows can cross(be non-projective) or not Projectivity Definition of a projective parse: There are no crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words Dependencies corresponding to a CFG tree must be projective i.e., by forming dependencies by taking 1 child of each category as head Most syntactic structure is projective like this, but dependency theory normally does allow non-projective structures to account for displaced constituents You can’t easily get the semantics of certain constructions right without these nonprojective dependencies e.g. From who did Bill buy the coffee yesterday? Who did Bill buy the coffee from yesterday Methods of Dependency Parsing Dynamic programming, Eisner(1996): $O(n^3)$ complexity Graph algorithms, McDonald et al.(2005): creating a Minimun Spanning Tree Constraint Satisfaction, Karlsson(1990) “Transition-based parsing” or “deterministic dependency parsing” Greedy choice of attachments guided by good machine learning classifiers E.g., MaltParser, Nivre et al.(2008) Greedy transition-based parsing, Nivre 2003 A simple form of greedy discriminative dependency parser The parser does a sequence of bottom-up actions Roughly like “shift” or “reduce” in a shift-reduce parser, but the “reduce” actions are specialized to create dependencies with head on left or right The parser has: a stack $\sigma$, written with top to the right which starts with the ROOT symbol a buffer $\beta$, written with top to the left which starts with the input sentence a set of dependency arcs A which starts off empty a set of actions Basic transition-based dependency parser Arc-standard transition-based parser E.g., Analysis of “I ate fish” MaltParser, Nivre and Hall 2005 How we choose the next action? Answer: Machine Learning! Each action is predicted by a discriminative classifier (e.g., softmax classifier) over legal move Max of 3 untyped choices; max of $\lvert R \rvert \times 2 + 1 $ when typed Features: top of stack word, POS; first in buffer word, POS; etc. There is NO search(in the simplest form) But you can profitably do a beam search if you wish(slower but better): You keep k good parse prefixes at each time step The model’s accuracy is fractionally below the state of the art in dependency parsing, but it provides very fast linear time parsing, with high accuracy, great for parsing the web Conventional Feature Representation Evaluation of Dependency Parsing: (labeled) dependency accuracy</summary></entry><entry><title type="html">cs224n - Lecture 3. Backprop and Neural Networks</title><link href="http://0.0.0.0:4000/cs224n_lec3" rel="alternate" type="text/html" title="cs224n - Lecture 3. Backprop and Neural Networks" /><published>2022-03-08T00:00:00+00:00</published><updated>2022-03-08T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec3</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec3">&lt;h3 id=&quot;named-entity-recognitionner&quot;&gt;Named Entity Recognition(NER)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Task: &lt;strong&gt;find&lt;/strong&gt; and &lt;strong&gt;classify&lt;/strong&gt; names in text&lt;/li&gt;
  &lt;li&gt;Possible uses:
    &lt;ul&gt;
      &lt;li&gt;Tracking mentions of particular entities in documents&lt;/li&gt;
      &lt;li&gt;For question answering, answers are usually named entities&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Often followed by Named Entity Linking/Canonicalization into Knowledge Base&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;simple-ner-window-classification-using-binary-logistic-classifier&quot;&gt;Simple NER: Window classification using binary logistic classifier&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using word vectors, build a context window of word vectors, then pass through a neural network and feed it to logistic classifier.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;idea: classify each word in its context window of neighboring words&lt;/li&gt;
  &lt;li&gt;Train logistic classifier on hand-labeled data to classify center word {yes/no} for each class based on a concatenation of word vectors in a window
    &lt;ul&gt;
      &lt;li&gt;In practice, we usually use multi-class softmax&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Example: Classify “Paris” as +/- location(binary for is_location) in context of sentence with window length 2:&lt;br /&gt;
  sentence $=$ “$\mbox{ the  museums  in  Paris  are  amazing  to  see   . }$”
  $x_{\text{window}} = [x_{\text{museums}}\quad x_{\text{in}}\quad x_{\text{Paris}}\quad x_{\text{are}}\quad x_{\text{amazing}}]^T$&lt;/li&gt;
  &lt;li&gt;Resulting vector $x_{\text{window}} = x\in \mathbb{R}^{5d}$, a column vector.&lt;/li&gt;
  &lt;li&gt;To classify all words: run classifier for each class on the vector centered on each word in the sentence&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec3_0.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;computing-gradients-by-hand&quot;&gt;Computing Gradients by Hand&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Matrix calculus: Fully vectorized gradients
    &lt;ul&gt;
      &lt;li&gt;“Multivariable calculus is just like single-variable calculus if you use matrices”&lt;/li&gt;
      &lt;li&gt;Much faster and more usful than non-vectorized gradients&lt;/li&gt;
      &lt;li&gt;But doing a non-vectorized gradient can be good for intuition; partial derivative for one parameter&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Vector gradient&lt;br /&gt;
  Given a function with &lt;em&gt;1&lt;/em&gt; output and &lt;em&gt;n&lt;/em&gt; inputs&lt;br /&gt;
  $f(x) = f(x_1, x_2, \ldots, x_n)$&lt;br /&gt;
  Its gradient is a vector of partial derivatives w.r.t each input&lt;br /&gt;
  \(\frac{\partial f}{\partial x} = \left[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n} \right]\)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jacobian Matrix: Generalization of the Gradient&lt;br /&gt;
  Given a function with &lt;strong&gt;&lt;em&gt;m&lt;/em&gt; outputs&lt;/strong&gt; and &lt;em&gt;n&lt;/em&gt; inputs&lt;br /&gt;
  $f(x) = [f_1(x_1, x_2, \ldots, x_n), \ldots, f_m(x_1, x_2, \ldots, x_n)]$&lt;br /&gt;
  It’s Jacobian is an &lt;strong&gt;$m \times n$&lt;/strong&gt; matrix of partial derivatives&lt;br /&gt;
  \(\begin{align*}
  \frac{\partial f}{\partial x} = 
  \begin{bmatrix}
  \frac{\partial f_1}{\partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\
  \vdots &amp;amp; \ddots &amp;amp; \vdots \\
  \frac{\partial f_m}{\partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial f_m}{\partial x_n}
  \end{bmatrix}
  \end{align*}\) $\quad\quad$ \(\begin{align*}\left( \frac{\partial f}{\partial x} \right)_{ij} = \frac{\partial f_i}{\partial x_j}\end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Chain Rule
    &lt;ul&gt;
      &lt;li&gt;For composition of one-variable functions: &lt;strong&gt;multiply derivatives&lt;/strong&gt;&lt;br /&gt;
  $z = g(y)$, $y = f(x)$&lt;br /&gt;
  \(\begin{align*}\frac{dz}{dx} = \frac{dz}{dy}\frac{dy}{dx}\end{align*}\)&lt;/li&gt;
      &lt;li&gt;For multiple variables at once: &lt;strong&gt;multiply Jacobians&lt;/strong&gt;&lt;br /&gt;
  $h = f(z)$&lt;br /&gt;
  $z = Wx + b$&lt;br /&gt;
  \(\begin{align*}\frac{\partial h}{\partial x} = \frac{\partial h}{\partial z}\frac{\partial z}{\partial x} = \cdots\end{align*}\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Example Jacobian: Elementwise activation function&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec3_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Other Jacobians&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec3_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Write out the Jacobians&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec3_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Derivative with respect to Matrix: Output shape&lt;br /&gt;
  for $\frac{\partial s}{\partial W}$, output $s$ is a scalar and input $W \in \mathbb{R}^{n\times m}$: &lt;em&gt;1&lt;/em&gt; by &lt;em&gt;nm&lt;/em&gt; Jacobian?&lt;br /&gt;
  $\rightarrow$ is inconvenient to do \(\theta^{\text{new}} = \theta^{\text{old}} - \alpha\nabla_\theta J(\theta)\) gradient descent.
    &lt;ul&gt;
      &lt;li&gt;Instead, use the &lt;strong&gt;shape convention&lt;/strong&gt;: the shape of the gradient is the shape of the parameters&lt;br /&gt;
  &lt;em&gt;n&lt;/em&gt; by &lt;em&gt;m&lt;/em&gt; matrix \(\begin{align*}\frac{\partial s}{\partial W} = 
  \begin{bmatrix} 
  \frac{\partial s}{\partial W_{11}} &amp;amp; \cdots &amp;amp; \frac{\partial s}{\partial W_{1m}} \\ 
  \vdots &amp;amp; \ddots &amp;amp; \vdots \\
  \frac{\partial s}{\partial W_{n1}} &amp;amp; \cdots &amp;amp; \frac{\partial s}{\partial W_{nm}}
  \end{bmatrix}\end{align*}\)&lt;/li&gt;
      &lt;li&gt;$\frac{\partial s}{\partial W} = \delta\frac{\partial z}{\partial W}$&lt;br /&gt;
  $z = Wx + b$&lt;br /&gt;
  \(\begin{align*}
  \frac{\partial s}{\partial W} &amp;amp;= \delta^T x^T \\
  &amp;amp; = \begin{bmatrix} \delta_1 \\ \vdots \\ \delta_n \end{bmatrix} [x_1, \ldots, x_m] = 
  \begin{bmatrix} \delta_1 x_1 &amp;amp; \cdots &amp;amp; \delta_1 x_m \\
  \vdots &amp;amp; \ddots &amp;amp; \vdots \\
  \delta_n x_1 &amp;amp; \cdots &amp;amp; \delta_n x_m \end{bmatrix}
  \end{align*}\)&lt;br /&gt;
  denote that $\delta$ is local error signal at &lt;em&gt;z&lt;/em&gt; and &lt;em&gt;x&lt;/em&gt; is local input signal.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-shape-should-derivatives-be&quot;&gt;What shape should derivatives be?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Two options:
    &lt;ol&gt;
      &lt;li&gt;Use Jacobian form as much as possible, reshape to follow the shape convention at the end:
        &lt;ul&gt;
          &lt;li&gt;at the end transpose $\frac{\partial s}{\partial b}$ to make the derivative a column vector, resulting in $\delta^T$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Always follow the shape convention
        &lt;ul&gt;
          &lt;li&gt;Look at dimensions to figure out when to transpose and/or reorder terms&lt;/li&gt;
          &lt;li&gt;The error message $\delta$ that arrives at a hidden layer has the same dimensionality as that hidden layer&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;backpropagation-chain-rule&quot;&gt;Backpropagation: Chain Rule&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Other trick:&lt;br /&gt;
  &lt;strong&gt;re-use&lt;/strong&gt; derivatives computed for higher layers in computing derivatives for lower layers to minimize computation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec4_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">Named Entity Recognition(NER) Task: find and classify names in text Possible uses: Tracking mentions of particular entities in documents For question answering, answers are usually named entities Often followed by Named Entity Linking/Canonicalization into Knowledge Base Simple NER: Window classification using binary logistic classifier Using word vectors, build a context window of word vectors, then pass through a neural network and feed it to logistic classifier. idea: classify each word in its context window of neighboring words Train logistic classifier on hand-labeled data to classify center word {yes/no} for each class based on a concatenation of word vectors in a window In practice, we usually use multi-class softmax Example: Classify “Paris” as +/- location(binary for is_location) in context of sentence with window length 2: sentence $=$ “$\mbox{ the museums in Paris are amazing to see . }$” $x_{\text{window}} = [x_{\text{museums}}\quad x_{\text{in}}\quad x_{\text{Paris}}\quad x_{\text{are}}\quad x_{\text{amazing}}]^T$ Resulting vector $x_{\text{window}} = x\in \mathbb{R}^{5d}$, a column vector. To classify all words: run classifier for each class on the vector centered on each word in the sentence Computing Gradients by Hand Matrix calculus: Fully vectorized gradients “Multivariable calculus is just like single-variable calculus if you use matrices” Much faster and more usful than non-vectorized gradients But doing a non-vectorized gradient can be good for intuition; partial derivative for one parameter Vector gradient Given a function with 1 output and n inputs $f(x) = f(x_1, x_2, \ldots, x_n)$ Its gradient is a vector of partial derivatives w.r.t each input \(\frac{\partial f}{\partial x} = \left[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n} \right]\) Jacobian Matrix: Generalization of the Gradient Given a function with m outputs and n inputs $f(x) = [f_1(x_1, x_2, \ldots, x_n), \ldots, f_m(x_1, x_2, \ldots, x_n)]$ It’s Jacobian is an $m \times n$ matrix of partial derivatives \(\begin{align*} \frac{\partial f}{\partial x} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial f_1}{\partial x_n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \frac{\partial f_m}{\partial x_1} &amp;amp; \cdots &amp;amp; \frac{\partial f_m}{\partial x_n} \end{bmatrix} \end{align*}\) $\quad\quad$ \(\begin{align*}\left( \frac{\partial f}{\partial x} \right)_{ij} = \frac{\partial f_i}{\partial x_j}\end{align*}\) Chain Rule For composition of one-variable functions: multiply derivatives $z = g(y)$, $y = f(x)$ \(\begin{align*}\frac{dz}{dx} = \frac{dz}{dy}\frac{dy}{dx}\end{align*}\) For multiple variables at once: multiply Jacobians $h = f(z)$ $z = Wx + b$ \(\begin{align*}\frac{\partial h}{\partial x} = \frac{\partial h}{\partial z}\frac{\partial z}{\partial x} = \cdots\end{align*}\) Example Jacobian: Elementwise activation function Other Jacobians Write out the Jacobians Derivative with respect to Matrix: Output shape for $\frac{\partial s}{\partial W}$, output $s$ is a scalar and input $W \in \mathbb{R}^{n\times m}$: 1 by nm Jacobian? $\rightarrow$ is inconvenient to do \(\theta^{\text{new}} = \theta^{\text{old}} - \alpha\nabla_\theta J(\theta)\) gradient descent. Instead, use the shape convention: the shape of the gradient is the shape of the parameters n by m matrix \(\begin{align*}\frac{\partial s}{\partial W} = \begin{bmatrix} \frac{\partial s}{\partial W_{11}} &amp;amp; \cdots &amp;amp; \frac{\partial s}{\partial W_{1m}} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \frac{\partial s}{\partial W_{n1}} &amp;amp; \cdots &amp;amp; \frac{\partial s}{\partial W_{nm}} \end{bmatrix}\end{align*}\) $\frac{\partial s}{\partial W} = \delta\frac{\partial z}{\partial W}$ $z = Wx + b$ \(\begin{align*} \frac{\partial s}{\partial W} &amp;amp;= \delta^T x^T \\ &amp;amp; = \begin{bmatrix} \delta_1 \\ \vdots \\ \delta_n \end{bmatrix} [x_1, \ldots, x_m] = \begin{bmatrix} \delta_1 x_1 &amp;amp; \cdots &amp;amp; \delta_1 x_m \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \delta_n x_1 &amp;amp; \cdots &amp;amp; \delta_n x_m \end{bmatrix} \end{align*}\) denote that $\delta$ is local error signal at z and x is local input signal. What shape should derivatives be? Two options: Use Jacobian form as much as possible, reshape to follow the shape convention at the end: at the end transpose $\frac{\partial s}{\partial b}$ to make the derivative a column vector, resulting in $\delta^T$ Always follow the shape convention Look at dimensions to figure out when to transpose and/or reorder terms The error message $\delta$ that arrives at a hidden layer has the same dimensionality as that hidden layer Backpropagation: Chain Rule Other trick: re-use derivatives computed for higher layers in computing derivatives for lower layers to minimize computation</summary></entry><entry><title type="html">cs224n - Lecture 2. Neural Classifiers</title><link href="http://0.0.0.0:4000/cs224n_lec2" rel="alternate" type="text/html" title="cs224n - Lecture 2. Neural Classifiers" /><published>2022-03-05T00:00:00+00:00</published><updated>2022-03-05T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec2</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec2">&lt;h3 id=&quot;review-main-idea-of-word2vec&quot;&gt;Review: Main idea of word2vec&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Start with random word vectors&lt;/li&gt;
  &lt;li&gt;Iterate through each word in the whole corpus&lt;/li&gt;
  &lt;li&gt;Try to predict surrounding words using word vectors: $P(o\mid c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Learning&lt;/strong&gt;: Update vectors so they can predict actual surrounding words better&lt;/li&gt;
  &lt;li&gt;Doing no more than this, this algorithm learns word vectors that capture well word similarity and meaningful directions in a wordspace.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;A “bag of words” model; doesn’t actually pay any attention to word order or position. The model makes the same predictions at each position; the probability estimate would be the same if it is next to the center word or a bit further away.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We want a model that gives a reasonably high probability estimate to &lt;em&gt;all&lt;/em&gt; words that occur in the context(at all often)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Word2vec maximizes objective function by putting similar words nearby in high dimensional vector space&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimization-gradient-descent&quot;&gt;Optimization: Gradient Descent&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;To learn good word vectors: minimize a cost function $J(\theta)$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Gradient Descent&lt;/strong&gt; is an algorithm to minimize $J(\theta)$ by changing $\theta$&lt;/li&gt;
  &lt;li&gt;idea: from current value of $\theta$, calculate gradient of $J(\theta)$, then take small step in the direction of &lt;em&gt;negative&lt;/em&gt; gradient. Repreat.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_1.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;theta_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;J&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_grad&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;stochastic-gradient-descent&quot;&gt;Stochastic Gradient Descent&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: $J(\theta)$ is a function of &lt;strong&gt;all&lt;/strong&gt; windows in the corpus (often, billions!); so $\nabla_\theta J(\theta)$ is very expensive to compute&lt;/li&gt;
  &lt;li&gt;Solution: Stochastic gradient descent(SGD)
    &lt;ul&gt;
      &lt;li&gt;Repeatedly sample windows, and update after each one, or each small batch&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;theta_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;J&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_grad&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Stochastic gradients with word vectors (Aside)
    &lt;ul&gt;
      &lt;li&gt;iteratively take gradients at each such window for SGD&lt;/li&gt;
      &lt;li&gt;But in each window, we only have at most &lt;em&gt;2m + 1&lt;/em&gt; words,&lt;br /&gt;
  so $\nabla_\theta J(\theta)$ is very sparse:&lt;br /&gt;
  \(\nabla_\theta J_t(\theta) = \begin{bmatrix} 0  \\ \vdots \\ \nabla_{v_{\text{like}}} \\ \vdots 0 \\ \nabla_{u_I} \\ \vdots \\ \nabla_{u_{\text{learning}}} \\ \vdots \end{bmatrix} \in \mathbb{R}^{2dV}\)&lt;/li&gt;
      &lt;li&gt;We might only update the word vectors that actually appear.&lt;/li&gt;
      &lt;li&gt;Solution: either you need sparse matrix update operations to only update certain &lt;strong&gt;rows&lt;/strong&gt;(in most DL packages) of full embedding matrices &lt;em&gt;U&lt;/em&gt; and &lt;em&gt;V&lt;/em&gt;, or you need to keep around a hash for word vectors.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec2_2.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;If you have millions of word vectors and do distributed computing, it is important to not have to send gigantic updates around.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2b-word2vec-algorithm-family-more-details&quot;&gt;2b. Word2vec algorithm family: More details&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Why two vectors? $\rightarrow$ Easier optimization. Average both at the end
    &lt;ul&gt;
      &lt;li&gt;But can implement the algorithm with just one vector per word, and it works slightly better, but it makes the algorithm much more complicated.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Two model variants:
    &lt;ol&gt;
      &lt;li&gt;Skip-grams(SG)&lt;br /&gt;
 Predict context(“outside”) words (position independent) given center word&lt;/li&gt;
      &lt;li&gt;Continuous Bag of Words(CBOW)&lt;br /&gt;
 Predict center word from (bag of) context words&lt;br /&gt;
  &lt;em&gt;We presented: Skip-gram model&lt;/em&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Additional efficiency in training:
    &lt;ul&gt;
      &lt;li&gt;Negative sampling&lt;br /&gt;
  &lt;em&gt;So far: Focus on naive softmax(simpler, but expensive training method)&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-skip-gram-model-with-negative-samplingsgns&quot;&gt;The skip-gram model with negative sampling(SGNS)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The normalization term is computationally expensive, especially on the denominator of $P(o\mid c)$.&lt;/li&gt;
  &lt;li&gt;Main idea: train binary logistic regressions for a true pair (center word and a word in its context window) versus several noise pairs (the center word paired with a random word)&lt;/li&gt;
  &lt;li&gt;From paper: &lt;em&gt;“Distributed Representations of Words and Phrases and their Compositionality” (Mikolov et al. 2013)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Overall objective function(to maximize):&lt;br /&gt;
  \(J(\theta) = \frac{1}{T}\sum_{t=1}^T J_t(\theta)\)&lt;br /&gt;
  \(J_t(\theta) = \log\sigma(u_o^T u_c) + \sum_{i=1}^k \mathbb{E}_{j~P(w)}\left[ \log\sigma(-u_j^T v_c) \right]\)&lt;br /&gt;
  where the logistic/sigmoid function: $\sigma(x) = \frac{1}{1+ e^{-x}}$&lt;/li&gt;
  &lt;li&gt;We maximize the probability of two words co-occuring in first log and minimize probability of noise words:&lt;br /&gt;
  $J_{\text{neg-sample}}(u_o, v_c, U) = -\log \sigma(u_o^T v_c) - \sum_{k\in { \text{K sampled indicies} }} \log \sigma(-u_k^T v_c)$&lt;/li&gt;
  &lt;li&gt;We take &lt;em&gt;k&lt;/em&gt; negative samples (using word probabilities)&lt;/li&gt;
  &lt;li&gt;Maximize probability that real outside word appears, minimize probability that random words appear around center word&lt;/li&gt;
  &lt;li&gt;Another trick: sample with $P(w) = U(w)^{3/4} / Z$, the unigram distribution $U(w)$ raised to the $3/4$ power (We provide this function in the starter code)&lt;/li&gt;
  &lt;li&gt;The power makes less frequent words be sampled more often&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;why-not-capture-co-occurrence-counts-directly&quot;&gt;Why not capture co-occurrence counts directly?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Building a co-occurrence matrix &lt;em&gt;X&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;2 options: windows vs. full document&lt;/li&gt;
      &lt;li&gt;Window: Similar to word2vec, use window around each word and captures some syntactic and semantic information&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec2_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;Word-document co-occurrence matrix will give general topics (all sports terms will have similar entries) learning to “Latent Semantic Analysis”; in tasks like information retrieval&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;co-occurrence-vectors&quot;&gt;Co-occurrence vectors&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Simple count co-occurrence vectors
    &lt;ul&gt;
      &lt;li&gt;Vectors increase in size with vocabulary&lt;/li&gt;
      &lt;li&gt;Very high dimensional: require a lot of storage (though sparse)&lt;/li&gt;
      &lt;li&gt;Subsequent classification models have sparsity issues $\rightarrow$ Models are less robust&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Low-dimensional vectors
    &lt;ul&gt;
      &lt;li&gt;idea: store “most” of the important information in a fixed, small number of directions: a dense vector&lt;/li&gt;
      &lt;li&gt;Usually 25-1000 directions, similar to word2vec&lt;/li&gt;
      &lt;li&gt;How to reduce the dimensionality?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;classic-method-dimensionality-reduction-on-x&quot;&gt;Classic Method: Dimensionality Reduction on X&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Singular Value Decomposition of co-occurrence matrix &lt;em&gt;X&lt;/em&gt;&lt;br /&gt;
  Factorizes &lt;em&gt;X&lt;/em&gt; into $U\Sigma V^T$, where &lt;em&gt;U&lt;/em&gt; and &lt;em&gt;V&lt;/em&gt; are orthonormal&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec2_4.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
  Corresponding to the columns without singular values in $\Sigma$, bottom rows in $V^T$ are ignored. The singular values inside the diagonal matrix $\Sigma$ are ordered from largest down to smallest. Retaining only &lt;em&gt;k&lt;/em&gt; singular values, in order to generalize, the lower dimensional representation $\hat{X}$ is the best rank &lt;em&gt;k&lt;/em&gt; approximation to &lt;em&gt;X&lt;/em&gt;, in terms of least squares.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;hacks-to-x-several-used-in-rohde-et-al-2005-in-coals&quot;&gt;Hacks to X (several used in Rohde et al. 2005 in COALS)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Running an SVD on a raw count co-occurrence matrix works poorly; In the mathematical assumptions of SVD, we are expecting to have normally distributed errors. But there are exceedingly common words like “a”, “the”, and “and”, and there is a very large number of rare words.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Scaling the counts in the cells can help &lt;strong&gt;a lot&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Problem: function words(&lt;em&gt;the, he, has&lt;/em&gt;) are too frequent $\rightarrow$ syntax has too much impact. Some fixes:
        &lt;ul&gt;
          &lt;li&gt;log the frequencies&lt;/li&gt;
          &lt;li&gt;$min(X,t)$, with $t\approx 100$&lt;/li&gt;
          &lt;li&gt;ignore the function words&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Ramped windows that count closer words more than further away words&lt;/li&gt;
  &lt;li&gt;Use Pearson correlations instead of counts, then set negative values to &lt;em&gt;0&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Etc.&lt;/li&gt;
  &lt;li&gt;Result:&lt;br /&gt;
  Interesting semantic patterns emerge in the scaled vectors; something like a word vector analogies.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec2_5.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;towards-glove-count-based-vs-direct-prediction&quot;&gt;Towards GloVe: Count based vs. direct prediction&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_6.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;encoding-meaning-components-in-vector-differences&quot;&gt;Encoding meaning components in vector differences&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Pennington, Socher, and Manning, EMNLP 2014&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;What properties needed to make vector analogies work?&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Crucial insight: Ratios of co-occurrence probabilities can encode meaning components&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_7.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_8.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Q: How can we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space?&lt;/li&gt;
  &lt;li&gt;A: Log-bilinear model: the dot product between two word vectors attempts to approximate the log of the probability of co-occurrence; \(w_i \cdot w_j = \log P(i|j)\)&lt;br /&gt;
  $\rightarrow$ with vector differences \(w_x \cdot (w_a - w_b) = \log \frac{P(x\mid a)}{P(x \mid b)}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;combining-the-best-of-both-worlds-glove&quot;&gt;Combining the best of both worlds: GloVe&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Pennington, Socher, and Manning, EMNLP 2014&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;With \(w_i \cdot w_j = \log P(i|j)\),&lt;br /&gt;
  explicit loss function \(J = \sum_{i,j=1}^V f(X_{ij})(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2\)&lt;br /&gt;
  to make the dot product to be similar to the log of the co-occurrence. To not have very common words dominate, capped the effect of high word counts using $f$ function. Optimize &lt;em&gt;J&lt;/em&gt; directly on the co-occurrence count matrix.
    &lt;ul&gt;
      &lt;li&gt;Fast training&lt;/li&gt;
      &lt;li&gt;Scalable to hugh corpora&lt;/li&gt;
      &lt;li&gt;Good performance even with small corpus and small vectors&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-evaluate-word-vectors&quot;&gt;How to evaluate word vectors?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Related to general evaluation in NLP: intrinsic vs. extrinsic&lt;/li&gt;
  &lt;li&gt;Intrinsic:
    &lt;ul&gt;
      &lt;li&gt;Evaluation on a specific/intermediate subtask&lt;/li&gt;
      &lt;li&gt;Fast to compute&lt;/li&gt;
      &lt;li&gt;Helps to understand that system&lt;/li&gt;
      &lt;li&gt;Not clear if really helpful unless correlation to real task is established&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Extrinsic:
    &lt;ul&gt;
      &lt;li&gt;Evaluation on a real task&lt;/li&gt;
      &lt;li&gt;Can take a long time to compute accuracy&lt;/li&gt;
      &lt;li&gt;Unclear if the subsystem is the problem or its interaction or other subsystems&lt;/li&gt;
      &lt;li&gt;If replacing exactly one subsystem with another improves accuracy $\rightarrow$ Winning!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intrinsic-word-vector-evaluation&quot;&gt;Intrinsic word vector evaluation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Word Vector Analogies&lt;br /&gt;
  |a:b :: c:?| $\rightarrow$ $d = \text{argmax}_i \frac{(x_b -x_a +x_c)^T x_i}{\lVert x_b -x_a +x_c\rVert}$&lt;br /&gt;
  (e.g. man:woman :: king:?)&lt;/li&gt;
  &lt;li&gt;Evalute word vectors by how well their cosine distance after addition captures intuitive semantic and syntactic analogy questions&lt;/li&gt;
  &lt;li&gt;Discarding the input words from the search!&lt;/li&gt;
  &lt;li&gt;Problem: What if the information is there but not linear?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;glove-visualizations&quot;&gt;GloVe Visualizations&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_9.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;analogy-evaluation-and-hyperparameters&quot;&gt;Analogy evaluation and hyperparameters&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_10.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_11.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;another-intrinsic-word-vector-evaluation&quot;&gt;Another intrinsic word vector evaluation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Word vector distances and their correlation with human judgements&lt;/li&gt;
  &lt;li&gt;Example dataset: WordSim353 &lt;a href=&quot;http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/&quot;&gt;http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_12.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_13.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Some ideas from Glove paper have been shown to improve skip-gram(SG) model also (e.g., average both vectors)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;extrinsic-word-vector-evaluation&quot;&gt;Extrinsic word vector evaluation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;All subsequent NLP tasks&lt;/li&gt;
  &lt;li&gt;One example where good word vectors should help directly: &lt;strong&gt;named entity recognition&lt;/strong&gt;: identifying references to a person, organization or location&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_14.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;word-senses&quot;&gt;Word senses&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Most words have lots of meanings
    &lt;ul&gt;
      &lt;li&gt;Especially common words&lt;/li&gt;
      &lt;li&gt;Especially words that have existed for a long time&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Does one vector caputre all these meanings or do we have a mess?&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;linear-algebric-structure-of-word-senses-with-applications-to-polysemy-arora--ma--tacl-2018&quot;&gt;&lt;em&gt;“Linear Algebric Structure of Word Senses, with Applications to Polysemy”, Arora, …, Ma, …, TACL 2018&lt;/em&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Different senses of a word reside in a linear superposition(weighted sum) in standard word embeddings like word2vec&lt;/li&gt;
  &lt;li&gt;\(v_{\text{pike}} = \alpha_1 v_{\text{pike}_2} + \alpha_2 v_{\text{pike}_2} + \alpha_3 v_{\text{pike}_3}\)&lt;br /&gt;
  where $\alpha_1 = \frac{f_1}{f_1+f_2+f_3}$, etc., for frequency &lt;em&gt;f&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Surprising result:&lt;br /&gt;
  Commonly, it is impossible to reconstruct the original components from their sum, but, because of ideas from &lt;em&gt;sparse coding&lt;/em&gt; you can actually separate out the senses(providing they are relatively common)!&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_15.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">Review: Main idea of word2vec Start with random word vectors Iterate through each word in the whole corpus Try to predict surrounding words using word vectors: $P(o\mid c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}$ Learning: Update vectors so they can predict actual surrounding words better Doing no more than this, this algorithm learns word vectors that capture well word similarity and meaningful directions in a wordspace. A “bag of words” model; doesn’t actually pay any attention to word order or position. The model makes the same predictions at each position; the probability estimate would be the same if it is next to the center word or a bit further away. We want a model that gives a reasonably high probability estimate to all words that occur in the context(at all often) Word2vec maximizes objective function by putting similar words nearby in high dimensional vector space Optimization: Gradient Descent To learn good word vectors: minimize a cost function $J(\theta)$ Gradient Descent is an algorithm to minimize $J(\theta)$ by changing $\theta$ idea: from current value of $\theta$, calculate gradient of $J(\theta)$, then take small step in the direction of negative gradient. Repreat. Algorithm: while True: theta_grad = evaluate_gradient(J, corpus, theta) theta = theta - alpha * theta_grad Stochastic Gradient Descent Problem: $J(\theta)$ is a function of all windows in the corpus (often, billions!); so $\nabla_\theta J(\theta)$ is very expensive to compute Solution: Stochastic gradient descent(SGD) Repeatedly sample windows, and update after each one, or each small batch Algorithm: while True: window = sample_window(corpus) theta_grad = evaluate_gradient(J, window, theta) theta = theta - alpha * theta_grad Stochastic gradients with word vectors (Aside) iteratively take gradients at each such window for SGD But in each window, we only have at most 2m + 1 words, so $\nabla_\theta J(\theta)$ is very sparse: \(\nabla_\theta J_t(\theta) = \begin{bmatrix} 0 \\ \vdots \\ \nabla_{v_{\text{like}}} \\ \vdots 0 \\ \nabla_{u_I} \\ \vdots \\ \nabla_{u_{\text{learning}}} \\ \vdots \end{bmatrix} \in \mathbb{R}^{2dV}\) We might only update the word vectors that actually appear. Solution: either you need sparse matrix update operations to only update certain rows(in most DL packages) of full embedding matrices U and V, or you need to keep around a hash for word vectors. If you have millions of word vectors and do distributed computing, it is important to not have to send gigantic updates around. 2b. Word2vec algorithm family: More details Why two vectors? $\rightarrow$ Easier optimization. Average both at the end But can implement the algorithm with just one vector per word, and it works slightly better, but it makes the algorithm much more complicated. Two model variants: Skip-grams(SG) Predict context(“outside”) words (position independent) given center word Continuous Bag of Words(CBOW) Predict center word from (bag of) context words We presented: Skip-gram model Additional efficiency in training: Negative sampling So far: Focus on naive softmax(simpler, but expensive training method) The skip-gram model with negative sampling(SGNS) The normalization term is computationally expensive, especially on the denominator of $P(o\mid c)$. Main idea: train binary logistic regressions for a true pair (center word and a word in its context window) versus several noise pairs (the center word paired with a random word) From paper: “Distributed Representations of Words and Phrases and their Compositionality” (Mikolov et al. 2013) Overall objective function(to maximize): \(J(\theta) = \frac{1}{T}\sum_{t=1}^T J_t(\theta)\) \(J_t(\theta) = \log\sigma(u_o^T u_c) + \sum_{i=1}^k \mathbb{E}_{j~P(w)}\left[ \log\sigma(-u_j^T v_c) \right]\) where the logistic/sigmoid function: $\sigma(x) = \frac{1}{1+ e^{-x}}$ We maximize the probability of two words co-occuring in first log and minimize probability of noise words: $J_{\text{neg-sample}}(u_o, v_c, U) = -\log \sigma(u_o^T v_c) - \sum_{k\in { \text{K sampled indicies} }} \log \sigma(-u_k^T v_c)$ We take k negative samples (using word probabilities) Maximize probability that real outside word appears, minimize probability that random words appear around center word Another trick: sample with $P(w) = U(w)^{3/4} / Z$, the unigram distribution $U(w)$ raised to the $3/4$ power (We provide this function in the starter code) The power makes less frequent words be sampled more often Why not capture co-occurrence counts directly? Building a co-occurrence matrix X 2 options: windows vs. full document Window: Similar to word2vec, use window around each word and captures some syntactic and semantic information Word-document co-occurrence matrix will give general topics (all sports terms will have similar entries) learning to “Latent Semantic Analysis”; in tasks like information retrieval Co-occurrence vectors Simple count co-occurrence vectors Vectors increase in size with vocabulary Very high dimensional: require a lot of storage (though sparse) Subsequent classification models have sparsity issues $\rightarrow$ Models are less robust Low-dimensional vectors idea: store “most” of the important information in a fixed, small number of directions: a dense vector Usually 25-1000 directions, similar to word2vec How to reduce the dimensionality? Classic Method: Dimensionality Reduction on X Singular Value Decomposition of co-occurrence matrix X Factorizes X into $U\Sigma V^T$, where U and V are orthonormal Corresponding to the columns without singular values in $\Sigma$, bottom rows in $V^T$ are ignored. The singular values inside the diagonal matrix $\Sigma$ are ordered from largest down to smallest. Retaining only k singular values, in order to generalize, the lower dimensional representation $\hat{X}$ is the best rank k approximation to X, in terms of least squares. Hacks to X (several used in Rohde et al. 2005 in COALS) Running an SVD on a raw count co-occurrence matrix works poorly; In the mathematical assumptions of SVD, we are expecting to have normally distributed errors. But there are exceedingly common words like “a”, “the”, and “and”, and there is a very large number of rare words. Scaling the counts in the cells can help a lot Problem: function words(the, he, has) are too frequent $\rightarrow$ syntax has too much impact. Some fixes: log the frequencies $min(X,t)$, with $t\approx 100$ ignore the function words Ramped windows that count closer words more than further away words Use Pearson correlations instead of counts, then set negative values to 0 Etc. Result: Interesting semantic patterns emerge in the scaled vectors; something like a word vector analogies. Towards GloVe: Count based vs. direct prediction Encoding meaning components in vector differences Pennington, Socher, and Manning, EMNLP 2014 What properties needed to make vector analogies work? Crucial insight: Ratios of co-occurrence probabilities can encode meaning components Q: How can we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space? A: Log-bilinear model: the dot product between two word vectors attempts to approximate the log of the probability of co-occurrence; \(w_i \cdot w_j = \log P(i|j)\) $\rightarrow$ with vector differences \(w_x \cdot (w_a - w_b) = \log \frac{P(x\mid a)}{P(x \mid b)}\) Combining the best of both worlds: GloVe Pennington, Socher, and Manning, EMNLP 2014 With \(w_i \cdot w_j = \log P(i|j)\), explicit loss function \(J = \sum_{i,j=1}^V f(X_{ij})(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2\) to make the dot product to be similar to the log of the co-occurrence. To not have very common words dominate, capped the effect of high word counts using $f$ function. Optimize J directly on the co-occurrence count matrix. Fast training Scalable to hugh corpora Good performance even with small corpus and small vectors How to evaluate word vectors? Related to general evaluation in NLP: intrinsic vs. extrinsic Intrinsic: Evaluation on a specific/intermediate subtask Fast to compute Helps to understand that system Not clear if really helpful unless correlation to real task is established Extrinsic: Evaluation on a real task Can take a long time to compute accuracy Unclear if the subsystem is the problem or its interaction or other subsystems If replacing exactly one subsystem with another improves accuracy $\rightarrow$ Winning! Intrinsic word vector evaluation Word Vector Analogies |a:b :: c:?| $\rightarrow$ $d = \text{argmax}_i \frac{(x_b -x_a +x_c)^T x_i}{\lVert x_b -x_a +x_c\rVert}$ (e.g. man:woman :: king:?) Evalute word vectors by how well their cosine distance after addition captures intuitive semantic and syntactic analogy questions Discarding the input words from the search! Problem: What if the information is there but not linear? GloVe Visualizations Analogy evaluation and hyperparameters Another intrinsic word vector evaluation Word vector distances and their correlation with human judgements Example dataset: WordSim353 http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/ Some ideas from Glove paper have been shown to improve skip-gram(SG) model also (e.g., average both vectors) Extrinsic word vector evaluation All subsequent NLP tasks One example where good word vectors should help directly: named entity recognition: identifying references to a person, organization or location Word senses Most words have lots of meanings Especially common words Especially words that have existed for a long time Does one vector caputre all these meanings or do we have a mess? “Linear Algebric Structure of Word Senses, with Applications to Polysemy”, Arora, …, Ma, …, TACL 2018 Different senses of a word reside in a linear superposition(weighted sum) in standard word embeddings like word2vec \(v_{\text{pike}} = \alpha_1 v_{\text{pike}_2} + \alpha_2 v_{\text{pike}_2} + \alpha_3 v_{\text{pike}_3}\) where $\alpha_1 = \frac{f_1}{f_1+f_2+f_3}$, etc., for frequency f Surprising result: Commonly, it is impossible to reconstruct the original components from their sum, but, because of ideas from sparse coding you can actually separate out the senses(providing they are relatively common)!</summary></entry><entry><title type="html">DevEnv Setup</title><link href="http://0.0.0.0:4000/DevEnv_Setup" rel="alternate" type="text/html" title="DevEnv Setup" /><published>2022-03-02T00:00:00+00:00</published><updated>2022-03-02T00:00:00+00:00</updated><id>http://0.0.0.0:4000/DevEnv_Setup</id><content type="html" xml:base="http://0.0.0.0:4000/DevEnv_Setup">&lt;ul&gt;
  &lt;li&gt;For purpose of setting local development environment on a new SSD storage, followed instructions below. Post for later use.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Document &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl&quot;&gt;Enable NVIDIA CUDA on WSL&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Install stable version of Windows 11&lt;/li&gt;
      &lt;li&gt;Enable WSL, install Ubuntu(20.04.3 LTS)&lt;br /&gt;
  On Windows &lt;strong&gt;Settings&lt;/strong&gt; app, select &lt;strong&gt;Check for updates&lt;/strong&gt; in the &lt;strong&gt;Windows Update&lt;/strong&gt; section and get the latest kernel(5.10.43.3 or higher)&lt;br /&gt;
  To check the version, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wsl cat /proc/version&lt;/code&gt; command in &lt;strong&gt;Powershell&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Install the GPU driver&lt;br /&gt;
  Download and install the NVIDIA CUDA enabled driver for WSL&lt;br /&gt;
  (Studio version: 511.65-desktop-win10-win11-64bit-international-nsd-dch-whql)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Install Docker Desktop app on Windows
    &lt;ul&gt;
      &lt;li&gt;Run:&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Result:
        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Windowed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Simulation&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Single&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;floating&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simulation&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Devices&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;used&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simulation&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;GPU&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Device&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Ampere&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capability&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;8.6&lt;/span&gt;

  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Compute&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;8.6&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CUDA&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NVIDIA&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GeForce&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RTX&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3070&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;mi&quot;&gt;47104&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bodies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;40.275&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ms&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;550.910&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;billion&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interactions&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;11018.199&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;single&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GFLOP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flops&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interaction&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Setting Docker image for TensorFlow-GPU
    &lt;ul&gt;
      &lt;li&gt;Pull the latest TensorFlow-GPU image
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker run -it --gpus all tensorflow/tensorflow:latest-gpu&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Install Anaconda on user:root(ref: &lt;a href=&quot;https://omhdydy.tistory.com/6&quot;&gt;blog&lt;/a&gt;)
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  # update and install prerequisites
  apt-get update
  apt-get install wget
  # get proper version of anaconda3
  wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh 
  sh Anaconda3-2021.11-Linux-x86_64.sh
  exec bash
  # create anaconda environment and install libraries(for stability)
  conda create -n !env_name pip python=3.7
  conda activate !env_name
  pip install tensorflow-gpu
  pip install ipykernel
  python -m ipykernel install --user --name !env_name --display-name !dispaly_name
  pip install jupyter
  # escape with Ctrl + p, Ctrl + q
  docker commit -m &quot;!message&quot; !container_id !image_name:tag
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;(Optional) Install TensorFlow Object Detection API
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  apt-get install git
  git clone --depth 1 https://github.com/tensorflow/models
  cd models/research/
  apt install -y protobuf-compiler
  # found a symlink err, fixed with running:
  # ln -s /usr/lib/x86_64-linux-gnu/libcuda.so.1 /usr/local/cuda/lib64/libcuda.so
  # and rerun: apt install -y protobuf-compiler
  protoc object_detection/protos/*.proto --python_out=.
  cd models/research/
  # install Object Detection API
  cp object_detection/packages/tf2/setup.py .
  python -m pip install --use-feature=2020-resolver .
  # run test
  python object_detection/builders/model_builder_tf2_test.py
  # rm -rf models (if desired)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;(Optional) Install JupyterLab Extensions and enable TensorBoard within Jupyterlab-Docker container&lt;br /&gt;
  Stable versions worked on my local environment
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  curl -sL https://deb.nodesource.com/setup_12.x | bash -
  apt-get install -y nodejs
  node --version # check: v12.22.10
  npm --version # check: 6.14.16
  pip install jupyterlab==2.3.2 
  pip install git+https://github.com/cliffwoolley/jupyter_tensorboard.git 
  pip install tensorboard==2.2
  jupyter labextension install jupyterlab_tensorboard
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
        &lt;p&gt;Commit and run container with any open port for JupyterLab&lt;br /&gt;
  e.g.&lt;/p&gt;
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  docker run --rm -it --gpus all -p 4000:4000 !image_name:tag
  conda activate !env_name
  jupyter lab --ip='0.0.0.0' --port=4000 --no-browser --allow-root
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
        &lt;p&gt;On your &lt;strong&gt;Windows&lt;/strong&gt;, open &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost:4000&lt;/code&gt; with browser&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Setting Docker image for Jekyll blog
    &lt;ul&gt;
      &lt;li&gt;Get latest Ubuntu image and install packages
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  docker run --rm -it -p 4000:4000 ubuntu
  apt-get update
  apt-get install git
  apt-get install vim ruby-full build-essential zlib1g-dev -y

  echo '# Install Ruby Gems to ~/gems' &amp;gt;&amp;gt; ~/.bashrc
  echo 'export GEM_HOME=&quot;$HOME/gems&quot;' &amp;gt;&amp;gt; ~/.bashrc
  echo 'export PATH=&quot;$HOME/gems/bin:$PATH&quot;' &amp;gt;&amp;gt; ~/.bashrc
  source ~/.bashrc
  gem install jekyll bundler
		
  jekyll -v # 4.2.1
  mkdir -p /root/blog_home
  echo 'export BLOG_HOME=&quot;/root/blog_home&quot;' &amp;gt;&amp;gt; ~/.bashrc
  echo '# Start jekyll' &amp;gt;&amp;gt; ~/.bashrc
  source ~/.bashrc
  cd $BLOG_HOME # Get any jekyll blog template here
  rm Gemfile.lock # if needed
  bundle install
  bundle exec jekyll serve --host 0.0.0.0 -p 4000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Darron Kwon</name></author><category term="blog" /><summary type="html">For purpose of setting local development environment on a new SSD storage, followed instructions below. Post for later use. Document Enable NVIDIA CUDA on WSL Install stable version of Windows 11 Enable WSL, install Ubuntu(20.04.3 LTS) On Windows Settings app, select Check for updates in the Windows Update section and get the latest kernel(5.10.43.3 or higher) To check the version, run wsl cat /proc/version command in Powershell. Install the GPU driver Download and install the NVIDIA CUDA enabled driver for WSL (Studio version: 511.65-desktop-win10-win11-64bit-international-nsd-dch-whql) Install Docker Desktop app on Windows Run: docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark Result: &amp;gt; Windowed mode &amp;gt; Simulation data stored in video memory &amp;gt; Single precision floating point simulation &amp;gt; 1 Devices used for simulation GPU Device 0: &quot;Ampere&quot; with compute capability 8.6 &amp;gt; Compute 8.6 CUDA device: [NVIDIA GeForce RTX 3070] 47104 bodies, total time for 10 iterations: 40.275 ms = 550.910 billion interactions per second = 11018.199 single-precision GFLOP/s at 20 flops per interaction Setting Docker image for TensorFlow-GPU Pull the latest TensorFlow-GPU image docker run -it --gpus all tensorflow/tensorflow:latest-gpu Install Anaconda on user:root(ref: blog) # update and install prerequisites apt-get update apt-get install wget # get proper version of anaconda3 wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh sh Anaconda3-2021.11-Linux-x86_64.sh exec bash # create anaconda environment and install libraries(for stability) conda create -n !env_name pip python=3.7 conda activate !env_name pip install tensorflow-gpu pip install ipykernel python -m ipykernel install --user --name !env_name --display-name !dispaly_name pip install jupyter # escape with Ctrl + p, Ctrl + q docker commit -m &quot;!message&quot; !container_id !image_name:tag (Optional) Install TensorFlow Object Detection API apt-get install git git clone --depth 1 https://github.com/tensorflow/models cd models/research/ apt install -y protobuf-compiler # found a symlink err, fixed with running: # ln -s /usr/lib/x86_64-linux-gnu/libcuda.so.1 /usr/local/cuda/lib64/libcuda.so # and rerun: apt install -y protobuf-compiler protoc object_detection/protos/*.proto --python_out=. cd models/research/ # install Object Detection API cp object_detection/packages/tf2/setup.py . python -m pip install --use-feature=2020-resolver . # run test python object_detection/builders/model_builder_tf2_test.py # rm -rf models (if desired) (Optional) Install JupyterLab Extensions and enable TensorBoard within Jupyterlab-Docker container Stable versions worked on my local environment curl -sL https://deb.nodesource.com/setup_12.x | bash - apt-get install -y nodejs node --version # check: v12.22.10 npm --version # check: 6.14.16 pip install jupyterlab==2.3.2 pip install git+https://github.com/cliffwoolley/jupyter_tensorboard.git pip install tensorboard==2.2 jupyter labextension install jupyterlab_tensorboard Commit and run container with any open port for JupyterLab e.g. docker run --rm -it --gpus all -p 4000:4000 !image_name:tag conda activate !env_name jupyter lab --ip='0.0.0.0' --port=4000 --no-browser --allow-root On your Windows, open localhost:4000 with browser Setting Docker image for Jekyll blog Get latest Ubuntu image and install packages docker run --rm -it -p 4000:4000 ubuntu apt-get update apt-get install git apt-get install vim ruby-full build-essential zlib1g-dev -y echo '# Install Ruby Gems to ~/gems' &amp;gt;&amp;gt; ~/.bashrc echo 'export GEM_HOME=&quot;$HOME/gems&quot;' &amp;gt;&amp;gt; ~/.bashrc echo 'export PATH=&quot;$HOME/gems/bin:$PATH&quot;' &amp;gt;&amp;gt; ~/.bashrc source ~/.bashrc gem install jekyll bundler jekyll -v # 4.2.1 mkdir -p /root/blog_home echo 'export BLOG_HOME=&quot;/root/blog_home&quot;' &amp;gt;&amp;gt; ~/.bashrc echo '# Start jekyll' &amp;gt;&amp;gt; ~/.bashrc source ~/.bashrc cd $BLOG_HOME # Get any jekyll blog template here rm Gemfile.lock # if needed bundle install bundle exec jekyll serve --host 0.0.0.0 -p 4000</summary></entry><entry><title type="html">cs224n - Lecture 1. Word Vectors</title><link href="http://0.0.0.0:4000/cs224n_lec1" rel="alternate" type="text/html" title="cs224n - Lecture 1. Word Vectors" /><published>2022-03-02T00:00:00+00:00</published><updated>2022-03-02T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec1</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec1">&lt;h3 id=&quot;objectives&quot;&gt;Objectives&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;The foundations of the effective modern methods for deep learning applied to NLP; from basics to key methods used in NLP: RNN, Attention, Transformers, etc.)&lt;/li&gt;
  &lt;li&gt;A big picture understanding of human languages and the difficulties in understanding and producing them&lt;/li&gt;
  &lt;li&gt;An understanding of and ability to build systems (in PyTorch) for some of the major problems in NLP: Word meaning, dependency parsing, machine translation, question answering&lt;br /&gt;
&lt;!-- *CS145: translate human language sentences into SQL --&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;NLP tasks:&lt;br /&gt;
  Easy: Spell Checking, Keyword Search, Finding Synonyms&lt;br /&gt;
  Medium: Parsing information from websites, documents, etc.&lt;br /&gt;
  Hard: Machine Translation, Semantic Analysis, Coreference, QA&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;word-meaning&quot;&gt;Word meaning&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Commonest linguistic way of thinking of meaning:&lt;br /&gt;
  signifier (symbol) $\Leftrightarrow$ signified (idea or thing)&lt;br /&gt;
  $=$ denotational semantics&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Common NLP solution&lt;/strong&gt;: Use, e.g., &lt;em&gt;WordNet&lt;/em&gt;, a thesaurus containing lists of &lt;strong&gt;synonym sets&lt;/strong&gt; and &lt;strong&gt;hypernyms&lt;/strong&gt;(“is a” relationships).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Problems with resources like WordNet&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Great as a resource but missing nuance&lt;/li&gt;
      &lt;li&gt;Missing new meanings of words; impossible to keep up-to-date&lt;/li&gt;
      &lt;li&gt;Subjective&lt;/li&gt;
      &lt;li&gt;Requires human labor to create and adapt&lt;/li&gt;
      &lt;li&gt;Can’t compute accurate word similarity&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;representing-words-as-discrete-symbols&quot;&gt;Representing words as discrete symbols&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;In traditional NLP, we regard words as discrete symbols - a &lt;em&gt;localist&lt;/em&gt; representation.&lt;br /&gt;
  $\rightarrow$ in a statistical machine learning systems, such symbols for words are separately represented by one-hot vectors. Thus we need to have huge vector dimension corresponding to the number of words in vocabulary.
    &lt;ul&gt;
      &lt;li&gt;But with discrete symbols, two vectors are orthogonal and there is no natural notion of similarity for one-hot vectors.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Solution:
    &lt;ul&gt;
      &lt;li&gt;Could try  to rely on WordNet’s list of synonyms to get similarity?&lt;br /&gt;
  But it is well-known to fail badly; incompleteness, etc.&lt;/li&gt;
      &lt;li&gt;Instead: learn to encode similarity in the vecotr themselves.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;representing-words-by-their-context&quot;&gt;Representing words by their context&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Distributional semantics: &lt;strong&gt;A word’s meaning is given by the words that frequently appear close-by&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;“You shall know a word by the company it keeps”&lt;/em&gt; (J. R. Firth 1957:11)&lt;/li&gt;
      &lt;li&gt;One of the most successful ideas of modern statistical NLP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;When a word &lt;em&gt;w&lt;/em&gt; appears in a text, its &lt;strong&gt;context&lt;/strong&gt; is the set of words that appear nearby(within a fixed-size window).&lt;/li&gt;
  &lt;li&gt;Use the many contexts of &lt;em&gt;w&lt;/em&gt; to build up a representation of &lt;em&gt;w&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;word-vectors&quot;&gt;Word vectors&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note: as a &lt;em&gt;distributed&lt;/em&gt; representation, &lt;em&gt;word vectors&lt;/em&gt; are also called &lt;em&gt;word embeddings&lt;/em&gt; or &lt;em&gt;(neural) words representations&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;word2vec&quot;&gt;Word2vec&lt;/h2&gt;
&lt;h3 id=&quot;word2vec-overview&quot;&gt;Word2vec: overview&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Word2vec&lt;/em&gt;(Mikolov et al. 2013) is a framework for learning word vectors&lt;/li&gt;
  &lt;li&gt;idea:
    &lt;ul&gt;
      &lt;li&gt;We have a large corpus(“body”) of text&lt;/li&gt;
      &lt;li&gt;Every word in a fixed vocabulary is representated by a vector&lt;/li&gt;
      &lt;li&gt;Go through each position &lt;em&gt;t&lt;/em&gt; in the text, which has a center word &lt;em&gt;c&lt;/em&gt; and context(“outside”) words &lt;em&gt;o&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Use the similarity of the word vectors for &lt;em&gt;c&lt;/em&gt; and &lt;em&gt;o&lt;/em&gt; to calculate the probability of &lt;em&gt;o&lt;/em&gt; given &lt;em&gt;c&lt;/em&gt;(or vice versa)&lt;/li&gt;
      &lt;li&gt;Keep adjusting the word vectors to maximize this probability&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;We can learn these word vectors from just a big pile of text by doing this distributional similarity task of being able to predict what words occur in the context of other words.&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Example windows and process for computing $P(w_{t+j}|w_t)$&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec1_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;word2vec-objective-function&quot;&gt;Word2vec: objective function&lt;/h3&gt;
&lt;p&gt;For each position $t = 1, \ldots, T$, predict context words within a window of fixed size &lt;em&gt;m&lt;/em&gt;, given center word $w_j$. Data likelihood:&lt;br /&gt;
\(\begin{align*}
L(\theta) = \prod_{t=1}^T \prod_{\substack{-m\leqq j\leqq m \\ j\ne 0}} P(w_{t+j}|w_t;\theta)
\end{align*}\)&lt;br /&gt;
where $\theta$ is all variables to be optimized.&lt;/p&gt;

&lt;p&gt;The objective function $J(\theta)$ is the (average) negative log likelihood:&lt;br /&gt;
\(\begin{align*}
J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T}\sum_{t=1}^T \sum_{\substack{-m\leqq j\leqq m \\ j\ne 0}}\log P(w_{t+j}|w_t;\theta)
\end{align*}\)&lt;br /&gt;
Minimizing objective function $\Leftrightarrow$ Maximizing predictive accuracy&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Question: How to calculate $P(w_{t+j}\vert w_t;\theta)$?&lt;/li&gt;
  &lt;li&gt;Answer: We will &lt;em&gt;use&lt;/em&gt; two vectors per word &lt;em&gt;w&lt;/em&gt;:
    &lt;ul&gt;
      &lt;li&gt;$v_w$ when &lt;em&gt;w&lt;/em&gt; is a center word&lt;/li&gt;
      &lt;li&gt;$u_w$ when &lt;em&gt;w&lt;/em&gt; is a context word&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Then for a centor word &lt;em&gt;c&lt;/em&gt; and a context word &lt;em&gt;o&lt;/em&gt;:&lt;br /&gt;
  \(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;word2vec-prediction-function&quot;&gt;Word2vec: prediction function&lt;/h3&gt;
&lt;p&gt;\(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\)&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$u_w^T v_c$: Dot product compares similarity of &lt;em&gt;o&lt;/em&gt; and &lt;em&gt;c&lt;/em&gt;.&lt;br /&gt;
 $u^T v = u\ .v = \sum_{i=1}^n u_i v_i$&lt;br /&gt;
 Larger dot product = larger probability&lt;/li&gt;
  &lt;li&gt;$\exp$: Exponentiation makes anything positive&lt;/li&gt;
  &lt;li&gt;$\sum_{w \in V}\exp(u_w^T v_c)$: Normalize over entire vocabulary to give probability distribution.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This is an example of the softmax function $\mathbb{R}^n \rightarrow (0,1)^n$(Open region) that maps arbitary values $x_i$ to a probability distribution $p_i$&lt;br /&gt;
  \(\mbox{softmax}(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^n\exp(x_j)} = p_i\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To train the model: Optimize value of parameters to minimize loss&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Recall: $\theta$ represents all the model parameters, in one long vector&lt;/li&gt;
      &lt;li&gt;In our case, with &lt;em&gt;d&lt;/em&gt;-dimensional vectors and &lt;em&gt;V&lt;/em&gt;-many words, we have: $\theta \in \mathbb{R}^{2dV}$&lt;/li&gt;
      &lt;li&gt;Remember: every word has two vectors&lt;/li&gt;
      &lt;li&gt;We optimize these parameters by walking down the gradient(gradient descent)&lt;/li&gt;
      &lt;li&gt;We compute &lt;strong&gt;all&lt;/strong&gt; vector gradients&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;word2vec-derivations-of-gradient&quot;&gt;Word2vec derivations of gradient&lt;/h3&gt;
&lt;p&gt;\(\begin{align*}
J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T}\sum_{t=1}^T \sum_{\substack{-m\leqq j\leqq m \\ j\ne 0}}\log P(w_{t+j}|w_t;\theta)
\end{align*}\)&lt;br /&gt;
\(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\)&lt;/p&gt;

&lt;p&gt;\(\frac{\partial}{\partial v_c} \log \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)} = \underbrace{\frac{\partial}{\partial v_c} \log \exp(u_o^T v_c)}_{(1)} - \underbrace{\frac{\partial}{\partial v_c} \log \sum_{w=1}^V \exp(u_w^T v_c)}_{(2)}\)&lt;br /&gt;
\(\begin{align*}\cdots (1) &amp;amp;= \frac{\partial}{\partial v_c} u_o^T v_c \\
							&amp;amp;= u_o \end{align*}\)&lt;br /&gt;
\(\begin{align*}\cdots (2) &amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \frac{\partial}{\partial v_c} \exp(u_x^T v_c) \\
							&amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \exp(u_x^T v_c) \cdot \frac{\partial}{\partial v_c} u_x^T v_c \\
							&amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \exp(u_x^T v_c) \cdot u_x \\
							\end{align*} \\\)&lt;br /&gt;
\(\begin{align*}
\frac{\partial}{\partial v_c} \log P(o|c) &amp;amp;= u_o - \frac{\sum_{x=1}^V \exp(u_x^T v_c)u_x}{\sum_{w=1}^V \exp(u_w^T v_c)} \\
	&amp;amp; = u_o - \sum_{x=1}^V \underbrace{\frac{\exp(u_x^T v_c)}{\sum_{w=1}^V \exp(u_w^T v_c)}}_{\mbox{softmax formula}} u_x \\
	&amp;amp; = u_o - \underbrace{\sum_{x=1}^V P(x|c) u_x}_{\mbox{Expectation}} \\
	&amp;amp; = \mbox{observed} - \mbox{expected} \end{align*}\)&lt;/p&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">Objectives The foundations of the effective modern methods for deep learning applied to NLP; from basics to key methods used in NLP: RNN, Attention, Transformers, etc.) A big picture understanding of human languages and the difficulties in understanding and producing them An understanding of and ability to build systems (in PyTorch) for some of the major problems in NLP: Word meaning, dependency parsing, machine translation, question answering NLP tasks: Easy: Spell Checking, Keyword Search, Finding Synonyms Medium: Parsing information from websites, documents, etc. Hard: Machine Translation, Semantic Analysis, Coreference, QA Word meaning Commonest linguistic way of thinking of meaning: signifier (symbol) $\Leftrightarrow$ signified (idea or thing) $=$ denotational semantics Common NLP solution: Use, e.g., WordNet, a thesaurus containing lists of synonym sets and hypernyms(“is a” relationships). Problems with resources like WordNet Great as a resource but missing nuance Missing new meanings of words; impossible to keep up-to-date Subjective Requires human labor to create and adapt Can’t compute accurate word similarity Representing words as discrete symbols In traditional NLP, we regard words as discrete symbols - a localist representation. $\rightarrow$ in a statistical machine learning systems, such symbols for words are separately represented by one-hot vectors. Thus we need to have huge vector dimension corresponding to the number of words in vocabulary. But with discrete symbols, two vectors are orthogonal and there is no natural notion of similarity for one-hot vectors. Solution: Could try to rely on WordNet’s list of synonyms to get similarity? But it is well-known to fail badly; incompleteness, etc. Instead: learn to encode similarity in the vecotr themselves. Representing words by their context Distributional semantics: A word’s meaning is given by the words that frequently appear close-by “You shall know a word by the company it keeps” (J. R. Firth 1957:11) One of the most successful ideas of modern statistical NLP When a word w appears in a text, its context is the set of words that appear nearby(within a fixed-size window). Use the many contexts of w to build up a representation of w Word vectors A dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts Note: as a distributed representation, word vectors are also called word embeddings or (neural) words representations Word2vec Word2vec: overview Word2vec(Mikolov et al. 2013) is a framework for learning word vectors idea: We have a large corpus(“body”) of text Every word in a fixed vocabulary is representated by a vector Go through each position t in the text, which has a center word c and context(“outside”) words o Use the similarity of the word vectors for c and o to calculate the probability of o given c(or vice versa) Keep adjusting the word vectors to maximize this probability We can learn these word vectors from just a big pile of text by doing this distributional similarity task of being able to predict what words occur in the context of other words. Example windows and process for computing $P(w_{t+j}|w_t)$ Word2vec: objective function For each position $t = 1, \ldots, T$, predict context words within a window of fixed size m, given center word $w_j$. Data likelihood: \(\begin{align*} L(\theta) = \prod_{t=1}^T \prod_{\substack{-m\leqq j\leqq m \\ j\ne 0}} P(w_{t+j}|w_t;\theta) \end{align*}\) where $\theta$ is all variables to be optimized. The objective function $J(\theta)$ is the (average) negative log likelihood: \(\begin{align*} J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T}\sum_{t=1}^T \sum_{\substack{-m\leqq j\leqq m \\ j\ne 0}}\log P(w_{t+j}|w_t;\theta) \end{align*}\) Minimizing objective function $\Leftrightarrow$ Maximizing predictive accuracy Question: How to calculate $P(w_{t+j}\vert w_t;\theta)$? Answer: We will use two vectors per word w: $v_w$ when w is a center word $u_w$ when w is a context word Then for a centor word c and a context word o: \(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\) Word2vec: prediction function \(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\) $u_w^T v_c$: Dot product compares similarity of o and c. $u^T v = u\ .v = \sum_{i=1}^n u_i v_i$ Larger dot product = larger probability $\exp$: Exponentiation makes anything positive $\sum_{w \in V}\exp(u_w^T v_c)$: Normalize over entire vocabulary to give probability distribution. This is an example of the softmax function $\mathbb{R}^n \rightarrow (0,1)^n$(Open region) that maps arbitary values $x_i$ to a probability distribution $p_i$ \(\mbox{softmax}(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^n\exp(x_j)} = p_i\) To train the model: Optimize value of parameters to minimize loss Recall: $\theta$ represents all the model parameters, in one long vector In our case, with d-dimensional vectors and V-many words, we have: $\theta \in \mathbb{R}^{2dV}$ Remember: every word has two vectors We optimize these parameters by walking down the gradient(gradient descent) We compute all vector gradients Word2vec derivations of gradient \(\begin{align*} J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T}\sum_{t=1}^T \sum_{\substack{-m\leqq j\leqq m \\ j\ne 0}}\log P(w_{t+j}|w_t;\theta) \end{align*}\) \(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\) \(\frac{\partial}{\partial v_c} \log \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)} = \underbrace{\frac{\partial}{\partial v_c} \log \exp(u_o^T v_c)}_{(1)} - \underbrace{\frac{\partial}{\partial v_c} \log \sum_{w=1}^V \exp(u_w^T v_c)}_{(2)}\) \(\begin{align*}\cdots (1) &amp;amp;= \frac{\partial}{\partial v_c} u_o^T v_c \\ &amp;amp;= u_o \end{align*}\) \(\begin{align*}\cdots (2) &amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \frac{\partial}{\partial v_c} \exp(u_x^T v_c) \\ &amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \exp(u_x^T v_c) \cdot \frac{\partial}{\partial v_c} u_x^T v_c \\ &amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \exp(u_x^T v_c) \cdot u_x \\ \end{align*} \\\) \(\begin{align*} \frac{\partial}{\partial v_c} \log P(o|c) &amp;amp;= u_o - \frac{\sum_{x=1}^V \exp(u_x^T v_c)u_x}{\sum_{w=1}^V \exp(u_w^T v_c)} \\ &amp;amp; = u_o - \sum_{x=1}^V \underbrace{\frac{\exp(u_x^T v_c)}{\sum_{w=1}^V \exp(u_w^T v_c)}}_{\mbox{softmax formula}} u_x \\ &amp;amp; = u_o - \underbrace{\sum_{x=1}^V P(x|c) u_x}_{\mbox{Expectation}} \\ &amp;amp; = \mbox{observed} - \mbox{expected} \end{align*}\)</summary></entry></feed>