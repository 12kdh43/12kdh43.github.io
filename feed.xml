<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2022-03-12T12:43:03+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Darron’s Devlog</title><entry><title type="html">cs224n - Lecture 2. Neural Classifiers</title><link href="http://0.0.0.0:4000/cs224n_lec2" rel="alternate" type="text/html" title="cs224n - Lecture 2. Neural Classifiers" /><published>2022-03-05T00:00:00+00:00</published><updated>2022-03-05T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec2</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec2">&lt;h3 id=&quot;review-main-idea-of-word2vec&quot;&gt;Review: Main idea of word2vec&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Start with random word vectors&lt;/li&gt;
  &lt;li&gt;Iterate through each word in the whole corpus&lt;/li&gt;
  &lt;li&gt;Try to predict surrounding words using word vectors: $P(o\mid c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Learning&lt;/strong&gt;: Update vectors so they can predict actual surrounding words better&lt;/li&gt;
  &lt;li&gt;Doing no more than this, this algorithm learns word vectors that capture well word similarity and meaningful directions in a wordspace.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;A “bag of words” model; doesn’t actually pay any attention to word order or position. The model makes the same predictions at each position; the probability estimate would be the same if it is next to the center word or a bit further away.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We want a model that gives a reasonably high probability estimate to &lt;em&gt;all&lt;/em&gt; words that occur in the context(at all often)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Word2vec maximizes objective function by putting similar words nearby in high dimensional vector space&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimization-gradient-descent&quot;&gt;Optimization: Gradient Descent&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;To learn good word vectors: minimize a cost function $J(\theta)$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Gradient Descent&lt;/strong&gt; is an algorithm to minimize $J(\theta)$ by changing $\theta$&lt;/li&gt;
  &lt;li&gt;idea: from current value of $\theta$, calculate gradient of $J(\theta)$, then take small step in the direction of &lt;em&gt;negative&lt;/em&gt; gradient. Repreat.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_1.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;theta_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;J&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_grad&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;stochastic-gradient-descent&quot;&gt;Stochastic Gradient Descent&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: $J(\theta)$ is a function of &lt;strong&gt;all&lt;/strong&gt; windows in the corpus (often, billions!); so $\nabla_\theta J(\theta)$ is very expensive to compute&lt;/li&gt;
  &lt;li&gt;Solution: Stochastic gradient descent(SGD)
    &lt;ul&gt;
      &lt;li&gt;Repeatedly sample windows, and update after each one, or each small batch&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;theta_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;J&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_grad&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Stochastic gradients with word vectors (Aside)
    &lt;ul&gt;
      &lt;li&gt;iteratively take gradients at each such window for SGD&lt;/li&gt;
      &lt;li&gt;But in each window, we only have at most &lt;em&gt;2m + 1&lt;/em&gt; words,&lt;br /&gt;
  so $\nabla_\theta J(\theta)$ is very sparse:&lt;br /&gt;
  \(\nabla_\theta J_t(\theta) = \begin{bmatrix} 0  \\ \vdots \\ \nabla_{v_{\text{like}}} \\ \vdots 0 \\ \nabla_{u_I} \\ \vdots \\ \nabla_{u_{\text{learning}}} \\ \vdots \end{bmatrix} \in \mathbb{R}^{2dV}\)&lt;/li&gt;
      &lt;li&gt;We might only update the word vectors that actually appear.&lt;/li&gt;
      &lt;li&gt;Solution: either you need sparse matrix update operations to only update certain &lt;strong&gt;rows&lt;/strong&gt;(in most DL packages) of full embedding matrices &lt;em&gt;U&lt;/em&gt; and &lt;em&gt;V&lt;/em&gt;, or you need to keep around a hash for word vectors.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec2_2.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;If you have millions of word vectors and do distributed computing, it is important to not have to send gigantic updates around.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2b-word2vec-algorithm-family-more-details&quot;&gt;2b. Word2vec algorithm family: More details&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Why two vectors? $\rightarrow$ Easier optimization. Average both at the end
    &lt;ul&gt;
      &lt;li&gt;But can implement the algorithm with just one vector per word, and it works slightly better, but it makes the algorithm much more complicated.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Two model variants:
    &lt;ol&gt;
      &lt;li&gt;Skip-grams(SG)&lt;br /&gt;
 Predict context(“outside”) words (position independent) given center word&lt;/li&gt;
      &lt;li&gt;Continuous Bag of Words(CBOW)&lt;br /&gt;
 Predict center word from (bag of) context words&lt;br /&gt;
  &lt;em&gt;We presented: Skip-gram model&lt;/em&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Additional efficiency in training:
    &lt;ul&gt;
      &lt;li&gt;Negative sampling&lt;br /&gt;
  &lt;em&gt;So far: Focus on naive softmax(simpler, but expensive training method)&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-skip-gram-model-with-negative-samplingsgns&quot;&gt;The skip-gram model with negative sampling(SGNS)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The normalization term is computationally expensive, especially on the denominator of $P(o\mid c)$.&lt;/li&gt;
  &lt;li&gt;Main idea: train binary logistic regressions for a true pair (center word and a word in its context window) versus several noise pairs (the center word paired with a random word)&lt;/li&gt;
  &lt;li&gt;From paper: &lt;em&gt;“Distributed Representations of Words and Phrases and their Compositionality” (Mikolov et al. 2013)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Overall objective function(to maximize):&lt;br /&gt;
  \(J(\theta) = \frac{1}{T}\sum_{t=1}^T J_t(\theta)\)&lt;br /&gt;
  \(J_t(\theta) = \log\sigma(u_o^T u_c) + \sum_{i=1}^k \mathbb{E}_{j~P(w)}\left[ \log\sigma(-u_j^T v_c) \right]\)&lt;br /&gt;
  where the logistic/sigmoid function: $\sigma(x) = \frac{1}{1+ e^{-x}}$&lt;/li&gt;
  &lt;li&gt;We maximize the probability of two words co-occuring in first log and minimize probability of noise words:&lt;br /&gt;
  $J_{\text{neg-sample}}(u_o, v_c, U) = -\log \sigma(u_o^T v_c) - \sum_{k\in { \text{K sampled indicies} }} \log \sigma(-u_k^T v_c)$&lt;/li&gt;
  &lt;li&gt;We take &lt;em&gt;k&lt;/em&gt; negative samples (using word probabilities)&lt;/li&gt;
  &lt;li&gt;Maximize probability that real outside word appears, minimize probability that random words appear around center word&lt;/li&gt;
  &lt;li&gt;Another trick: sample with $P(w) = U(w)^{3/4} / Z$, the unigram distribution $U(w)$ raised to the $3/4$ power (We provide this function in the starter code)&lt;/li&gt;
  &lt;li&gt;The power makes less frequent words be sampled more often&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;why-not-capture-co-occurrence-counts-directly&quot;&gt;Why not capture co-occurrence counts directly?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Building a co-occurrence matrix &lt;em&gt;X&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;2 options: windows vs. full document&lt;/li&gt;
      &lt;li&gt;Window: Similar to word2vec, use window around each word and captures some syntactic and semantic information&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec2_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;Word-document co-occurrence matrix will give general topics (all sports terms will have similar entries) learning to “Latent Semantic Analysis”; in tasks like information retrieval&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;co-occurrence-vectors&quot;&gt;Co-occurrence vectors&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Simple count co-occurrence vectors
    &lt;ul&gt;
      &lt;li&gt;Vectors increase in size with vocabulary&lt;/li&gt;
      &lt;li&gt;Very high dimensional: require a lot of storage (though sparse)&lt;/li&gt;
      &lt;li&gt;Subsequent classification models have sparsity issues $\rightarrow$ Models are less robust&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Low-dimensional vectors
    &lt;ul&gt;
      &lt;li&gt;idea: store “most” of the important information in a fixed, small number of directions: a dense vector&lt;/li&gt;
      &lt;li&gt;Usually 25-1000 directions, similar to word2vec&lt;/li&gt;
      &lt;li&gt;How to reduce the dimensionality?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;classic-method-dimensionality-reduction-on-x&quot;&gt;Classic Method: Dimensionality Reduction on X&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Singular Value Decomposition of co-occurrence matrix &lt;em&gt;X&lt;/em&gt;&lt;br /&gt;
  Factorizes &lt;em&gt;X&lt;/em&gt; into $U\Sigma V^T$, where &lt;em&gt;U&lt;/em&gt; and &lt;em&gt;V&lt;/em&gt; are orthonormal&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec2_4.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
  Corresponding to the columns without singular values in $\Sigma$, bottom rows in $V^T$ are ignored. The singular values inside the diagonal matrix $\Sigma$ are ordered from largest down to smallest. Retaining only &lt;em&gt;k&lt;/em&gt; singular values, in order to generalize, the lower dimensional representation $\hat{X}$ is the best rank &lt;em&gt;k&lt;/em&gt; approximation to &lt;em&gt;X&lt;/em&gt;, in terms of least squares.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;hacks-to-x-several-used-in-rohde-et-al-2005-in-coals&quot;&gt;Hacks to X (several used in Rohde et al. 2005 in COALS)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Running an SVD on a raw count co-occurrence matrix works poorly; In the mathematical assumptions of SVD, we are expecting to have normally distributed errors. But there are exceedingly common words like “a”, “the”, and “and”, and there is a very large number of rare words.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Scaling the counts in the cells can help &lt;strong&gt;a lot&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Problem: function words(&lt;em&gt;the, he, has&lt;/em&gt;) are too frequent $\rightarrow$ syntax has too much impact. Some fixes:
        &lt;ul&gt;
          &lt;li&gt;log the frequencies&lt;/li&gt;
          &lt;li&gt;$min(X,t)$, with $t\approx 100$&lt;/li&gt;
          &lt;li&gt;ignore the function words&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Ramped windows that count closer words more than further away words&lt;/li&gt;
  &lt;li&gt;Use Pearson correlations instead of counts, then set negative values to &lt;em&gt;0&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Etc.&lt;/li&gt;
  &lt;li&gt;Result:&lt;br /&gt;
  Interesting semantic patterns emerge in the scaled vectors; something like a word vector analogies.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs224n/lec2_5.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;towards-glove-count-based-vs-direct-prediction&quot;&gt;Towards GloVe: Count based vs. direct prediction&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_6.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;encoding-meaning-components-in-vector-differences&quot;&gt;Encoding meaning components in vector differences&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Pennington, Socher, and Manning, EMNLP 2014&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;What properties needed to make vector analogies work?&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Crucial insight: Ratios of co-occurrence probabilities can encode meaning components&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_7.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_8.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Q: How can we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space?&lt;/li&gt;
  &lt;li&gt;A: Log-bilinear model: the dot product between two word vectors attempts to approximate the log of the probability of co-occurrence; \(w_i \cdot w_j = \log P(i|j)\)&lt;br /&gt;
  $\rightarrow$ with vector differences \(w_x \cdot (w_a - w_b) = \log \frac{P(x\mid a)}{P(x \mid b)}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;combining-the-best-of-both-worlds-glove&quot;&gt;Combining the best of both worlds: GloVe&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Pennington, Socher, and Manning, EMNLP 2014&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;With \(w_i \cdot w_j = \log P(i|j)\),&lt;br /&gt;
  explicit loss function \(J = \sum_{i,j=1}^V f(X_{ij})(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2\)&lt;br /&gt;
  to make the dot product to be similar to the log of the co-occurrence. To not have very common words dominate, capped the effect of high word counts using $f$ function. Optimize &lt;em&gt;J&lt;/em&gt; directly on the co-occurrence count matrix.
    &lt;ul&gt;
      &lt;li&gt;Fast training&lt;/li&gt;
      &lt;li&gt;Scalable to hugh corpora&lt;/li&gt;
      &lt;li&gt;Good performance even with small corpus and small vectors&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-evaluate-word-vectors&quot;&gt;How to evaluate word vectors?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Related to general evaluation in NLP: intrinsic vs. extrinsic&lt;/li&gt;
  &lt;li&gt;Intrinsic:
    &lt;ul&gt;
      &lt;li&gt;Evaluation on a specific/intermediate subtask&lt;/li&gt;
      &lt;li&gt;Fast to compute&lt;/li&gt;
      &lt;li&gt;Helps to understand that system&lt;/li&gt;
      &lt;li&gt;Not clear if really helpful unless correlation to real task is established&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Extrinsic:
    &lt;ul&gt;
      &lt;li&gt;Evaluation on a real task&lt;/li&gt;
      &lt;li&gt;Can take a long time to compute accuracy&lt;/li&gt;
      &lt;li&gt;Unclear if the subsystem is the problem or its interaction or other subsystems&lt;/li&gt;
      &lt;li&gt;If replacing exactly one subsystem with another improves accuracy $\rightarrow$ Winning!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intrinsic-word-vector-evaluation&quot;&gt;Intrinsic word vector evaluation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Word Vector Analogies&lt;br /&gt;
  |a:b :: c:?| $\rightarrow$ $d = \text{argmax}_i \frac{(x_b -x_a +x_c)^T x_i}{\lVert x_b -x_a +x_c\rVert}$&lt;br /&gt;
  (e.g. man:woman :: king:?)&lt;/li&gt;
  &lt;li&gt;Evalute word vectors by how well their cosine distance after addition captures intuitive semantic and syntactic analogy questions&lt;/li&gt;
  &lt;li&gt;Discarding the input words from the search!&lt;/li&gt;
  &lt;li&gt;Problem: What if the information is there but not linear?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;glove-visualizations&quot;&gt;GloVe Visualizations&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_9.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;analogy-evaluation-and-hyperparameters&quot;&gt;Analogy evaluation and hyperparameters&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_10.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_11.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;another-intrinsic-word-vector-evaluation&quot;&gt;Another intrinsic word vector evaluation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Word vector distances and their correlation with human judgements&lt;/li&gt;
  &lt;li&gt;Example dataset: WordSim353 &lt;a href=&quot;http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/&quot;&gt;http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_12.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_13.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Some ideas from Glove paper have been shown to improve skip-gram(SG) model also (e.g., average both vectors)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;extrinsic-word-vector-evaluation&quot;&gt;Extrinsic word vector evaluation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;All subsequent NLP tasks&lt;/li&gt;
  &lt;li&gt;One example where good word vectors should help directly: &lt;strong&gt;named entity recognition&lt;/strong&gt;: identifying references to a person, organization or location&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs224n/lec2_14.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;word-senses&quot;&gt;Word senses&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Most words have lots of meanings
    &lt;ul&gt;
      &lt;li&gt;Especially common words&lt;/li&gt;
      &lt;li&gt;Especially words that have existed for a long time&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Does one vector caputre all these meanings or do we have a mess?&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;linear-algebric-structure-of-word-senses-with-applications-to-polysemy-arora--ma--tacl-2018&quot;&gt;&lt;em&gt;“Linear Algebric Structure of Word Senses, with Applications to Polysemy”, Arora, …, Ma, …, TACL 2018&lt;/em&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Different senses of a word reside in a linear superposition(weighted sum) in standard word embeddings like word2vec&lt;/li&gt;
  &lt;li&gt;\(v_{\text{pike}} = \alpha_1 v_{\text{pike}_2} + \alpha_2 v_{\text{pike}_2} + \alpha_3 v_{\text{pike}_3}\)&lt;br /&gt;
  where $\alpha_1 = \frac{f_1}{f_1+f_2+f_3}$, etc., for frequency &lt;em&gt;f&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Surprising result:&lt;br /&gt;
  Commonly, it is impossible to reconstruct the original components from their sum, but, because of ideas from &lt;em&gt;sparse coding&lt;/em&gt; you can actually separate out the senses(providing they are relatively common)!&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec2_15.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">Review: Main idea of word2vec Start with random word vectors Iterate through each word in the whole corpus Try to predict surrounding words using word vectors: $P(o\mid c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}$ Learning: Update vectors so they can predict actual surrounding words better Doing no more than this, this algorithm learns word vectors that capture well word similarity and meaningful directions in a wordspace. A “bag of words” model; doesn’t actually pay any attention to word order or position. The model makes the same predictions at each position; the probability estimate would be the same if it is next to the center word or a bit further away. We want a model that gives a reasonably high probability estimate to all words that occur in the context(at all often) Word2vec maximizes objective function by putting similar words nearby in high dimensional vector space Optimization: Gradient Descent To learn good word vectors: minimize a cost function $J(\theta)$ Gradient Descent is an algorithm to minimize $J(\theta)$ by changing $\theta$ idea: from current value of $\theta$, calculate gradient of $J(\theta)$, then take small step in the direction of negative gradient. Repreat. Algorithm: while True: theta_grad = evaluate_gradient(J, corpus, theta) theta = theta - alpha * theta_grad Stochastic Gradient Descent Problem: $J(\theta)$ is a function of all windows in the corpus (often, billions!); so $\nabla_\theta J(\theta)$ is very expensive to compute Solution: Stochastic gradient descent(SGD) Repeatedly sample windows, and update after each one, or each small batch Algorithm: while True: window = sample_window(corpus) theta_grad = evaluate_gradient(J, window, theta) theta = theta - alpha * theta_grad Stochastic gradients with word vectors (Aside) iteratively take gradients at each such window for SGD But in each window, we only have at most 2m + 1 words, so $\nabla_\theta J(\theta)$ is very sparse: \(\nabla_\theta J_t(\theta) = \begin{bmatrix} 0 \\ \vdots \\ \nabla_{v_{\text{like}}} \\ \vdots 0 \\ \nabla_{u_I} \\ \vdots \\ \nabla_{u_{\text{learning}}} \\ \vdots \end{bmatrix} \in \mathbb{R}^{2dV}\) We might only update the word vectors that actually appear. Solution: either you need sparse matrix update operations to only update certain rows(in most DL packages) of full embedding matrices U and V, or you need to keep around a hash for word vectors. If you have millions of word vectors and do distributed computing, it is important to not have to send gigantic updates around. 2b. Word2vec algorithm family: More details Why two vectors? $\rightarrow$ Easier optimization. Average both at the end But can implement the algorithm with just one vector per word, and it works slightly better, but it makes the algorithm much more complicated. Two model variants: Skip-grams(SG) Predict context(“outside”) words (position independent) given center word Continuous Bag of Words(CBOW) Predict center word from (bag of) context words We presented: Skip-gram model Additional efficiency in training: Negative sampling So far: Focus on naive softmax(simpler, but expensive training method) The skip-gram model with negative sampling(SGNS) The normalization term is computationally expensive, especially on the denominator of $P(o\mid c)$. Main idea: train binary logistic regressions for a true pair (center word and a word in its context window) versus several noise pairs (the center word paired with a random word) From paper: “Distributed Representations of Words and Phrases and their Compositionality” (Mikolov et al. 2013) Overall objective function(to maximize): \(J(\theta) = \frac{1}{T}\sum_{t=1}^T J_t(\theta)\) \(J_t(\theta) = \log\sigma(u_o^T u_c) + \sum_{i=1}^k \mathbb{E}_{j~P(w)}\left[ \log\sigma(-u_j^T v_c) \right]\) where the logistic/sigmoid function: $\sigma(x) = \frac{1}{1+ e^{-x}}$ We maximize the probability of two words co-occuring in first log and minimize probability of noise words: $J_{\text{neg-sample}}(u_o, v_c, U) = -\log \sigma(u_o^T v_c) - \sum_{k\in { \text{K sampled indicies} }} \log \sigma(-u_k^T v_c)$ We take k negative samples (using word probabilities) Maximize probability that real outside word appears, minimize probability that random words appear around center word Another trick: sample with $P(w) = U(w)^{3/4} / Z$, the unigram distribution $U(w)$ raised to the $3/4$ power (We provide this function in the starter code) The power makes less frequent words be sampled more often Why not capture co-occurrence counts directly? Building a co-occurrence matrix X 2 options: windows vs. full document Window: Similar to word2vec, use window around each word and captures some syntactic and semantic information Word-document co-occurrence matrix will give general topics (all sports terms will have similar entries) learning to “Latent Semantic Analysis”; in tasks like information retrieval Co-occurrence vectors Simple count co-occurrence vectors Vectors increase in size with vocabulary Very high dimensional: require a lot of storage (though sparse) Subsequent classification models have sparsity issues $\rightarrow$ Models are less robust Low-dimensional vectors idea: store “most” of the important information in a fixed, small number of directions: a dense vector Usually 25-1000 directions, similar to word2vec How to reduce the dimensionality? Classic Method: Dimensionality Reduction on X Singular Value Decomposition of co-occurrence matrix X Factorizes X into $U\Sigma V^T$, where U and V are orthonormal Corresponding to the columns without singular values in $\Sigma$, bottom rows in $V^T$ are ignored. The singular values inside the diagonal matrix $\Sigma$ are ordered from largest down to smallest. Retaining only k singular values, in order to generalize, the lower dimensional representation $\hat{X}$ is the best rank k approximation to X, in terms of least squares. Hacks to X (several used in Rohde et al. 2005 in COALS) Running an SVD on a raw count co-occurrence matrix works poorly; In the mathematical assumptions of SVD, we are expecting to have normally distributed errors. But there are exceedingly common words like “a”, “the”, and “and”, and there is a very large number of rare words. Scaling the counts in the cells can help a lot Problem: function words(the, he, has) are too frequent $\rightarrow$ syntax has too much impact. Some fixes: log the frequencies $min(X,t)$, with $t\approx 100$ ignore the function words Ramped windows that count closer words more than further away words Use Pearson correlations instead of counts, then set negative values to 0 Etc. Result: Interesting semantic patterns emerge in the scaled vectors; something like a word vector analogies. Towards GloVe: Count based vs. direct prediction Encoding meaning components in vector differences Pennington, Socher, and Manning, EMNLP 2014 What properties needed to make vector analogies work? Crucial insight: Ratios of co-occurrence probabilities can encode meaning components Q: How can we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space? A: Log-bilinear model: the dot product between two word vectors attempts to approximate the log of the probability of co-occurrence; \(w_i \cdot w_j = \log P(i|j)\) $\rightarrow$ with vector differences \(w_x \cdot (w_a - w_b) = \log \frac{P(x\mid a)}{P(x \mid b)}\) Combining the best of both worlds: GloVe Pennington, Socher, and Manning, EMNLP 2014 With \(w_i \cdot w_j = \log P(i|j)\), explicit loss function \(J = \sum_{i,j=1}^V f(X_{ij})(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2\) to make the dot product to be similar to the log of the co-occurrence. To not have very common words dominate, capped the effect of high word counts using $f$ function. Optimize J directly on the co-occurrence count matrix. Fast training Scalable to hugh corpora Good performance even with small corpus and small vectors How to evaluate word vectors? Related to general evaluation in NLP: intrinsic vs. extrinsic Intrinsic: Evaluation on a specific/intermediate subtask Fast to compute Helps to understand that system Not clear if really helpful unless correlation to real task is established Extrinsic: Evaluation on a real task Can take a long time to compute accuracy Unclear if the subsystem is the problem or its interaction or other subsystems If replacing exactly one subsystem with another improves accuracy $\rightarrow$ Winning! Intrinsic word vector evaluation Word Vector Analogies |a:b :: c:?| $\rightarrow$ $d = \text{argmax}_i \frac{(x_b -x_a +x_c)^T x_i}{\lVert x_b -x_a +x_c\rVert}$ (e.g. man:woman :: king:?) Evalute word vectors by how well their cosine distance after addition captures intuitive semantic and syntactic analogy questions Discarding the input words from the search! Problem: What if the information is there but not linear? GloVe Visualizations Analogy evaluation and hyperparameters Another intrinsic word vector evaluation Word vector distances and their correlation with human judgements Example dataset: WordSim353 http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/ Some ideas from Glove paper have been shown to improve skip-gram(SG) model also (e.g., average both vectors) Extrinsic word vector evaluation All subsequent NLP tasks One example where good word vectors should help directly: named entity recognition: identifying references to a person, organization or location Word senses Most words have lots of meanings Especially common words Especially words that have existed for a long time Does one vector caputre all these meanings or do we have a mess? “Linear Algebric Structure of Word Senses, with Applications to Polysemy”, Arora, …, Ma, …, TACL 2018 Different senses of a word reside in a linear superposition(weighted sum) in standard word embeddings like word2vec \(v_{\text{pike}} = \alpha_1 v_{\text{pike}_2} + \alpha_2 v_{\text{pike}_2} + \alpha_3 v_{\text{pike}_3}\) where $\alpha_1 = \frac{f_1}{f_1+f_2+f_3}$, etc., for frequency f Surprising result: Commonly, it is impossible to reconstruct the original components from their sum, but, because of ideas from sparse coding you can actually separate out the senses(providing they are relatively common)!</summary></entry><entry><title type="html">DevEnv Setup</title><link href="http://0.0.0.0:4000/DevEnv_Setup" rel="alternate" type="text/html" title="DevEnv Setup" /><published>2022-03-02T00:00:00+00:00</published><updated>2022-03-02T00:00:00+00:00</updated><id>http://0.0.0.0:4000/DevEnv_Setup</id><content type="html" xml:base="http://0.0.0.0:4000/DevEnv_Setup">&lt;ul&gt;
  &lt;li&gt;For purpose of setting local development environment on a new SSD storage, followed instructions below. Post for later use.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Document &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl&quot;&gt;Enable NVIDIA CUDA on WSL&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Install stable version of Windows 11&lt;/li&gt;
      &lt;li&gt;Enable WSL, install Ubuntu(20.04.3 LTS)&lt;br /&gt;
  On Windows &lt;strong&gt;Settings&lt;/strong&gt; app, select &lt;strong&gt;Check for updates&lt;/strong&gt; in the &lt;strong&gt;Windows Update&lt;/strong&gt; section and get the latest kernel(5.10.43.3 or higher)&lt;br /&gt;
  To check the version, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wsl cat /proc/version&lt;/code&gt; command in &lt;strong&gt;Powershell&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Install the GPU driver&lt;br /&gt;
  Download and install the NVIDIA CUDA enabled driver for WSL&lt;br /&gt;
  (Studio version: 511.65-desktop-win10-win11-64bit-international-nsd-dch-whql)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Install Docker Desktop app on Windows
    &lt;ul&gt;
      &lt;li&gt;Run:&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Result:
        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Windowed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Simulation&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Single&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;floating&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simulation&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Devices&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;used&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simulation&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;GPU&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Device&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Ampere&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capability&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;8.6&lt;/span&gt;

  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Compute&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;8.6&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CUDA&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NVIDIA&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GeForce&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RTX&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3070&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;mi&quot;&gt;47104&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bodies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;40.275&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ms&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;550.910&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;billion&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interactions&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;11018.199&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;single&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GFLOP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flops&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interaction&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Setting Docker image for TensorFlow-GPU
    &lt;ul&gt;
      &lt;li&gt;Pull the latest TensorFlow-GPU image
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker run -it --gpus all tensorflow/tensorflow:latest-gpu&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Install Anaconda on user:root(ref: &lt;a href=&quot;https://omhdydy.tistory.com/6&quot;&gt;blog&lt;/a&gt;)
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  # update and install prerequisites
  apt-get update
  apt-get install wget
  # get proper version of anaconda3
  wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh 
  sh Anaconda3-2021.11-Linux-x86_64.sh
  exec bash
  # create anaconda environment and install libraries(for stability)
  conda create -n !env_name pip python=3.7
  conda activate !env_name
  pip install tensorflow-gpu
  pip install ipykernel
  python -m ipykernel install --user --name !env_name --display-name !dispaly_name
  pip install jupyter
  # escape with Ctrl + p, Ctrl + q
  docker commit -m &quot;!message&quot; !container_id !image_name:tag
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;(Optional) Install TensorFlow Object Detection API
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  apt-get install git
  git clone --depth 1 https://github.com/tensorflow/models
  cd models/research/
  apt install -y protobuf-compiler
  # found a symlink err, fixed with running:
  # ln -s /usr/lib/x86_64-linux-gnu/libcuda.so.1 /usr/local/cuda/lib64/libcuda.so
  # and rerun: apt install -y protobuf-compiler
  protoc object_detection/protos/*.proto --python_out=.
  cd models/research/
  # install Object Detection API
  cp object_detection/packages/tf2/setup.py .
  python -m pip install --use-feature=2020-resolver .
  # run test
  python object_detection/builders/model_builder_tf2_test.py
  # rm -rf models (if desired)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;(Optional) Install JupyterLab Extensions and enable TensorBoard within Jupyterlab-Docker container&lt;br /&gt;
  Stable versions worked on my local environment
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  curl -sL https://deb.nodesource.com/setup_12.x | bash -
  apt-get install -y nodejs
  node --version # check: v12.22.10
  npm --version # check: 6.14.16
  pip install jupyterlab==2.3.2 
  pip install git+https://github.com/cliffwoolley/jupyter_tensorboard.git 
  pip install tensorboard==2.2
  jupyter labextension install jupyterlab_tensorboard
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
        &lt;p&gt;Commit and run container with any open port for JupyterLab&lt;br /&gt;
  e.g.&lt;/p&gt;
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  docker run --rm -it --gpus all -p 4000:4000 !image_name:tag
  conda activate !env_name
  jupyter lab --ip='0.0.0.0' --port=4000 --no-browser --allow-root
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
        &lt;p&gt;On your &lt;strong&gt;Windows&lt;/strong&gt;, open &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost:4000&lt;/code&gt; with browser&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Setting Docker image for Jekyll blog
    &lt;ul&gt;
      &lt;li&gt;Get latest Ubuntu image and install packages
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  docker run --rm -it -p 4000:4000 ubuntu
  apt-get update
  apt-get install git
  apt-get install vim ruby-full build-essential zlib1g-dev -y

  echo '# Install Ruby Gems to ~/gems' &amp;gt;&amp;gt; ~/.bashrc
  echo 'export GEM_HOME=&quot;$HOME/gems&quot;' &amp;gt;&amp;gt; ~/.bashrc
  echo 'export PATH=&quot;$HOME/gems/bin:$PATH&quot;' &amp;gt;&amp;gt; ~/.bashrc
  source ~/.bashrc
  gem install jekyll bundler
		
  jekyll -v # 4.2.1
  mkdir -p /root/blog_home
  echo 'export BLOG_HOME=&quot;/root/blog_home&quot;' &amp;gt;&amp;gt; ~/.bashrc
  echo '# Start jekyll' &amp;gt;&amp;gt; ~/.bashrc
  source ~/.bashrc
  cd $BLOG_HOME # Get any jekyll blog template here
  rm Gemfile.lock # if needed
  bundle install
  bundle exec jekyll serve --host 0.0.0.0 -p 4000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Darron Kwon</name></author><category term="blog" /><summary type="html">For purpose of setting local development environment on a new SSD storage, followed instructions below. Post for later use. Document Enable NVIDIA CUDA on WSL Install stable version of Windows 11 Enable WSL, install Ubuntu(20.04.3 LTS) On Windows Settings app, select Check for updates in the Windows Update section and get the latest kernel(5.10.43.3 or higher) To check the version, run wsl cat /proc/version command in Powershell. Install the GPU driver Download and install the NVIDIA CUDA enabled driver for WSL (Studio version: 511.65-desktop-win10-win11-64bit-international-nsd-dch-whql) Install Docker Desktop app on Windows Run: docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark Result: &amp;gt; Windowed mode &amp;gt; Simulation data stored in video memory &amp;gt; Single precision floating point simulation &amp;gt; 1 Devices used for simulation GPU Device 0: &quot;Ampere&quot; with compute capability 8.6 &amp;gt; Compute 8.6 CUDA device: [NVIDIA GeForce RTX 3070] 47104 bodies, total time for 10 iterations: 40.275 ms = 550.910 billion interactions per second = 11018.199 single-precision GFLOP/s at 20 flops per interaction Setting Docker image for TensorFlow-GPU Pull the latest TensorFlow-GPU image docker run -it --gpus all tensorflow/tensorflow:latest-gpu Install Anaconda on user:root(ref: blog) # update and install prerequisites apt-get update apt-get install wget # get proper version of anaconda3 wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh sh Anaconda3-2021.11-Linux-x86_64.sh exec bash # create anaconda environment and install libraries(for stability) conda create -n !env_name pip python=3.7 conda activate !env_name pip install tensorflow-gpu pip install ipykernel python -m ipykernel install --user --name !env_name --display-name !dispaly_name pip install jupyter # escape with Ctrl + p, Ctrl + q docker commit -m &quot;!message&quot; !container_id !image_name:tag (Optional) Install TensorFlow Object Detection API apt-get install git git clone --depth 1 https://github.com/tensorflow/models cd models/research/ apt install -y protobuf-compiler # found a symlink err, fixed with running: # ln -s /usr/lib/x86_64-linux-gnu/libcuda.so.1 /usr/local/cuda/lib64/libcuda.so # and rerun: apt install -y protobuf-compiler protoc object_detection/protos/*.proto --python_out=. cd models/research/ # install Object Detection API cp object_detection/packages/tf2/setup.py . python -m pip install --use-feature=2020-resolver . # run test python object_detection/builders/model_builder_tf2_test.py # rm -rf models (if desired) (Optional) Install JupyterLab Extensions and enable TensorBoard within Jupyterlab-Docker container Stable versions worked on my local environment curl -sL https://deb.nodesource.com/setup_12.x | bash - apt-get install -y nodejs node --version # check: v12.22.10 npm --version # check: 6.14.16 pip install jupyterlab==2.3.2 pip install git+https://github.com/cliffwoolley/jupyter_tensorboard.git pip install tensorboard==2.2 jupyter labextension install jupyterlab_tensorboard Commit and run container with any open port for JupyterLab e.g. docker run --rm -it --gpus all -p 4000:4000 !image_name:tag conda activate !env_name jupyter lab --ip='0.0.0.0' --port=4000 --no-browser --allow-root On your Windows, open localhost:4000 with browser Setting Docker image for Jekyll blog Get latest Ubuntu image and install packages docker run --rm -it -p 4000:4000 ubuntu apt-get update apt-get install git apt-get install vim ruby-full build-essential zlib1g-dev -y echo '# Install Ruby Gems to ~/gems' &amp;gt;&amp;gt; ~/.bashrc echo 'export GEM_HOME=&quot;$HOME/gems&quot;' &amp;gt;&amp;gt; ~/.bashrc echo 'export PATH=&quot;$HOME/gems/bin:$PATH&quot;' &amp;gt;&amp;gt; ~/.bashrc source ~/.bashrc gem install jekyll bundler jekyll -v # 4.2.1 mkdir -p /root/blog_home echo 'export BLOG_HOME=&quot;/root/blog_home&quot;' &amp;gt;&amp;gt; ~/.bashrc echo '# Start jekyll' &amp;gt;&amp;gt; ~/.bashrc source ~/.bashrc cd $BLOG_HOME # Get any jekyll blog template here rm Gemfile.lock # if needed bundle install bundle exec jekyll serve --host 0.0.0.0 -p 4000</summary></entry><entry><title type="html">cs224n - Lecture 1. Word Vectors</title><link href="http://0.0.0.0:4000/cs224n_lec1" rel="alternate" type="text/html" title="cs224n - Lecture 1. Word Vectors" /><published>2022-03-02T00:00:00+00:00</published><updated>2022-03-02T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs224n_lec1</id><content type="html" xml:base="http://0.0.0.0:4000/cs224n_lec1">&lt;h3 id=&quot;objectives&quot;&gt;Objectives&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;The foundations of the effective modern methods for deep learning applied to NLP; from basics to key methods used in NLP: RNN, Attention, Transformers, etc.)&lt;/li&gt;
  &lt;li&gt;A big picture understanding of human languages and the difficulties in understanding and producing them&lt;/li&gt;
  &lt;li&gt;An understanding of and ability to build systems (in PyTorch) for some of the major problems in NLP: Word meaning, dependency parsing, machine translation, question answering&lt;br /&gt;
&lt;!-- *CS145: translate human language sentences into SQL --&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;NLP tasks:&lt;br /&gt;
  Easy: Spell Checking, Keyword Search, Finding Synonyms&lt;br /&gt;
  Medium: Parsing information from websites, documents, etc.&lt;br /&gt;
  Hard: Machine Translation, Semantic Analysis, Coreference, QA&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;word-meaning&quot;&gt;Word meaning&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Commonest linguistic way of thinking of meaning:&lt;br /&gt;
  signifier (symbol) $\Leftrightarrow$ signified (idea or thing)&lt;br /&gt;
  $=$ denotational semantics&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Common NLP solution&lt;/strong&gt;: Use, e.g., &lt;em&gt;WordNet&lt;/em&gt;, a thesaurus containing lists of &lt;strong&gt;synonym sets&lt;/strong&gt; and &lt;strong&gt;hypernyms&lt;/strong&gt;(“is a” relationships).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Problems with resources like WordNet&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Great as a resource but missing nuance&lt;/li&gt;
      &lt;li&gt;Missing new meanings of words; impossible to keep up-to-date&lt;/li&gt;
      &lt;li&gt;Subjective&lt;/li&gt;
      &lt;li&gt;Requires human labor to create and adapt&lt;/li&gt;
      &lt;li&gt;Can’t compute accurate word similarity&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;representing-words-as-discrete-symbols&quot;&gt;Representing words as discrete symbols&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;In traditional NLP, we regard words as discrete symbols - a &lt;em&gt;localist&lt;/em&gt; representation.&lt;br /&gt;
  $\rightarrow$ in a statistical machine learning systems, such symbols for words are separately represented by one-hot vectors. Thus we need to have huge vector dimension corresponding to the number of words in vocabulary.
    &lt;ul&gt;
      &lt;li&gt;But with discrete symbols, two vectors are orthogonal and there is no natural notion of similarity for one-hot vectors.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Solution:
    &lt;ul&gt;
      &lt;li&gt;Could try  to rely on WordNet’s list of synonyms to get similarity?&lt;br /&gt;
  But it is well-known to fail badly; incompleteness, etc.&lt;/li&gt;
      &lt;li&gt;Instead: learn to encode similarity in the vecotr themselves.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;representing-words-by-their-context&quot;&gt;Representing words by their context&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Distributional semantics: &lt;strong&gt;A word’s meaning is given by the words that frequently appear close-by&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;“You shall know a word by the company it keeps”&lt;/em&gt; (J. R. Firth 1957:11)&lt;/li&gt;
      &lt;li&gt;One of the most successful ideas of modern statistical NLP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;When a word &lt;em&gt;w&lt;/em&gt; appears in a text, its &lt;strong&gt;context&lt;/strong&gt; is the set of words that appear nearby(within a fixed-size window).&lt;/li&gt;
  &lt;li&gt;Use the many contexts of &lt;em&gt;w&lt;/em&gt; to build up a representation of &lt;em&gt;w&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;word-vectors&quot;&gt;Word vectors&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note: as a &lt;em&gt;distributed&lt;/em&gt; representation, &lt;em&gt;word vectors&lt;/em&gt; are also called &lt;em&gt;word embeddings&lt;/em&gt; or &lt;em&gt;(neural) words representations&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;word2vec&quot;&gt;Word2vec&lt;/h2&gt;
&lt;h3 id=&quot;word2vec-overview&quot;&gt;Word2vec: overview&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Word2vec&lt;/em&gt;(Mikolov et al. 2013) is a framework for learning word vectors&lt;/li&gt;
  &lt;li&gt;idea:
    &lt;ul&gt;
      &lt;li&gt;We have a large corpus(“body”) of text&lt;/li&gt;
      &lt;li&gt;Every word in a fixed vocabulary is representated by a vector&lt;/li&gt;
      &lt;li&gt;Go through each position &lt;em&gt;t&lt;/em&gt; in the text, which has a center word &lt;em&gt;c&lt;/em&gt; and context(“outside”) words &lt;em&gt;o&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Use the similarity of the word vectors for &lt;em&gt;c&lt;/em&gt; and &lt;em&gt;o&lt;/em&gt; to calculate the probability of &lt;em&gt;o&lt;/em&gt; given &lt;em&gt;c&lt;/em&gt;(or vice versa)&lt;/li&gt;
      &lt;li&gt;Keep adjusting the word vectors to maximize this probability&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;We can learn these word vectors from just a big pile of text by doing this distributional similarity task of being able to predict what words occur in the context of other words.&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Example windows and process for computing $P(w_{t+j}|w_t)$&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs224n/lec1_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;word2vec-objective-function&quot;&gt;Word2vec: objective function&lt;/h3&gt;
&lt;p&gt;For each position $t = 1, \ldots, T$, predict context words within a window of fixed size &lt;em&gt;m&lt;/em&gt;, given center word $w_j$. Data likelihood:&lt;br /&gt;
\(\begin{align*}
L(\theta) = \prod_{t=1}^T \prod_{\substack{-m\leqq j\leqq m \\ j\ne 0}} P(w_{t+j}|w_t;\theta)
\end{align*}\)&lt;br /&gt;
where $\theta$ is all variables to be optimized.&lt;/p&gt;

&lt;p&gt;The objective function $J(\theta)$ is the (average) negative log likelihood:&lt;br /&gt;
\(\begin{align*}
J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T}\sum_{t=1}^T \sum_{\substack{-m\leqq j\leqq m \\ j\ne 0}}\log P(w_{t+j}|w_t;\theta)
\end{align*}\)&lt;br /&gt;
Minimizing objective function $\Leftrightarrow$ Maximizing predictive accuracy&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Question: How to calculate $P(w_{t+j}\vert w_t;\theta)$?&lt;/li&gt;
  &lt;li&gt;Answer: We will &lt;em&gt;use&lt;/em&gt; two vectors per word &lt;em&gt;w&lt;/em&gt;:
    &lt;ul&gt;
      &lt;li&gt;$v_w$ when &lt;em&gt;w&lt;/em&gt; is a center word&lt;/li&gt;
      &lt;li&gt;$u_w$ when &lt;em&gt;w&lt;/em&gt; is a context word&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Then for a centor word &lt;em&gt;c&lt;/em&gt; and a context word &lt;em&gt;o&lt;/em&gt;:&lt;br /&gt;
  \(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;word2vec-prediction-function&quot;&gt;Word2vec: prediction function&lt;/h3&gt;
&lt;p&gt;\(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\)&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$u_w^T v_c$: Dot product compares similarity of &lt;em&gt;o&lt;/em&gt; and &lt;em&gt;c&lt;/em&gt;.&lt;br /&gt;
 $u^T v = u\ .v = \sum_{i=1}^n u_i v_i$&lt;br /&gt;
 Larger dot product = larger probability&lt;/li&gt;
  &lt;li&gt;$\exp$: Exponentiation makes anything positive&lt;/li&gt;
  &lt;li&gt;$\sum_{w \in V}\exp(u_w^T v_c)$: Normalize over entire vocabulary to give probability distribution.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This is an example of the softmax function $\mathbb{R}^n \rightarrow (0,1)^n$(Open region) that maps arbitary values $x_i$ to a probability distribution $p_i$&lt;br /&gt;
  \(\mbox{softmax}(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^n\exp(x_j)} = p_i\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To train the model: Optimize value of parameters to minimize loss&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Recall: $\theta$ represents all the model parameters, in one long vector&lt;/li&gt;
      &lt;li&gt;In our case, with &lt;em&gt;d&lt;/em&gt;-dimensional vectors and &lt;em&gt;V&lt;/em&gt;-many words, we have: $\theta \in \mathbb{R}^{2dV}$&lt;/li&gt;
      &lt;li&gt;Remember: every word has two vectors&lt;/li&gt;
      &lt;li&gt;We optimize these parameters by walking down the gradient(gradient descent)&lt;/li&gt;
      &lt;li&gt;We compute &lt;strong&gt;all&lt;/strong&gt; vector gradients&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;word2vec-derivations-of-gradient&quot;&gt;Word2vec derivations of gradient&lt;/h3&gt;
&lt;p&gt;\(\begin{align*}
J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T}\sum_{t=1}^T \sum_{\substack{-m\leqq j\leqq m \\ j\ne 0}}\log P(w_{t+j}|w_t;\theta)
\end{align*}\)&lt;br /&gt;
\(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\)&lt;/p&gt;

&lt;p&gt;\(\frac{\partial}{\partial v_c} \log \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)} = \underbrace{\frac{\partial}{\partial v_c} \log \exp(u_o^T v_c)}_{(1)} - \underbrace{\frac{\partial}{\partial v_c} \log \sum_{w=1}^V \exp(u_w^T v_c)}_{(2)}\)&lt;br /&gt;
\(\begin{align*}\cdots (1) &amp;amp;= \frac{\partial}{\partial v_c} u_o^T v_c \\
							&amp;amp;= u_o \end{align*}\)&lt;br /&gt;
\(\begin{align*}\cdots (2) &amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \frac{\partial}{\partial v_c} \exp(u_x^T v_c) \\
							&amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \exp(u_x^T v_c) \cdot \frac{\partial}{\partial v_c} u_x^T v_c \\
							&amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \exp(u_x^T v_c) \cdot u_x \\
							\end{align*} \\\)&lt;br /&gt;
\(\begin{align*}
\frac{\partial}{\partial v_c} \log P(o|c) &amp;amp;= u_o - \frac{\sum_{x=1}^V \exp(u_x^T v_c)u_x}{\sum_{w=1}^V \exp(u_w^T v_c)} \\
	&amp;amp; = u_o - \sum_{x=1}^V \underbrace{\frac{\exp(u_x^T v_c)}{\sum_{w=1}^V \exp(u_w^T v_c)}}_{\mbox{softmax formula}} u_x \\
	&amp;amp; = u_o - \underbrace{\sum_{x=1}^V P(x|c) u_x}_{\mbox{Expectation}} \\
	&amp;amp; = \mbox{observed} - \mbox{expected} \end{align*}\)&lt;/p&gt;</content><author><name>Darron Kwon</name></author><category term="cs224n" /><summary type="html">Objectives The foundations of the effective modern methods for deep learning applied to NLP; from basics to key methods used in NLP: RNN, Attention, Transformers, etc.) A big picture understanding of human languages and the difficulties in understanding and producing them An understanding of and ability to build systems (in PyTorch) for some of the major problems in NLP: Word meaning, dependency parsing, machine translation, question answering NLP tasks: Easy: Spell Checking, Keyword Search, Finding Synonyms Medium: Parsing information from websites, documents, etc. Hard: Machine Translation, Semantic Analysis, Coreference, QA Word meaning Commonest linguistic way of thinking of meaning: signifier (symbol) $\Leftrightarrow$ signified (idea or thing) $=$ denotational semantics Common NLP solution: Use, e.g., WordNet, a thesaurus containing lists of synonym sets and hypernyms(“is a” relationships). Problems with resources like WordNet Great as a resource but missing nuance Missing new meanings of words; impossible to keep up-to-date Subjective Requires human labor to create and adapt Can’t compute accurate word similarity Representing words as discrete symbols In traditional NLP, we regard words as discrete symbols - a localist representation. $\rightarrow$ in a statistical machine learning systems, such symbols for words are separately represented by one-hot vectors. Thus we need to have huge vector dimension corresponding to the number of words in vocabulary. But with discrete symbols, two vectors are orthogonal and there is no natural notion of similarity for one-hot vectors. Solution: Could try to rely on WordNet’s list of synonyms to get similarity? But it is well-known to fail badly; incompleteness, etc. Instead: learn to encode similarity in the vecotr themselves. Representing words by their context Distributional semantics: A word’s meaning is given by the words that frequently appear close-by “You shall know a word by the company it keeps” (J. R. Firth 1957:11) One of the most successful ideas of modern statistical NLP When a word w appears in a text, its context is the set of words that appear nearby(within a fixed-size window). Use the many contexts of w to build up a representation of w Word vectors A dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts Note: as a distributed representation, word vectors are also called word embeddings or (neural) words representations Word2vec Word2vec: overview Word2vec(Mikolov et al. 2013) is a framework for learning word vectors idea: We have a large corpus(“body”) of text Every word in a fixed vocabulary is representated by a vector Go through each position t in the text, which has a center word c and context(“outside”) words o Use the similarity of the word vectors for c and o to calculate the probability of o given c(or vice versa) Keep adjusting the word vectors to maximize this probability We can learn these word vectors from just a big pile of text by doing this distributional similarity task of being able to predict what words occur in the context of other words. Example windows and process for computing $P(w_{t+j}|w_t)$ Word2vec: objective function For each position $t = 1, \ldots, T$, predict context words within a window of fixed size m, given center word $w_j$. Data likelihood: \(\begin{align*} L(\theta) = \prod_{t=1}^T \prod_{\substack{-m\leqq j\leqq m \\ j\ne 0}} P(w_{t+j}|w_t;\theta) \end{align*}\) where $\theta$ is all variables to be optimized. The objective function $J(\theta)$ is the (average) negative log likelihood: \(\begin{align*} J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T}\sum_{t=1}^T \sum_{\substack{-m\leqq j\leqq m \\ j\ne 0}}\log P(w_{t+j}|w_t;\theta) \end{align*}\) Minimizing objective function $\Leftrightarrow$ Maximizing predictive accuracy Question: How to calculate $P(w_{t+j}\vert w_t;\theta)$? Answer: We will use two vectors per word w: $v_w$ when w is a center word $u_w$ when w is a context word Then for a centor word c and a context word o: \(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\) Word2vec: prediction function \(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\) $u_w^T v_c$: Dot product compares similarity of o and c. $u^T v = u\ .v = \sum_{i=1}^n u_i v_i$ Larger dot product = larger probability $\exp$: Exponentiation makes anything positive $\sum_{w \in V}\exp(u_w^T v_c)$: Normalize over entire vocabulary to give probability distribution. This is an example of the softmax function $\mathbb{R}^n \rightarrow (0,1)^n$(Open region) that maps arbitary values $x_i$ to a probability distribution $p_i$ \(\mbox{softmax}(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^n\exp(x_j)} = p_i\) To train the model: Optimize value of parameters to minimize loss Recall: $\theta$ represents all the model parameters, in one long vector In our case, with d-dimensional vectors and V-many words, we have: $\theta \in \mathbb{R}^{2dV}$ Remember: every word has two vectors We optimize these parameters by walking down the gradient(gradient descent) We compute all vector gradients Word2vec derivations of gradient \(\begin{align*} J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T}\sum_{t=1}^T \sum_{\substack{-m\leqq j\leqq m \\ j\ne 0}}\log P(w_{t+j}|w_t;\theta) \end{align*}\) \(P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)}\) \(\frac{\partial}{\partial v_c} \log \frac{\exp(u_o^T v_c)}{\sum_{w \in V}\exp(u_w^T v_c)} = \underbrace{\frac{\partial}{\partial v_c} \log \exp(u_o^T v_c)}_{(1)} - \underbrace{\frac{\partial}{\partial v_c} \log \sum_{w=1}^V \exp(u_w^T v_c)}_{(2)}\) \(\begin{align*}\cdots (1) &amp;amp;= \frac{\partial}{\partial v_c} u_o^T v_c \\ &amp;amp;= u_o \end{align*}\) \(\begin{align*}\cdots (2) &amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \frac{\partial}{\partial v_c} \exp(u_x^T v_c) \\ &amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \exp(u_x^T v_c) \cdot \frac{\partial}{\partial v_c} u_x^T v_c \\ &amp;amp;= \frac{1}{\sum_{w=1}^V \exp(u_w^T v_c)} \cdot \sum_{x=1}^V \exp(u_x^T v_c) \cdot u_x \\ \end{align*} \\\) \(\begin{align*} \frac{\partial}{\partial v_c} \log P(o|c) &amp;amp;= u_o - \frac{\sum_{x=1}^V \exp(u_x^T v_c)u_x}{\sum_{w=1}^V \exp(u_w^T v_c)} \\ &amp;amp; = u_o - \sum_{x=1}^V \underbrace{\frac{\exp(u_x^T v_c)}{\sum_{w=1}^V \exp(u_w^T v_c)}}_{\mbox{softmax formula}} u_x \\ &amp;amp; = u_o - \underbrace{\sum_{x=1}^V P(x|c) u_x}_{\mbox{Expectation}} \\ &amp;amp; = \mbox{observed} - \mbox{expected} \end{align*}\)</summary></entry><entry><title type="html">Mask R-CNN</title><link href="http://0.0.0.0:4000/Mask_R-CNN" rel="alternate" type="text/html" title="Mask R-CNN" /><published>2022-02-15T00:00:00+00:00</published><updated>2022-02-15T00:00:00+00:00</updated><id>http://0.0.0.0:4000/Mask_R-CNN</id><content type="html" xml:base="http://0.0.0.0:4000/Mask_R-CNN">&lt;h2 id=&quot;mask-r-cnn&quot;&gt;Mask R-CNN&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick, ICCV 2017&lt;/em&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/facebookresearch/Detectron&quot;&gt;https://github.com/facebookresearch/Detectron&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;whats-different&quot;&gt;What’s different?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Models so far&lt;br /&gt;
  R-CNN: 2-stage model for Object detection&lt;br /&gt;
  Fast R-CNN: RoI on feature map&lt;br /&gt;
  Faster R-CNN: RPN network&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Instance Segmentation&lt;/em&gt;&lt;br /&gt;
  Combining to tasks:
    &lt;ul&gt;
      &lt;li&gt;Object detection(Fast/Faster R-CNN): classify individual objects and localize each using a bounding box.&lt;/li&gt;
      &lt;li&gt;Semantic segmentation(FCN; Fully Convolutional Network): classify each pixel into a fixed set of categories without differentiating object instances.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Mask R-CNN:&lt;br /&gt;
  1) Model for &lt;em&gt;instance segmentation&lt;/em&gt;: Mask prediction branch&lt;br /&gt;
  2) FPN(feature pyramid network) before RPN&lt;br /&gt;
  3) RoI align&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mask-prediction&quot;&gt;Mask prediction&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Mask loss&lt;br /&gt;
  In the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask(ones to the object and zeros elsewhere) for each RoI. Defined multi-task loss on each sampled RoI: $L = L_{cls} + L_{box} + L_{mask}$&lt;br /&gt;
  The mask branch has a $Km^2$-dimensional output for each RoI, which encodes &lt;em&gt;K&lt;/em&gt; binary masks of resolution &lt;em&gt;m\times m&lt;/em&gt;, one for each of the &lt;em&gt;K&lt;/em&gt; classes. To this we apply a per-pixel sigmoid, and define $L_{mask}$ as the &lt;strong&gt;average binary cross-entropy loss&lt;/strong&gt;.  For an RoI associated with ground-truth class &lt;em&gt;k&lt;/em&gt;, $L_{mask}$ is only defined on the &lt;em&gt;k&lt;/em&gt;-th mask(other mask outputs do not contribute to the loss).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Decouples&lt;/em&gt; mask and class prediction&lt;br /&gt;
  This definition of mask loss allows the network to generate masks for each class without competition among classes; we rely on the dedicated classification branch to predict the class label used to select the output mask.&lt;br /&gt;
  With a per-pixel sigmoid and a binary loss, masks do not compete across classes; in contrast to FCNs for semantic segmentation using a per-pixel softmax and a multinomial cross-entropy loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mask Representation&lt;br /&gt;
  Unlike class labels or box offsets, extracting the spatial structure of masks can be addressed naturally by the pixel-to-pixel correspondence provided by convolutions.&lt;br /&gt;
  Predicting an $m\times m$ mask from each RoI using an FCN, allows each layer in the mask branch to have $m\times m$ object spatial layout without collapsing it into a vector representation that lacks spatial dimensions.&lt;br /&gt;
  This pixel-to-pixel behavior requires RoI features, small cropped feature maps, to be well aligned to faithfully preserve the explicit per-pixel spatial correspondence; &lt;em&gt;RoIAlign&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;roialign&quot;&gt;RoIAlign&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RoIPool(or RoI Pooling)&lt;br /&gt;
  &lt;em&gt;Quantizes&lt;/em&gt; a floating-number RoI to the discrete granularity(integerize by &lt;em&gt;rounding&lt;/em&gt;) of the feature map, its result is then subdivided into spatial bins, and finally feature values covered by each bin are aggregated(usually by max pooling).
    &lt;ul&gt;
      &lt;li&gt;Problem: Quantizations introduce misalignments between the RoI and the extracted features. This may not impact classification, which is robust to small translations, but it has a large negative effect on predicting pixel-accurate masks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RoIAlign layer&lt;br /&gt;
  Instead of any quantization of the RoI boundaries or bins, use bilinear interpolation(&lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial transformer networks&lt;/a&gt;) to compute the exact values of the input features at four regularly sampled locations in each RoI bin, and aggregate the result(using max or average).&lt;br /&gt;
  e.g.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_1.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  (from CS231n lecture)&lt;br /&gt;
  Feature $f_{xy}$ for point $(x, y)$ is a linear combination of features at its four neighboring grid cells:&lt;br /&gt;
  $f_{xy} = \sum_{i,j=1}^2 f_{i,j} \text{max}(0, 1 - \left\vert x - x_i \right\vert) \text{max}(0, 1 - \left\vert y - y_i \right\vert)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;RoIAlign improves mask accuracy by relative 10% to 50%, showing bigger gains under stricter localization metrics.&lt;br /&gt;
&lt;!--
  [Understanding Region of Interest — (RoI Pooling)](https://towardsdatascience.com/understanding-region-of-interest-part-1-roi-pooling-e4f5dd65bb44)  
  [Understanding Region of Interest — (RoI Align and RoI Warp)](https://towardsdatascience.com/understanding-region-of-interest-part-2-roi-align-and-roi-warp-f795196fc193)  
--&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;backbone&lt;/em&gt;:  Faster R-CNN with an FPN(ResNet-FPN)
    &lt;ul&gt;
      &lt;li&gt;FPN, Feature pyramid network(&lt;a href=&quot;https://arxiv.org/abs/1612.03144&quot;&gt;Lin et al.&lt;/a&gt;):&lt;br /&gt;
  Uses a top-down architecture with lateral connections to build an in-network feature pyramid from a single-scale input.&lt;/li&gt;
      &lt;li&gt;RPN:&lt;br /&gt;
  RoI align on each FPN feature maps&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;head&lt;/em&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  Add a fully convolutional mask prediction branch, extending the Faster R-CNN box heads from the ResNet and FPN papers. Train with additional mask loss.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Comparison to the sota methods in instance segmentation&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/papers/Mask_R-CNN_3.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/papers/Mask_R-CNN_4.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ablations
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_5.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Multinomial vs. Independent Masks&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_6.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Class-Specific vs. Class-Agnostic Masks&lt;/strong&gt;&lt;br /&gt;
  Interestingly, Mask R-CNN with classagnostic masks(predicting a single &lt;em&gt;m×m&lt;/em&gt; output regardless of class)) is nearly as effective as class-specific masks(default; &lt;em&gt;m×m&lt;/em&gt; mask per class).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;RoIAlign&lt;/strong&gt;&lt;br /&gt;
  ResNet50-C4 backbone of stride 16&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_7.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  ResNet-50-C5 backbone of stride 32&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_8.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  Note that with RoIAlign, using stride-32 C5 features is more accurate than using stride-16 C4 features. Used with FPN, which has finer multi-level strides, RoIAlign shows better result.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Mask branch&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_9.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bounding Box Detection Results&lt;/strong&gt;&lt;br /&gt;
  Our approach largely closes the gap between object detection and the more 	challenging instance segmentation task.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/papers/Mask_R-CNN_10.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;mask-r-cnn-for-human-pose-estimation&quot;&gt;Mask R-CNN for Human Pose Estimation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;By modeling a keypoint’s location as a one-hot mask, and adopt Mask R-CNN to predict &lt;em&gt;K&lt;/em&gt; masks, one for each of &lt;em&gt;K&lt;/em&gt; keypoint types, this framework can easily be extended to human pose estimation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Main Results and Ablations&lt;/strong&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_11.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_12.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/Mask_R-CNN_13.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$\therefore$ We have &lt;em&gt;a unified model that can simultaneously predict boxes, segments, and keypoints&lt;/em&gt; while running at 5 fps.&lt;/p&gt;

&lt;!-- https://ganghee-lee.tistory.com/40 --&gt;</content><author><name>Darron Kwon</name></author><category term="papers" /><summary type="html">Mask R-CNN Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick, ICCV 2017 https://github.com/facebookresearch/Detectron What’s different? Models so far R-CNN: 2-stage model for Object detection Fast R-CNN: RoI on feature map Faster R-CNN: RPN network Instance Segmentation Combining to tasks: Object detection(Fast/Faster R-CNN): classify individual objects and localize each using a bounding box. Semantic segmentation(FCN; Fully Convolutional Network): classify each pixel into a fixed set of categories without differentiating object instances. Mask R-CNN: 1) Model for instance segmentation: Mask prediction branch 2) FPN(feature pyramid network) before RPN 3) RoI align Mask prediction Mask loss In the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask(ones to the object and zeros elsewhere) for each RoI. Defined multi-task loss on each sampled RoI: $L = L_{cls} + L_{box} + L_{mask}$ The mask branch has a $Km^2$-dimensional output for each RoI, which encodes K binary masks of resolution m\times m, one for each of the K classes. To this we apply a per-pixel sigmoid, and define $L_{mask}$ as the average binary cross-entropy loss. For an RoI associated with ground-truth class k, $L_{mask}$ is only defined on the k-th mask(other mask outputs do not contribute to the loss). Decouples mask and class prediction This definition of mask loss allows the network to generate masks for each class without competition among classes; we rely on the dedicated classification branch to predict the class label used to select the output mask. With a per-pixel sigmoid and a binary loss, masks do not compete across classes; in contrast to FCNs for semantic segmentation using a per-pixel softmax and a multinomial cross-entropy loss. Mask Representation Unlike class labels or box offsets, extracting the spatial structure of masks can be addressed naturally by the pixel-to-pixel correspondence provided by convolutions. Predicting an $m\times m$ mask from each RoI using an FCN, allows each layer in the mask branch to have $m\times m$ object spatial layout without collapsing it into a vector representation that lacks spatial dimensions. This pixel-to-pixel behavior requires RoI features, small cropped feature maps, to be well aligned to faithfully preserve the explicit per-pixel spatial correspondence; RoIAlign. RoIAlign RoIPool(or RoI Pooling) Quantizes a floating-number RoI to the discrete granularity(integerize by rounding) of the feature map, its result is then subdivided into spatial bins, and finally feature values covered by each bin are aggregated(usually by max pooling). Problem: Quantizations introduce misalignments between the RoI and the extracted features. This may not impact classification, which is robust to small translations, but it has a large negative effect on predicting pixel-accurate masks. RoIAlign layer Instead of any quantization of the RoI boundaries or bins, use bilinear interpolation(Spatial transformer networks) to compute the exact values of the input features at four regularly sampled locations in each RoI bin, and aggregate the result(using max or average). e.g. (from CS231n lecture) Feature $f_{xy}$ for point $(x, y)$ is a linear combination of features at its four neighboring grid cells: $f_{xy} = \sum_{i,j=1}^2 f_{i,j} \text{max}(0, 1 - \left\vert x - x_i \right\vert) \text{max}(0, 1 - \left\vert y - y_i \right\vert)$ RoIAlign improves mask accuracy by relative 10% to 50%, showing bigger gains under stricter localization metrics. Network Architecture backbone: Faster R-CNN with an FPN(ResNet-FPN) FPN, Feature pyramid network(Lin et al.): Uses a top-down architecture with lateral connections to build an in-network feature pyramid from a single-scale input. RPN: RoI align on each FPN feature maps head: Add a fully convolutional mask prediction branch, extending the Faster R-CNN box heads from the ResNet and FPN papers. Train with additional mask loss. Experiments Comparison to the sota methods in instance segmentation Ablations Architecture Multinomial vs. Independent Masks Class-Specific vs. Class-Agnostic Masks Interestingly, Mask R-CNN with classagnostic masks(predicting a single m×m output regardless of class)) is nearly as effective as class-specific masks(default; m×m mask per class). RoIAlign ResNet50-C4 backbone of stride 16 ResNet-50-C5 backbone of stride 32 Note that with RoIAlign, using stride-32 C5 features is more accurate than using stride-16 C4 features. Used with FPN, which has finer multi-level strides, RoIAlign shows better result. Mask branch Bounding Box Detection Results Our approach largely closes the gap between object detection and the more challenging instance segmentation task. Mask R-CNN for Human Pose Estimation By modeling a keypoint’s location as a one-hot mask, and adopt Mask R-CNN to predict K masks, one for each of K keypoint types, this framework can easily be extended to human pose estimation. Main Results and Ablations: $\therefore$ We have a unified model that can simultaneously predict boxes, segments, and keypoints while running at 5 fps.</summary></entry><entry><title type="html">Starfish detection w/ TF Object Detection API</title><link href="http://0.0.0.0:4000/starfish_detection" rel="alternate" type="text/html" title="Starfish detection w/ TF Object Detection API" /><published>2022-02-14T15:00:00+00:00</published><updated>2022-02-14T15:00:00+00:00</updated><id>http://0.0.0.0:4000/starfish_detection</id><content type="html" xml:base="http://0.0.0.0:4000/starfish_detection">&lt;h2 id=&quot;tensorflow---help-protect-the-great-barrier-reef&quot;&gt;TensorFlow - Help Protect the Great Barrier Reef&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Worked in Feb. 2022. to study object detection model&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Task:&lt;br /&gt;
  Underwater + Small object detection&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Score(IoU=0.50:0.95):&lt;br /&gt;
  &lt;em&gt;mAP@100: 0.364686 / AR@100: 0.491768 / Expected F2: 0.459727&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Direct link: &lt;a href=&quot;https://www.kaggle.com/kwondalhyeon/starfish-detection-w-tf-object-detection-api?scriptVersionId=87885389&quot;&gt;kaggle notebook&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;iframe src=&quot;https://www.kaggle.com/embed/kwondalhyeon/starfish-detection-w-tf-object-detection-api?kernelSessionId=87885389&quot; height=&quot;1200&quot; style=&quot;margin: 0 auto; width: 100%; max-width: 100%;&quot; frameborder=&quot;0&quot; scrolling=&quot;auto&quot; title=&quot;Starfish detection w/ TF Object Detection API&quot;&gt;&lt;/iframe&gt;</content><author><name>Darron Kwon</name></author><category term="projects" /><summary type="html">TensorFlow - Help Protect the Great Barrier Reef Worked in Feb. 2022. to study object detection model Task: Underwater + Small object detection Score(IoU=0.50:0.95): mAP@100: 0.364686 / AR@100: 0.491768 / Expected F2: 0.459727 Direct link: kaggle notebook</summary></entry><entry><title type="html">cs231n - Lecture 15. Detection and Segmentation</title><link href="http://0.0.0.0:4000/cs231n_lec15" rel="alternate" type="text/html" title="cs231n - Lecture 15. Detection and Segmentation" /><published>2022-02-07T00:00:00+00:00</published><updated>2022-02-07T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cs231n_lec15</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec15">&lt;h3 id=&quot;computer-vision-tasks&quot;&gt;Computer Vision Tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Image Classification: No spatial extent&lt;/li&gt;
  &lt;li&gt;Semantic Segmentation: No objects, just pixels&lt;/li&gt;
  &lt;li&gt;Object Detection/ Instance Segmentation: Multiple objects&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;semantic-segmentation&quot;&gt;Semantic Segmentation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Paired training data:&lt;br /&gt;
  For each training image, &lt;strong&gt;each pixel is labeled&lt;/strong&gt; with a semantic category.&lt;/li&gt;
  &lt;li&gt;At test time, classify each pixel of a new image.&lt;/li&gt;
  &lt;li&gt;Problem:&lt;br /&gt;
  Classifying with only single pixel does not include context information.&lt;/li&gt;
  &lt;li&gt;Idea:
    &lt;ul&gt;
      &lt;li&gt;Sliding Window&lt;br /&gt;
  Extract patch from full image, classify center pixel with CNN.&lt;br /&gt;
  $\color{red}{(-)}$ Very inefficient, not reusing shared features between overlapping patches.&lt;/li&gt;
      &lt;li&gt;Convolution&lt;br /&gt;
  Encode the entire image with conv net, and do semantic segmentation on top.&lt;br /&gt;
  $\color{red}{(-)}$ CNN architectures often change the spatial sizes, but semantic segmentation requires the output size to be same as input size.&lt;/li&gt;
      &lt;li&gt;Fully Convolutional&lt;br /&gt;
  Design a network with &lt;strong&gt;only&lt;/strong&gt; convolutional layers without downsampling operators&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
  $\color{red}{(-)}$ convolutions at original image resolution is very expensive&lt;br /&gt;
  $\rightarrow$ Design convolutional network with &lt;strong&gt;downsampling and upsampling&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Downsampling: Pooling, strided convolution&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In-Network Upsampling: Unpooling, strided transpose convolution&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unpooling:&lt;br /&gt;
  Nearest Neighbor: copy-paste to extended region&lt;br /&gt;
  “Bed of Nails”: no positional argument, pad with zeros&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Max Unpooling: use positions from poolying layer ahead, pad with zeros&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learnable Downsampling: Strided convolution&lt;br /&gt;
  Output is a dot product between filter and input&lt;br /&gt;
  Stride gives ratio between movement in input and output&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learnable Upsampling: Transposed convolution&lt;br /&gt;
  Input gives weight for filter&lt;br /&gt;
  Output contains copies of the filter weighted by the input, summing at where at overlaps in the output&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Summary&lt;br /&gt;
  Label each pixel in the image with a category label&lt;br /&gt;
  Don’t differentiate instances, only care about pixels&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;object-detection&quot;&gt;Object Detection&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multiple Objects:&lt;br /&gt;
  Each image needs a different number of outputs;&lt;br /&gt;
  $\rightarrow$ Apply a CNN to many different crops of the image, CNN classifies each crop as object or background.&lt;br /&gt;
  $\color{red}{(-)}$ Need to apply CNN to huge number of locations, scales, and aspect ratios, very computationally expensive.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;r-cnn&quot;&gt;R-CNN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_4.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Girshick et al, “Rich feature hierarchies for accurate object detection and
semantic segmentation”, CVPR 2014&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;2-stage Detector: Region Proposal + Region Classification
    &lt;ol&gt;
      &lt;li&gt;Image as input&lt;/li&gt;
      &lt;li&gt;Crop bounding boxes with Selective Search&lt;br /&gt;
 Warp into same size pixels for CNN model&lt;/li&gt;
      &lt;li&gt;Input Warped images into CNN&lt;/li&gt;
      &lt;li&gt;Run classification on each&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;ol&gt;
      &lt;li&gt;Region Proposals: Selective Search&lt;br /&gt;
 Find “blobby” image regions that are likely to contain objects.&lt;br /&gt;
 Relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU.&lt;/li&gt;
      &lt;li&gt;CNN:&lt;br /&gt;
 For a pre-trained CNN architecture, change the number of classes on the last classification layer(detection classes &lt;em&gt;N&lt;/em&gt; + background &lt;em&gt;1&lt;/em&gt;), fine-tune with dataset for Object Detection. From the region proposal input, outputs a fixed-length feature vector.&lt;/li&gt;
      &lt;li&gt;SVM: Category-Specific Linear SVMs&lt;br /&gt;
 Positive: ground-truth boxes&lt;br /&gt;
 Negative: IoU under 0.3&lt;br /&gt;
 Scores each feature vector for classes, classifies whether each one is positive/negative(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_object&lt;/code&gt;).&lt;/li&gt;
      &lt;li&gt;Non-Maximum Suppression: with concept of &lt;strong&gt;IoU&lt;/strong&gt;&lt;br /&gt;
 Intersection over Union; area of intersection divided by area of union&lt;br /&gt;
 If there are two boxes with IoU over 0.5, consider them proposed on the same object, leave one with the highest score.&lt;/li&gt;
      &lt;li&gt;Bounding Box Regression: adjust boxes from Selective Search
        &lt;ul&gt;
          &lt;li&gt;Algorithm:&lt;br /&gt;
 Assume a bounding box $P^i = (P_x^i, P_y^i, P_w^i, P_h^i)$,&lt;br /&gt;
 Ground-truth box $G = (G_x, G_y, G_w, G_h)$.&lt;br /&gt;
 Define a function $d$, mapping $P$ close to $G$;&lt;br /&gt;
 \(\hat{G}_x = P_w d_x(P) + P_x\)&lt;br /&gt;
 \(\hat{G}_y = P_h d_y(P) + P_y\)&lt;br /&gt;
 \(\hat{G}_w = P_w \mbox{exp}(d_w(P))\)&lt;br /&gt;
 \(\hat{G}_h = P_h \mbox{exp}(d_h(P))\)&lt;br /&gt;
 where $d_{\star}(P) = w_{\star}^T \phi_5(P)$, is modeled as a linear function(learnable weight vector &lt;em&gt;w&lt;/em&gt;) of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POOL5&lt;/code&gt; features of proposal &lt;em&gt;P&lt;/em&gt;($\phi_5(P)$). We learn $w_{\star}$ by optimizing the regularized least squares objective(Ridge regression)&lt;br /&gt;
  &lt;em&gt;Learnable parameters on: 2, 3, 5&lt;/em&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Summary:&lt;br /&gt;
  Score: 53.7% on Pascal VOC 2010&lt;br /&gt;
  Problem:&lt;br /&gt;
      1. Low Performance; Warping images into 224x224 size for AlexNet&lt;br /&gt;
      2. Slow; Using all candidates from Selective Search&lt;br /&gt;
      3. Not GPU-optimized; Using Selective Search and SVM&lt;br /&gt;
      4. No Back Propagation; Not sharing computations&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fast-r-cnn&quot;&gt;Fast R-CNN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_5.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Girshick, “Fast R-CNN”, ICCV 2015&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;idea:&lt;br /&gt;
  Pass the image through convnet &lt;strong&gt;before&lt;/strong&gt; cropping. Crop the conv feature instead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;ol&gt;
      &lt;li&gt;Pass the full image through pre-trained CNN and extract feature maps.&lt;/li&gt;
      &lt;li&gt;Get RoIs from a proposal method(Selective Search) and crop by RoI Pooling, get fixed size feature vectors.&lt;/li&gt;
      &lt;li&gt;With RoI feature vectors, pass some fully connected layers and split into two branches.&lt;/li&gt;
      &lt;li&gt;1) pass softmax and classify the class of RoI. no SVM used. 2) Run bounding box regression.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Cropping Features: RoI Pool &lt;!--[](href) link to terminologies --&gt;
    &lt;ol&gt;
      &lt;li&gt;Project RoI proposals(on input image) onto CNN image features.&lt;/li&gt;
      &lt;li&gt;Divide into subregions.&lt;/li&gt;
      &lt;li&gt;Run pooling(Max-pool) within each subregion.&lt;br /&gt;
  $\rightarrow$ Region features always be the same size regardless of input region size&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;faster-r-cnn&quot;&gt;Faster R-CNN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_6.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Ren et al, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”, NIPS 2015&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;idea:&lt;br /&gt;
  Fast R-CNN is not GPU-optimized; runtime dominated by region proposals.&lt;br /&gt;
  By inserting Region Proposal Network(RPN), implemented end-to-end architecture.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;ol&gt;
      &lt;li&gt;Pass the full image through pre-trained CNN and extract feature maps.&lt;/li&gt;
      &lt;li&gt;RPN:&lt;br /&gt;
 For &lt;em&gt;K&lt;/em&gt; different anchor boxes of different size and scale at each point in the feature map, predict whether it contains an object(binary classification), and also predict a corrections from the anchor to the ground-truth box(regress 4 numbers per pixel).&lt;br /&gt;
 &lt;img src=&quot;/assets/images/cs231n_lec15_7.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;Jointly train with 4 losses:&lt;br /&gt;
 1) RPN classify object / not object&lt;br /&gt;
 2) RPN regress box coordinates&lt;br /&gt;
 3) Final classification score (object classes)&lt;br /&gt;
 4) Final box coordinates&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Glossing over many details:
    &lt;ul&gt;
      &lt;li&gt;Ignore overlapping proposals with non-max suppression&lt;/li&gt;
      &lt;li&gt;How are anchors determined?&lt;/li&gt;
      &lt;li&gt;How do we sample positive / negative samples for training the RPN?&lt;/li&gt;
      &lt;li&gt;How to parameterize bounding box regression?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Two-stage object detector:
    &lt;ul&gt;
      &lt;li&gt;First stage: Run once per image
        &lt;ul&gt;
          &lt;li&gt;Backbone network&lt;/li&gt;
          &lt;li&gt;Region proposal network(RPN)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Second stage: Run once per region
        &lt;ul&gt;
          &lt;li&gt;Crop features: RoI pool/ align&lt;/li&gt;
          &lt;li&gt;Predict object class&lt;/li&gt;
          &lt;li&gt;Prediction bbox offset&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;single-stage-object-detectors-yolo--ssd--retinanet&quot;&gt;Single-Stage Object Detectors: YOLO / SSD / RetinaNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Algorithm:
    &lt;ol&gt;
      &lt;li&gt;Divide input imgae into grid&lt;/li&gt;
      &lt;li&gt;Image a set of &lt;strong&gt;base boxes&lt;/strong&gt; centered at each grid cell&lt;/li&gt;
      &lt;li&gt;Within each grid cell:
        &lt;ul&gt;
          &lt;li&gt;Regress from each of the &lt;em&gt;B&lt;/em&gt; base boxes to a final box with 5 numbers(dx, dy, dh, dw, confidence)&lt;/li&gt;
          &lt;li&gt;Predict scores for each of &lt;em&gt;C&lt;/em&gt; classes(including background as a class)&lt;/li&gt;
          &lt;li&gt;Looks a lot like RPN, but category-specific&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_8.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;instance-segmentation-mask-r-cnn&quot;&gt;Instance Segmentation: Mask R-CNN&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;He et al, “Mask R-CNN”, ICCV 2017&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_9.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_10.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_11.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_12.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_13.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;open-source-frameworks&quot;&gt;Open Source Frameworks&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/object_detection&quot;&gt;TensorFlow Detection API&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;&gt;Detectron2(Pytorch)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;beyond-2d-object-detection&quot;&gt;Beyond 2D Object Detection&lt;/h2&gt;

&lt;h3 id=&quot;object-detection--captioning-dense-captioning&quot;&gt;Object Detection + Captioning: Dense Captioning&lt;/h3&gt;

&lt;h3 id=&quot;dense-video-captioning-timestep-t&quot;&gt;Dense Video Captioning: timestep “T”&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_14.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;objects--relationships-scene-graphs&quot;&gt;Objects + Relationships: Scene Graphs&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_15.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_16.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3d-object-detection&quot;&gt;3D Object Detection&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_17.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;2D bounding box: (x, y, w, h)&lt;br /&gt;
  $\rightarrow$ 3D oriented bounding box: (x, y, z, w, h, l, r, p, y)&lt;br /&gt;
  $\rightarrow$ Simplified bbox: no roll &amp;amp; pitch&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Simple Camera Model:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_18.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
  A point on the image plane corresponds to a &lt;strong&gt;ray&lt;/strong&gt; in the 3D space&lt;br /&gt;
  A 2D bounding box on an image is a &lt;strong&gt;frustrum&lt;/strong&gt; in the 3D space&lt;br /&gt;
  Localize an object in 3D: The object can be anywhere in the &lt;strong&gt;camera viewing frustrum&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Monocular Camera:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_19.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;80%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;Same idea as Faster RCNN, but proposals are in 3D&lt;/li&gt;
      &lt;li&gt;3D bounding box proposal, regress 3D box parameters + class score&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;3D Shape Prediction: Mesh R-CNN
&lt;img src=&quot;/assets/images/cs231n_lec15_20.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Computer Vision Tasks Image Classification: No spatial extent Semantic Segmentation: No objects, just pixels Object Detection/ Instance Segmentation: Multiple objects Semantic Segmentation Paired training data: For each training image, each pixel is labeled with a semantic category. At test time, classify each pixel of a new image. Problem: Classifying with only single pixel does not include context information. Idea: Sliding Window Extract patch from full image, classify center pixel with CNN. $\color{red}{(-)}$ Very inefficient, not reusing shared features between overlapping patches. Convolution Encode the entire image with conv net, and do semantic segmentation on top. $\color{red}{(-)}$ CNN architectures often change the spatial sizes, but semantic segmentation requires the output size to be same as input size. Fully Convolutional Design a network with only convolutional layers without downsampling operators $\color{red}{(-)}$ convolutions at original image resolution is very expensive $\rightarrow$ Design convolutional network with downsampling and upsampling Downsampling: Pooling, strided convolution In-Network Upsampling: Unpooling, strided transpose convolution Unpooling: Nearest Neighbor: copy-paste to extended region “Bed of Nails”: no positional argument, pad with zeros Max Unpooling: use positions from poolying layer ahead, pad with zeros Learnable Downsampling: Strided convolution Output is a dot product between filter and input Stride gives ratio between movement in input and output Learnable Upsampling: Transposed convolution Input gives weight for filter Output contains copies of the filter weighted by the input, summing at where at overlaps in the output Summary Label each pixel in the image with a category label Don’t differentiate instances, only care about pixels Object Detection Multiple Objects: Each image needs a different number of outputs; $\rightarrow$ Apply a CNN to many different crops of the image, CNN classifies each crop as object or background. $\color{red}{(-)}$ Need to apply CNN to huge number of locations, scales, and aspect ratios, very computationally expensive. R-CNN Girshick et al, “Rich feature hierarchies for accurate object detection and semantic segmentation”, CVPR 2014 2-stage Detector: Region Proposal + Region Classification Image as input Crop bounding boxes with Selective Search Warp into same size pixels for CNN model Input Warped images into CNN Run classification on each Algorithm: Region Proposals: Selective Search Find “blobby” image regions that are likely to contain objects. Relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU. CNN: For a pre-trained CNN architecture, change the number of classes on the last classification layer(detection classes N + background 1), fine-tune with dataset for Object Detection. From the region proposal input, outputs a fixed-length feature vector. SVM: Category-Specific Linear SVMs Positive: ground-truth boxes Negative: IoU under 0.3 Scores each feature vector for classes, classifies whether each one is positive/negative(is_object). Non-Maximum Suppression: with concept of IoU Intersection over Union; area of intersection divided by area of union If there are two boxes with IoU over 0.5, consider them proposed on the same object, leave one with the highest score. Bounding Box Regression: adjust boxes from Selective Search Algorithm: Assume a bounding box $P^i = (P_x^i, P_y^i, P_w^i, P_h^i)$, Ground-truth box $G = (G_x, G_y, G_w, G_h)$. Define a function $d$, mapping $P$ close to $G$; \(\hat{G}_x = P_w d_x(P) + P_x\) \(\hat{G}_y = P_h d_y(P) + P_y\) \(\hat{G}_w = P_w \mbox{exp}(d_w(P))\) \(\hat{G}_h = P_h \mbox{exp}(d_h(P))\) where $d_{\star}(P) = w_{\star}^T \phi_5(P)$, is modeled as a linear function(learnable weight vector w) of the POOL5 features of proposal P($\phi_5(P)$). We learn $w_{\star}$ by optimizing the regularized least squares objective(Ridge regression) Learnable parameters on: 2, 3, 5 Summary: Score: 53.7% on Pascal VOC 2010 Problem: 1. Low Performance; Warping images into 224x224 size for AlexNet 2. Slow; Using all candidates from Selective Search 3. Not GPU-optimized; Using Selective Search and SVM 4. No Back Propagation; Not sharing computations Fast R-CNN Girshick, “Fast R-CNN”, ICCV 2015 idea: Pass the image through convnet before cropping. Crop the conv feature instead. Algorithm: Pass the full image through pre-trained CNN and extract feature maps. Get RoIs from a proposal method(Selective Search) and crop by RoI Pooling, get fixed size feature vectors. With RoI feature vectors, pass some fully connected layers and split into two branches. 1) pass softmax and classify the class of RoI. no SVM used. 2) Run bounding box regression. Cropping Features: RoI Pool Project RoI proposals(on input image) onto CNN image features. Divide into subregions. Run pooling(Max-pool) within each subregion. $\rightarrow$ Region features always be the same size regardless of input region size Faster R-CNN Ren et al, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”, NIPS 2015 idea: Fast R-CNN is not GPU-optimized; runtime dominated by region proposals. By inserting Region Proposal Network(RPN), implemented end-to-end architecture. Algorithm: Pass the full image through pre-trained CNN and extract feature maps. RPN: For K different anchor boxes of different size and scale at each point in the feature map, predict whether it contains an object(binary classification), and also predict a corrections from the anchor to the ground-truth box(regress 4 numbers per pixel). Jointly train with 4 losses: 1) RPN classify object / not object 2) RPN regress box coordinates 3) Final classification score (object classes) 4) Final box coordinates Glossing over many details: Ignore overlapping proposals with non-max suppression How are anchors determined? How do we sample positive / negative samples for training the RPN? How to parameterize bounding box regression? Two-stage object detector: First stage: Run once per image Backbone network Region proposal network(RPN) Second stage: Run once per region Crop features: RoI pool/ align Predict object class Prediction bbox offset Single-Stage Object Detectors: YOLO / SSD / RetinaNet Algorithm: Divide input imgae into grid Image a set of base boxes centered at each grid cell Within each grid cell: Regress from each of the B base boxes to a final box with 5 numbers(dx, dy, dh, dw, confidence) Predict scores for each of C classes(including background as a class) Looks a lot like RPN, but category-specific Instance Segmentation: Mask R-CNN He et al, “Mask R-CNN”, ICCV 2017 Open Source Frameworks TensorFlow Detection API Detectron2(Pytorch) Beyond 2D Object Detection Object Detection + Captioning: Dense Captioning Dense Video Captioning: timestep “T” Objects + Relationships: Scene Graphs 3D Object Detection 2D bounding box: (x, y, w, h) $\rightarrow$ 3D oriented bounding box: (x, y, z, w, h, l, r, p, y) $\rightarrow$ Simplified bbox: no roll &amp;amp; pitch Simple Camera Model: A point on the image plane corresponds to a ray in the 3D space A 2D bounding box on an image is a frustrum in the 3D space Localize an object in 3D: The object can be anywhere in the camera viewing frustrum Monocular Camera: Same idea as Faster RCNN, but proposals are in 3D 3D bounding box proposal, regress 3D box parameters + class score 3D Shape Prediction: Mesh R-CNN</summary></entry><entry><title type="html">cs231n - Lecture 14. Visualizing and Understanding</title><link href="http://0.0.0.0:4000/cs231n_lec14" rel="alternate" type="text/html" title="cs231n - Lecture 14. Visualizing and Understanding" /><published>2022-02-02T15:00:00+00:00</published><updated>2022-02-02T15:00:00+00:00</updated><id>http://0.0.0.0:4000/cs231n_lec14</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec14">&lt;h2 id=&quot;whats-going-on-inside-convnets&quot;&gt;What’s going on inside ConvNets?&lt;/h2&gt;

&lt;h3 id=&quot;visualizing-what-models-have-learned&quot;&gt;Visualizing what models have learned&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;First layer: Visualize Filters&lt;br /&gt;
  At the first layer, we can visualize the raw weights and see gabor-like features. While the higher layers are about the weights to the activations from the layer before, it is not very interpretable.&lt;/li&gt;
  &lt;li&gt;Last layer: Visualize Representations(feature vector)
    &lt;ul&gt;
      &lt;li&gt;Nearest neighbors in feature space&lt;/li&gt;
      &lt;li&gt;Dimensionality reduction: clustering with similarity; using simple algorithm(PCA) or more complex one(t-SNE)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Visualizing Activations:&lt;br /&gt;
  &lt;em&gt;Yosinski et al, “Understanding Neural Networks Through Deep Visualization”, ICML DL Workshop 2014&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;understanding-input-pixels&quot;&gt;Understanding input pixels&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Maximally Activating Patches&lt;br /&gt;
  Run many images, record values of chosen channel(layer or neuron) and visualize image patches that correspond to maximal activations.&lt;/li&gt;
  &lt;li&gt;Saliency via Occlusion: &lt;em&gt;Zeiler and Fergus, “Visualizing and Understanding Convolutional Networks”, ECCV 2014&lt;/em&gt;.&lt;br /&gt;
  Mask part of the image before feeding to CNN, slide the occluder and check how much predicted probabilities change. Found that when there are multiple objects, the classification performance improved as the false class object masked.&lt;/li&gt;
  &lt;li&gt;Which pixels matter: Saliency via Backprop
    &lt;ul&gt;
      &lt;li&gt;Visualize the data gradient
        &lt;ol&gt;
          &lt;li&gt;foward an image&lt;/li&gt;
          &lt;li&gt;set &lt;em&gt;activations&lt;/em&gt; in layer of interest to all zero, except for a 1.0 for a neuron of interest&lt;/li&gt;
          &lt;li&gt;backprop to image input&lt;/li&gt;
          &lt;li&gt;squish the channels of gradients to get 1-dimensional activation map(or we can run segmentation using grabcut on this heatmap).&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;Can also find biases; to see what false classifier actually see&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Intermediate Features via (guided) backprop
    &lt;ul&gt;
      &lt;li&gt;Deconvolution-based approach:
        &lt;ol&gt;
          &lt;li&gt;Feed image into net&lt;/li&gt;
          &lt;li&gt;Pick a layer, set the gradient there to be all zero except for one 1 for some neuron of interest&lt;/li&gt;
          &lt;li&gt;Backprop to image(use guided backprop to pass the positive influence; activation with positive values, using modified relu or deconvnet)&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Visualizing CNN features: Gradient Ascent
  (Guided) backprop: Find the part of an image that a neuron responds to.&lt;br /&gt;
  Gradient ascent: Generate a synthetic image that maximally activates a neuron.&lt;br /&gt;
  \(I^{\ast} = \mbox{argmax}_I f(I) + R(I)\); (neuron value + regularizer)
    &lt;ul&gt;
      &lt;li&gt;Optimization-based approach: freeze/fix the weights and run “image update” to find images that maximize the score of chosen class.&lt;br /&gt;
  &lt;em&gt;Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, “Deep Inside Convolutional Networks: Visualizing Image Classification Models and Saliency Maps”, Workshop at ICLR, 2014&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;Find images that maximize some class score: \(\mbox{argmax}_I S_c(I) - \lambda {\lVert I \rVert}^2_2\)
        &lt;ol&gt;
          &lt;li&gt;initialize image to zeros&lt;/li&gt;
          &lt;li&gt;forward image to compute current scores&lt;/li&gt;
          &lt;li&gt;set the &lt;em&gt;gradient&lt;/em&gt; of the scores vector to be &lt;em&gt;I&lt;/em&gt;, then backprop to image&lt;/li&gt;
          &lt;li&gt;make a small update to the image&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;Instead of using L2 norm, we can use better regularizer(Gaussian blur image, Clip pixels with small values/gradients to 0, …)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimize in FC6 latent space instead of pixel space:&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec14_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec14_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adversarial-perturbations&quot;&gt;Adversarial perturbations&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Fooling Images / Adversarial Examples
    &lt;ol&gt;
      &lt;li&gt;Start from an arbitrary image&lt;/li&gt;
      &lt;li&gt;Pick an arbitrary class&lt;/li&gt;
      &lt;li&gt;Modify the image to maximize the class&lt;/li&gt;
      &lt;li&gt;Repeat until network is fooled&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Ian Goodfellow, “Explaining and Harnessing Adversarial Examples”, 2014&lt;/em&gt;&lt;br /&gt;
  Classifier is vulnerable to adversarical perturbation because of its linear nature. Check &lt;a href=&quot;https://www.youtube.com/watch?v=CIfsB_EYsVI&quot;&gt;Ian Goodfellow’s lecture&lt;/a&gt; from 2017&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Moosavi-Dezfooli, Seyed-Mohsen, et al. “Universal adversarial perturbations”, IEEE, 2017.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;style-transfer&quot;&gt;Style Transfer&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;DeepDream: Amplify existing features&lt;br /&gt;
  Rather than synthesizing an image to maximize a specific neuron, instead try to &lt;strong&gt;amplify&lt;/strong&gt; the neuron activations at some layer in the network.&lt;/li&gt;
  &lt;li&gt;Choose an image and a layer in a CNN; repeat:
    &lt;ol&gt;
      &lt;li&gt;Forward: compute activations at chosen layer&lt;/li&gt;
      &lt;li&gt;Set gradient of chosen layer &lt;em&gt;equal to its activation&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Backward: Compute gradient on image&lt;/li&gt;
      &lt;li&gt;Update image&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec14_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feature Inversion&lt;br /&gt;
  &lt;em&gt;Mahendran and Vedaldi, “Understanding Deep Image Representations by Inverting Them”, CVPR 2015&lt;/em&gt;&lt;br /&gt;
  Given a CNN feature vector for an image, find a new image that:
    &lt;ul&gt;
      &lt;li&gt;Matches the given feature vector&lt;/li&gt;
      &lt;li&gt;“looks natural” (image prior regularization)&lt;br /&gt;
  \(\begin{align*}
  x^{\ast} &amp;amp;= \underset{x\in\mathbb{R}^{H \times W \times C}}{\mbox{argmin}} l(\Phi(x), \Phi_0) + \lambda \mathcal{R}(x) \\
      &amp;amp; \mbox{where loss } l = {\lVert \Phi(x) - \Phi_0 \rVert}^2
  \end{align*}\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Texture Synthesis: Nearest Neighbor&lt;br /&gt;
  Given a sample patch of some texture, generate a bigger image of the same texture&lt;br /&gt;
  Generate pixels one at a time in scanline order; form neighborhood of already generated pixels and copy nearest neighbor from input&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Neural Texture Synthesis: Gram Matrix&lt;br /&gt;
  a pair-wise statistics; interpret given &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CxHxW&lt;/code&gt; features as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HxW&lt;/code&gt; grid of C-dimensional vectors. By computing outer product and sum up for all spacial locations($G=V^T V$), it gives &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CxC&lt;/code&gt; matrix measuring co-occurrence.
    &lt;ol&gt;
      &lt;li&gt;Pretrain a CNN on ImageNet (VGG-19)&lt;/li&gt;
      &lt;li&gt;Run input texture forward through CNN, record activations on every layer&lt;/li&gt;
      &lt;li&gt;At each layer compute the Gram matrix&lt;/li&gt;
      &lt;li&gt;Initialize generated image from random noise&lt;/li&gt;
      &lt;li&gt;Pass generated image through CNN, compute Gram matrix on each layer&lt;/li&gt;
      &lt;li&gt;Compute loss: weighted sum of L2 distance between Gram matrices&lt;/li&gt;
      &lt;li&gt;Backprop to get gradient on image&lt;/li&gt;
      &lt;li&gt;Make gradient step on image&lt;/li&gt;
      &lt;li&gt;GOTO 5
&lt;img src=&quot;/assets/images/cs231n_lec14_3.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Neural Style Transfer: Feature + Gram Reconstruction
    &lt;ol&gt;
      &lt;li&gt;Extract content targets (ConvNet activations of all layers for the given image)&lt;/li&gt;
      &lt;li&gt;Extract style targets (Gram matrices of ConvNet activations of all layers)&lt;/li&gt;
      &lt;li&gt;Optimize over image to have:
        &lt;ul&gt;
          &lt;li&gt;The content of the content image(activations match content)&lt;/li&gt;
          &lt;li&gt;The style of the stlye image(Gram matrices of activations match style)&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec14_4.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;Problem: requires many forward, backward passes; VGG is very slow&lt;/li&gt;
          &lt;li&gt;Solution: Train another neural network; &lt;strong&gt;Fast Style Transfer&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Fast Style Transfer
  &lt;img src=&quot;/assets/images/cs231n_lec14_5.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">What’s going on inside ConvNets? Visualizing what models have learned First layer: Visualize Filters At the first layer, we can visualize the raw weights and see gabor-like features. While the higher layers are about the weights to the activations from the layer before, it is not very interpretable. Last layer: Visualize Representations(feature vector) Nearest neighbors in feature space Dimensionality reduction: clustering with similarity; using simple algorithm(PCA) or more complex one(t-SNE) Visualizing Activations: Yosinski et al, “Understanding Neural Networks Through Deep Visualization”, ICML DL Workshop 2014. Understanding input pixels Maximally Activating Patches Run many images, record values of chosen channel(layer or neuron) and visualize image patches that correspond to maximal activations. Saliency via Occlusion: Zeiler and Fergus, “Visualizing and Understanding Convolutional Networks”, ECCV 2014. Mask part of the image before feeding to CNN, slide the occluder and check how much predicted probabilities change. Found that when there are multiple objects, the classification performance improved as the false class object masked. Which pixels matter: Saliency via Backprop Visualize the data gradient foward an image set activations in layer of interest to all zero, except for a 1.0 for a neuron of interest backprop to image input squish the channels of gradients to get 1-dimensional activation map(or we can run segmentation using grabcut on this heatmap). Can also find biases; to see what false classifier actually see Intermediate Features via (guided) backprop Deconvolution-based approach: Feed image into net Pick a layer, set the gradient there to be all zero except for one 1 for some neuron of interest Backprop to image(use guided backprop to pass the positive influence; activation with positive values, using modified relu or deconvnet) Visualizing CNN features: Gradient Ascent (Guided) backprop: Find the part of an image that a neuron responds to. Gradient ascent: Generate a synthetic image that maximally activates a neuron. \(I^{\ast} = \mbox{argmax}_I f(I) + R(I)\); (neuron value + regularizer) Optimization-based approach: freeze/fix the weights and run “image update” to find images that maximize the score of chosen class. Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, “Deep Inside Convolutional Networks: Visualizing Image Classification Models and Saliency Maps”, Workshop at ICLR, 2014. Find images that maximize some class score: \(\mbox{argmax}_I S_c(I) - \lambda {\lVert I \rVert}^2_2\) initialize image to zeros forward image to compute current scores set the gradient of the scores vector to be I, then backprop to image make a small update to the image Instead of using L2 norm, we can use better regularizer(Gaussian blur image, Clip pixels with small values/gradients to 0, …) Optimize in FC6 latent space instead of pixel space: Adversarial perturbations Fooling Images / Adversarial Examples Start from an arbitrary image Pick an arbitrary class Modify the image to maximize the class Repeat until network is fooled Ian Goodfellow, “Explaining and Harnessing Adversarial Examples”, 2014 Classifier is vulnerable to adversarical perturbation because of its linear nature. Check Ian Goodfellow’s lecture from 2017 Moosavi-Dezfooli, Seyed-Mohsen, et al. “Universal adversarial perturbations”, IEEE, 2017. Style Transfer DeepDream: Amplify existing features Rather than synthesizing an image to maximize a specific neuron, instead try to amplify the neuron activations at some layer in the network. Choose an image and a layer in a CNN; repeat: Forward: compute activations at chosen layer Set gradient of chosen layer equal to its activation Backward: Compute gradient on image Update image Feature Inversion Mahendran and Vedaldi, “Understanding Deep Image Representations by Inverting Them”, CVPR 2015 Given a CNN feature vector for an image, find a new image that: Matches the given feature vector “looks natural” (image prior regularization) \(\begin{align*} x^{\ast} &amp;amp;= \underset{x\in\mathbb{R}^{H \times W \times C}}{\mbox{argmin}} l(\Phi(x), \Phi_0) + \lambda \mathcal{R}(x) \\ &amp;amp; \mbox{where loss } l = {\lVert \Phi(x) - \Phi_0 \rVert}^2 \end{align*}\) Texture Synthesis: Nearest Neighbor Given a sample patch of some texture, generate a bigger image of the same texture Generate pixels one at a time in scanline order; form neighborhood of already generated pixels and copy nearest neighbor from input Neural Texture Synthesis: Gram Matrix a pair-wise statistics; interpret given CxHxW features as HxW grid of C-dimensional vectors. By computing outer product and sum up for all spacial locations($G=V^T V$), it gives CxC matrix measuring co-occurrence. Pretrain a CNN on ImageNet (VGG-19) Run input texture forward through CNN, record activations on every layer At each layer compute the Gram matrix Initialize generated image from random noise Pass generated image through CNN, compute Gram matrix on each layer Compute loss: weighted sum of L2 distance between Gram matrices Backprop to get gradient on image Make gradient step on image GOTO 5 Neural Style Transfer: Feature + Gram Reconstruction Extract content targets (ConvNet activations of all layers for the given image) Extract style targets (Gram matrices of ConvNet activations of all layers) Optimize over image to have: The content of the content image(activations match content) The style of the stlye image(Gram matrices of activations match style) Problem: requires many forward, backward passes; VGG is very slow Solution: Train another neural network; Fast Style Transfer Fast Style Transfer</summary></entry><entry><title type="html">Unsupervised Representation Learning by Predicting Image Rotations</title><link href="http://0.0.0.0:4000/Rotation" rel="alternate" type="text/html" title="Unsupervised Representation Learning by Predicting Image Rotations" /><published>2022-01-24T15:00:00+00:00</published><updated>2022-01-24T15:00:00+00:00</updated><id>http://0.0.0.0:4000/Rotation</id><content type="html" xml:base="http://0.0.0.0:4000/Rotation">&lt;h2 id=&quot;unsupervised-representation-learning-by-predicting-image-rotations&quot;&gt;Unsupervised Representation Learning by Predicting Image Rotations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Gidaris et al. 2018&lt;/em&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/gidariss/FeatureLearningRotNet&quot;&gt;https://github.com/gidariss/FeatureLearningRotNet&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ConvNet:&lt;br /&gt;
  (+) Unparalleled capacity to learn high level semantic image features&lt;br /&gt;
  (-) Require massive amounts of manually labeled data, expensive and impractical to scale&lt;br /&gt;
  $\rightarrow$ &lt;em&gt;Unsupervised Learning&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Unsupervised semantic feature learning:&lt;br /&gt;
  Learn image features by training ConvNets to recognize the 2d rotated images as input. With apparently simple task, provides a very powerful supervisory signal for semantic feature learning(Conv). Evaluated in various unsupervised feature learning benchmarks, exceeds SotA performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;featurelearningrotnet&quot;&gt;FeatureLearningRotNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;How To:&lt;br /&gt;
  First define a small set of discrete geometric transformations, then each of those transformations are applied to each image on the dataset and produced transformed images are fed to ConvNet model that is trained to recognize the transformation of each image.
    &lt;ul&gt;
      &lt;li&gt;Set of geometric transformations define the classification pretext task that the ConvNet has to learn; to achieve unsupervised semantic feature learning, it is important to properly choose those geometric transformations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Purpose: to define the geometric transformations as rotations of 4 different degrees, ConvNet trained on the 4-way image classification task of recognizing one of the four Maximizing prob. $F^y(x^{y^{*}})$, probability of transformation &lt;em&gt;y&lt;/em&gt; predicted by &lt;em&gt;F&lt;/em&gt;, when given X is transformed by the transformation $y^{*}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;With idea: In order a ConvNet model to be able recognize the rotation transformations, it will require to understand the concept of the objects depicted in the image	such as their location, type, and pose.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;define a set of &lt;em&gt;K&lt;/em&gt; discrete geometric transformations \(G = \{g(\cdot\vert y)\}_{y=1}^K\), where $g(.\vert y)$ applies to input X, transformed image $X^y = g(X\vert y)$&lt;/li&gt;
  &lt;li&gt;ConvNet model F(.) gets as input an image $X^{y^{\ast}}$, to recognize unknown $y^{\ast}$ yields as output a probability distribution over all possible transformations	\(F(X^{y^{\ast}}\vert\theta) = \{ F^y(X^{y^{\ast}}\vert\theta) \}_{y=1}^K\), output &lt;em&gt;F&lt;/em&gt; returns probs for all classes $y$.&lt;/li&gt;
  &lt;li&gt;Therefore, &lt;em&gt;N&lt;/em&gt; training images \(D = \{ X_i \}_{i=0}^N\), the self-supervised training objective that ConvNet must learn to solve is:&lt;br /&gt;
 \(\mbox{min}_{\theta}\frac{1}{N}\sum_{i=1}^N \mbox{loss}(X_i,\theta)\),&lt;br /&gt;
 where the loss function is defined as:&lt;br /&gt;
 \(\mbox{loss}(X_i,\theta) = -\frac{1}{K}\sum_{y=1}^K \log(F^y(g(X_i|y)|\theta))\)&lt;br /&gt;
 (negative sum of log probs &lt;em&gt;F&lt;/em&gt; for all classes &lt;em&gt;y&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;2d image rotations:&lt;br /&gt;
  $Rot(X, \phi)$, operator that rotates image X by $\phi$ degrees&lt;br /&gt;
  In this case 0, 90, 180, 270; K=4 for G, where $g(X|y)=Rot(X,(y-1)90)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;forcing-the-learning-of-semantic-features&quot;&gt;Forcing the learning of semantic features&lt;/h3&gt;
&lt;p&gt;Fact that it is essentially impossible for a ConvNet model to effectively perform the above rotation recognition task, unless it has first learnt to recognize and detect classes of objects as well as their &lt;strong&gt;semantic parts&lt;/strong&gt; in images.&lt;br /&gt;
$\rightarrow$ &lt;strong&gt;ATTENTION MAPS&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/papers/papers_rotation_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;By comparing the attention maps from two models trained on supervised and unsupervised way, we observe that both models seem to focus on roughly &lt;strong&gt;the same image regions&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/papers/papers_rotation_1.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Also, trained on the proposed rotation recognition task, &lt;strong&gt;visualized layer filters&lt;/strong&gt; learnt appear to have a big variety of edge filters on multiple orientations and multiple frequencies, then the filters learnt by the supervised task.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Absence of low-level visual artifacts&lt;/strong&gt;:&lt;br /&gt;
  An additional important advantage of using image rotations over other geometric transformations, is that they do not leave e any easily detectable low-level visual artifacts that will lead the ConvNet to learn trivial features with no practical value for the vision perception tasks.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Well-posedness&lt;/strong&gt;:&lt;br /&gt;
  Human captured images tend to depict objects in an “up-standing” position. When defining the rotation recognition task, there is usually no ambiguity of what is the rotation transformation.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Implementing image rotations&lt;/strong&gt;:&lt;br /&gt;
  Flip and transpose.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$*$ Activation-based Attention Maps from &lt;em&gt;“Paying More Attention to Attention”, Zagoruyko et al., 2017&lt;/em&gt; - &lt;a href=&quot;https://arxiv.org/abs/1612.03928&quot;&gt;https://arxiv.org/abs/1612.03928&lt;/a&gt;&lt;br /&gt;
  Activation tensor of a conv. layer: $A\in R^{C\times H\times W}$ consists of &lt;em&gt;C&lt;/em&gt; feautre planes with spatial dimensions &lt;em&gt;H&lt;/em&gt;x&lt;em&gt;W&lt;/em&gt;&lt;br /&gt;
  Activation-based mapping function &lt;em&gt;F&lt;/em&gt; w.r.t that layer: $\mathcal{F}: R^{C\times H\times W} \rightarrow R^{H\times W}$&lt;br /&gt;
  With implicit assumption: Absolute value of a hidden neuron activation(that results when the network is evaluated on given input) can be used as an indication about the importance of that neuron w.r.t. the specific input.&lt;br /&gt;
  By considering, therefore, the absolute values of the elements of tensor A,	we construct a spatial attention map by computing statistics of these values	across the channel dimension(&lt;em&gt;C&lt;/em&gt;)
    &lt;ul&gt;
      &lt;li&gt;sum of abs: $F_{sum}(A)=\sum_{i=1}^C\vert A_i\vert$&lt;/li&gt;
      &lt;li&gt;sum of abs, raised to the power of p(&amp;gt;1): $F_{sum}^p(A) = \sum_{i=1}^C\vert A_i\vert^p$&lt;/li&gt;
      &lt;li&gt;max of abs, raised to the pwoer of p(&amp;gt;1): $F_{max}^p(A) = \mbox{max}_{i=1,C}\vert A_i\vert^p$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;transfer-learning&quot;&gt;Transfer Learning&lt;/h3&gt;
&lt;p&gt;With a model trained on proposed rotation recognition task with unlabeled data, freeze its early conv. layers and attach the layers from a supervised model, evaluate on a supervised task with a subset of labeled data.&lt;/p&gt;

&lt;h3 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h3&gt;

&lt;h4 id=&quot;cifar-10-experiments&quot;&gt;CIFAR-10 Experiments&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;RotNet&lt;/em&gt; implementation details&lt;/strong&gt;:&lt;br /&gt;
  Network-In-Network (NIN) architectures (&lt;em&gt;Lin et al., 2013&lt;/em&gt;)&lt;br /&gt;
  Pretask train: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;optimizer = SGD, batch_size = 128, momentum = 0.9, weight_decay = 5e−4, lr = 0.1, lr_decay = 0.2 (after 30, 60, 80 epochs), num_epochs = 100&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evaluation of the learned feature hierarchies&lt;/strong&gt;:&lt;br /&gt;
  Using the CIFAR-10 training images, train three &lt;em&gt;RotNet&lt;/em&gt; models which have 3, 4, and 5 conv. blocks respectively. Afterwards, on top of the feature maps generated by each conv. block of each &lt;em&gt;RotNet&lt;/em&gt; model, add classifiers trained in a supervised way on the object recognition task of CIFAR-10; consists of 3 FC layers. The accuracy results of CIFAR-10 test set:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_2.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;
    &lt;ol&gt;
      &lt;li&gt;In all cases the feature maps generated by the 2nd conv. block achieve the highest accuracy, i.e., between 88.26% and 89.06%. Then the accuracy gradually degrades, which we assume is because they start becoming more and more specific on the self-supervised task of rotation prediction.&lt;/li&gt;
      &lt;li&gt;Observe that increasing the total depth of the RotNet models leads to increased object recognition performance by the feature maps generated by earlier layers. We assume that this is because increasing the depth of the model and thus the complexity of its head (i.e., top ConvNet layers) allows the features of earlier layers to be less specific to the rotation prediction task.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Exploring the quality of the learned features w.r.t. the number of recognized rotations&lt;/strong&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_3.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;Observed that 4 discrete rotations as proposed achieved better performance over other cases.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Comparison against supervised and other unsupervised methods&lt;/strong&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_4.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_5.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;Model using the feature maps generated by the 2nd conv. block of a RotNet model with 4 conv. blocks in total.&lt;br /&gt;
  (a) &lt;em&gt;RotNet + non-linear&lt;/em&gt;: a non-linear classifier with 3 fully connected layers&lt;br /&gt;
  (b) &lt;em&gt;RotNet +conv.&lt;/em&gt;: three conv. layers + a linear prediction layer&lt;/li&gt;
      &lt;li&gt;Achieved best result among the unsupervised approaches&lt;/li&gt;
      &lt;li&gt;Very close to the fully supervised NIN model&lt;/li&gt;
      &lt;li&gt;Observed that fine-tuning the unsupervised learned features further improves the classification performance&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Correlation between object classification task and rotation prediction task&lt;/strong&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_6.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;As the ability of the RotNet features for solving the rotation prediction task improves(as the rotation prediction accuracy increases), their ability to help solving the object recognition task improves as well(the object recognition accuracy also increases).&lt;/li&gt;
      &lt;li&gt;Object recognition accuracy converges fast w.r.t. the number of training epochs used for solving the pretext task of rotation prediction.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Semi-supervised setting&lt;/strong&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_7.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;br /&gt;
  Train a 4 block RotNet model on the rotation prediction task using the entire image dataset of CIFAR-10, then train on top of its feature maps object classifiers using only a subset of the available images and their corresponding labels. As feature maps we use those from 2nd conv. block of the RotNet model. As a classifier we use a set of convolutional layers of the same e architecture as the 3rd conv. block of a NIN model plus a linear classifier, all randomly initialized.  For training the object classifier we use for each category 20, 100, 400, 1000, or 5000 image examples. Comapred with a supervised model that is trained only on the available examples each time:
    &lt;ul&gt;
      &lt;li&gt;Observed that our unsupervised trained model exceeds in this semi-supervised setting the supervised model when the number of examples per category drops below 1000; can be useful when there are only small subset of labeled data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;evaluation-of-self-supervised-features-trained-in-imagenet&quot;&gt;Evaluation of self-supervised features trained in ImageNet&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Train a &lt;em&gt;RotNet&lt;/em&gt; model on the training images of the ImageNet dataset and evaluate the performance of the self-self-supervised features on the image classification tasks of ImageNet, Places, and PASCAL VOC datasets and on the object detection and object segmentation tasks of PASCAL VOC.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Implementation details&lt;/strong&gt;:&lt;br /&gt;
  Based on an AlexNet architecture, without local response normalization units, dropout units, or groups in the colvolutional layers, while it includes batch normalization units after each linear layer (either convolutional or fully connected).&lt;br /&gt;
  Train with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SGD, batch_size=192, momentum=0.9, weight_decay=5e-4, lr=0.01, lr_decay=0.1(after 10, 20 epochs), num_epochs=30&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ImageNet classification task&lt;/strong&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_8.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;Observed that &lt;em&gt;our approach surpasses all the other methods by a significant margin&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transfer learning evaluation on PASCAL VOC&lt;/strong&gt;:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_9.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Places classification task:&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/papers/papers_rotation_10.png&quot; alt=&quot;png&quot; width=&quot;100%&amp;quot;, height=&amp;quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="papers" /><summary type="html">Unsupervised Representation Learning by Predicting Image Rotations Gidaris et al. 2018 https://github.com/gidariss/FeatureLearningRotNet ConvNet: (+) Unparalleled capacity to learn high level semantic image features (-) Require massive amounts of manually labeled data, expensive and impractical to scale $\rightarrow$ Unsupervised Learning Unsupervised semantic feature learning: Learn image features by training ConvNets to recognize the 2d rotated images as input. With apparently simple task, provides a very powerful supervisory signal for semantic feature learning(Conv). Evaluated in various unsupervised feature learning benchmarks, exceeds SotA performance. FeatureLearningRotNet How To: First define a small set of discrete geometric transformations, then each of those transformations are applied to each image on the dataset and produced transformed images are fed to ConvNet model that is trained to recognize the transformation of each image. Set of geometric transformations define the classification pretext task that the ConvNet has to learn; to achieve unsupervised semantic feature learning, it is important to properly choose those geometric transformations. Purpose: to define the geometric transformations as rotations of 4 different degrees, ConvNet trained on the 4-way image classification task of recognizing one of the four Maximizing prob. $F^y(x^{y^{*}})$, probability of transformation y predicted by F, when given X is transformed by the transformation $y^{*}$. With idea: In order a ConvNet model to be able recognize the rotation transformations, it will require to understand the concept of the objects depicted in the image such as their location, type, and pose. Overview define a set of K discrete geometric transformations \(G = \{g(\cdot\vert y)\}_{y=1}^K\), where $g(.\vert y)$ applies to input X, transformed image $X^y = g(X\vert y)$ ConvNet model F(.) gets as input an image $X^{y^{\ast}}$, to recognize unknown $y^{\ast}$ yields as output a probability distribution over all possible transformations \(F(X^{y^{\ast}}\vert\theta) = \{ F^y(X^{y^{\ast}}\vert\theta) \}_{y=1}^K\), output F returns probs for all classes $y$. Therefore, N training images \(D = \{ X_i \}_{i=0}^N\), the self-supervised training objective that ConvNet must learn to solve is: \(\mbox{min}_{\theta}\frac{1}{N}\sum_{i=1}^N \mbox{loss}(X_i,\theta)\), where the loss function is defined as: \(\mbox{loss}(X_i,\theta) = -\frac{1}{K}\sum_{y=1}^K \log(F^y(g(X_i|y)|\theta))\) (negative sum of log probs F for all classes y) 2d image rotations: $Rot(X, \phi)$, operator that rotates image X by $\phi$ degrees In this case 0, 90, 180, 270; K=4 for G, where $g(X|y)=Rot(X,(y-1)90)$ Forcing the learning of semantic features Fact that it is essentially impossible for a ConvNet model to effectively perform the above rotation recognition task, unless it has first learnt to recognize and detect classes of objects as well as their semantic parts in images. $\rightarrow$ ATTENTION MAPS By comparing the attention maps from two models trained on supervised and unsupervised way, we observe that both models seem to focus on roughly the same image regions. Also, trained on the proposed rotation recognition task, visualized layer filters learnt appear to have a big variety of edge filters on multiple orientations and multiple frequencies, then the filters learnt by the supervised task. Absence of low-level visual artifacts: An additional important advantage of using image rotations over other geometric transformations, is that they do not leave e any easily detectable low-level visual artifacts that will lead the ConvNet to learn trivial features with no practical value for the vision perception tasks. Well-posedness: Human captured images tend to depict objects in an “up-standing” position. When defining the rotation recognition task, there is usually no ambiguity of what is the rotation transformation. Implementing image rotations: Flip and transpose. $*$ Activation-based Attention Maps from “Paying More Attention to Attention”, Zagoruyko et al., 2017 - https://arxiv.org/abs/1612.03928 Activation tensor of a conv. layer: $A\in R^{C\times H\times W}$ consists of C feautre planes with spatial dimensions HxW Activation-based mapping function F w.r.t that layer: $\mathcal{F}: R^{C\times H\times W} \rightarrow R^{H\times W}$ With implicit assumption: Absolute value of a hidden neuron activation(that results when the network is evaluated on given input) can be used as an indication about the importance of that neuron w.r.t. the specific input. By considering, therefore, the absolute values of the elements of tensor A, we construct a spatial attention map by computing statistics of these values across the channel dimension(C) sum of abs: $F_{sum}(A)=\sum_{i=1}^C\vert A_i\vert$ sum of abs, raised to the power of p(&amp;gt;1): $F_{sum}^p(A) = \sum_{i=1}^C\vert A_i\vert^p$ max of abs, raised to the pwoer of p(&amp;gt;1): $F_{max}^p(A) = \mbox{max}_{i=1,C}\vert A_i\vert^p$ Transfer Learning With a model trained on proposed rotation recognition task with unlabeled data, freeze its early conv. layers and attach the layers from a supervised model, evaluate on a supervised task with a subset of labeled data. Experimental Results CIFAR-10 Experiments RotNet implementation details: Network-In-Network (NIN) architectures (Lin et al., 2013) Pretask train: optimizer = SGD, batch_size = 128, momentum = 0.9, weight_decay = 5e−4, lr = 0.1, lr_decay = 0.2 (after 30, 60, 80 epochs), num_epochs = 100 Evaluation of the learned feature hierarchies: Using the CIFAR-10 training images, train three RotNet models which have 3, 4, and 5 conv. blocks respectively. Afterwards, on top of the feature maps generated by each conv. block of each RotNet model, add classifiers trained in a supervised way on the object recognition task of CIFAR-10; consists of 3 FC layers. The accuracy results of CIFAR-10 test set: In all cases the feature maps generated by the 2nd conv. block achieve the highest accuracy, i.e., between 88.26% and 89.06%. Then the accuracy gradually degrades, which we assume is because they start becoming more and more specific on the self-supervised task of rotation prediction. Observe that increasing the total depth of the RotNet models leads to increased object recognition performance by the feature maps generated by earlier layers. We assume that this is because increasing the depth of the model and thus the complexity of its head (i.e., top ConvNet layers) allows the features of earlier layers to be less specific to the rotation prediction task. Exploring the quality of the learned features w.r.t. the number of recognized rotations: Observed that 4 discrete rotations as proposed achieved better performance over other cases. Comparison against supervised and other unsupervised methods: Model using the feature maps generated by the 2nd conv. block of a RotNet model with 4 conv. blocks in total. (a) RotNet + non-linear: a non-linear classifier with 3 fully connected layers (b) RotNet +conv.: three conv. layers + a linear prediction layer Achieved best result among the unsupervised approaches Very close to the fully supervised NIN model Observed that fine-tuning the unsupervised learned features further improves the classification performance Correlation between object classification task and rotation prediction task: As the ability of the RotNet features for solving the rotation prediction task improves(as the rotation prediction accuracy increases), their ability to help solving the object recognition task improves as well(the object recognition accuracy also increases). Object recognition accuracy converges fast w.r.t. the number of training epochs used for solving the pretext task of rotation prediction. Semi-supervised setting: Train a 4 block RotNet model on the rotation prediction task using the entire image dataset of CIFAR-10, then train on top of its feature maps object classifiers using only a subset of the available images and their corresponding labels. As feature maps we use those from 2nd conv. block of the RotNet model. As a classifier we use a set of convolutional layers of the same e architecture as the 3rd conv. block of a NIN model plus a linear classifier, all randomly initialized. For training the object classifier we use for each category 20, 100, 400, 1000, or 5000 image examples. Comapred with a supervised model that is trained only on the available examples each time: Observed that our unsupervised trained model exceeds in this semi-supervised setting the supervised model when the number of examples per category drops below 1000; can be useful when there are only small subset of labeled data. Evaluation of self-supervised features trained in ImageNet Train a RotNet model on the training images of the ImageNet dataset and evaluate the performance of the self-self-supervised features on the image classification tasks of ImageNet, Places, and PASCAL VOC datasets and on the object detection and object segmentation tasks of PASCAL VOC. Implementation details: Based on an AlexNet architecture, without local response normalization units, dropout units, or groups in the colvolutional layers, while it includes batch normalization units after each linear layer (either convolutional or fully connected). Train with SGD, batch_size=192, momentum=0.9, weight_decay=5e-4, lr=0.01, lr_decay=0.1(after 10, 20 epochs), num_epochs=30 ImageNet classification task: Observed that our approach surpasses all the other methods by a significant margin Transfer learning evaluation on PASCAL VOC: Places classification task:</summary></entry><entry><title type="html">cs231n - Lecture 13. Self-Supervised Learning</title><link href="http://0.0.0.0:4000/cs231n_lec13" rel="alternate" type="text/html" title="cs231n - Lecture 13. Self-Supervised Learning" /><published>2022-01-19T15:00:00+00:00</published><updated>2022-01-19T15:00:00+00:00</updated><id>http://0.0.0.0:4000/cs231n_lec13</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec13">&lt;h2 id=&quot;self-supervised-learning&quot;&gt;Self-Supervised Learning&lt;/h2&gt;

&lt;h3 id=&quot;generative-vs-self-supervised-learning&quot;&gt;Generative vs. Self-supervised Learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Both aim to learn from data without manual label annotation&lt;/li&gt;
  &lt;li&gt;Generative learning aims to model &lt;strong&gt;data distribution&lt;/strong&gt; $p_{data}(x)$,&lt;br /&gt;
  e.g., generating realistic images.&lt;/li&gt;
  &lt;li&gt;Self-supervised learning methods solve “pretext” tasks that produce &lt;strong&gt;good features&lt;/strong&gt; for downstream tasks.
    &lt;ul&gt;
      &lt;li&gt;Learn with supervised learning objectives, e.g., classification, regression.&lt;/li&gt;
      &lt;li&gt;Labels of these pretext tasks are generated &lt;em&gt;automatically&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;self-supervised-pretext-tasks&quot;&gt;Self-supervised pretext tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Example: learn to predict image transformations / complete corrupted images;&lt;br /&gt;
  e.g. image completion, rotation prediction, “jigsaw puzzle”, coloriztaion.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Solving the pretext tasks allow the model to learn good features.&lt;/li&gt;
  &lt;li&gt;We can automatically generate labels for the pretext tasks.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Learning to generate pixel-level details is often unnecessary; learn high-level semantic features with pretext tasks instead(only encode high-level features sufficient enough to distinguish different objects, Contrastive Methods): &lt;a href=&quot;https://aeon.co/essays/your-brain-does-not-process-information-and-it-is-not-a-computer&quot; target=&quot;_blank&quot;&gt;Epstein, 2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-evaluate-a-self-supervised-learning-method&quot;&gt;How to evaluate a self-supervised learning method?&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Self-supervised learning:&lt;br /&gt;
 With lots of unlabeled data, learn good feature extractors from self-supervised pretext tasks, e.g., predicting image rotations.&lt;/li&gt;
  &lt;li&gt;Supervised Learning:&lt;br /&gt;
 With small amount of labeled data on the target task, attach a shallow network on the feature extractor; train the shallow network and evaluate on the target task, e.g., classification, detection.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;pretext-tasks-from-image-transformations&quot;&gt;Pretext tasks from image transformations&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Predict Rotations&lt;br /&gt;
  &lt;em&gt;Gidaris et al., 2018&lt;/em&gt; - &lt;a href=&quot;/Rotation&quot;&gt;(Paper Review)&lt;/a&gt; 
  &lt;img src=&quot;/assets/images/cs231n_lec13_17.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Predict Relative Patch Locations&lt;br /&gt;
  &lt;em&gt;Doersch et al., 2015&lt;/em&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Solving “jigsaw puzzles”; shuffled patches &lt;br /&gt;
  &lt;em&gt;Noroozi &amp;amp; Favaro, 2016&lt;/em&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_1.png&quot; alt=&quot;png&quot; width=&quot;65%&amp;quot;, height=&amp;quot;65%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Predict Missing Pixels(Inpainting); encoder-decoder&lt;br /&gt;
  &lt;em&gt;Pathak et al., 2016&lt;/em&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_2.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Image Coloring; Split-brain Autoencoder&lt;br /&gt;
  &lt;em&gt;Richard Zhang/ Phillip Isola&lt;/em&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_3.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Video Coloring; from t=0 reference frame to the later frames&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_4.png&quot; alt=&quot;png&quot; width=&quot;65%&amp;quot;, height=&amp;quot;65%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-pretext-tasks&quot;&gt;Summary: Pretext tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Pretext tasks focus on “visual common sense”; by image transformations, can learn without supervision(big labeled data).&lt;/li&gt;
  &lt;li&gt;The models are forced learn good features about natural images, e.g., semantic representation of an object category, in order to solve the pretext tasks.&lt;/li&gt;
  &lt;li&gt;We don’t care about the performance of these pretext tasks, but rather how useful the learned features are for downstream tasks.&lt;/li&gt;
  &lt;li&gt;$\color{red}{Problems}$: 1) coming up with individual pretext tasks is tedious, and 2) the learned representations may not be general; tied to a specific pretext task.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;contrastive-representation-learning&quot;&gt;Contrastive Representation Learning&lt;/h2&gt;
&lt;p&gt;For a more general pretext task,&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_5.png&quot; alt=&quot;png&quot; width=&quot;75%&amp;quot;, height=&amp;quot;75%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;a-formulation-of-contrastive-learning&quot;&gt;A formulation of contrastive learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;What we want:&lt;br /&gt;
  $\mbox{score}(f(x), f(x^+)) » score(f(x), f(x^-))$&lt;br /&gt;
  &lt;em&gt;x&lt;/em&gt;: reference sample, &lt;em&gt;x+: positive sample&lt;/em&gt;, &lt;em&gt;x-&lt;/em&gt;: negative sample&lt;br /&gt;
  Given a chosen score function, we aim to learn an &lt;strong&gt;encoder function&lt;/strong&gt; &lt;em&gt;f&lt;/em&gt; that yields high score for positive pairs and low scores for negative pairs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loss function given &lt;em&gt;1&lt;/em&gt; positive sample and &lt;em&gt;N-1&lt;/em&gt; negative samples:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_6.png&quot; alt=&quot;png&quot; width=&quot;65%&amp;quot;, height=&amp;quot;65%&quot; /&gt;&lt;br /&gt;
  seems familiar with &lt;strong&gt;Cross entropy loss&lt;/strong&gt; for a N-way softmax classifier!&lt;br /&gt;
  i.e., learn to find the positive sample from the N samples&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Commonly known as the InfoNCE loss(&lt;em&gt;van den Oord et al., 2018&lt;/em&gt;)&lt;br /&gt;
  A &lt;em&gt;lower bound&lt;/em&gt; on the mutual information between $f(x)$ and $f(x^+)$&lt;br /&gt;
  \(\rightarrow MI[f(x), f(x^+)] - \log(N) \ge -L\)&lt;br /&gt;
  The larger the negative sample size(N), the tighter the bound&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;simclr-a-simple-framework-for-contrastive-learning&quot;&gt;SimCLR: A Simple Framework for Contrastive Learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Chen et al., 2020&lt;/em&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_7.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Cosine similarity as the score function:&lt;br /&gt;
  \(s(u, v) = \frac{u^T v}{\lVert u \rVert \lVert v \rVert}\)&lt;/li&gt;
  &lt;li&gt;Use a projection network &lt;strong&gt;&lt;em&gt;h(.)&lt;/em&gt;&lt;/strong&gt; to project features to a space where contrastive learning is applied.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generate positive samples through data augmentation:&lt;br /&gt;
  random cropping, random color distortion, and random blur.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_8.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_9.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Evaluate: Freeze feature encoder, train(finetune) on a supervised downstream task&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- simCLR v1 &amp; v2 리뷰: https://rauleun.github.io/SimCLR--&gt;
&lt;h4 id=&quot;simclr-design-choices-projection-headzg&quot;&gt;SimCLR design choices: Projection head($z=g(.)$)&lt;/h4&gt;
&lt;p&gt;Linear / non-linear projection heads improve representation learning.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A possible explanation:
    &lt;ul&gt;
      &lt;li&gt;contrastive learning objective may discard useful information for downstream tasks.&lt;/li&gt;
      &lt;li&gt;representation space &lt;strong&gt;z&lt;/strong&gt; is trained to be invariant to data transformation.&lt;/li&gt;
      &lt;li&gt;by leveraging the projection head &lt;strong&gt;g(.)&lt;/strong&gt;, more information can be preserved in the &lt;strong&gt;h&lt;/strong&gt; representation space&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;simclr-design-choices-large-batch-size&quot;&gt;SimCLR design choices: Large batch size&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec13_10.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
Large training batch size is crucial for SimCLR, but it causes large memory footprint during backpropagation; requires distributed training on TPUs.&lt;/p&gt;

&lt;h3 id=&quot;momentum-contrastive-learning-moco&quot;&gt;Momentum Contrastive Learning (MoCo)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;He et al., 2020&lt;/em&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_11.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Key differences to SimCLR:
    &lt;ul&gt;
      &lt;li&gt;Keep a running queue of keys (negative samples).&lt;/li&gt;
      &lt;li&gt;Compute gradients and update the encoder only through the queries.&lt;/li&gt;
      &lt;li&gt;Decouple min-batch size with the number of keys: can support a large number of negative samples.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec13_12.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;moco-v2&quot;&gt;MoCo V2&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Chen et al., 2020&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;A hybrid of ideas from SimCLR and MoCo:&lt;br /&gt;
  From SimCLR: non-linear projection head and strong data augmentation.&lt;br /&gt;
  From MoCo: momentum-updated queues that allow training on a large number of negative samples (no TPU required).&lt;/li&gt;
  &lt;li&gt;Key takeaways(vs. SimCLR, MoCo V1):
    &lt;ul&gt;
      &lt;li&gt;Non-linear projection head and strong data augmentation are crucial for contrastive learning.&lt;/li&gt;
      &lt;li&gt;Decoupling mini-batch size with negative sample size allows MoCo-V2 to outperform SimCLR with smaller batch size (256 vs. 8192).&lt;/li&gt;
      &lt;li&gt;Achieved with much smaller memory footprint.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;instance-vs-sequence-contrastive-learning&quot;&gt;Instance vs. Sequence Contrastive Learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Instance-level contrastive learning:&lt;br /&gt;
  Based on positive &amp;amp; negative instances.&lt;br /&gt;
  E.g., SimCLR, MoCo&lt;/li&gt;
  &lt;li&gt;Sequence-level contrastive learning:&lt;br /&gt;
  Based on sequential / temporal orders.&lt;br /&gt;
  E.g., Contrastive Predictive Coding (CPC)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;contrastive-predictive-coding-cpc&quot;&gt;Contrastive Predictive Coding (CPC)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;van den Oord et al., 2018&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Contrastive: contrast between “right” and “wrong” sequences using contrastive learning.&lt;/li&gt;
  &lt;li&gt;Predictive: the model has to predict future patterns given the current context.&lt;/li&gt;
  &lt;li&gt;Coding: the model learns useful feature vectors, or “code”, for downstream tasks, similar to other context self-supervised methods.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec13_13.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Encode all samples in a sequence into vectors $z_t = g_{\mbox{enc}}(x_t)$&lt;/li&gt;
  &lt;li&gt;Summarize context (e.g., half of a sequence) into a context code $c_t$ using an auto-regressive model ($g_{\mbox{ar}}$). The original paper uses GRU-RNN here.&lt;/li&gt;
  &lt;li&gt;Compute InfoNCE loss between the context $c_t$ and future code $z_{t+k}$ using the following time-dependent score funtion: $s_k(z_{t+k}, c_t) = z_{t+k}^T W_k c_t$, where $W_k$ is a trainable matrix.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Summary(CPC):&lt;br /&gt;
  Contrast “right” sequence with “wrong” sequence.&lt;br /&gt;
  InfoNCE loss with a time-dependent score function.&lt;br /&gt;
  Can be applied to a variety of learning problems, but not as effective in learning image representations compared to instance-level methods.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-examples&quot;&gt;Other examples&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec13_14.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_15.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_16.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Self-Supervised Learning Generative vs. Self-supervised Learning Both aim to learn from data without manual label annotation Generative learning aims to model data distribution $p_{data}(x)$, e.g., generating realistic images. Self-supervised learning methods solve “pretext” tasks that produce good features for downstream tasks. Learn with supervised learning objectives, e.g., classification, regression. Labels of these pretext tasks are generated automatically. Self-supervised pretext tasks Example: learn to predict image transformations / complete corrupted images; e.g. image completion, rotation prediction, “jigsaw puzzle”, coloriztaion. Solving the pretext tasks allow the model to learn good features. We can automatically generate labels for the pretext tasks. Learning to generate pixel-level details is often unnecessary; learn high-level semantic features with pretext tasks instead(only encode high-level features sufficient enough to distinguish different objects, Contrastive Methods): Epstein, 2016 How to evaluate a self-supervised learning method? Self-supervised learning: With lots of unlabeled data, learn good feature extractors from self-supervised pretext tasks, e.g., predicting image rotations. Supervised Learning: With small amount of labeled data on the target task, attach a shallow network on the feature extractor; train the shallow network and evaluate on the target task, e.g., classification, detection. Pretext tasks from image transformations Predict Rotations Gidaris et al., 2018 - (Paper Review) Predict Relative Patch Locations Doersch et al., 2015 Solving “jigsaw puzzles”; shuffled patches Noroozi &amp;amp; Favaro, 2016 Predict Missing Pixels(Inpainting); encoder-decoder Pathak et al., 2016 Image Coloring; Split-brain Autoencoder Richard Zhang/ Phillip Isola Video Coloring; from t=0 reference frame to the later frames Summary: Pretext tasks Pretext tasks focus on “visual common sense”; by image transformations, can learn without supervision(big labeled data). The models are forced learn good features about natural images, e.g., semantic representation of an object category, in order to solve the pretext tasks. We don’t care about the performance of these pretext tasks, but rather how useful the learned features are for downstream tasks. $\color{red}{Problems}$: 1) coming up with individual pretext tasks is tedious, and 2) the learned representations may not be general; tied to a specific pretext task. Contrastive Representation Learning For a more general pretext task, A formulation of contrastive learning What we want: $\mbox{score}(f(x), f(x^+)) » score(f(x), f(x^-))$ x: reference sample, x+: positive sample, x-: negative sample Given a chosen score function, we aim to learn an encoder function f that yields high score for positive pairs and low scores for negative pairs. Loss function given 1 positive sample and N-1 negative samples: seems familiar with Cross entropy loss for a N-way softmax classifier! i.e., learn to find the positive sample from the N samples Commonly known as the InfoNCE loss(van den Oord et al., 2018) A lower bound on the mutual information between $f(x)$ and $f(x^+)$ \(\rightarrow MI[f(x), f(x^+)] - \log(N) \ge -L\) The larger the negative sample size(N), the tighter the bound SimCLR: A Simple Framework for Contrastive Learning Chen et al., 2020 Cosine similarity as the score function: \(s(u, v) = \frac{u^T v}{\lVert u \rVert \lVert v \rVert}\) Use a projection network h(.) to project features to a space where contrastive learning is applied. Generate positive samples through data augmentation: random cropping, random color distortion, and random blur. Evaluate: Freeze feature encoder, train(finetune) on a supervised downstream task SimCLR design choices: Projection head($z=g(.)$) Linear / non-linear projection heads improve representation learning. A possible explanation: contrastive learning objective may discard useful information for downstream tasks. representation space z is trained to be invariant to data transformation. by leveraging the projection head g(.), more information can be preserved in the h representation space SimCLR design choices: Large batch size Large training batch size is crucial for SimCLR, but it causes large memory footprint during backpropagation; requires distributed training on TPUs. Momentum Contrastive Learning (MoCo) He et al., 2020 Key differences to SimCLR: Keep a running queue of keys (negative samples). Compute gradients and update the encoder only through the queries. Decouple min-batch size with the number of keys: can support a large number of negative samples. MoCo V2 Chen et al., 2020 A hybrid of ideas from SimCLR and MoCo: From SimCLR: non-linear projection head and strong data augmentation. From MoCo: momentum-updated queues that allow training on a large number of negative samples (no TPU required). Key takeaways(vs. SimCLR, MoCo V1): Non-linear projection head and strong data augmentation are crucial for contrastive learning. Decoupling mini-batch size with negative sample size allows MoCo-V2 to outperform SimCLR with smaller batch size (256 vs. 8192). Achieved with much smaller memory footprint. Instance vs. Sequence Contrastive Learning Instance-level contrastive learning: Based on positive &amp;amp; negative instances. E.g., SimCLR, MoCo Sequence-level contrastive learning: Based on sequential / temporal orders. E.g., Contrastive Predictive Coding (CPC) Contrastive Predictive Coding (CPC) van den Oord et al., 2018 Contrastive: contrast between “right” and “wrong” sequences using contrastive learning. Predictive: the model has to predict future patterns given the current context. Coding: the model learns useful feature vectors, or “code”, for downstream tasks, similar to other context self-supervised methods. Encode all samples in a sequence into vectors $z_t = g_{\mbox{enc}}(x_t)$ Summarize context (e.g., half of a sequence) into a context code $c_t$ using an auto-regressive model ($g_{\mbox{ar}}$). The original paper uses GRU-RNN here. Compute InfoNCE loss between the context $c_t$ and future code $z_{t+k}$ using the following time-dependent score funtion: $s_k(z_{t+k}, c_t) = z_{t+k}^T W_k c_t$, where $W_k$ is a trainable matrix. Summary(CPC): Contrast “right” sequence with “wrong” sequence. InfoNCE loss with a time-dependent score function. Can be applied to a variety of learning problems, but not as effective in learning image representations compared to instance-level methods. Other examples</summary></entry><entry><title type="html">cs231n - Lecture 12. Generative Models</title><link href="http://0.0.0.0:4000/cs231n_lec12" rel="alternate" type="text/html" title="cs231n - Lecture 12. Generative Models" /><published>2022-01-09T15:00:00+00:00</published><updated>2022-01-09T15:00:00+00:00</updated><id>http://0.0.0.0:4000/cs231n_lec12</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec12">&lt;h3 id=&quot;supervised-vs-unsupervised&quot;&gt;Supervised vs. Unsupervised&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Supervised Learning:&lt;br /&gt;
  Data: $(x,y)$; &lt;em&gt;y&lt;/em&gt; is label&lt;br /&gt;
  Goal: Learn a function to map $x\rightarrow y$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unsupervised Learning:&lt;br /&gt;
  Data: &lt;em&gt;x&lt;/em&gt;; no labels&lt;br /&gt;
  Goal: Learn some underlying hidden structure of the data&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;generative-modeling&quot;&gt;Generative Modeling&lt;/h3&gt;
&lt;p&gt;Given training data, generate new samples from same distribution&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Objectives:
    &lt;ol&gt;
      &lt;li&gt;Learn $p_{\scriptstyle\text{model}}(x)$ that approximates $p_{\scriptstyle\text{data}}(x)$&lt;/li&gt;
      &lt;li&gt;Sampling new &lt;em&gt;x&lt;/em&gt; from $p_{\scriptstyle\text{model}}(x)$&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Formulate as density estimation problems:
    &lt;ul&gt;
      &lt;li&gt;Explicit density estimation: explicitly define and solve for $p_{\scriptstyle\text{model}}(x)$.&lt;/li&gt;
      &lt;li&gt;Implicit density estimation: learn model that can sample from $p_{\scriptstyle\text{model}}(x)$ without explicitly defining it.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Why Generative Models?&lt;br /&gt;
  Realistic samples for artwork, super-resolution, colorization, etc.&lt;br /&gt;
  Learn useful features for downstream tasks such as classification.&lt;br /&gt;
  Getting insights from high-dimensional data (physics, medical imaging, etc.)&lt;br /&gt;
  Modeling physical world for simulation and planning (robotics and reinforcement learning applications)&lt;br /&gt;
  …&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pixelrnn-and-pixelcnn-a-brief-overview&quot;&gt;PixelRNN and PixelCNN; a brief overview&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Fully visible belief network (FVBN)&lt;br /&gt;
  is an explicit density model, defines tractable density function using chain rule to decompose the likelihood of an image &lt;em&gt;x&lt;/em&gt; into product of &lt;em&gt;1&lt;/em&gt;-d distributions:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_0.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
  Then maximize likelihood of training data. It is a complex distribution over pixel values, express using a neural network.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pixelrnn-van-der-oord-et-al-2016&quot;&gt;PixelRNN, &lt;em&gt;van der Oord et al., 2016&lt;/em&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Generate image pixels starting from corner, dependency on previous pixels modeled using an RNN(LSTM). 
  &lt;img src=&quot;/assets/images/cs231n_lec12_1.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;br /&gt;
  Drawback: sequential generation is slow in both training and inference&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pixelcnn-van-der-oord-et-al-2016&quot;&gt;PixelCNN, &lt;em&gt;van der Oord et al., 2016&lt;/em&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generate image pixels starting from corner, ependency on previous pixels modeled using a CNN over context region(&lt;strong&gt;masked convolution&lt;/strong&gt;)&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_2.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;br /&gt;
  Training is faster than PixelRNN: it can parallelize convolutions since context region values known from training images.&lt;br /&gt;
  Generation is still slow: for a 32x32 image, we need to do forward passes of the network &lt;em&gt;1024&lt;/em&gt; times for a single image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Improving PixelCNN performance&lt;br /&gt;
  Gated convolutional layers, Short-cut connections, Discretized logistic loss, Multi-scale, Training tricks, etc.&lt;br /&gt;
  See also PixelCNN++, &lt;em&gt;Salimans et al., 2017&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Pros:&lt;br /&gt;
  Can explicitly compute likelihood &lt;em&gt;p(x)&lt;/em&gt;&lt;br /&gt;
  Easy to optimize&lt;br /&gt;
  Good samples&lt;/li&gt;
  &lt;li&gt;Cons:&lt;br /&gt;
  Sequential generation is slow&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variational-autoencodervae&quot;&gt;Variational Autoencoder(VAE)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;VAE is an explicit density model, defines intractable(approximate) density function with latent &lt;strong&gt;z&lt;/strong&gt;:&lt;br /&gt;
  $p_\theta(x) = \int p_\theta(z)p_\theta(x|z)\, dz$&lt;br /&gt;
  No dependencies among pixels, can generate all pixels at the same time. But cannot optimize directly, derive and optimize lower bound on likelihood instead&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background-autoencoders&quot;&gt;Background: Autoencoders&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_3.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;z&lt;/strong&gt; usually smaller than &lt;strong&gt;x&lt;/strong&gt;: with dimensionality reduction to capture meaningful factors of variation in data. Train such that features can be used to reconstruct original data($\hat{x}$)&lt;/li&gt;
      &lt;li&gt;“Autoencoding”; encoding input itself(&lt;em&gt;L2&lt;/em&gt; loss)&lt;/li&gt;
      &lt;li&gt;After training, throw away decoder and adjust to the final task&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder can be used to initialize a supervised model;
  Transfer from large, unlabeled dataset(Autoencoder) to small, labeled dataset and fine-tune; train for final task.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_4.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;But we can’t generate new images from an autoencoder because we don’t know the space of &lt;strong&gt;z&lt;/strong&gt;. $\rightarrow$ Variational Autoencoders for a generative model.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;variational-autoencoders-probabilistic-spin-on-autoencoders&quot;&gt;Variational Autoencoders: Probabilistic spin on autoencoders&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Assume training data \(\left\{ x^{(i)}\right\} _{i=1}^N\) is generated from the distribution of unobserved (latent) representation &lt;strong&gt;z&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Intuition from autoencoders: &lt;strong&gt;x&lt;/strong&gt; is an image, &lt;strong&gt;z&lt;/strong&gt; is latent factors used to generate &lt;strong&gt;x&lt;/strong&gt;: attributes, orientation, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_5.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We want to estimate the true parameters $\theta^*$ of this generative model given training data &lt;em&gt;x&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Model representation:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;p(z)&lt;/em&gt;: Choose prior to be simple, e.g. Gaussian.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;z&lt;/strong&gt;: Reasonable for latent attributes, e.g. pose, how much smile.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;p(x|z)&lt;/em&gt;: Generating images, conditional probability is complex&lt;br /&gt;
  $\rightarrow$ represent with neural network&lt;/li&gt;
      &lt;li&gt;$p_\theta(x)$: Learn model parameters to maximize likelihood of training data&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;variational-autoencoders-intractability&quot;&gt;Variational Autoencoders: Intractability&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Data likelihood:&lt;br /&gt;
  $p_\theta(x) = \int p_\theta(z)p_\theta(x|z)\, dz$&lt;br /&gt;
  where $p_\theta(z)$ is a Simple Gaussian prior and $p_\theta(x|z)$ is a decoder neural network, it is intractable to compute &lt;em&gt;p(x|z)&lt;/em&gt; for every &lt;em&gt;z&lt;/em&gt;.&lt;br /&gt;
  while &lt;em&gt;Monte Carlo estimation&lt;/em&gt;-$\log p(x) \approx \log\frac{1}{k}\sum_{i=1}^k p(x|z^{(i)})$, where $z^{(i)}\sim p(z)$- is too high variance.&lt;/li&gt;
  &lt;li&gt;divided by intractable $p_\theta(x)$, Posterior density also intractable:&lt;br /&gt;
  $p_\theta(z|x) = p_\theta(x|z)p_\theta(z)/p_\theta(x)$&lt;/li&gt;
  &lt;li&gt;Solution:&lt;br /&gt;
  In addition to modeling $p_\theta(x|z)$, learn $q_\phi(z|x)$ that approximates the true posterior $p_\theta(z|x)$. $q_\phi$, approximate posterior allows us to derive a lower bound on the data likelihood that is tractable, which can be optimized.&lt;br /&gt;
  &lt;strong&gt;Variational inference&lt;/strong&gt; is to approximate the unknown posterior distribution from only the observed data &lt;em&gt;x&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

\[\begin{align*}
\log p_\theta(x^{(i)})
&amp;amp;= \mathbf{E}_{z~q_\phi(z|x^{(i)})}\left[ \log p_\theta(x^{(i)}) \right] \quad \textit(p_\theta(x^{(i)})\ does\ not\ depend\ on\ z) \\
&amp;amp;= \mathbf{E}_z \left[
	\log\frac{p_\theta(x^{(i)}|z)p_\theta(z)}{p_\theta(z|x^{(i)})} \right] \quad \textit(Bayes'\ Rule) \\
&amp;amp;= \mathbf{E}_z \left[ 
	\log\frac{p_\theta(x^{(i)}|z)p_\theta(z)}{p_\theta(z|x^{(i)})}
		\frac{q_\phi(z|x^{(i)})}{q_\phi(z|x^{(i)})} \right] \quad \textit(Multiply\ by\ constant)\\
&amp;amp;= \mathbf{E}_z \left[\log p_\theta(x^{(i)}|z) \right]
	- \mathbf{E}_z \left[ \log\frac{q_\phi(z|x^{(i)})}{p_\theta(z)}\right]
	+ \mathbf{E}_z \left[ \log\frac{q_\phi(z|x^{(i)})}{p_\theta(z|x^{(i)})}\right] \quad \textit(Logarithms) \\
&amp;amp;= \mathbf{E}_z \left[\log p_\theta(x^{(i)}|z) \right]
	- D_{KL}(q_\phi(z|x^{(i)})|p_\theta(z)) + D_{KL}(q_\phi(z|x^{(i)})|p_\theta(z|x^{(i)}))
\end{align*}\]

&lt;ul&gt;
  &lt;li&gt;With taking expectation with respect to &lt;em&gt;z&lt;/em&gt;(using encoder network) let us write nice &lt;em&gt;KL&lt;/em&gt; terms;
    &lt;ul&gt;
      &lt;li&gt;\(\mathbf{E}_z \left[\log p_\theta(x^{(i)}\vert z) \right]\): &lt;strong&gt;Decoder&lt;/strong&gt; network gives $p_\theta(x\vert z)$, can compute estimate of this term through sampling(need some trick to differentiate through sampling). It reconstruct the input data.&lt;/li&gt;
      &lt;li&gt;\(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z))\): KL term between Gaussian for encoder and &lt;em&gt;z&lt;/em&gt; prior has nice closed-form solution. &lt;strong&gt;Encoder&lt;/strong&gt; makes approximate posterior distribution close to prior.&lt;/li&gt;
      &lt;li&gt;\(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z\vert x^{(i)}))\): is intractable, we can’t compute this term; but we know KL divergence always greater than &lt;em&gt;0&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To maximize the data likelihood, we can rewrite&lt;br /&gt;
\(\begin{align*}
\log p_\theta (x^{(i)}) &amp;amp;= \mathbf{E}_z \left[ \log p _\theta (x^{(i)}\vert z) \right]
                      - D_{KL}(q_\phi (z\vert x^{(i)})\vert p_\theta (z)) + D_{KL}(q_\phi (z\vert x^{(i)})\vert p_\theta (z\vert x^{(i)})) \\
                      &amp;amp;= \mathcal{L}(x^{(i)},\theta ,\phi ) + (C\ge 0)
\end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;\(\mathcal{L}(x^{(i)},\theta,\phi)\): &lt;em&gt;Decoder - Encoder&lt;/em&gt;&lt;br /&gt;
  &lt;strong&gt;Tractable lower bound&lt;/strong&gt; which we can take gradient of and optimize. Maximizing this &lt;em&gt;evidence lower bound(ELBO)&lt;/em&gt;, we can maximize $\log p_\theta(x)$. Later, we take minus on this term for the loss function of a neural network.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_6.png&quot; alt=&quot;png&quot; width=&quot;45%&amp;quot;, height=&amp;quot;45%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Encoder part; \(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z))\)&lt;br /&gt;
  We choose &lt;em&gt;q(z)&lt;/em&gt; as a Gaussian distribution, $q(z\vert x) = N(\mu_{z\vert x}, \Sigma_{z\vert x})$. Computing the KL divergence, \(D_{KL}(N(\mu_{z\vert x}, \Sigma_{z\vert x}))\vert N(0,I))\), having analytical solution.&lt;/li&gt;
  &lt;li&gt;Reparameterization trick &lt;em&gt;z&lt;/em&gt;:&lt;br /&gt;
  to make sampling differentiable, input sample $\epsilon\sim N(0,I)$ to the graph $z = \mu_{z\vert x} + \epsilon\sigma_{z\vert x}$; where $\mu, \sigma$ are the part of computation graph.&lt;/li&gt;
  &lt;li&gt;Decoder part;&lt;br /&gt;
  Maximize likelihood of original input being reconstructed, $\hat{x}-x$.&lt;/li&gt;
  &lt;li&gt;For every minibatch of input data, compute $\mathcal{L}$ graph forward pass and backprop.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_7.png&quot; alt=&quot;png&quot; width=&quot;75%&amp;quot;, height=&amp;quot;75%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;variational-autoencoders-generating-data&quot;&gt;Variational Autoencoders: Generating Data&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_8.png&quot; alt=&quot;png&quot; width=&quot;55%&amp;quot;, height=&amp;quot;55%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Diagonal prior on &lt;strong&gt;z&lt;/strong&gt; for independent latent variables&lt;/li&gt;
  &lt;li&gt;Different dimensions of &lt;strong&gt;z&lt;/strong&gt; encode interpretable factors of variation;&lt;br /&gt;
  Also good feature representation taht can be computed using $q_\phi(z\vert x)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-1&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Probabilistic spin to traditional autoencoders, allows generating data&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Defines an intractable density; derive and optimize a (variational) lower bound&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Pros:&lt;br /&gt;
  Principled approach to generative models&lt;br /&gt;
  Interpretable latent space&lt;br /&gt;
  Allows inference of $q(z\vert x)$, can be useful feature representation for other tasks  - Cons:&lt;br /&gt;
  Maximizes lower bound of likelihood: not as good evaluation as tractable model&lt;br /&gt;
  Samples &lt;em&gt;mean&lt;/em&gt;; blurrier and lower quality compared to state-of-the-art (GANs)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;generative-adversarial-networksgans&quot;&gt;Generative Adversarial Networks(GANs)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_9.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
idea: Use a discriminator network to tell whether the generate image is within data distribution (“real”) or not&lt;/p&gt;

&lt;h3 id=&quot;training-gans-two-player-game&quot;&gt;Training GANs: Two-player game&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_10.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
Discriminator network: try to distinguish between real and fake images&lt;br /&gt;
Generator network: try to fool discriminator by generating real-looking images&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Train jointly in &lt;strong&gt;minimax game&lt;/strong&gt;;&lt;br /&gt;
  Minimax objective function:&lt;br /&gt;
  \(\mbox{min}_{\theta_g} \mbox{max}_{\theta_d}\left[\mathbb{E}_{x\sim {p_{data}}}\log D_{\theta_d}(x) + \mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z))) \right]\)&lt;br /&gt;
  where $\theta_g$ is an objective for the generator objective and $\theta_d$ for the discriminator
    &lt;ul&gt;
      &lt;li&gt;$D_{\theta_d}(x)$: Discriminator outputs likelihood in &lt;em&gt;(0,1)&lt;/em&gt; of real image&lt;/li&gt;
      &lt;li&gt;$D_{\theta_d}(G_{\theta_g}(z))$: Discriminator output for generated fake data &lt;em&gt;G(z)&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Discriminator($\theta_d$) wants to &lt;strong&gt;maximize objective&lt;/strong&gt; such that &lt;em&gt;D(x)&lt;/em&gt; is close to &lt;em&gt;1&lt;/em&gt;(real) and &lt;em&gt;D(G(z))&lt;/em&gt; is close to &lt;em&gt;0&lt;/em&gt;(fake)&lt;/li&gt;
  &lt;li&gt;Generator($\theta_g$) wants to &lt;em&gt;minimize objective&lt;/em&gt; such that &lt;em&gt;D(G(z))&lt;/em&gt; is close to &lt;em&gt;1&lt;/em&gt;(to fool discriminator)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We alternate the minimax objection function with:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Gradient ascent&lt;/strong&gt; on discriminator&lt;br /&gt;
 \(\mbox{max}_{\theta_d}\left[\mathbb{E}_{x\sim p_{data}}\log D_{\theta_d}(x) + \mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z))) \right]\)&lt;/li&gt;
  &lt;li&gt;1) &lt;strong&gt;Gradient descent&lt;/strong&gt; on generator&lt;br /&gt;
 \(\mbox{min}_{\theta_g}\mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z)))\)
    &lt;ul&gt;
      &lt;li&gt;In practice, optimizing this generator objective does not work well;&lt;br /&gt;
  When sample is likely fake, want to learn from it to improve generator (move to the right on &lt;em&gt;X&lt;/em&gt; axis), but gradient near &lt;em&gt;0&lt;/em&gt; in &lt;em&gt;X&lt;/em&gt; axis is relatively flat; Gradient signal is dominated by region where sample is already good(near &lt;em&gt;1&lt;/em&gt;).&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_11.png&quot; alt=&quot;png&quot; width=&quot;30%&amp;quot;, height=&amp;quot;30%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;2) &lt;strong&gt;Instead: Gradient ascent&lt;/strong&gt; on generator, different objective&lt;br /&gt;
 \(\mbox{max}_{\theta_d}\mathbb{E}_{z\sim p(z)}\log(D_{\theta_d}(G_{\theta_g}(z)))\)&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Rather than minimizing likelihood of discriminator being correct, maximize likelihood of discriminator being wrong. Same objective of fooling discriminator, but now higher gradient signal for bad samples.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_12.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GAN training Algorithm&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_13.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;70%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After training, use generator network to generate new images&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gan-convolutional-architectures&quot;&gt;GAN: Convolutional Architectures&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generator is an upsampling network with fractionally-strided convolutions&lt;br /&gt;
  Discriminator is a convolutional network&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Architecture guidelines for stable Deep Conv GANs&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Replace any pooling layers with strided convolutions(discriminator) and fractional-strided convolutions(generator).&lt;/li&gt;
      &lt;li&gt;Use batchnorm in both network.&lt;/li&gt;
      &lt;li&gt;Remove fully connected hidden layers for deeper architecture.&lt;/li&gt;
      &lt;li&gt;Use ReLU activation in generator for all layers except for the output, which uses Tanh.&lt;/li&gt;
      &lt;li&gt;Use LeakyReLU activation in the discriminator for all layers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gan-interpretable-vector-math&quot;&gt;GAN: Interpretable Vector Math&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_14.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;works similar to a language model&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2017-explosion-of-gans&quot;&gt;2017: Explosion of GANs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;“The GAN Zoo”, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://github.com/hindupuravinash/the-gan-zoo&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;check &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://github.com/soumith/ganhacks&lt;/code&gt; for tips and tricks for training GANs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;scene-graphs-to-gans&quot;&gt;Scene graphs to GANs&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_15.png&quot; alt=&quot;png&quot; width=&quot;30%&amp;quot;, height=&amp;quot;30%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Specifying exactly what kind of image you want to generate. The explicit structure in scene graphs provides better image generation for complex scenes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-gans&quot;&gt;Summary: GANs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Don’t work with an explicit density function&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Take game-theoretic approach: learn to generate from training distribution through 2-player game&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Pros:
    &lt;ul&gt;
      &lt;li&gt;Beautiful, state-of-the-art samples&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cons:
    &lt;ul&gt;
      &lt;li&gt;Trickier / more unstable to train&lt;/li&gt;
      &lt;li&gt;Can’t solve inference queries such as $p(x)$, $p(z\vert x)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Active areas of research:
    &lt;ul&gt;
      &lt;li&gt;Better loss functions, more stable training (Wasserstein GAN, LSGAN, many others)&lt;/li&gt;
      &lt;li&gt;Conditional GANs, GANs for all kinds of applications&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Useful Resources on Generative Models&lt;br /&gt;
  CS236: Deep Generative Models (Stanford)&lt;br /&gt;
  CS 294-158 Deep Unsupervised Learning (Berkeley)&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Supervised vs. Unsupervised Supervised Learning: Data: $(x,y)$; y is label Goal: Learn a function to map $x\rightarrow y$ Unsupervised Learning: Data: x; no labels Goal: Learn some underlying hidden structure of the data Generative Modeling Given training data, generate new samples from same distribution Objectives: Learn $p_{\scriptstyle\text{model}}(x)$ that approximates $p_{\scriptstyle\text{data}}(x)$ Sampling new x from $p_{\scriptstyle\text{model}}(x)$ Formulate as density estimation problems: Explicit density estimation: explicitly define and solve for $p_{\scriptstyle\text{model}}(x)$. Implicit density estimation: learn model that can sample from $p_{\scriptstyle\text{model}}(x)$ without explicitly defining it. Why Generative Models? Realistic samples for artwork, super-resolution, colorization, etc. Learn useful features for downstream tasks such as classification. Getting insights from high-dimensional data (physics, medical imaging, etc.) Modeling physical world for simulation and planning (robotics and reinforcement learning applications) … PixelRNN and PixelCNN; a brief overview Fully visible belief network (FVBN) is an explicit density model, defines tractable density function using chain rule to decompose the likelihood of an image x into product of 1-d distributions: Then maximize likelihood of training data. It is a complex distribution over pixel values, express using a neural network. PixelRNN, van der Oord et al., 2016 Generate image pixels starting from corner, dependency on previous pixels modeled using an RNN(LSTM). Drawback: sequential generation is slow in both training and inference PixelCNN, van der Oord et al., 2016 Generate image pixels starting from corner, ependency on previous pixels modeled using a CNN over context region(masked convolution) Training is faster than PixelRNN: it can parallelize convolutions since context region values known from training images. Generation is still slow: for a 32x32 image, we need to do forward passes of the network 1024 times for a single image. Improving PixelCNN performance Gated convolutional layers, Short-cut connections, Discretized logistic loss, Multi-scale, Training tricks, etc. See also PixelCNN++, Salimans et al., 2017 Summary Pros: Can explicitly compute likelihood p(x) Easy to optimize Good samples Cons: Sequential generation is slow Variational Autoencoder(VAE) VAE is an explicit density model, defines intractable(approximate) density function with latent z: $p_\theta(x) = \int p_\theta(z)p_\theta(x|z)\, dz$ No dependencies among pixels, can generate all pixels at the same time. But cannot optimize directly, derive and optimize lower bound on likelihood instead Background: Autoencoders Unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data z usually smaller than x: with dimensionality reduction to capture meaningful factors of variation in data. Train such that features can be used to reconstruct original data($\hat{x}$) “Autoencoding”; encoding input itself(L2 loss) After training, throw away decoder and adjust to the final task Encoder can be used to initialize a supervised model; Transfer from large, unlabeled dataset(Autoencoder) to small, labeled dataset and fine-tune; train for final task. But we can’t generate new images from an autoencoder because we don’t know the space of z. $\rightarrow$ Variational Autoencoders for a generative model. Variational Autoencoders: Probabilistic spin on autoencoders Assume training data \(\left\{ x^{(i)}\right\} _{i=1}^N\) is generated from the distribution of unobserved (latent) representation z Intuition from autoencoders: x is an image, z is latent factors used to generate x: attributes, orientation, etc. We want to estimate the true parameters $\theta^*$ of this generative model given training data x. Model representation: p(z): Choose prior to be simple, e.g. Gaussian. z: Reasonable for latent attributes, e.g. pose, how much smile. p(x|z): Generating images, conditional probability is complex $\rightarrow$ represent with neural network $p_\theta(x)$: Learn model parameters to maximize likelihood of training data Variational Autoencoders: Intractability Data likelihood: $p_\theta(x) = \int p_\theta(z)p_\theta(x|z)\, dz$ where $p_\theta(z)$ is a Simple Gaussian prior and $p_\theta(x|z)$ is a decoder neural network, it is intractable to compute p(x|z) for every z. while Monte Carlo estimation-$\log p(x) \approx \log\frac{1}{k}\sum_{i=1}^k p(x|z^{(i)})$, where $z^{(i)}\sim p(z)$- is too high variance. divided by intractable $p_\theta(x)$, Posterior density also intractable: $p_\theta(z|x) = p_\theta(x|z)p_\theta(z)/p_\theta(x)$ Solution: In addition to modeling $p_\theta(x|z)$, learn $q_\phi(z|x)$ that approximates the true posterior $p_\theta(z|x)$. $q_\phi$, approximate posterior allows us to derive a lower bound on the data likelihood that is tractable, which can be optimized. Variational inference is to approximate the unknown posterior distribution from only the observed data x \[\begin{align*} \log p_\theta(x^{(i)}) &amp;amp;= \mathbf{E}_{z~q_\phi(z|x^{(i)})}\left[ \log p_\theta(x^{(i)}) \right] \quad \textit(p_\theta(x^{(i)})\ does\ not\ depend\ on\ z) \\ &amp;amp;= \mathbf{E}_z \left[ \log\frac{p_\theta(x^{(i)}|z)p_\theta(z)}{p_\theta(z|x^{(i)})} \right] \quad \textit(Bayes'\ Rule) \\ &amp;amp;= \mathbf{E}_z \left[ \log\frac{p_\theta(x^{(i)}|z)p_\theta(z)}{p_\theta(z|x^{(i)})} \frac{q_\phi(z|x^{(i)})}{q_\phi(z|x^{(i)})} \right] \quad \textit(Multiply\ by\ constant)\\ &amp;amp;= \mathbf{E}_z \left[\log p_\theta(x^{(i)}|z) \right] - \mathbf{E}_z \left[ \log\frac{q_\phi(z|x^{(i)})}{p_\theta(z)}\right] + \mathbf{E}_z \left[ \log\frac{q_\phi(z|x^{(i)})}{p_\theta(z|x^{(i)})}\right] \quad \textit(Logarithms) \\ &amp;amp;= \mathbf{E}_z \left[\log p_\theta(x^{(i)}|z) \right] - D_{KL}(q_\phi(z|x^{(i)})|p_\theta(z)) + D_{KL}(q_\phi(z|x^{(i)})|p_\theta(z|x^{(i)})) \end{align*}\] With taking expectation with respect to z(using encoder network) let us write nice KL terms; \(\mathbf{E}_z \left[\log p_\theta(x^{(i)}\vert z) \right]\): Decoder network gives $p_\theta(x\vert z)$, can compute estimate of this term through sampling(need some trick to differentiate through sampling). It reconstruct the input data. \(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z))\): KL term between Gaussian for encoder and z prior has nice closed-form solution. Encoder makes approximate posterior distribution close to prior. \(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z\vert x^{(i)}))\): is intractable, we can’t compute this term; but we know KL divergence always greater than 0. To maximize the data likelihood, we can rewrite \(\begin{align*} \log p_\theta (x^{(i)}) &amp;amp;= \mathbf{E}_z \left[ \log p _\theta (x^{(i)}\vert z) \right] - D_{KL}(q_\phi (z\vert x^{(i)})\vert p_\theta (z)) + D_{KL}(q_\phi (z\vert x^{(i)})\vert p_\theta (z\vert x^{(i)})) \\ &amp;amp;= \mathcal{L}(x^{(i)},\theta ,\phi ) + (C\ge 0) \end{align*}\) \(\mathcal{L}(x^{(i)},\theta,\phi)\): Decoder - Encoder Tractable lower bound which we can take gradient of and optimize. Maximizing this evidence lower bound(ELBO), we can maximize $\log p_\theta(x)$. Later, we take minus on this term for the loss function of a neural network. Encoder part; \(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z))\) We choose q(z) as a Gaussian distribution, $q(z\vert x) = N(\mu_{z\vert x}, \Sigma_{z\vert x})$. Computing the KL divergence, \(D_{KL}(N(\mu_{z\vert x}, \Sigma_{z\vert x}))\vert N(0,I))\), having analytical solution. Reparameterization trick z: to make sampling differentiable, input sample $\epsilon\sim N(0,I)$ to the graph $z = \mu_{z\vert x} + \epsilon\sigma_{z\vert x}$; where $\mu, \sigma$ are the part of computation graph. Decoder part; Maximize likelihood of original input being reconstructed, $\hat{x}-x$. For every minibatch of input data, compute $\mathcal{L}$ graph forward pass and backprop. Variational Autoencoders: Generating Data Diagonal prior on z for independent latent variables Different dimensions of z encode interpretable factors of variation; Also good feature representation taht can be computed using $q_\phi(z\vert x)$. Summary Probabilistic spin to traditional autoencoders, allows generating data Defines an intractable density; derive and optimize a (variational) lower bound Pros: Principled approach to generative models Interpretable latent space Allows inference of $q(z\vert x)$, can be useful feature representation for other tasks - Cons: Maximizes lower bound of likelihood: not as good evaluation as tractable model Samples mean; blurrier and lower quality compared to state-of-the-art (GANs) Generative Adversarial Networks(GANs) idea: Use a discriminator network to tell whether the generate image is within data distribution (“real”) or not Training GANs: Two-player game Discriminator network: try to distinguish between real and fake images Generator network: try to fool discriminator by generating real-looking images Train jointly in minimax game; Minimax objective function: \(\mbox{min}_{\theta_g} \mbox{max}_{\theta_d}\left[\mathbb{E}_{x\sim {p_{data}}}\log D_{\theta_d}(x) + \mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z))) \right]\) where $\theta_g$ is an objective for the generator objective and $\theta_d$ for the discriminator $D_{\theta_d}(x)$: Discriminator outputs likelihood in (0,1) of real image $D_{\theta_d}(G_{\theta_g}(z))$: Discriminator output for generated fake data G(z) Discriminator($\theta_d$) wants to maximize objective such that D(x) is close to 1(real) and D(G(z)) is close to 0(fake) Generator($\theta_g$) wants to minimize objective such that D(G(z)) is close to 1(to fool discriminator) We alternate the minimax objection function with: Gradient ascent on discriminator \(\mbox{max}_{\theta_d}\left[\mathbb{E}_{x\sim p_{data}}\log D_{\theta_d}(x) + \mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z))) \right]\) 1) Gradient descent on generator \(\mbox{min}_{\theta_g}\mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z)))\) In practice, optimizing this generator objective does not work well; When sample is likely fake, want to learn from it to improve generator (move to the right on X axis), but gradient near 0 in X axis is relatively flat; Gradient signal is dominated by region where sample is already good(near 1). 2) Instead: Gradient ascent on generator, different objective \(\mbox{max}_{\theta_d}\mathbb{E}_{z\sim p(z)}\log(D_{\theta_d}(G_{\theta_g}(z)))\) Rather than minimizing likelihood of discriminator being correct, maximize likelihood of discriminator being wrong. Same objective of fooling discriminator, but now higher gradient signal for bad samples. GAN training Algorithm After training, use generator network to generate new images GAN: Convolutional Architectures Generator is an upsampling network with fractionally-strided convolutions Discriminator is a convolutional network Architecture guidelines for stable Deep Conv GANs Replace any pooling layers with strided convolutions(discriminator) and fractional-strided convolutions(generator). Use batchnorm in both network. Remove fully connected hidden layers for deeper architecture. Use ReLU activation in generator for all layers except for the output, which uses Tanh. Use LeakyReLU activation in the discriminator for all layers. GAN: Interpretable Vector Math works similar to a language model 2017: Explosion of GANs “The GAN Zoo”, https://github.com/hindupuravinash/the-gan-zoo check https://github.com/soumith/ganhacks for tips and tricks for training GANs Scene graphs to GANs Specifying exactly what kind of image you want to generate. The explicit structure in scene graphs provides better image generation for complex scenes. Summary: GANs Don’t work with an explicit density function Take game-theoretic approach: learn to generate from training distribution through 2-player game Pros: Beautiful, state-of-the-art samples Cons: Trickier / more unstable to train Can’t solve inference queries such as $p(x)$, $p(z\vert x)$ Active areas of research: Better loss functions, more stable training (Wasserstein GAN, LSGAN, many others) Conditional GANs, GANs for all kinds of applications Useful Resources on Generative Models CS236: Deep Generative Models (Stanford) CS 294-158 Deep Unsupervised Learning (Berkeley)</summary></entry></feed>