<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2022-02-16T08:59:55+09:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Darron’s Devlog</title><entry><title type="html">Starfish detection w/ TF Object Detection API</title><link href="http://0.0.0.0:4000/starfish_detection" rel="alternate" type="text/html" title="Starfish detection w/ TF Object Detection API" /><published>2022-02-15T00:00:00+09:00</published><updated>2022-02-15T00:00:00+09:00</updated><id>http://0.0.0.0:4000/starfish_detection</id><content type="html" xml:base="http://0.0.0.0:4000/starfish_detection">&lt;h2 id=&quot;tensorflow---help-protect-the-great-barrier-reef&quot;&gt;TensorFlow - Help Protect the Great Barrier Reef&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Worked in Feb. 2022. to study object detection model&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Score(IoU=0.50:0.95):&lt;br /&gt;
  &lt;em&gt;mAP@100: 0.364686 / AR@100: 0.491768 / Expected F2: 0.459727&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Direct link: &lt;a href=&quot;https://www.kaggle.com/kwondalhyeon/starfish-detection-w-tf-object-detection-api?scriptVersionId=87885389&quot;&gt;kaggle notebook&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;iframe src=&quot;https://www.kaggle.com/embed/kwondalhyeon/starfish-detection-w-tf-object-detection-api?kernelSessionId=87885389&quot; height=&quot;1200&quot; style=&quot;margin: 0 auto; width: 100%; max-width: 100%;&quot; frameborder=&quot;0&quot; scrolling=&quot;auto&quot; title=&quot;Starfish detection w/ TF Object Detection API&quot;&gt;&lt;/iframe&gt;</content><author><name>Darron Kwon</name></author><category term="projects" /><summary type="html">TensorFlow - Help Protect the Great Barrier Reef Worked in Feb. 2022. to study object detection model Score(IoU=0.50:0.95): mAP@100: 0.364686 / AR@100: 0.491768 / Expected F2: 0.459727 Direct link: kaggle notebook</summary></entry><entry><title type="html">Unsupervised Representation Learning by Predicting Image Rotations</title><link href="http://0.0.0.0:4000/Rotation" rel="alternate" type="text/html" title="Unsupervised Representation Learning by Predicting Image Rotations" /><published>2022-01-25T00:00:00+09:00</published><updated>2022-01-25T00:00:00+09:00</updated><id>http://0.0.0.0:4000/Rotation</id><content type="html" xml:base="http://0.0.0.0:4000/Rotation">&lt;h2 id=&quot;unsupervised-representation-learning-by-predicting-image-rotations&quot;&gt;Unsupervised Representation Learning by Predicting Image Rotations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Gidaris et al. 2018&lt;/em&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://github.com/gidariss/FeatureLearningRotNet&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ConvNet:&lt;br /&gt;
  (+) Unparalleled capacity to learn high level semantic image features&lt;br /&gt;
  (-) Require massive amounts of manually labeled data, expensive and impractical to scale&lt;br /&gt;
  $\rightarrow$ &lt;em&gt;Unsupervised Learning&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Unsupervised semantic feature learning:&lt;br /&gt;
  Learn image features by training ConvNets to recognize the 2d rotated images as input. With apparently simple task, provides a very powerful supervisory signal for semantic feature learning(Conv). Evaluated in various unsupervised feature learning benchmarks, exceeds SotA performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;featurelearningrotnet&quot;&gt;FeatureLearningRotNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;How To:&lt;br /&gt;
  First define a small set of discrete geometric transformations, then each of those transformations are applied to each image on the dataset and produced transformed images are fed to ConvNet model that is trained to recognize the transformation of each image.
    &lt;ul&gt;
      &lt;li&gt;Set of geometric transformations define the classification pretext task that the ConvNet has to learn; to achieve unsupervised semantic feature learning, it is important to properly choose those geometric transformations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Purpose: to define the geometric transformations as rotations of 4 different degrees, ConvNet trained on the 4-way image classification task of recognizing one of the four Maximizing prob. $F^y(x^{y^{*}})$, probability of transformation &lt;em&gt;y&lt;/em&gt; predicted by &lt;em&gt;F&lt;/em&gt;, when given X is transformed by the transformation $y^{*}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;With idea: In order a ConvNet model to be able recognize the rotation transformations, it will require to understand the concept of the objects depicted in the image	such as their location, type, and pose.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;define a set of &lt;em&gt;K&lt;/em&gt; discrete geometric transformations \(G = \{g(\cdot\vert y)\}_{y=1}^K\), where $g(.\vert y)$ applies to input X, transformed image $X^y = g(X\vert y)$&lt;/li&gt;
  &lt;li&gt;ConvNet model F(.) gets as input an image $X^{y^{\ast}}$, to recognize unknown $y^{\ast}$ yields as output a probability distribution over all possible transformations	\(F(X^{y^{\ast}}\vert\theta) = \{ F^y(X^{y^{\ast}}\vert\theta) \}_{y=1}^K\), output &lt;em&gt;F&lt;/em&gt; returns probs for all classes $y$.&lt;/li&gt;
  &lt;li&gt;Therefore, &lt;em&gt;N&lt;/em&gt; training images \(D = \{ X_i \}_{i=0}^N\), the self-supervised training objective that ConvNet must learn to solve is:&lt;br /&gt;
 \(\mbox{min}_{\theta}\frac{1}{N}\sum_{i=1}^N \mbox{loss}(X_i,\theta)\),&lt;br /&gt;
 where the loss function is defined as:&lt;br /&gt;
 \(\mbox{loss}(X_i,\theta) = -\frac{1}{K}\sum_{y=1}^K \log(F^y(g(X_i|y)|\theta))\)&lt;br /&gt;
 (negative sum of log probs &lt;em&gt;F&lt;/em&gt; for all classes &lt;em&gt;y&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;2d image rotations:&lt;br /&gt;
  $Rot(X, \phi)$, operator that rotates image X by $\phi$ degrees&lt;br /&gt;
  In this case 0, 90, 180, 270; K=4 for G, where $g(X|y)=Rot(X,(y-1)90)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;forcing-the-learning-of-semantic-features&quot;&gt;Forcing the learning of semantic features&lt;/h3&gt;
&lt;p&gt;Fact that it is essentially impossible for a ConvNet model to effectively perform the above rotation recognition task, unless it has first learnt to recognize and detect classes of objects as well as their &lt;strong&gt;semantic parts&lt;/strong&gt; in images.&lt;br /&gt;
$\rightarrow$ &lt;strong&gt;ATTENTION MAPS&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/papers_rotation_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;By comparing the attention maps from two models trained on supervised and unsupervised way, we observe that both models seem to focus on roughly &lt;strong&gt;the same image regions&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/papers_rotation_1.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Also, trained on the proposed rotation recognition task, &lt;strong&gt;visualized layer filters&lt;/strong&gt; learnt appear to have a big variety of edge filters on multiple orientations and multiple frequencies, then the filters learnt by the supervised task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$*$ Activation-based Attention Maps from &lt;em&gt;“Paying More Attention to Attention”, Zagoruyko et al., 2017&lt;/em&gt; - &lt;a href=&quot;https://arxiv.org/abs/1612.03928&quot;&gt;LINK&lt;/a&gt;&lt;br /&gt;
  Activation tensor of a conv. layer: $A\in R^{C\times H\times W}$ consists of &lt;em&gt;C&lt;/em&gt; feautre planes with spatial dimensions &lt;em&gt;H&lt;/em&gt;x&lt;em&gt;W&lt;/em&gt;&lt;br /&gt;
  Activation-based mapping function &lt;em&gt;F&lt;/em&gt; w.r.t that layer: $\mathcal{F}: R^{C\times H\times W} \rightarrow R^{H\times W}$&lt;br /&gt;
  With implicit assumption: Absolute value of a hidden neuron activation(that results when the network is evaluated on given input) can be used as an indication about the importance of that neuron w.r.t. the specific input.&lt;br /&gt;
  By considering, therefore, the absolute values of the elements of tensor A,	we construct a spatial attention map by computing statistics of these values	across the channel dimension(&lt;em&gt;C&lt;/em&gt;)&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;sum of abs: $F_{sum}(A)=\sum_{i=1}^C\vert A_i\vert$&lt;/li&gt;
      &lt;li&gt;sum of abs, raised to the power of p(&amp;gt;1): $F_{sum}^p(A) = \sum_{i=1}^C\vert A_i\vert^p$&lt;/li&gt;
      &lt;li&gt;max of abs, raised to the pwoer of p(&amp;gt;1): $F_{max}^p(A) = \mbox{max}_{i=1,C}\vert A_i\vert^p$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;transfer-learning&quot;&gt;Transfer Learning&lt;/h3&gt;
&lt;p&gt;With a model trained on proposed rotation recognition task with unlabeled data, freeze its early conv. layers and attach the layers from a supervised model, evaluate on a supervised task with a subset of labeled data.&lt;/p&gt;</content><author><name>Darron Kwon</name></author><category term="papers" /><summary type="html">Unsupervised Representation Learning by Predicting Image Rotations Gidaris et al. 2018 https://github.com/gidariss/FeatureLearningRotNet ConvNet: (+) Unparalleled capacity to learn high level semantic image features (-) Require massive amounts of manually labeled data, expensive and impractical to scale $\rightarrow$ Unsupervised Learning Unsupervised semantic feature learning: Learn image features by training ConvNets to recognize the 2d rotated images as input. With apparently simple task, provides a very powerful supervisory signal for semantic feature learning(Conv). Evaluated in various unsupervised feature learning benchmarks, exceeds SotA performance. FeatureLearningRotNet How To: First define a small set of discrete geometric transformations, then each of those transformations are applied to each image on the dataset and produced transformed images are fed to ConvNet model that is trained to recognize the transformation of each image. Set of geometric transformations define the classification pretext task that the ConvNet has to learn; to achieve unsupervised semantic feature learning, it is important to properly choose those geometric transformations. Purpose: to define the geometric transformations as rotations of 4 different degrees, ConvNet trained on the 4-way image classification task of recognizing one of the four Maximizing prob. $F^y(x^{y^{*}})$, probability of transformation y predicted by F, when given X is transformed by the transformation $y^{*}$. With idea: In order a ConvNet model to be able recognize the rotation transformations, it will require to understand the concept of the objects depicted in the image such as their location, type, and pose. Overview define a set of K discrete geometric transformations \(G = \{g(\cdot\vert y)\}_{y=1}^K\), where $g(.\vert y)$ applies to input X, transformed image $X^y = g(X\vert y)$ ConvNet model F(.) gets as input an image $X^{y^{\ast}}$, to recognize unknown $y^{\ast}$ yields as output a probability distribution over all possible transformations \(F(X^{y^{\ast}}\vert\theta) = \{ F^y(X^{y^{\ast}}\vert\theta) \}_{y=1}^K\), output F returns probs for all classes $y$. Therefore, N training images \(D = \{ X_i \}_{i=0}^N\), the self-supervised training objective that ConvNet must learn to solve is: \(\mbox{min}_{\theta}\frac{1}{N}\sum_{i=1}^N \mbox{loss}(X_i,\theta)\), where the loss function is defined as: \(\mbox{loss}(X_i,\theta) = -\frac{1}{K}\sum_{y=1}^K \log(F^y(g(X_i|y)|\theta))\) (negative sum of log probs F for all classes y) 2d image rotations: $Rot(X, \phi)$, operator that rotates image X by $\phi$ degrees In this case 0, 90, 180, 270; K=4 for G, where $g(X|y)=Rot(X,(y-1)90)$ Forcing the learning of semantic features Fact that it is essentially impossible for a ConvNet model to effectively perform the above rotation recognition task, unless it has first learnt to recognize and detect classes of objects as well as their semantic parts in images. $\rightarrow$ ATTENTION MAPS By comparing the attention maps from two models trained on supervised and unsupervised way, we observe that both models seem to focus on roughly the same image regions. Also, trained on the proposed rotation recognition task, visualized layer filters learnt appear to have a big variety of edge filters on multiple orientations and multiple frequencies, then the filters learnt by the supervised task. $*$ Activation-based Attention Maps from “Paying More Attention to Attention”, Zagoruyko et al., 2017 - LINK Activation tensor of a conv. layer: $A\in R^{C\times H\times W}$ consists of C feautre planes with spatial dimensions HxW Activation-based mapping function F w.r.t that layer: $\mathcal{F}: R^{C\times H\times W} \rightarrow R^{H\times W}$ With implicit assumption: Absolute value of a hidden neuron activation(that results when the network is evaluated on given input) can be used as an indication about the importance of that neuron w.r.t. the specific input. By considering, therefore, the absolute values of the elements of tensor A, we construct a spatial attention map by computing statistics of these values across the channel dimension(C) sum of abs: $F_{sum}(A)=\sum_{i=1}^C\vert A_i\vert$ sum of abs, raised to the power of p(&amp;gt;1): $F_{sum}^p(A) = \sum_{i=1}^C\vert A_i\vert^p$ max of abs, raised to the pwoer of p(&amp;gt;1): $F_{max}^p(A) = \mbox{max}_{i=1,C}\vert A_i\vert^p$ Transfer Learning With a model trained on proposed rotation recognition task with unlabeled data, freeze its early conv. layers and attach the layers from a supervised model, evaluate on a supervised task with a subset of labeled data.</summary></entry><entry><title type="html">cs231n - Lecture 13. Self-Supervised Learning</title><link href="http://0.0.0.0:4000/cs231n_lec13" rel="alternate" type="text/html" title="cs231n - Lecture 13. Self-Supervised Learning" /><published>2022-01-20T00:00:00+09:00</published><updated>2022-01-20T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec13</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec13">&lt;h2 id=&quot;self-supervised-learning&quot;&gt;Self-Supervised Learning&lt;/h2&gt;

&lt;h3 id=&quot;generative-vs-self-supervised-learning&quot;&gt;Generative vs. Self-supervised Learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Both aim to learn from data without manual label annotation&lt;/li&gt;
  &lt;li&gt;Generative learning aims to model &lt;strong&gt;data distribution&lt;/strong&gt; $p_{data}(x)$,&lt;br /&gt;
  e.g., generating realistic images.&lt;/li&gt;
  &lt;li&gt;Self-supervised learning methods solve “pretext” tasks that produce &lt;strong&gt;good features&lt;/strong&gt; for downstream tasks.
    &lt;ul&gt;
      &lt;li&gt;Learn with supervised learning objectives, e.g., classification, regression.&lt;/li&gt;
      &lt;li&gt;Labels of these pretext tasks are generated &lt;em&gt;automatically&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;self-supervised-pretext-tasks&quot;&gt;Self-supervised pretext tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Example: learn to predict image transformations / complete corrupted images;&lt;br /&gt;
  e.g. image completion, rotation prediction, “jigsaw puzzle”, coloriztaion.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Solving the pretext tasks allow the model to learn good features.&lt;/li&gt;
  &lt;li&gt;We can automatically generate labels for the pretext tasks.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Learning to generate pixel-level details is often unnecessary; learn high-level semantic features with pretext tasks instead(only encode high-level features sufficient enough to distinguish different objects, Contrastive Methods): &lt;a href=&quot;https://aeon.co/essays/your-brain-does-not-process-information-and-it-is-not-a-computer&quot; target=&quot;_blank&quot;&gt;Epstein, 2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-evaluate-a-self-supervised-learning-method&quot;&gt;How to evaluate a self-supervised learning method?&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Self-supervised learning:&lt;br /&gt;
 With lots of unlabeled data, learn good feature extractors from self-supervised pretext tasks, e.g., predicting image rotations.&lt;/li&gt;
  &lt;li&gt;Supervised Learning:&lt;br /&gt;
 With small amount of labeled data on the target task, attach a shallow network on the feature extractor; train the shallow network and evaluate on the target task, e.g., classification, detection.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;pretext-tasks-from-image-transformations&quot;&gt;Pretext tasks from image transformations&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Predict Rotations&lt;br /&gt;
  &lt;em&gt;Gidaris et al., 2018&lt;/em&gt; - &lt;a href=&quot;/Rotation&quot;&gt;(Paper Review)&lt;/a&gt; 
  &lt;img src=&quot;/assets/images/cs231n_lec13_17.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Predict Relative Patch Locations&lt;br /&gt;
  &lt;em&gt;Doersch et al., 2015&lt;/em&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Solving “jigsaw puzzles”; shuffled patches &lt;br /&gt;
  &lt;em&gt;Noroozi &amp;amp; Favaro, 2016&lt;/em&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_1.png&quot; alt=&quot;png&quot; width=&quot;65%&amp;quot;, height=&amp;quot;65%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Predict Missing Pixels(Inpainting); encoder-decoder&lt;br /&gt;
  &lt;em&gt;Pathak et al., 2016&lt;/em&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_2.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Image Coloring; Split-brain Autoencoder&lt;br /&gt;
  &lt;em&gt;Richard Zhang/ Phillip Isola&lt;/em&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_3.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Video Coloring; from t=0 reference frame to the later frames&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_4.png&quot; alt=&quot;png&quot; width=&quot;65%&amp;quot;, height=&amp;quot;65%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-pretext-tasks&quot;&gt;Summary: Pretext tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Pretext tasks focus on “visual common sense”; by image transformations, can learn without supervision(big labeled data).&lt;/li&gt;
  &lt;li&gt;The models are forced learn good features about natural images, e.g., semantic representation of an object category, in order to solve the pretext tasks.&lt;/li&gt;
  &lt;li&gt;We don’t care about the performance of these pretext tasks, but rather how useful the learned features are for downstream tasks.&lt;/li&gt;
  &lt;li&gt;$\color{red}{Problems}$: 1) coming up with individual pretext tasks is tedious, and 2) the learned representations may not be general; tied to a specific pretext task.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;contrastive-representation-learning&quot;&gt;Contrastive Representation Learning&lt;/h2&gt;
&lt;p&gt;For a more general pretext task,&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_5.png&quot; alt=&quot;png&quot; width=&quot;75%&amp;quot;, height=&amp;quot;75%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;a-formulation-of-contrastive-learning&quot;&gt;A formulation of contrastive learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;What we want:&lt;br /&gt;
  $\mbox{score}(f(x), f(x^+)) » score(f(x), f(x^-))$&lt;br /&gt;
  &lt;em&gt;x&lt;/em&gt;: reference sample, &lt;em&gt;x+: positive sample&lt;/em&gt;, &lt;em&gt;x-&lt;/em&gt;: negative sample&lt;br /&gt;
  Given a chosen score function, we aim to learn an &lt;strong&gt;encoder function&lt;/strong&gt; &lt;em&gt;f&lt;/em&gt; that yields high score for positive pairs and low scores for negative pairs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loss function given &lt;em&gt;1&lt;/em&gt; positive sample and &lt;em&gt;N-1&lt;/em&gt; negative samples:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_6.png&quot; alt=&quot;png&quot; width=&quot;65%&amp;quot;, height=&amp;quot;65%&quot; /&gt;&lt;br /&gt;
  seems familiar with &lt;strong&gt;Cross entropy loss&lt;/strong&gt; for a N-way softmax classifier!&lt;br /&gt;
  i.e., learn to find the positive sample from the N samples&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Commonly known as the InfoNCE loss(&lt;em&gt;van den Oord et al., 2018&lt;/em&gt;)&lt;br /&gt;
  A &lt;em&gt;lower bound&lt;/em&gt; on the mutual information between $f(x)$ and $f(x^+)$&lt;br /&gt;
  \(\rightarrow MI[f(x), f(x^+)] - \log(N) \ge -L\)&lt;br /&gt;
  The larger the negative sample size(N), the tighter the bound&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;simclr-a-simple-framework-for-contrastive-learning&quot;&gt;SimCLR: A Simple Framework for Contrastive Learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Chen et al., 2020&lt;/em&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_7.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Cosine similarity as the score function:&lt;br /&gt;
  \(s(u, v) = \frac{u^T v}{\lVert u \rVert \lVert v \rVert}\)&lt;/li&gt;
  &lt;li&gt;Use a projection network &lt;strong&gt;&lt;em&gt;h(.)&lt;/em&gt;&lt;/strong&gt; to project features to a space where contrastive learning is applied.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generate positive samples through data augmentation:&lt;br /&gt;
  random cropping, random color distortion, and random blur.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_8.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_9.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Evaluate: Freeze feature encoder, train(finetune) on a supervised downstream task&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- simCLR v1 &amp; v2 리뷰: https://rauleun.github.io/SimCLR--&gt;
&lt;h4 id=&quot;simclr-design-choices-projection-headzg&quot;&gt;SimCLR design choices: Projection head($z=g(.)$)&lt;/h4&gt;
&lt;p&gt;Linear / non-linear projection heads improve representation learning.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A possible explanation:
    &lt;ul&gt;
      &lt;li&gt;contrastive learning objective may discard useful information for downstream tasks.&lt;/li&gt;
      &lt;li&gt;representation space &lt;strong&gt;z&lt;/strong&gt; is trained to be invariant to data transformation.&lt;/li&gt;
      &lt;li&gt;by leveraging the projection head &lt;strong&gt;g(.)&lt;/strong&gt;, more information can be preserved in the &lt;strong&gt;h&lt;/strong&gt; representation space&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;simclr-design-choices-large-batch-size&quot;&gt;SimCLR design choices: Large batch size&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec13_10.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
Large training batch size is crucial for SimCLR, but it causes large memory footprint during backpropagation; requires distributed training on TPUs.&lt;/p&gt;

&lt;h3 id=&quot;momentum-contrastive-learning-moco&quot;&gt;Momentum Contrastive Learning (MoCo)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;He et al., 2020&lt;/em&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_11.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Key differences to SimCLR:
    &lt;ul&gt;
      &lt;li&gt;Keep a running queue of keys (negative samples).&lt;/li&gt;
      &lt;li&gt;Compute gradients and update the encoder only through the queries.&lt;/li&gt;
      &lt;li&gt;Decouple min-batch size with the number of keys: can support a large number of negative samples.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec13_12.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;moco-v2&quot;&gt;MoCo V2&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Chen et al., 2020&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;A hybrid of ideas from SimCLR and MoCo:&lt;br /&gt;
  From SimCLR: non-linear projection head and strong data augmentation.&lt;br /&gt;
  From MoCo: momentum-updated queues that allow training on a large number of negative samples (no TPU required).&lt;/li&gt;
  &lt;li&gt;Key takeaways(vs. SimCLR, MoCo V1):
    &lt;ul&gt;
      &lt;li&gt;Non-linear projection head and strong data augmentation are crucial for contrastive learning.&lt;/li&gt;
      &lt;li&gt;Decoupling mini-batch size with negative sample size allows MoCo-V2 to outperform SimCLR with smaller batch size (256 vs. 8192).&lt;/li&gt;
      &lt;li&gt;Achieved with much smaller memory footprint.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;instance-vs-sequence-contrastive-learning&quot;&gt;Instance vs. Sequence Contrastive Learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Instance-level contrastive learning:&lt;br /&gt;
  Based on positive &amp;amp; negative instances.&lt;br /&gt;
  E.g., SimCLR, MoCo&lt;/li&gt;
  &lt;li&gt;Sequence-level contrastive learning:&lt;br /&gt;
  Based on sequential / temporal orders.&lt;br /&gt;
  E.g., Contrastive Predictive Coding (CPC)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;contrastive-predictive-coding-cpc&quot;&gt;Contrastive Predictive Coding (CPC)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;van den Oord et al., 2018&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Contrastive: contrast between “right” and “wrong” sequences using contrastive learning.&lt;/li&gt;
  &lt;li&gt;Predictive: the model has to predict future patterns given the current context.&lt;/li&gt;
  &lt;li&gt;Coding: the model learns useful feature vectors, or “code”, for downstream tasks, similar to other context self-supervised methods.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec13_13.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Encode all samples in a sequence into vectors $z_t = g_{\mbox{enc}}(x_t)$&lt;/li&gt;
  &lt;li&gt;Summarize context (e.g., half of a sequence) into a context code $c_t$ using an auto-regressive model ($g_{\mbox{ar}}$). The original paper uses GRU-RNN here.&lt;/li&gt;
  &lt;li&gt;Compute InfoNCE loss between the context $c_t$ and future code $z_{t+k}$ using the following time-dependent score funtion: $s_k(z_{t+k}, c_t) = z_{t+k}^T W_k c_t$, where $W_k$ is a trainable matrix.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Summary(CPC):&lt;br /&gt;
  Contrast “right” sequence with “wrong” sequence.&lt;br /&gt;
  InfoNCE loss with a time-dependent score function.&lt;br /&gt;
  Can be applied to a variety of learning problems, but not as effective in learning image representations compared to instance-level methods.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-examples&quot;&gt;Other examples&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec13_14.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_15.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_16.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Self-Supervised Learning Generative vs. Self-supervised Learning Both aim to learn from data without manual label annotation Generative learning aims to model data distribution $p_{data}(x)$, e.g., generating realistic images. Self-supervised learning methods solve “pretext” tasks that produce good features for downstream tasks. Learn with supervised learning objectives, e.g., classification, regression. Labels of these pretext tasks are generated automatically. Self-supervised pretext tasks Example: learn to predict image transformations / complete corrupted images; e.g. image completion, rotation prediction, “jigsaw puzzle”, coloriztaion. Solving the pretext tasks allow the model to learn good features. We can automatically generate labels for the pretext tasks. Learning to generate pixel-level details is often unnecessary; learn high-level semantic features with pretext tasks instead(only encode high-level features sufficient enough to distinguish different objects, Contrastive Methods): Epstein, 2016 How to evaluate a self-supervised learning method? Self-supervised learning: With lots of unlabeled data, learn good feature extractors from self-supervised pretext tasks, e.g., predicting image rotations. Supervised Learning: With small amount of labeled data on the target task, attach a shallow network on the feature extractor; train the shallow network and evaluate on the target task, e.g., classification, detection. Pretext tasks from image transformations Predict Rotations Gidaris et al., 2018 - (Paper Review) Predict Relative Patch Locations Doersch et al., 2015 Solving “jigsaw puzzles”; shuffled patches Noroozi &amp;amp; Favaro, 2016 Predict Missing Pixels(Inpainting); encoder-decoder Pathak et al., 2016 Image Coloring; Split-brain Autoencoder Richard Zhang/ Phillip Isola Video Coloring; from t=0 reference frame to the later frames Summary: Pretext tasks Pretext tasks focus on “visual common sense”; by image transformations, can learn without supervision(big labeled data). The models are forced learn good features about natural images, e.g., semantic representation of an object category, in order to solve the pretext tasks. We don’t care about the performance of these pretext tasks, but rather how useful the learned features are for downstream tasks. $\color{red}{Problems}$: 1) coming up with individual pretext tasks is tedious, and 2) the learned representations may not be general; tied to a specific pretext task. Contrastive Representation Learning For a more general pretext task, A formulation of contrastive learning What we want: $\mbox{score}(f(x), f(x^+)) » score(f(x), f(x^-))$ x: reference sample, x+: positive sample, x-: negative sample Given a chosen score function, we aim to learn an encoder function f that yields high score for positive pairs and low scores for negative pairs. Loss function given 1 positive sample and N-1 negative samples: seems familiar with Cross entropy loss for a N-way softmax classifier! i.e., learn to find the positive sample from the N samples Commonly known as the InfoNCE loss(van den Oord et al., 2018) A lower bound on the mutual information between $f(x)$ and $f(x^+)$ \(\rightarrow MI[f(x), f(x^+)] - \log(N) \ge -L\) The larger the negative sample size(N), the tighter the bound SimCLR: A Simple Framework for Contrastive Learning Chen et al., 2020 Cosine similarity as the score function: \(s(u, v) = \frac{u^T v}{\lVert u \rVert \lVert v \rVert}\) Use a projection network h(.) to project features to a space where contrastive learning is applied. Generate positive samples through data augmentation: random cropping, random color distortion, and random blur. Evaluate: Freeze feature encoder, train(finetune) on a supervised downstream task SimCLR design choices: Projection head($z=g(.)$) Linear / non-linear projection heads improve representation learning. A possible explanation: contrastive learning objective may discard useful information for downstream tasks. representation space z is trained to be invariant to data transformation. by leveraging the projection head g(.), more information can be preserved in the h representation space SimCLR design choices: Large batch size Large training batch size is crucial for SimCLR, but it causes large memory footprint during backpropagation; requires distributed training on TPUs. Momentum Contrastive Learning (MoCo) He et al., 2020 Key differences to SimCLR: Keep a running queue of keys (negative samples). Compute gradients and update the encoder only through the queries. Decouple min-batch size with the number of keys: can support a large number of negative samples. MoCo V2 Chen et al., 2020 A hybrid of ideas from SimCLR and MoCo: From SimCLR: non-linear projection head and strong data augmentation. From MoCo: momentum-updated queues that allow training on a large number of negative samples (no TPU required). Key takeaways(vs. SimCLR, MoCo V1): Non-linear projection head and strong data augmentation are crucial for contrastive learning. Decoupling mini-batch size with negative sample size allows MoCo-V2 to outperform SimCLR with smaller batch size (256 vs. 8192). Achieved with much smaller memory footprint. Instance vs. Sequence Contrastive Learning Instance-level contrastive learning: Based on positive &amp;amp; negative instances. E.g., SimCLR, MoCo Sequence-level contrastive learning: Based on sequential / temporal orders. E.g., Contrastive Predictive Coding (CPC) Contrastive Predictive Coding (CPC) van den Oord et al., 2018 Contrastive: contrast between “right” and “wrong” sequences using contrastive learning. Predictive: the model has to predict future patterns given the current context. Coding: the model learns useful feature vectors, or “code”, for downstream tasks, similar to other context self-supervised methods. Encode all samples in a sequence into vectors $z_t = g_{\mbox{enc}}(x_t)$ Summarize context (e.g., half of a sequence) into a context code $c_t$ using an auto-regressive model ($g_{\mbox{ar}}$). The original paper uses GRU-RNN here. Compute InfoNCE loss between the context $c_t$ and future code $z_{t+k}$ using the following time-dependent score funtion: $s_k(z_{t+k}, c_t) = z_{t+k}^T W_k c_t$, where $W_k$ is a trainable matrix. Summary(CPC): Contrast “right” sequence with “wrong” sequence. InfoNCE loss with a time-dependent score function. Can be applied to a variety of learning problems, but not as effective in learning image representations compared to instance-level methods. Other examples</summary></entry><entry><title type="html">cs231n - Lecture 12. Generative Models</title><link href="http://0.0.0.0:4000/cs231n_lec12" rel="alternate" type="text/html" title="cs231n - Lecture 12. Generative Models" /><published>2022-01-10T00:00:00+09:00</published><updated>2022-01-10T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec12</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec12">&lt;h3 id=&quot;supervised-vs-unsupervised&quot;&gt;Supervised vs. Unsupervised&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Supervised Learning:&lt;br /&gt;
  Data: $(x,y)$; &lt;em&gt;y&lt;/em&gt; is label&lt;br /&gt;
  Goal: Learn a function to map $x\rightarrow y$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unsupervised Learning:&lt;br /&gt;
  Data: &lt;em&gt;x&lt;/em&gt;; no labels&lt;br /&gt;
  Goal: Learn some underlying hidden structure of the data&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;generative-modeling&quot;&gt;Generative Modeling&lt;/h3&gt;
&lt;p&gt;Given training data, generate new samples from same distribution&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Objectives:
    &lt;ol&gt;
      &lt;li&gt;Learn $p_{\scriptstyle\text{model}}(x)$ that approximates $p_{\scriptstyle\text{data}}(x)$&lt;/li&gt;
      &lt;li&gt;Sampling new &lt;em&gt;x&lt;/em&gt; from $p_{\scriptstyle\text{model}}(x)$&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Formulate as density estimation problems:
    &lt;ul&gt;
      &lt;li&gt;Explicit density estimation: explicitly define and solve for $p_{\scriptstyle\text{model}}(x)$.&lt;/li&gt;
      &lt;li&gt;Implicit density estimation: learn model that can sample from $p_{\scriptstyle\text{model}}(x)$ without explicitly defining it.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Why Generative Models?&lt;br /&gt;
  Realistic samples for artwork, super-resolution, colorization, etc.&lt;br /&gt;
  Learn useful features for downstream tasks such as classification.&lt;br /&gt;
  Getting insights from high-dimensional data (physics, medical imaging, etc.)&lt;br /&gt;
  Modeling physical world for simulation and planning (robotics and reinforcement learning applications)&lt;br /&gt;
  …&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pixelrnn-and-pixelcnn-a-brief-overview&quot;&gt;PixelRNN and PixelCNN; a brief overview&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Fully visible belief network (FVBN)&lt;br /&gt;
  is an explicit density model, defines tractable density function using chain rule to decompose the likelihood of an image &lt;em&gt;x&lt;/em&gt; into product of &lt;em&gt;1&lt;/em&gt;-d distributions:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_0.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
  Then maximize likelihood of training data. It is a complex distribution over pixel values, express using a neural network.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pixelrnn-van-der-oord-et-al-2016&quot;&gt;PixelRNN, &lt;em&gt;van der Oord et al., 2016&lt;/em&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Generate image pixels starting from corner, dependency on previous pixels modeled using an RNN(LSTM). 
  &lt;img src=&quot;/assets/images/cs231n_lec12_1.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;br /&gt;
  Drawback: sequential generation is slow in both training and inference&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pixelcnn-van-der-oord-et-al-2016&quot;&gt;PixelCNN, &lt;em&gt;van der Oord et al., 2016&lt;/em&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generate image pixels starting from corner, ependency on previous pixels modeled using a CNN over context region(&lt;strong&gt;masked convolution&lt;/strong&gt;)&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_2.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;br /&gt;
  Training is faster than PixelRNN: it can parallelize convolutions since context region values known from training images.&lt;br /&gt;
  Generation is still slow: for a 32x32 image, we need to do forward passes of the network &lt;em&gt;1024&lt;/em&gt; times for a single image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Improving PixelCNN performance&lt;br /&gt;
  Gated convolutional layers, Short-cut connections, Discretized logistic loss, Multi-scale, Training tricks, etc.&lt;br /&gt;
  See also PixelCNN++, &lt;em&gt;Salimans et al., 2017&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Pros:&lt;br /&gt;
  Can explicitly compute likelihood &lt;em&gt;p(x)&lt;/em&gt;&lt;br /&gt;
  Easy to optimize&lt;br /&gt;
  Good samples&lt;/li&gt;
  &lt;li&gt;Cons:&lt;br /&gt;
  Sequential generation is slow&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variational-autoencodervae&quot;&gt;Variational Autoencoder(VAE)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;VAE is an explicit density model, defines intractable(approximate) density function with latent &lt;strong&gt;z&lt;/strong&gt;:&lt;br /&gt;
  $p_\theta(x) = \int p_\theta(z)p_\theta(x|z)\, dz$&lt;br /&gt;
  No dependencies among pixels, can generate all pixels at the same time. But cannot optimize directly, derive and optimize lower bound on likelihood instead&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background-autoencoders&quot;&gt;Background: Autoencoders&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_3.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;z&lt;/strong&gt; usually smaller than &lt;strong&gt;x&lt;/strong&gt;: with dimensionality reduction to capture meaningful factors of variation in data. Train such that features can be used to reconstruct original data($\hat{x}$)&lt;/li&gt;
      &lt;li&gt;“Autoencoding”; encoding input itself(&lt;em&gt;L2&lt;/em&gt; loss)&lt;/li&gt;
      &lt;li&gt;After training, throw away decoder and adjust to the final task&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder can be used to initialize a supervised model;
  Transfer from large, unlabeled dataset(Autoencoder) to small, labeled dataset and fine-tune; train for final task.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_4.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;But we can’t generate new images from an autoencoder because we don’t know the space of &lt;strong&gt;z&lt;/strong&gt;. $\rightarrow$ Variational Autoencoders for a generative model.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;variational-autoencoders-probabilistic-spin-on-autoencoders&quot;&gt;Variational Autoencoders: Probabilistic spin on autoencoders&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Assume training data \(\left\{ x^{(i)}\right\} _{i=1}^N\) is generated from the distribution of unobserved (latent) representation &lt;strong&gt;z&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Intuition from autoencoders: &lt;strong&gt;x&lt;/strong&gt; is an image, &lt;strong&gt;z&lt;/strong&gt; is latent factors used to generate &lt;strong&gt;x&lt;/strong&gt;: attributes, orientation, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_5.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We want to estimate the true parameters $\theta^*$ of this generative model given training data &lt;em&gt;x&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Model representation:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;p(z)&lt;/em&gt;: Choose prior to be simple, e.g. Gaussian.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;z&lt;/strong&gt;: Reasonable for latent attributes, e.g. pose, how much smile.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;p(x|z)&lt;/em&gt;: Generating images, conditional probability is complex&lt;br /&gt;
  $\rightarrow$ represent with neural network&lt;/li&gt;
      &lt;li&gt;$p_\theta(x)$: Learn model parameters to maximize likelihood of training data&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;variational-autoencoders-intractability&quot;&gt;Variational Autoencoders: Intractability&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Data likelihood:&lt;br /&gt;
  $p_\theta(x) = \int p_\theta(z)p_\theta(x|z)\, dz$&lt;br /&gt;
  where $p_\theta(z)$ is a Simple Gaussian prior and $p_\theta(x|z)$ is a decoder neural network, it is intractable to compute &lt;em&gt;p(x|z)&lt;/em&gt; for every &lt;em&gt;z&lt;/em&gt;.&lt;br /&gt;
  while &lt;em&gt;Monte Carlo estimation&lt;/em&gt;-$\log p(x) \approx \log\frac{1}{k}\sum_{i=1}^k p(x|z^{(i)})$, where $z^{(i)}\sim p(z)$- is too high variance.&lt;/li&gt;
  &lt;li&gt;divided by intractable $p_\theta(x)$, Posterior density also intractable:&lt;br /&gt;
  $p_\theta(z|x) = p_\theta(x|z)p_\theta(z)/p_\theta(x)$&lt;/li&gt;
  &lt;li&gt;Solution:&lt;br /&gt;
  In addition to modeling $p_\theta(x|z)$, learn $q_\phi(z|x)$ that approximates the true posterior $p_\theta(z|x)$. $q_\phi$, approximate posterior allows us to derive a lower bound on the data likelihood that is tractable, which can be optimized.&lt;br /&gt;
  &lt;strong&gt;Variational inference&lt;/strong&gt; is to approximate the unknown posterior distribution from only the observed data &lt;em&gt;x&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

\[\begin{align*}
\log p_\theta(x^{(i)})
&amp;amp;= \mathbf{E}_{z~q_\phi(z|x^{(i)})}\left[ \log p_\theta(x^{(i)}) \right] \quad \textit(p_\theta(x^{(i)})\ does\ not\ depend\ on\ z) \\
&amp;amp;= \mathbf{E}_z \left[
	\log\frac{p_\theta(x^{(i)}|z)p_\theta(z)}{p_\theta(z|x^{(i)})} \right] \quad \textit(Bayes'\ Rule) \\
&amp;amp;= \mathbf{E}_z \left[ 
	\log\frac{p_\theta(x^{(i)}|z)p_\theta(z)}{p_\theta(z|x^{(i)})}
		\frac{q_\phi(z|x^{(i)})}{q_\phi(z|x^{(i)})} \right] \quad \textit(Multiply\ by\ constant)\\
&amp;amp;= \mathbf{E}_z \left[\log p_\theta(x^{(i)}|z) \right]
	- \mathbf{E}_z \left[ \log\frac{q_\phi(z|x^{(i)})}{p_\theta(z)}\right]
	+ \mathbf{E}_z \left[ \log\frac{q_\phi(z|x^{(i)})}{p_\theta(z|x^{(i)})}\right] \quad \textit(Logarithms) \\
&amp;amp;= \mathbf{E}_z \left[\log p_\theta(x^{(i)}|z) \right]
	- D_{KL}(q_\phi(z|x^{(i)})|p_\theta(z)) + D_{KL}(q_\phi(z|x^{(i)})|p_\theta(z|x^{(i)}))
\end{align*}\]

&lt;ul&gt;
  &lt;li&gt;With taking expectation with respect to &lt;em&gt;z&lt;/em&gt;(using encoder network) let us write nice &lt;em&gt;KL&lt;/em&gt; terms;
    &lt;ul&gt;
      &lt;li&gt;\(\mathbf{E}_z \left[\log p_\theta(x^{(i)}\vert z) \right]\): &lt;strong&gt;Decoder&lt;/strong&gt; network gives $p_\theta(x\vert z)$, can compute estimate of this term through sampling(need some trick to differentiate through sampling). It reconstruct the input data.&lt;/li&gt;
      &lt;li&gt;\(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z))\): KL term between Gaussian for encoder and &lt;em&gt;z&lt;/em&gt; prior has nice closed-form solution. &lt;strong&gt;Encoder&lt;/strong&gt; makes approximate posterior distribution close to prior.&lt;/li&gt;
      &lt;li&gt;\(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z\vert x^{(i)}))\): is intractable, we can’t compute this term; but we know KL divergence always greater than &lt;em&gt;0&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To maximize the data likelihood, we can rewrite&lt;br /&gt;
\(\begin{align*}
\log p_\theta (x^{(i)}) &amp;amp;= \mathbf{E}_z \left[ \log p _\theta (x^{(i)}\vert z) \right]
                      - D_{KL}(q_\phi (z\vert x^{(i)})\vert p_\theta (z)) + D_{KL}(q_\phi (z\vert x^{(i)})\vert p_\theta (z\vert x^{(i)})) \\
                      &amp;amp;= \mathcal{L}(x^{(i)},\theta ,\phi ) + (C\ge 0)
\end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;\(\mathcal{L}(x^{(i)},\theta,\phi)\): &lt;em&gt;Decoder - Encoder&lt;/em&gt;&lt;br /&gt;
  &lt;strong&gt;Tractable lower bound&lt;/strong&gt; which we can take gradient of and optimize. Maximizing this &lt;em&gt;evidence lower bound(ELBO)&lt;/em&gt;, we can maximize $\log p_\theta(x)$. Later, we take minus on this term for the loss function of a neural network.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_6.png&quot; alt=&quot;png&quot; width=&quot;45%&amp;quot;, height=&amp;quot;45%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Encoder part; \(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z))\)&lt;br /&gt;
  We choose &lt;em&gt;q(z)&lt;/em&gt; as a Gaussian distribution, $q(z\vert x) = N(\mu_{z\vert x}, \Sigma_{z\vert x})$. Computing the KL divergence, \(D_{KL}(N(\mu_{z\vert x}, \Sigma_{z\vert x}))\vert N(0,I))\), having analytical solution.&lt;/li&gt;
  &lt;li&gt;Reparameterization trick &lt;em&gt;z&lt;/em&gt;:&lt;br /&gt;
  to make sampling differentiable, input sample $\epsilon\sim N(0,I)$ to the graph $z = \mu_{z\vert x} + \epsilon\sigma_{z\vert x}$; where $\mu, \sigma$ are the part of computation graph.&lt;/li&gt;
  &lt;li&gt;Decoder part;&lt;br /&gt;
  Maximize likelihood of original input being reconstructed, $\hat{x}-x$.&lt;/li&gt;
  &lt;li&gt;For every minibatch of input data, compute $\mathcal{L}$ graph forward pass and backprop.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_7.png&quot; alt=&quot;png&quot; width=&quot;75%&amp;quot;, height=&amp;quot;75%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;variational-autoencoders-generating-data&quot;&gt;Variational Autoencoders: Generating Data&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_8.png&quot; alt=&quot;png&quot; width=&quot;55%&amp;quot;, height=&amp;quot;55%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Diagonal prior on &lt;strong&gt;z&lt;/strong&gt; for independent latent variables&lt;/li&gt;
  &lt;li&gt;Different dimensions of &lt;strong&gt;z&lt;/strong&gt; encode interpretable factors of variation;&lt;br /&gt;
  Also good feature representation taht can be computed using $q_\phi(z\vert x)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-1&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Probabilistic spin to traditional autoencoders, allows generating data&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Defines an intractable density; derive and optimize a (variational) lower bound&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Pros:&lt;br /&gt;
  Principled approach to generative models&lt;br /&gt;
  Interpretable latent space&lt;br /&gt;
  Allows inference of $q(z\vert x)$, can be useful feature representation for other tasks  - Cons:&lt;br /&gt;
  Maximizes lower bound of likelihood: not as good evaluation as tractable model&lt;br /&gt;
  Samples &lt;em&gt;mean&lt;/em&gt;; blurrier and lower quality compared to state-of-the-art (GANs)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;generative-adversarial-networksgans&quot;&gt;Generative Adversarial Networks(GANs)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_9.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
idea: Use a discriminator network to tell whether the generate image is within data distribution (“real”) or not&lt;/p&gt;

&lt;h3 id=&quot;training-gans-two-player-game&quot;&gt;Training GANs: Two-player game&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_10.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
Discriminator network: try to distinguish between real and fake images&lt;br /&gt;
Generator network: try to fool discriminator by generating real-looking images&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Train jointly in &lt;strong&gt;minimax game&lt;/strong&gt;;&lt;br /&gt;
  Minimax objective function:&lt;br /&gt;
  \(\mbox{min}_{\theta_g} \mbox{max}_{\theta_d}\left[\mathbb{E}_{x\sim {p_{data}}}\log D_{\theta_d}(x) + \mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z))) \right]\)&lt;br /&gt;
  where $\theta_g$ is an objective for the generator objective and $\theta_d$ for the discriminator
    &lt;ul&gt;
      &lt;li&gt;$D_{\theta_d}(x)$: Discriminator outputs likelihood in &lt;em&gt;(0,1)&lt;/em&gt; of real image&lt;/li&gt;
      &lt;li&gt;$D_{\theta_d}(G_{\theta_g}(z))$: Discriminator output for generated fake data &lt;em&gt;G(z)&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Discriminator($\theta_d$) wants to &lt;strong&gt;maximize objective&lt;/strong&gt; such that &lt;em&gt;D(x)&lt;/em&gt; is close to &lt;em&gt;1&lt;/em&gt;(real) and &lt;em&gt;D(G(z))&lt;/em&gt; is close to &lt;em&gt;0&lt;/em&gt;(fake)&lt;/li&gt;
  &lt;li&gt;Generator($\theta_g$) wants to &lt;em&gt;minimize objective&lt;/em&gt; such that &lt;em&gt;D(G(z))&lt;/em&gt; is close to &lt;em&gt;1&lt;/em&gt;(to fool discriminator)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We alternate the minimax objection function with:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Gradient ascent&lt;/strong&gt; on discriminator&lt;br /&gt;
 \(\mbox{max}_{\theta_d}\left[\mathbb{E}_{x\sim p_{data}}\log D_{\theta_d}(x) + \mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z))) \right]\)&lt;/li&gt;
  &lt;li&gt;1) &lt;strong&gt;Gradient descent&lt;/strong&gt; on generator&lt;br /&gt;
 \(\mbox{min}_{\theta_g}\mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z)))\)
    &lt;ul&gt;
      &lt;li&gt;In practice, optimizing this generator objective does not work well;&lt;br /&gt;
  When sample is likely fake, want to learn from it to improve generator (move to the right on &lt;em&gt;X&lt;/em&gt; axis), but gradient near &lt;em&gt;0&lt;/em&gt; in &lt;em&gt;X&lt;/em&gt; axis is relatively flat; Gradient signal is dominated by region where sample is already good(near &lt;em&gt;1&lt;/em&gt;).&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_11.png&quot; alt=&quot;png&quot; width=&quot;30%&amp;quot;, height=&amp;quot;30%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;2) &lt;strong&gt;Instead: Gradient ascent&lt;/strong&gt; on generator, different objective&lt;br /&gt;
 \(\mbox{max}_{\theta_d}\mathbb{E}_{z\sim p(z)}\log(D_{\theta_d}(G_{\theta_g}(z)))\)&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Rather than minimizing likelihood of discriminator being correct, maximize likelihood of discriminator being wrong. Same objective of fooling discriminator, but now higher gradient signal for bad samples.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_12.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GAN training Algorithm&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_13.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;70%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After training, use generator network to generate new images&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gan-convolutional-architectures&quot;&gt;GAN: Convolutional Architectures&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generator is an upsampling network with fractionally-strided convolutions&lt;br /&gt;
  Discriminator is a convolutional network&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Architecture guidelines for stable Deep Conv GANs&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Replace any pooling layers with strided convolutions(discriminator) and fractional-strided convolutions(generator).&lt;/li&gt;
      &lt;li&gt;Use batchnorm in both network.&lt;/li&gt;
      &lt;li&gt;Remove fully connected hidden layers for deeper architecture.&lt;/li&gt;
      &lt;li&gt;Use ReLU activation in generator for all layers except for the output, which uses Tanh.&lt;/li&gt;
      &lt;li&gt;Use LeakyReLU activation in the discriminator for all layers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gan-interpretable-vector-math&quot;&gt;GAN: Interpretable Vector Math&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_14.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;works similar to a language model&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2017-explosion-of-gans&quot;&gt;2017: Explosion of GANs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;“The GAN Zoo”, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://github.com/hindupuravinash/the-gan-zoo&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;check &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://github.com/soumith/ganhacks&lt;/code&gt; for tips and tricks for training GANs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;scene-graphs-to-gans&quot;&gt;Scene graphs to GANs&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_15.png&quot; alt=&quot;png&quot; width=&quot;30%&amp;quot;, height=&amp;quot;30%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Specifying exactly what kind of image you want to generate. The explicit structure in scene graphs provides better image generation for complex scenes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-gans&quot;&gt;Summary: GANs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Don’t work with an explicit density function&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Take game-theoretic approach: learn to generate from training distribution through 2-player game&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Pros:
    &lt;ul&gt;
      &lt;li&gt;Beautiful, state-of-the-art samples&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cons:
    &lt;ul&gt;
      &lt;li&gt;Trickier / more unstable to train&lt;/li&gt;
      &lt;li&gt;Can’t solve inference queries such as $p(x)$, $p(z\vert x)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Active areas of research:
    &lt;ul&gt;
      &lt;li&gt;Better loss functions, more stable training (Wasserstein GAN, LSGAN, many others)&lt;/li&gt;
      &lt;li&gt;Conditional GANs, GANs for all kinds of applications&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Useful Resources on Generative Models&lt;br /&gt;
  CS236: Deep Generative Models (Stanford)&lt;br /&gt;
  CS 294-158 Deep Unsupervised Learning (Berkeley)&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Supervised vs. Unsupervised Supervised Learning: Data: $(x,y)$; y is label Goal: Learn a function to map $x\rightarrow y$ Unsupervised Learning: Data: x; no labels Goal: Learn some underlying hidden structure of the data Generative Modeling Given training data, generate new samples from same distribution Objectives: Learn $p_{\scriptstyle\text{model}}(x)$ that approximates $p_{\scriptstyle\text{data}}(x)$ Sampling new x from $p_{\scriptstyle\text{model}}(x)$ Formulate as density estimation problems: Explicit density estimation: explicitly define and solve for $p_{\scriptstyle\text{model}}(x)$. Implicit density estimation: learn model that can sample from $p_{\scriptstyle\text{model}}(x)$ without explicitly defining it. Why Generative Models? Realistic samples for artwork, super-resolution, colorization, etc. Learn useful features for downstream tasks such as classification. Getting insights from high-dimensional data (physics, medical imaging, etc.) Modeling physical world for simulation and planning (robotics and reinforcement learning applications) … PixelRNN and PixelCNN; a brief overview Fully visible belief network (FVBN) is an explicit density model, defines tractable density function using chain rule to decompose the likelihood of an image x into product of 1-d distributions: Then maximize likelihood of training data. It is a complex distribution over pixel values, express using a neural network. PixelRNN, van der Oord et al., 2016 Generate image pixels starting from corner, dependency on previous pixels modeled using an RNN(LSTM). Drawback: sequential generation is slow in both training and inference PixelCNN, van der Oord et al., 2016 Generate image pixels starting from corner, ependency on previous pixels modeled using a CNN over context region(masked convolution) Training is faster than PixelRNN: it can parallelize convolutions since context region values known from training images. Generation is still slow: for a 32x32 image, we need to do forward passes of the network 1024 times for a single image. Improving PixelCNN performance Gated convolutional layers, Short-cut connections, Discretized logistic loss, Multi-scale, Training tricks, etc. See also PixelCNN++, Salimans et al., 2017 Summary Pros: Can explicitly compute likelihood p(x) Easy to optimize Good samples Cons: Sequential generation is slow Variational Autoencoder(VAE) VAE is an explicit density model, defines intractable(approximate) density function with latent z: $p_\theta(x) = \int p_\theta(z)p_\theta(x|z)\, dz$ No dependencies among pixels, can generate all pixels at the same time. But cannot optimize directly, derive and optimize lower bound on likelihood instead Background: Autoencoders Unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data z usually smaller than x: with dimensionality reduction to capture meaningful factors of variation in data. Train such that features can be used to reconstruct original data($\hat{x}$) “Autoencoding”; encoding input itself(L2 loss) After training, throw away decoder and adjust to the final task Encoder can be used to initialize a supervised model; Transfer from large, unlabeled dataset(Autoencoder) to small, labeled dataset and fine-tune; train for final task. But we can’t generate new images from an autoencoder because we don’t know the space of z. $\rightarrow$ Variational Autoencoders for a generative model. Variational Autoencoders: Probabilistic spin on autoencoders Assume training data \(\left\{ x^{(i)}\right\} _{i=1}^N\) is generated from the distribution of unobserved (latent) representation z Intuition from autoencoders: x is an image, z is latent factors used to generate x: attributes, orientation, etc. We want to estimate the true parameters $\theta^*$ of this generative model given training data x. Model representation: p(z): Choose prior to be simple, e.g. Gaussian. z: Reasonable for latent attributes, e.g. pose, how much smile. p(x|z): Generating images, conditional probability is complex $\rightarrow$ represent with neural network $p_\theta(x)$: Learn model parameters to maximize likelihood of training data Variational Autoencoders: Intractability Data likelihood: $p_\theta(x) = \int p_\theta(z)p_\theta(x|z)\, dz$ where $p_\theta(z)$ is a Simple Gaussian prior and $p_\theta(x|z)$ is a decoder neural network, it is intractable to compute p(x|z) for every z. while Monte Carlo estimation-$\log p(x) \approx \log\frac{1}{k}\sum_{i=1}^k p(x|z^{(i)})$, where $z^{(i)}\sim p(z)$- is too high variance. divided by intractable $p_\theta(x)$, Posterior density also intractable: $p_\theta(z|x) = p_\theta(x|z)p_\theta(z)/p_\theta(x)$ Solution: In addition to modeling $p_\theta(x|z)$, learn $q_\phi(z|x)$ that approximates the true posterior $p_\theta(z|x)$. $q_\phi$, approximate posterior allows us to derive a lower bound on the data likelihood that is tractable, which can be optimized. Variational inference is to approximate the unknown posterior distribution from only the observed data x \[\begin{align*} \log p_\theta(x^{(i)}) &amp;amp;= \mathbf{E}_{z~q_\phi(z|x^{(i)})}\left[ \log p_\theta(x^{(i)}) \right] \quad \textit(p_\theta(x^{(i)})\ does\ not\ depend\ on\ z) \\ &amp;amp;= \mathbf{E}_z \left[ \log\frac{p_\theta(x^{(i)}|z)p_\theta(z)}{p_\theta(z|x^{(i)})} \right] \quad \textit(Bayes'\ Rule) \\ &amp;amp;= \mathbf{E}_z \left[ \log\frac{p_\theta(x^{(i)}|z)p_\theta(z)}{p_\theta(z|x^{(i)})} \frac{q_\phi(z|x^{(i)})}{q_\phi(z|x^{(i)})} \right] \quad \textit(Multiply\ by\ constant)\\ &amp;amp;= \mathbf{E}_z \left[\log p_\theta(x^{(i)}|z) \right] - \mathbf{E}_z \left[ \log\frac{q_\phi(z|x^{(i)})}{p_\theta(z)}\right] + \mathbf{E}_z \left[ \log\frac{q_\phi(z|x^{(i)})}{p_\theta(z|x^{(i)})}\right] \quad \textit(Logarithms) \\ &amp;amp;= \mathbf{E}_z \left[\log p_\theta(x^{(i)}|z) \right] - D_{KL}(q_\phi(z|x^{(i)})|p_\theta(z)) + D_{KL}(q_\phi(z|x^{(i)})|p_\theta(z|x^{(i)})) \end{align*}\] With taking expectation with respect to z(using encoder network) let us write nice KL terms; \(\mathbf{E}_z \left[\log p_\theta(x^{(i)}\vert z) \right]\): Decoder network gives $p_\theta(x\vert z)$, can compute estimate of this term through sampling(need some trick to differentiate through sampling). It reconstruct the input data. \(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z))\): KL term between Gaussian for encoder and z prior has nice closed-form solution. Encoder makes approximate posterior distribution close to prior. \(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z\vert x^{(i)}))\): is intractable, we can’t compute this term; but we know KL divergence always greater than 0. To maximize the data likelihood, we can rewrite \(\begin{align*} \log p_\theta (x^{(i)}) &amp;amp;= \mathbf{E}_z \left[ \log p _\theta (x^{(i)}\vert z) \right] - D_{KL}(q_\phi (z\vert x^{(i)})\vert p_\theta (z)) + D_{KL}(q_\phi (z\vert x^{(i)})\vert p_\theta (z\vert x^{(i)})) \\ &amp;amp;= \mathcal{L}(x^{(i)},\theta ,\phi ) + (C\ge 0) \end{align*}\) \(\mathcal{L}(x^{(i)},\theta,\phi)\): Decoder - Encoder Tractable lower bound which we can take gradient of and optimize. Maximizing this evidence lower bound(ELBO), we can maximize $\log p_\theta(x)$. Later, we take minus on this term for the loss function of a neural network. Encoder part; \(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z))\) We choose q(z) as a Gaussian distribution, $q(z\vert x) = N(\mu_{z\vert x}, \Sigma_{z\vert x})$. Computing the KL divergence, \(D_{KL}(N(\mu_{z\vert x}, \Sigma_{z\vert x}))\vert N(0,I))\), having analytical solution. Reparameterization trick z: to make sampling differentiable, input sample $\epsilon\sim N(0,I)$ to the graph $z = \mu_{z\vert x} + \epsilon\sigma_{z\vert x}$; where $\mu, \sigma$ are the part of computation graph. Decoder part; Maximize likelihood of original input being reconstructed, $\hat{x}-x$. For every minibatch of input data, compute $\mathcal{L}$ graph forward pass and backprop. Variational Autoencoders: Generating Data Diagonal prior on z for independent latent variables Different dimensions of z encode interpretable factors of variation; Also good feature representation taht can be computed using $q_\phi(z\vert x)$. Summary Probabilistic spin to traditional autoencoders, allows generating data Defines an intractable density; derive and optimize a (variational) lower bound Pros: Principled approach to generative models Interpretable latent space Allows inference of $q(z\vert x)$, can be useful feature representation for other tasks - Cons: Maximizes lower bound of likelihood: not as good evaluation as tractable model Samples mean; blurrier and lower quality compared to state-of-the-art (GANs) Generative Adversarial Networks(GANs) idea: Use a discriminator network to tell whether the generate image is within data distribution (“real”) or not Training GANs: Two-player game Discriminator network: try to distinguish between real and fake images Generator network: try to fool discriminator by generating real-looking images Train jointly in minimax game; Minimax objective function: \(\mbox{min}_{\theta_g} \mbox{max}_{\theta_d}\left[\mathbb{E}_{x\sim {p_{data}}}\log D_{\theta_d}(x) + \mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z))) \right]\) where $\theta_g$ is an objective for the generator objective and $\theta_d$ for the discriminator $D_{\theta_d}(x)$: Discriminator outputs likelihood in (0,1) of real image $D_{\theta_d}(G_{\theta_g}(z))$: Discriminator output for generated fake data G(z) Discriminator($\theta_d$) wants to maximize objective such that D(x) is close to 1(real) and D(G(z)) is close to 0(fake) Generator($\theta_g$) wants to minimize objective such that D(G(z)) is close to 1(to fool discriminator) We alternate the minimax objection function with: Gradient ascent on discriminator \(\mbox{max}_{\theta_d}\left[\mathbb{E}_{x\sim p_{data}}\log D_{\theta_d}(x) + \mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z))) \right]\) 1) Gradient descent on generator \(\mbox{min}_{\theta_g}\mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z)))\) In practice, optimizing this generator objective does not work well; When sample is likely fake, want to learn from it to improve generator (move to the right on X axis), but gradient near 0 in X axis is relatively flat; Gradient signal is dominated by region where sample is already good(near 1). 2) Instead: Gradient ascent on generator, different objective \(\mbox{max}_{\theta_d}\mathbb{E}_{z\sim p(z)}\log(D_{\theta_d}(G_{\theta_g}(z)))\) Rather than minimizing likelihood of discriminator being correct, maximize likelihood of discriminator being wrong. Same objective of fooling discriminator, but now higher gradient signal for bad samples. GAN training Algorithm After training, use generator network to generate new images GAN: Convolutional Architectures Generator is an upsampling network with fractionally-strided convolutions Discriminator is a convolutional network Architecture guidelines for stable Deep Conv GANs Replace any pooling layers with strided convolutions(discriminator) and fractional-strided convolutions(generator). Use batchnorm in both network. Remove fully connected hidden layers for deeper architecture. Use ReLU activation in generator for all layers except for the output, which uses Tanh. Use LeakyReLU activation in the discriminator for all layers. GAN: Interpretable Vector Math works similar to a language model 2017: Explosion of GANs “The GAN Zoo”, https://github.com/hindupuravinash/the-gan-zoo check https://github.com/soumith/ganhacks for tips and tricks for training GANs Scene graphs to GANs Specifying exactly what kind of image you want to generate. The explicit structure in scene graphs provides better image generation for complex scenes. Summary: GANs Don’t work with an explicit density function Take game-theoretic approach: learn to generate from training distribution through 2-player game Pros: Beautiful, state-of-the-art samples Cons: Trickier / more unstable to train Can’t solve inference queries such as $p(x)$, $p(z\vert x)$ Active areas of research: Better loss functions, more stable training (Wasserstein GAN, LSGAN, many others) Conditional GANs, GANs for all kinds of applications Useful Resources on Generative Models CS236: Deep Generative Models (Stanford) CS 294-158 Deep Unsupervised Learning (Berkeley)</summary></entry><entry><title type="html">cs231n - Lecture 11. Attention and Transformers</title><link href="http://0.0.0.0:4000/cs231n_lec11" rel="alternate" type="text/html" title="cs231n - Lecture 11. Attention and Transformers" /><published>2022-01-05T00:00:00+09:00</published><updated>2022-01-05T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec11</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec11">&lt;h2 id=&quot;attention-with-rnns&quot;&gt;Attention with RNNs&lt;/h2&gt;

&lt;h3 id=&quot;image-captioning-using-spatial-features&quot;&gt;Image Captioning using spatial features&lt;/h3&gt;
&lt;p&gt;Input: Image &lt;em&gt;I&lt;/em&gt;&lt;br /&gt;
Output: Sequence &lt;strong&gt;y&lt;/strong&gt; $= y_1, y_2, \ldots, y_T$&lt;br /&gt;
Encoder: $h_0 = f_W(z)$, where &lt;em&gt;z&lt;/em&gt; is spatial CNN features, $f_W(\cdot)$ is an MLP&lt;br /&gt;
Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector &lt;em&gt;c&lt;/em&gt; is often $c=h_0$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
Problem: Input is “bottlenecked” through &lt;em&gt;c&lt;/em&gt;; especially in a long descriptions.  Model needs to encode everything it wants to say within &lt;em&gt;c&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Attention idea: New context vector &lt;em&gt;c_t&lt;/em&gt; at every time step&lt;br /&gt;
Each context vector will attend to different image regions&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alignment scores(scalars): $H \times W$ matrix &lt;strong&gt;&lt;em&gt;e&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
  $e_{t,i,j} = f_{\mbox{att}}(h_{t-1}, z_{i,j})$&lt;br /&gt;
  where $f_{\mbox{att}}(\cdot)$ is an MLP&lt;/li&gt;
  &lt;li&gt;Normalize to get attention weights:&lt;br /&gt;
  $a_{t,:,:} = \mbox{softmax}(e_{t,:,:})$,&lt;br /&gt;
  $0&amp;lt;a_{t,i,j}&amp;lt;1$, attention values sum to &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Compute context vector &lt;em&gt;c&lt;/em&gt;: multiply &lt;em&gt;CNN features&lt;/em&gt; and &lt;em&gt;Attention weights&lt;/em&gt;&lt;br /&gt;
  $c_t = \sum_{i,j} a_{t,i,j} z_{t,i,j}$&lt;/li&gt;
  &lt;li&gt;Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, \color{red}{c_t})$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_1.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;50%&quot; /&gt;&lt;br /&gt;
Each timestep of decode uses a different context vector that looks(attend) at different parts of the input image. This entire process is differentiable; model chooses its own attention weights. No attention supervision is required.&lt;/p&gt;

&lt;h3 id=&quot;similar-tasks-in-nlp---language-translation-example&quot;&gt;Similar tasks in NLP - Language translation example&lt;/h3&gt;
&lt;p&gt;Vanilla Encoder-Decoder setting:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Input: sequence &lt;strong&gt;x&lt;/strong&gt; $= x_1, x_2, \ldots, x_T$&lt;/li&gt;
  &lt;li&gt;Output: sequence &lt;strong&gt;y&lt;/strong&gt; $= y_1, y_2, \ldots, y_T$&lt;/li&gt;
  &lt;li&gt;Encoder: $h_0 = f_W(z)$, where $z_t = \mbox{RNN}(x_t, u_{t-1})$, $f_W(\cdot)$ is MLP, &lt;em&gt;u&lt;/em&gt; is the hidden RNN state&lt;/li&gt;
  &lt;li&gt;Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector &lt;em&gt;c&lt;/em&gt; is often $c=h_0$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
Attention in NLP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alignment scores(scalars):&lt;br /&gt;
  $e_{t,i} = f_{\mbox{att}}(h_{t-1}, z_t)$, where $f_{\mbox{att}}(\cdot)$ is an MLP&lt;/li&gt;
  &lt;li&gt;Normalize to get attention weights:&lt;br /&gt;
  $a_{t,:} = \mbox{softmax}(e_{t,:})$,&lt;br /&gt;
  $0&amp;lt;a_{t,i}&amp;lt;1$, attention values sum to &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Compute context vector &lt;em&gt;c&lt;/em&gt;:&lt;br /&gt;
  $c_t = \sum_i a_{t,i} z_{t,i}$&lt;/li&gt;
  &lt;li&gt;Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, \color{red}c_t)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Heatmap: visualization of attention weights; without any attention supervision, model learns different word orderings for different languages&lt;/p&gt;

&lt;h2 id=&quot;general-attention-layer&quot;&gt;General Attention Layer&lt;/h2&gt;
&lt;p&gt;Attention in image captioning before
&lt;img src=&quot;/assets/images/cs231n_lec11_4.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;single-query-setting&quot;&gt;Single query setting&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_5.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
&lt;strong&gt;Inputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;input vectors: &lt;strong&gt;x&lt;/strong&gt;(shape: $N\times D$)&lt;br /&gt;
  Attention operation is permutation invariant; produces the same output regardless of the order of elements(features) in the input vector. Stretch $H\times W = N$ into &lt;em&gt;N&lt;/em&gt; vectors, transform $H\times W\times D$ features into $N\times D$ input vectors &lt;strong&gt;x&lt;/strong&gt;(similar to attention in NLP).&lt;/li&gt;
  &lt;li&gt;Query: &lt;strong&gt;h&lt;/strong&gt;(shape: D)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alignment
    &lt;ul&gt;
      &lt;li&gt;Change $f_{\mbox{att}}(\cdot)$ to a simple dot product:&lt;br /&gt;
  $e_i = h\cdot x_i$; only works well with key &amp;amp; value transformation trick&lt;/li&gt;
      &lt;li&gt;Change $f_{\mbox{att}}(\cdot)$ to a &lt;strong&gt;scaled&lt;/strong&gt; dot product:&lt;br /&gt;
  $e_i = h\cdot x_i / \sqrt{D}$;&lt;br /&gt;
  Larger dimensions means more terms in the dot product sum. So, the variance of the logits is higher. Large magnitude(length) vectors will produce much higher logits. Then, the post-softmax distribution(&lt;em&gt;e&lt;/em&gt;) has lower-entropy(high uncertainty) assuming logits are &lt;em&gt;I.I.D&lt;/em&gt;. Ultimately, these large magnitude vectors will cause softmax to peak and assign very little weight to all others. To reduce this effect, divide by $sqrt{D}$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$&lt;/li&gt;
  &lt;li&gt;Output: $\mathbf{c} = \sum_i a_i x_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;br /&gt;
	- context vector: &lt;strong&gt;c&lt;/strong&gt;(shape: D)&lt;/p&gt;

&lt;h3 id=&quot;multiple-query-setting&quot;&gt;Multiple query setting&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_6.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
&lt;strong&gt;Inputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;input vectors: &lt;strong&gt;x&lt;/strong&gt;(shape: $N\times D$)&lt;/li&gt;
  &lt;li&gt;Queries: &lt;strong&gt;q&lt;/strong&gt;(shape: $M\times D$); multiple query vectors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alignment: $e_{i,j} = q_j\cdot x_i / \sqrt{D}$&lt;/li&gt;
  &lt;li&gt;Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$&lt;/li&gt;
  &lt;li&gt;Output: $y_j = \sum_i a_{i,j} x_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;context vectors: &lt;strong&gt;y&lt;/strong&gt;(shape: D);&lt;br /&gt;
  each query creates a new output context vector&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;weight-layers-added&quot;&gt;Weight layers added&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_7.png&quot; alt=&quot;png&quot; width=&quot;45%&amp;quot;, height=&amp;quot;45%&quot; /&gt;&lt;br /&gt;
Notice that the input vectors &lt;strong&gt;x&lt;/strong&gt; are used for both the alignment(&lt;strong&gt;e&lt;/strong&gt;) and attention calculations(&lt;strong&gt;y&lt;/strong&gt;); We can add more expressivity to the layer by adding a different FC layer before each of the two steps. The input and output dimensions can now change depending on the key and value FC layers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;input vectors: &lt;strong&gt;x&lt;/strong&gt;(shape: $N\times D$)&lt;/li&gt;
  &lt;li&gt;Queries: &lt;strong&gt;q&lt;/strong&gt;(shape: $M\times D_k$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Key vectors: $\mathbf{k} = \mathbf{x}W_k$&lt;/li&gt;
  &lt;li&gt;Value vectors: $\mathbf{v} = \mathbf{x}W_v$&lt;/li&gt;
  &lt;li&gt;Alignment: $e_{i,j} = q_j\cdot k_i / \sqrt{D}$&lt;/li&gt;
  &lt;li&gt;Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$&lt;/li&gt;
  &lt;li&gt;Output: $y_j = \sum_i a_{i,j} v_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;context vectors: &lt;strong&gt;y&lt;/strong&gt;(shape: $D_v$);&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;self-attention-layer&quot;&gt;Self attention layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_8.png&quot; alt=&quot;png&quot; width=&quot;45%&amp;quot;, height=&amp;quot;45%&quot; /&gt;&lt;br /&gt;
Recall that the query vector was a function of the input vectors; Encoder $h_0=f_W(z)$, where &lt;em&gt;z&lt;/em&gt; is spatial CNN features, $f_W(\cdot)$ is an MLP. We can calculate the query vectors from the input vectors, defining a “self-attention” layer. No input query vectors anymore, instead query vectors are calculated using a FC layer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;input vectors: &lt;strong&gt;x&lt;/strong&gt;(shape: $N\times D$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Key vectors: $\mathbf{k} = \mathbf{x}W_k$&lt;/li&gt;
  &lt;li&gt;Value vectors: $\mathbf{v} = \mathbf{x}W_v$&lt;/li&gt;
  &lt;li&gt;Query vectors: $\mathbf{q} = \mathbf{x}W_q$&lt;/li&gt;
  &lt;li&gt;Alignment: $e_{i,j} = q_j\cdot k_i / \sqrt{D}$&lt;/li&gt;
  &lt;li&gt;Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$&lt;/li&gt;
  &lt;li&gt;Output: $y_j = \sum_i a_{i,j} v_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;context vectors: &lt;strong&gt;y&lt;/strong&gt;(shape: $D_v$)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;positional-encoding&quot;&gt;&lt;em&gt;Positional encoding&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_9.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;br /&gt;
Self attention attends over sets of inputs; is permutation invariant. To encode the ordered sequences(e.g. language, image), concatenate special positional encoding $p_j$ to each input vector $x_j$.&lt;/p&gt;

&lt;p&gt;$\mathit{pos}: N\rightarrow R^d$ to process the position &lt;em&gt;j&lt;/em&gt; of the vector into a &lt;em&gt;d&lt;/em&gt;-dimensional vector; $p_j = \mathit{pos}(j)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Desiderata&lt;/strong&gt; of $\mathit{pos}(\cdot)$:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Should output a &lt;strong&gt;unique&lt;/strong&gt; encoding for each time-step(word’s position in a sentence).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Distance&lt;/strong&gt; between any two time-steps should be consistent across sentences with different lengths(variable inputs).&lt;/li&gt;
  &lt;li&gt;Model should generalize to &lt;strong&gt;longer&lt;/strong&gt; sentences without any efforts. Its values should be bounded.&lt;/li&gt;
  &lt;li&gt;Must be &lt;strong&gt;deterministic&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Options&lt;/strong&gt; for $\mathit{pos}(\cdot)$:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Learn a lookup table:
    &lt;ul&gt;
      &lt;li&gt;Learn parameters to use for $\mathit{pos}(t)$ for $t \in [0,T)$&lt;/li&gt;
      &lt;li&gt;Lookup table contains $T\times d$ parameters&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Design a fixed function with the desiderata&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec11_10.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;masked-self-attention-layer&quot;&gt;Masked self-attention layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_11.png&quot; alt=&quot;png&quot; width=&quot;45%&amp;quot;, height=&amp;quot;45%&quot; /&gt;&lt;br /&gt;
Manually set alignment scores to $-\infty$, prevent vectors from looking at future vectors.&lt;/p&gt;

&lt;h3 id=&quot;multi-head-self-attention-layer&quot;&gt;Multi-head self attention layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_12.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
Multiple self-attention heads in parallel; similar to ensemble&lt;/p&gt;

&lt;h3 id=&quot;comparing-rnns-to-transformers&quot;&gt;Comparing RNNs to Transformers&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;RNNs&lt;/strong&gt;&lt;br /&gt;
&lt;span style=&quot;color:green&quot;&gt;(+) LSTMs work reasonably well for long sequences.&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:red&quot;&gt;(-) Expects an ordered sequences of inputs&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:red&quot;&gt;(-) Sequential computation: subsequent hidden states can only be computed after the previous ones are done.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Transformers&lt;/strong&gt;&lt;br /&gt;
&lt;span style=&quot;color:green&quot;&gt;(+) Good at long sequences. Each attention calculation looks at all inputs.&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:green&quot;&gt;(+) Can operate over unordered sets or ordered sequences with positional encodings.&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:green&quot;&gt;(+) Parallel computation: All alignment and attention scores for all inputs can be done in parallel.&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:red&quot;&gt;(-) Requires a lot of memory: N x M alignment and attention scalers need to be calculated and stored for a single self-attention head.&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;transformers&quot;&gt;Transformers&lt;/h2&gt;
&lt;h3 id=&quot;image-captioning-using-transformers&quot;&gt;Image Captioning using transformers&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_13.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;No recurrence at all&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Input: Image &lt;strong&gt;I&lt;/strong&gt;&lt;br /&gt;
Output: Sequence &lt;strong&gt;y&lt;/strong&gt; $= y_1, y_2, \ldots, y_T$&lt;br /&gt;
Encoder: $c = T_W(z)$, where &lt;em&gt;z&lt;/em&gt; is spatial CNN features, $T_W(\cdot)$ is the transformer encoder&lt;br /&gt;
Decoder: $y_t = T_D(y_{0:t-1}, c)$, where $T_D(\cdot)$ is the transformer decoder&lt;/p&gt;

&lt;h4 id=&quot;the-transformer-encoder-block&quot;&gt;The Transformer encoder block&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_14.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
Inputs: Set of vectors &lt;strong&gt;x&lt;/strong&gt;&lt;br /&gt;
Outputs: Set of vectors &lt;strong&gt;y&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Self-attention is the only interaction between vectors; Layer norm and MLP operate independently per vector. Highly scalable, highly parallelizable, but high memory usage.&lt;/p&gt;

&lt;h4 id=&quot;the-transformer-decoder-block&quot;&gt;The Transformer Decoder block&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_15.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
Inputs: Set of vectors &lt;strong&gt;x&lt;/strong&gt; and Set of context vector &lt;strong&gt;c&lt;/strong&gt;&lt;br /&gt;
Outputs: Set of vectors &lt;strong&gt;y&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Masked Self-attention only interacts with past inputs(&lt;em&gt;x&lt;/em&gt;, or previous output $y_{t-1}$). Multi-head attention block is NOT self-attention; it attends over the transformer encoder outputs. In this phase, we inject image features into the decoder. Highly scalable, highly parallelizable, but high memory usage.&lt;/p&gt;

&lt;h3 id=&quot;image-captioning-using-only-transformers&quot;&gt;Image Captioning using ONLY transformers&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Transformers from pixels to language&lt;br /&gt;
  &lt;em&gt;Dosovitskiy et al, “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”, ArXiv 2020&lt;/em&gt;  &lt;a href=&quot;https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax.ipynb&quot; target=&quot;_blank&quot;&gt;colab notebook link&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note: in Google Colab - TPU runtime setting&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# TPU initialization
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolver&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distribute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cluster_resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TPUClusterResolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'grpc://'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'COLAB_TPU_ADDR'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experimental_connect_to_cluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experimental&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initialize_tpu_system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;strategy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distribute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TPUStrategy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# compile in strategy.scope
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strategy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'adam'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SparseCategoricalCrossentropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_logits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sparse_categorical_accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Adding &lt;strong&gt;attention&lt;/strong&gt; to RNNs allows them to “attend” to different parts of the input at every time step&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;general attention layer&lt;/strong&gt; is a new type of layer that can be used to design new neural network architectures&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transformers&lt;/strong&gt; are a type of layer that uses self-attention and layer norm.
    &lt;ul&gt;
      &lt;li&gt;It is highly scalable and highly parallelizable&lt;/li&gt;
      &lt;li&gt;Faster training, larger models, better performance across vision and language tasks&lt;/li&gt;
      &lt;li&gt;They are quickly replacing RNNs, LSTMs, and may even replace convolutions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Attention with RNNs Image Captioning using spatial features Input: Image I Output: Sequence y $= y_1, y_2, \ldots, y_T$ Encoder: $h_0 = f_W(z)$, where z is spatial CNN features, $f_W(\cdot)$ is an MLP Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector c is often $c=h_0$ Problem: Input is “bottlenecked” through c; especially in a long descriptions. Model needs to encode everything it wants to say within c Attention idea: New context vector c_t at every time step Each context vector will attend to different image regions Alignment scores(scalars): $H \times W$ matrix e $e_{t,i,j} = f_{\mbox{att}}(h_{t-1}, z_{i,j})$ where $f_{\mbox{att}}(\cdot)$ is an MLP Normalize to get attention weights: $a_{t,:,:} = \mbox{softmax}(e_{t,:,:})$, $0&amp;lt;a_{t,i,j}&amp;lt;1$, attention values sum to 1 Compute context vector c: multiply CNN features and Attention weights $c_t = \sum_{i,j} a_{t,i,j} z_{t,i,j}$ Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, \color{red}{c_t})$ Each timestep of decode uses a different context vector that looks(attend) at different parts of the input image. This entire process is differentiable; model chooses its own attention weights. No attention supervision is required. Similar tasks in NLP - Language translation example Vanilla Encoder-Decoder setting: Input: sequence x $= x_1, x_2, \ldots, x_T$ Output: sequence y $= y_1, y_2, \ldots, y_T$ Encoder: $h_0 = f_W(z)$, where $z_t = \mbox{RNN}(x_t, u_{t-1})$, $f_W(\cdot)$ is MLP, u is the hidden RNN state Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector c is often $c=h_0$ Attention in NLP Alignment scores(scalars): $e_{t,i} = f_{\mbox{att}}(h_{t-1}, z_t)$, where $f_{\mbox{att}}(\cdot)$ is an MLP Normalize to get attention weights: $a_{t,:} = \mbox{softmax}(e_{t,:})$, $0&amp;lt;a_{t,i}&amp;lt;1$, attention values sum to 1 Compute context vector c: $c_t = \sum_i a_{t,i} z_{t,i}$ Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, \color{red}c_t)$ Heatmap: visualization of attention weights; without any attention supervision, model learns different word orderings for different languages General Attention Layer Attention in image captioning before Single query setting Inputs input vectors: x(shape: $N\times D$) Attention operation is permutation invariant; produces the same output regardless of the order of elements(features) in the input vector. Stretch $H\times W = N$ into N vectors, transform $H\times W\times D$ features into $N\times D$ input vectors x(similar to attention in NLP). Query: h(shape: D) Operations Alignment Change $f_{\mbox{att}}(\cdot)$ to a simple dot product: $e_i = h\cdot x_i$; only works well with key &amp;amp; value transformation trick Change $f_{\mbox{att}}(\cdot)$ to a scaled dot product: $e_i = h\cdot x_i / \sqrt{D}$; Larger dimensions means more terms in the dot product sum. So, the variance of the logits is higher. Large magnitude(length) vectors will produce much higher logits. Then, the post-softmax distribution(e) has lower-entropy(high uncertainty) assuming logits are I.I.D. Ultimately, these large magnitude vectors will cause softmax to peak and assign very little weight to all others. To reduce this effect, divide by $sqrt{D}$. Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$ Output: $\mathbf{c} = \sum_i a_i x_i$ Outputs - context vector: c(shape: D) Multiple query setting Inputs input vectors: x(shape: $N\times D$) Queries: q(shape: $M\times D$); multiple query vectors Operations Alignment: $e_{i,j} = q_j\cdot x_i / \sqrt{D}$ Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$ Output: $y_j = \sum_i a_{i,j} x_i$ Outputs context vectors: y(shape: D); each query creates a new output context vector Weight layers added Notice that the input vectors x are used for both the alignment(e) and attention calculations(y); We can add more expressivity to the layer by adding a different FC layer before each of the two steps. The input and output dimensions can now change depending on the key and value FC layers. Inputs input vectors: x(shape: $N\times D$) Queries: q(shape: $M\times D_k$) Operations Key vectors: $\mathbf{k} = \mathbf{x}W_k$ Value vectors: $\mathbf{v} = \mathbf{x}W_v$ Alignment: $e_{i,j} = q_j\cdot k_i / \sqrt{D}$ Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$ Output: $y_j = \sum_i a_{i,j} v_i$ Outputs context vectors: y(shape: $D_v$); Self attention layer Recall that the query vector was a function of the input vectors; Encoder $h_0=f_W(z)$, where z is spatial CNN features, $f_W(\cdot)$ is an MLP. We can calculate the query vectors from the input vectors, defining a “self-attention” layer. No input query vectors anymore, instead query vectors are calculated using a FC layer. Inputs input vectors: x(shape: $N\times D$) Operations Key vectors: $\mathbf{k} = \mathbf{x}W_k$ Value vectors: $\mathbf{v} = \mathbf{x}W_v$ Query vectors: $\mathbf{q} = \mathbf{x}W_q$ Alignment: $e_{i,j} = q_j\cdot k_i / \sqrt{D}$ Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$ Output: $y_j = \sum_i a_{i,j} v_i$ Outputs context vectors: y(shape: $D_v$) Positional encoding Self attention attends over sets of inputs; is permutation invariant. To encode the ordered sequences(e.g. language, image), concatenate special positional encoding $p_j$ to each input vector $x_j$. $\mathit{pos}: N\rightarrow R^d$ to process the position j of the vector into a d-dimensional vector; $p_j = \mathit{pos}(j)$ Desiderata of $\mathit{pos}(\cdot)$: Should output a unique encoding for each time-step(word’s position in a sentence). Distance between any two time-steps should be consistent across sentences with different lengths(variable inputs). Model should generalize to longer sentences without any efforts. Its values should be bounded. Must be deterministic. Options for $\mathit{pos}(\cdot)$: Learn a lookup table: Learn parameters to use for $\mathit{pos}(t)$ for $t \in [0,T)$ Lookup table contains $T\times d$ parameters Design a fixed function with the desiderata Masked self-attention layer Manually set alignment scores to $-\infty$, prevent vectors from looking at future vectors. Multi-head self attention layer Multiple self-attention heads in parallel; similar to ensemble Comparing RNNs to Transformers RNNs (+) LSTMs work reasonably well for long sequences. (-) Expects an ordered sequences of inputs (-) Sequential computation: subsequent hidden states can only be computed after the previous ones are done. Transformers (+) Good at long sequences. Each attention calculation looks at all inputs. (+) Can operate over unordered sets or ordered sequences with positional encodings. (+) Parallel computation: All alignment and attention scores for all inputs can be done in parallel. (-) Requires a lot of memory: N x M alignment and attention scalers need to be calculated and stored for a single self-attention head. Transformers Image Captioning using transformers No recurrence at all Input: Image I Output: Sequence y $= y_1, y_2, \ldots, y_T$ Encoder: $c = T_W(z)$, where z is spatial CNN features, $T_W(\cdot)$ is the transformer encoder Decoder: $y_t = T_D(y_{0:t-1}, c)$, where $T_D(\cdot)$ is the transformer decoder The Transformer encoder block Inputs: Set of vectors x Outputs: Set of vectors y Self-attention is the only interaction between vectors; Layer norm and MLP operate independently per vector. Highly scalable, highly parallelizable, but high memory usage. The Transformer Decoder block Inputs: Set of vectors x and Set of context vector c Outputs: Set of vectors y Masked Self-attention only interacts with past inputs(x, or previous output $y_{t-1}$). Multi-head attention block is NOT self-attention; it attends over the transformer encoder outputs. In this phase, we inject image features into the decoder. Highly scalable, highly parallelizable, but high memory usage. Image Captioning using ONLY transformers Transformers from pixels to language Dosovitskiy et al, “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”, ArXiv 2020 colab notebook link Note: in Google Colab - TPU runtime setting import tensorflow as tf import os # TPU initialization resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR']) tf.config.experimental_connect_to_cluster(resolver) tf.tpu.experimental.initialize_tpu_system(resolver) strategy = tf.distribute.TPUStrategy(resolver) # compile in strategy.scope def create_model(): return tf.keras.Sequential( [tf.keras.layers.Conv2D(256, 3, activation='relu', input_shape=(28, 28, 1)), tf.keras.layers.Conv2D(256, 3, activation='relu'), tf.keras.layers.Flatten(), tf.keras.layers.Dense(256, activation='relu'), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dense(10)]) with strategy.scope(): model = create_model() model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['sparse_categorical_accuracy']) Summary Adding attention to RNNs allows them to “attend” to different parts of the input at every time step The general attention layer is a new type of layer that can be used to design new neural network architectures Transformers are a type of layer that uses self-attention and layer norm. It is highly scalable and highly parallelizable Faster training, larger models, better performance across vision and language tasks They are quickly replacing RNNs, LSTMs, and may even replace convolutions.</summary></entry><entry><title type="html">cs231n - Lecture 10. Recurrent Neural Networks</title><link href="http://0.0.0.0:4000/cs231n_lec10" rel="alternate" type="text/html" title="cs231n - Lecture 10. Recurrent Neural Networks" /><published>2022-01-04T00:00:00+09:00</published><updated>2022-01-04T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec10</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec10">&lt;h2 id=&quot;rnn-process-sequences&quot;&gt;RNN: Process Sequences&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;one to one; vanilla neural networks&lt;/li&gt;
  &lt;li&gt;one to many; e.g. Image Captioning(image to sequence of words)&lt;/li&gt;
  &lt;li&gt;many to one; e.g. Action Prediction(video sequence to action class)&lt;/li&gt;
  &lt;li&gt;many to many(1); e.g. Video Captioning(video sequence to caption)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;many to many(2); e.g. Video Classification on frame level&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Why existing convnets are insufficient?:&lt;br /&gt;
  Variable sequence length inputs and outputs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_1.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;70%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Key idea: RNNs have an “internal state” that is updated as a sequence is processed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN hidden state update:&lt;br /&gt;
  \(h_t = f_W(h_{t-1}, x_t)\)&lt;br /&gt;
  The same function and the same set of parameters are used at every time step.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RNN output generation: \(y_t = f_{W_hy}(h_t)\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Simple(Vanilla) RNN: The state consists of a single hidden vector &lt;em&gt;h&lt;/em&gt;&lt;br /&gt;
  $h_t = \mbox{tanh}(W_hh h_{t-1} + W_{xh}x_t)$&lt;br /&gt;
  $y_t = W_{hy}h_t$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sequence-to-sequenceseq2seq-many-to-one--one-to-many&quot;&gt;Sequence to Sequence(Seq2Seq): Many-to-One + One-to-Many&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Many-to-One: Encode input sequence in a single vector&lt;br /&gt;
  One-to-Many: Produce output sequence from single input vector&lt;br /&gt;
  Encoder produces the last hidden state $h_T$ and decoder uses it as a default $h_0$. Weights($W_1, W_2$) are re-used for each procedure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Example: Character-level Language Model Sampling&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_2.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;backpropagation&quot;&gt;Backpropagation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Backpropagation through time: Computationally Expensive&lt;br /&gt;
  Forward through entire sequence to compute loss, then backward through entire sequence to compute gradient.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Truncated&lt;/strong&gt; Backpropagation through time:&lt;br /&gt;
  Run forward and backward through &lt;strong&gt;chunks of the sequence&lt;/strong&gt; instead of whole sequence. Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rnn-tradeoffs&quot;&gt;RNN tradeoffs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RNN Advantages:
    &lt;ul&gt;
      &lt;li&gt;Can process any length input&lt;/li&gt;
      &lt;li&gt;Computation for step t can (in theory) use information from many steps back&lt;/li&gt;
      &lt;li&gt;Model size doesn’t increase for longer input&lt;/li&gt;
      &lt;li&gt;Same weights applied on every timestep, so there is symmetry in how inputs are processed.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN Disadvantages:
    &lt;ul&gt;
      &lt;li&gt;Recurrent computation is slow&lt;/li&gt;
      &lt;li&gt;In practice, difficult to access information from many steps back&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;image-captioning-cnn--rnn&quot;&gt;Image Captioning: CNN + RNN&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Instead of the final FC layer and the classifier in CNN, use FC output &lt;em&gt;v&lt;/em&gt;(say 4096 length vector) to formulate the default hidden state $h_0$ in RNN.
    &lt;ul&gt;
      &lt;li&gt;before: $h = \mbox{tanh}(W_{xh}\ast x+W_{hh}\ast h)$&lt;/li&gt;
      &lt;li&gt;now: $h=\mbox{tanh}(W_{xh}\ast x + W_{hh}\ast h + W_{ih}\ast v)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN for Image Captioning&lt;br /&gt;
  Re-sample the previous output $y_{t-1}$ as the next input $x_t$, iterate untill $y_t$ sample takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;END&amp;gt;&lt;/code&gt; token.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;visual-question-answering-rnns-with-attention&quot;&gt;Visual Question Answering: RNNs with Attention&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;other-tasks&quot;&gt;Other tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Visual Dialog: Conversations about images&lt;/li&gt;
  &lt;li&gt;Visual Language Navigation: Go to the living room&lt;br /&gt;
  Agent encodes instructions in language and uses an RNN to generate a series of movements as the visual input changes after each move.&lt;/li&gt;
  &lt;li&gt;Visual Question Answering: Dataset Bias&lt;br /&gt;
  With different types(Image + Question + Answer) of data used, model performances are better.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;long-short-term-memory-lstm&quot;&gt;Long Short Term Memory (LSTM)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Vanilla RNN&lt;br /&gt;
  \(h_t = \mbox{tanh}(W_{hh}h_{t-1} + W_{xh}x_t) \\
      = \mbox{tanh}\left(
          (W_{hh} \ W_{hx}) {\begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}}
                  \right) \\
      = \mbox{tanh}\left(
          W {\begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}}
                  \right)\)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(\frac{\partial h_t}{\partial h_{t-1}} = \mbox{tanh}' (W_{hh}h_{t-1} + W_{xh}x_t)W_{hh}\)&lt;br /&gt;
  $\frac{\partial L}{\partial W} = \sum_{t=1}^T \frac{\partial L_t}{\partial W}$&lt;/p&gt;

\[\begin{align*}
  \frac{\partial L_T}{\partial W} &amp;amp;= \frac{\partial L_T}{\partial h_T}
                                      \frac{\partial h_t}{\partial h_{t-1}}\cdots
                                      \frac{\partial h_1}{\partial W} \\
                                   &amp;amp;= \frac{\partial L_T}{\partial h_T}(\prod_{t=2}^T \frac{\partial h_t}{\partial h_{t-1}})\frac{\partial h_1}{\partial W} \\
                                  &amp;amp;= \frac{\partial L_T}{\partial h_T}(\prod_{t=2}^T \mbox{tanh}'(W_{hh}h_{t-1} + W_{xh}x_t))W_{hh}^{T-1} \frac{\partial h_1}{\partial W}
  \end{align*}\]
  &lt;/li&gt;
  &lt;li&gt;Problem&lt;br /&gt;
  As the output of &lt;em&gt;tanh&lt;/em&gt; function are in range of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[-1,1]&lt;/code&gt; and almost smaller than 1, vanilla RNN has &lt;strong&gt;&lt;em&gt;vanishing gradients&lt;/em&gt;&lt;/strong&gt;. If we assume no non-linearity, the gradient will be \(\frac{\partial L_T}{\partial W} = \frac{\partial L_T}{\partial h_T}W_{hh}^{T-1}\frac{\partial h_1}{\partial W}\). In this case, when the largest singular value is greater than 1, we have exploding gradients, while the value is smaller than 1, we have vanishing gradients.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# dimensionality of hidden state
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# number of time steps
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Whh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# forward pass of an RNN (ignoring inputs x)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Whh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	
&lt;span class=&quot;c1&quot;&gt;# backward pass of the RNN
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dhs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dhs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#start off the chain with random gradient
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reversed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dhs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# backprop through the nonlinearity
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;dhs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Whh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# backprop into previous hidden state
&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# &quot;Whh.T&quot; multiplied by &quot;T&quot; times!
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For exploding gradients: control with gradient clipping.&lt;br /&gt;
  For vanishing gradients: change the architecture, LSTM introduced.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LSTM:&lt;br /&gt;
  \(\begin{pmatrix} i \\ f \\ o \\ g \end{pmatrix} =
  \begin{pmatrix} \sigma \\ \sigma \\ \sigma \\ \mbox{tanh}\end{pmatrix} W \begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}\)&lt;br /&gt;
  \(c_t = f \odot c_{t-1} + i \odot g\), &lt;em&gt;memory cell update&lt;/em&gt;&lt;br /&gt;
  \(h_t = o \odot \mbox{tanh}(c_t)\), &lt;em&gt;hidden state update&lt;/em&gt;&lt;br /&gt;
  where &lt;em&gt;W&lt;/em&gt; is a stack of $W_h$ and $W_x$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_4.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
i: Input gate, whether to write to cell&lt;br /&gt;
f: Forget gate, Whether to erase cell&lt;br /&gt;
o: Output gate, How much to reveal cell&lt;br /&gt;
g: Gate gate, How much to write to cell&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_5.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Backpropagation from $c_t$ to $c_{t-1}$ only elementwise multiplication by &lt;em&gt;f&lt;/em&gt;, no matrix multiply by &lt;em&gt;W&lt;/em&gt;. Notice that the gradient contains the &lt;em&gt;f&lt;/em&gt; gate’s vector of activations; it allows better control of gradients values, using suitable parameter updates of the forget gate. Also notice that are added through the &lt;em&gt;f, i, g,&lt;/em&gt; and &lt;em&gt;o&lt;/em&gt; gates, we can have better balancing of gradient values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Recall: “PlainNets” vs. ResNets&lt;br /&gt;
  ResNet is to PlainNet what LSTM is to RNN, kind of.&lt;br /&gt;
  &lt;em&gt;Additive skip connections&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Do LSTMs solve the vanishing gradient problem?:&lt;br /&gt;
  The LSTM architecture makes it easier for the RNN to preserve information over many timesteps. e.g. If $f=1$ and $i=0$, then the information of that cell is preserved indefinitely. By contrast, it’s harder for vanilla RNN to learn a recurrent weight matrix $W_h$ that preserves information in hidden state.&lt;br /&gt;
  LSTM doesn’t guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;in between: Highway Networks, &lt;em&gt;Srivastava et al, 2015, [arXiv:1505.00387v2]&lt;/em&gt;&lt;br /&gt;
  A new architecture designed to ease gradient-based training of very deep networks. To regulate the flow of information and enlarge the possibility of studying extremely deep and efficient architectures.&lt;br /&gt;
  $g = T(x, W_T)$, $y = g \odot H(x, W_H) + (1-g)\odot x$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-rnn-variants&quot;&gt;Other RNN Variants&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Neural Architecture Search(NAS) with Reinforcement Learning, &lt;em&gt;Zoph et Le, 2017&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;RNN to design model; idea that we can represent the model architecture with a variable-length string.&lt;/li&gt;
      &lt;li&gt;Apply reinforcement learning on a neural network to maximize the accuracy(as a reward) on validation set, find a good architecture.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GRU; smaller LSTM, &lt;em&gt;“Learning phrase representations using rnn encoder-decoder for statistical machine translation”, Cho et al., 2014&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“An Empirical Exploration of Recurrent Network Architectures”, Jozefowicz et al., 2015&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;LSTM: A Search Space Odyssey, Greff et al., 2015&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;recurrence-for-vision&quot;&gt;Recurrence for Vision&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;LSTM wer a good default choice until this year&lt;/li&gt;
  &lt;li&gt;Use variants like GRU if you want faster compute and less parameters&lt;/li&gt;
  &lt;li&gt;Use transformers (next lecture) as they are dominating NLP models&lt;/li&gt;
  &lt;li&gt;almost everyday there is a new vision transformer model&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RNNs allow a lot of flexibility in architecture design&lt;/li&gt;
  &lt;li&gt;Vanilla RNNs are simple but don’t work very well&lt;/li&gt;
  &lt;li&gt;Common to use LSTM or GRU: their additive interactions improve gradient flow&lt;/li&gt;
  &lt;li&gt;Backward flow of gradients in RNN can explode or vanish. Exploding is controlled with gradient clipping. Vanishing is controlled with additive interactions (LSTM)&lt;/li&gt;
  &lt;li&gt;Better/simpler architectures are a hot topic of current research, as well as new paradigms for reasoning over sequences&lt;/li&gt;
  &lt;li&gt;Better understanding (both theoretical and empirical) is needed.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">RNN: Process Sequences one to one; vanilla neural networks one to many; e.g. Image Captioning(image to sequence of words) many to one; e.g. Action Prediction(video sequence to action class) many to many(1); e.g. Video Captioning(video sequence to caption) many to many(2); e.g. Video Classification on frame level Why existing convnets are insufficient?: Variable sequence length inputs and outputs Key idea: RNNs have an “internal state” that is updated as a sequence is processed. RNN hidden state update: \(h_t = f_W(h_{t-1}, x_t)\) The same function and the same set of parameters are used at every time step. RNN output generation: \(y_t = f_{W_hy}(h_t)\) Simple(Vanilla) RNN: The state consists of a single hidden vector h $h_t = \mbox{tanh}(W_hh h_{t-1} + W_{xh}x_t)$ $y_t = W_{hy}h_t$ Sequence to Sequence(Seq2Seq): Many-to-One + One-to-Many Many-to-One: Encode input sequence in a single vector One-to-Many: Produce output sequence from single input vector Encoder produces the last hidden state $h_T$ and decoder uses it as a default $h_0$. Weights($W_1, W_2$) are re-used for each procedure. Example: Character-level Language Model Sampling Backpropagation Backpropagation through time: Computationally Expensive Forward through entire sequence to compute loss, then backward through entire sequence to compute gradient. Truncated Backpropagation through time: Run forward and backward through chunks of the sequence instead of whole sequence. Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps. RNN tradeoffs RNN Advantages: Can process any length input Computation for step t can (in theory) use information from many steps back Model size doesn’t increase for longer input Same weights applied on every timestep, so there is symmetry in how inputs are processed. RNN Disadvantages: Recurrent computation is slow In practice, difficult to access information from many steps back Image Captioning: CNN + RNN Instead of the final FC layer and the classifier in CNN, use FC output v(say 4096 length vector) to formulate the default hidden state $h_0$ in RNN. before: $h = \mbox{tanh}(W_{xh}\ast x+W_{hh}\ast h)$ now: $h=\mbox{tanh}(W_{xh}\ast x + W_{hh}\ast h + W_{ih}\ast v)$ RNN for Image Captioning Re-sample the previous output $y_{t-1}$ as the next input $x_t$, iterate untill $y_t$ sample takes &amp;lt;END&amp;gt; token. Visual Question Answering: RNNs with Attention Other tasks Visual Dialog: Conversations about images Visual Language Navigation: Go to the living room Agent encodes instructions in language and uses an RNN to generate a series of movements as the visual input changes after each move. Visual Question Answering: Dataset Bias With different types(Image + Question + Answer) of data used, model performances are better. Long Short Term Memory (LSTM) Vanilla RNN \(h_t = \mbox{tanh}(W_{hh}h_{t-1} + W_{xh}x_t) \\ = \mbox{tanh}\left( (W_{hh} \ W_{hx}) {\begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}} \right) \\ = \mbox{tanh}\left( W {\begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}} \right)\) \(\frac{\partial h_t}{\partial h_{t-1}} = \mbox{tanh}' (W_{hh}h_{t-1} + W_{xh}x_t)W_{hh}\) $\frac{\partial L}{\partial W} = \sum_{t=1}^T \frac{\partial L_t}{\partial W}$ \[\begin{align*} \frac{\partial L_T}{\partial W} &amp;amp;= \frac{\partial L_T}{\partial h_T} \frac{\partial h_t}{\partial h_{t-1}}\cdots \frac{\partial h_1}{\partial W} \\ &amp;amp;= \frac{\partial L_T}{\partial h_T}(\prod_{t=2}^T \frac{\partial h_t}{\partial h_{t-1}})\frac{\partial h_1}{\partial W} \\ &amp;amp;= \frac{\partial L_T}{\partial h_T}(\prod_{t=2}^T \mbox{tanh}'(W_{hh}h_{t-1} + W_{xh}x_t))W_{hh}^{T-1} \frac{\partial h_1}{\partial W} \end{align*}\] Problem As the output of tanh function are in range of [-1,1] and almost smaller than 1, vanilla RNN has vanishing gradients. If we assume no non-linearity, the gradient will be \(\frac{\partial L_T}{\partial W} = \frac{\partial L_T}{\partial h_T}W_{hh}^{T-1}\frac{\partial h_1}{\partial W}\). In this case, when the largest singular value is greater than 1, we have exploding gradients, while the value is smaller than 1, we have vanishing gradients. H = 5 # dimensionality of hidden state T = 50 # number of time steps Whh = np.random.randn(H, H) # forward pass of an RNN (ignoring inputs x) hs = {} ss = {} hs[-1] = np.random.randn(H) for t in xrange(T): ss[t] = np.dot(Whh, hs[t-1]) hs[t] = np.maximum(0, ss[t]) # backward pass of the RNN dhs = {} dss = {} dhs[T-1] = np.random.randn(H) #start off the chain with random gradient for t in reversed(xrange(T)): dss[t] = (hs[t] &amp;gt; 0) * dhs[t] # backprop through the nonlinearity dhs[t-1] = np.dot(Whh.T, dss[t]) # backprop into previous hidden state # &quot;Whh.T&quot; multiplied by &quot;T&quot; times! For exploding gradients: control with gradient clipping. For vanishing gradients: change the architecture, LSTM introduced. LSTM: \(\begin{pmatrix} i \\ f \\ o \\ g \end{pmatrix} = \begin{pmatrix} \sigma \\ \sigma \\ \sigma \\ \mbox{tanh}\end{pmatrix} W \begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}\) \(c_t = f \odot c_{t-1} + i \odot g\), memory cell update \(h_t = o \odot \mbox{tanh}(c_t)\), hidden state update where W is a stack of $W_h$ and $W_x$ i: Input gate, whether to write to cell f: Forget gate, Whether to erase cell o: Output gate, How much to reveal cell g: Gate gate, How much to write to cell Backpropagation from $c_t$ to $c_{t-1}$ only elementwise multiplication by f, no matrix multiply by W. Notice that the gradient contains the f gate’s vector of activations; it allows better control of gradients values, using suitable parameter updates of the forget gate. Also notice that are added through the f, i, g, and o gates, we can have better balancing of gradient values. Recall: “PlainNets” vs. ResNets ResNet is to PlainNet what LSTM is to RNN, kind of. Additive skip connections Do LSTMs solve the vanishing gradient problem?: The LSTM architecture makes it easier for the RNN to preserve information over many timesteps. e.g. If $f=1$ and $i=0$, then the information of that cell is preserved indefinitely. By contrast, it’s harder for vanilla RNN to learn a recurrent weight matrix $W_h$ that preserves information in hidden state. LSTM doesn’t guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies. in between: Highway Networks, Srivastava et al, 2015, [arXiv:1505.00387v2] A new architecture designed to ease gradient-based training of very deep networks. To regulate the flow of information and enlarge the possibility of studying extremely deep and efficient architectures. $g = T(x, W_T)$, $y = g \odot H(x, W_H) + (1-g)\odot x$ Other RNN Variants Neural Architecture Search(NAS) with Reinforcement Learning, Zoph et Le, 2017 RNN to design model; idea that we can represent the model architecture with a variable-length string. Apply reinforcement learning on a neural network to maximize the accuracy(as a reward) on validation set, find a good architecture. GRU; smaller LSTM, “Learning phrase representations using rnn encoder-decoder for statistical machine translation”, Cho et al., 2014 “An Empirical Exploration of Recurrent Network Architectures”, Jozefowicz et al., 2015 LSTM: A Search Space Odyssey, Greff et al., 2015 Recurrence for Vision LSTM wer a good default choice until this year Use variants like GRU if you want faster compute and less parameters Use transformers (next lecture) as they are dominating NLP models almost everyday there is a new vision transformer model Summary RNNs allow a lot of flexibility in architecture design Vanilla RNNs are simple but don’t work very well Common to use LSTM or GRU: their additive interactions improve gradient flow Backward flow of gradients in RNN can explode or vanish. Exploding is controlled with gradient clipping. Vanishing is controlled with additive interactions (LSTM) Better/simpler architectures are a hot topic of current research, as well as new paradigms for reasoning over sequences Better understanding (both theoretical and empirical) is needed.</summary></entry><entry><title type="html">cs231n - Lecture 9. CNN Architectures</title><link href="http://0.0.0.0:4000/cs231n_lec9" rel="alternate" type="text/html" title="cs231n - Lecture 9. CNN Architectures" /><published>2022-01-03T00:00:00+09:00</published><updated>2022-01-03T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec9</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec9">&lt;h2 id=&quot;review&quot;&gt;Review&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;LeCun et al., 1998&lt;/em&gt;&lt;br /&gt;
  $5\times 5$ Conv filters applied at stride &lt;em&gt;1&lt;/em&gt;&lt;br /&gt;
  $2\times 2$ Subsampling (Pooling) layers applied at stride &lt;em&gt;2&lt;/em&gt;&lt;br /&gt;
  i.e. architecture is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[CONV-POOL-CONV-POOL-FC-FC]&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stride: Downsample output activations&lt;br /&gt;
  Padding: Preserve input spatial dimensions in output activations&lt;br /&gt;
  Filter: Each conv filter outputs a “slice” in the activation&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;case-studies&quot;&gt;Case Studies&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec9_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;alexnet-first-cnn-based-winner&quot;&gt;AlexNet: First CNN-based winner&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Architecture: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MaxPOOL3-FC6-FC7-FC8]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;Input: $227\times 227\times 3$ images&lt;/li&gt;
      &lt;li&gt;First layer(CONV1):&lt;br /&gt;
  &lt;em&gt;96&lt;/em&gt; $11\times 11$ filters applied at stride &lt;em&gt;4&lt;/em&gt;, pad &lt;em&gt;0&lt;/em&gt;&lt;br /&gt;
  Output volume: $W’ = (W-F+2P)/S + 1 \rightarrow$ $55\times 55\times 96$&lt;br /&gt;
  Parameters: $(11* 11* 3 +1)* 96 =$ &lt;strong&gt;36K&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Second layer(POOL1):&lt;br /&gt;
  $3\times 3\times$ filters applied at stride &lt;em&gt;2&lt;/em&gt;&lt;br /&gt;
  Output volume: $27\times 27\times 96$&lt;br /&gt;
  Parameters: 0&lt;br /&gt;
  $\vdots$&lt;/li&gt;
      &lt;li&gt;CONV2($27\times 27\times 256$):&lt;br /&gt;
  &lt;em&gt;256&lt;/em&gt; $5\times 5$ filters applied at stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;2&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;MAX POOL2($13\times 13\times 256):&lt;br /&gt;
  $3\times 3\times$ filters applied at stride &lt;em&gt;2&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;CONV3($13\times 13\times 384$):&lt;br /&gt;
  &lt;em&gt;384&lt;/em&gt; $3\times 3$ filters applied at stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;CONV4($13\times 13\times 384$):&lt;br /&gt;
  &lt;em&gt;384&lt;/em&gt; $3\times 3$ filters applied at stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;CONV5($13\times 13\times 256$):&lt;br /&gt;
  &lt;em&gt;256&lt;/em&gt; $3\times 3$ filters applied at stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;MAX POOL3($6\times 6\times 256$):&lt;br /&gt;
  $3\times 3\times$ filters applied at stride &lt;em&gt;2&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;FC6(4096): &lt;em&gt;4096&lt;/em&gt; neurons&lt;/li&gt;
      &lt;li&gt;FC7(4096): &lt;em&gt;4096&lt;/em&gt; neurons&lt;/li&gt;
      &lt;li&gt;FC8(1000): &lt;em&gt;1000&lt;/em&gt; neurons (class scores)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Historical note:
    &lt;ul&gt;
      &lt;li&gt;Network spread across &lt;em&gt;2&lt;/em&gt; GPUs, half the neurons (feature maps) on each GPU.&lt;/li&gt;
      &lt;li&gt;CONV1, CONV2, CONV4, CONV5: Connections only with feature maps on same GPU&lt;/li&gt;
      &lt;li&gt;CONV3, FC6, FC7, FC8: Connections with all feature maps in preceding layer, communication across GPUs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Details/Retrospectives:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Krizhevsky et al. 2012&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;first use of ReLU&lt;/li&gt;
      &lt;li&gt;used Norm layers (not common anymore)&lt;/li&gt;
      &lt;li&gt;heavy data augmentation&lt;/li&gt;
      &lt;li&gt;dropout &lt;em&gt;0.5&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;batch size &lt;em&gt;128&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;SGD Momentum &lt;em&gt;0.9&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Learning rate &lt;em&gt;1e-2&lt;/em&gt;, reduced by &lt;em&gt;10&lt;/em&gt; manually when val accuracy plateaus&lt;/li&gt;
      &lt;li&gt;L2 weight decay &lt;em&gt;5e-4&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;7 CNN ensemble: &lt;em&gt;18.2%&lt;/em&gt; $\rightarrow$ &lt;em&gt;15.4%&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;zfnet-improved-hyperparameters-over-alexnet&quot;&gt;ZFNet: Improved hyperparameters over AlexNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;AlexNet but:
    &lt;ul&gt;
      &lt;li&gt;CONV1: change from ($11\times 11$ stride &lt;em&gt;4&lt;/em&gt;) to ($7\times 7$ stride &lt;em&gt;2&lt;/em&gt;)&lt;/li&gt;
      &lt;li&gt;CONV3,4,5: instead of &lt;em&gt;384&lt;/em&gt;, &lt;em&gt;384&lt;/em&gt;, &lt;em&gt;256&lt;/em&gt; filters use &lt;em&gt;512&lt;/em&gt;, &lt;em&gt;1024&lt;/em&gt;, &lt;em&gt;512&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;ImageNet top 5 error: &lt;em&gt;16.4%&lt;/em&gt; -&amp;gt; &lt;em&gt;11.7%&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Zeiler and Fergus, 2013&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vggnet-deeper-networks&quot;&gt;VGGNet: Deeper Networks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Small filters, Deeper networks
    &lt;ul&gt;
      &lt;li&gt;8 layers (AlexNet) $\rightarrow$ 16 - 19 layers (VGG16Net)&lt;/li&gt;
      &lt;li&gt;Only $3\times 3$ CONV stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;1&lt;/em&gt; and $2\times 2$ MAX POOL with stride &lt;em&gt;2&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;11.7%&lt;/em&gt; top 5 error(ZFNet) $\rightarrow$ &lt;em&gt;7.3%&lt;/em&gt; top 5 error in ILSVRC’14&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Why use smaller filters?&lt;br /&gt;
  :Stack of three $3\times 3$ conv (stride &lt;em&gt;1&lt;/em&gt;) layers has same effective receptive field as one $7\times 7$ conv layer, but with deeper, more non-linearities and fewer parameters&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec9_1.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TOTAL memory: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;24M * 4 bytes ~= 96MB&lt;/code&gt; / image (for a forward pass)&lt;br /&gt;
  TOTAL params: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;138M&lt;/code&gt; parameters&lt;br /&gt;
  Most memory is in early CONV, Most params are in late FC&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Details:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Simonyan and Zisserman, 2014&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;ILSVRC’14 2nd in classification, 1st in localization&lt;/li&gt;
      &lt;li&gt;Similar training procedure as &lt;em&gt;Krizhevsky 2012&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;No Local Response Normalisation (LRN)&lt;/li&gt;
      &lt;li&gt;Use VGG16 or VGG19 (VGG19 only slightly better, more memory)&lt;/li&gt;
      &lt;li&gt;Use ensembles for best results&lt;/li&gt;
      &lt;li&gt;FC7 features generalize well to other	tasks&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;googlenet&quot;&gt;GoogLeNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Inception module&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;design a good local network topology(network within a network) and then stack these modules on top of each other&lt;/li&gt;
      &lt;li&gt;Apply parallel filter operations on the input from previous layer: Multiple receptive field sizes for convolution(1x1, 3x3, 5x5), Pooling(3x3)&lt;/li&gt;
      &lt;li&gt;Concatenate all filter outputs together channel-wise&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“Bottlenect”&lt;/em&gt; layers to reduce computational complexity of inception:
    &lt;ul&gt;
      &lt;li&gt;use 1x1 conv to reduce feature channel size; alternatively, interpret it as applying the same FC layer on each input pixel&lt;/li&gt;
      &lt;li&gt;preserves spatial dimensions, reduces depth&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec9_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Full GoogLeNet Architecture:
    &lt;ul&gt;
      &lt;li&gt;Stem Network: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[Conv-POOL-2x CONV-POOL]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Stack Inception modules: with dimension reduction on top of each other&lt;/li&gt;
      &lt;li&gt;Classifier output: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[(H*W*c)-Avg POOL-(1*1*c)-FC-Softmax]&lt;/code&gt;&lt;br /&gt;
  Global average pooling layer before final FC layer, avoids expensive FC layers&lt;/li&gt;
      &lt;li&gt;Auxiliary classification layers: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[AvgPool-1x1 Conv-FC-FC-Softmax]&lt;/code&gt;&lt;br /&gt;
  to inject additional gradient at lower layers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Details:
    &lt;ul&gt;
      &lt;li&gt;Deeper networks, with computational efficiency&lt;/li&gt;
      &lt;li&gt;ILSVRC’14 classification winner (&lt;em&gt;6.7%&lt;/em&gt; top 5 error)&lt;/li&gt;
      &lt;li&gt;22 layers&lt;/li&gt;
      &lt;li&gt;Only 5 million parameters(12x less than AlexNet, 27x less than VGG-16)&lt;/li&gt;
      &lt;li&gt;Efficient “Inception” module&lt;/li&gt;
      &lt;li&gt;No FC layers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;resnet&quot;&gt;ResNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;From 2015, “Revolution of Depth”; more than 100 layers&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Stacking deeper layers on a “plain” convolutional neural network results in lower both test and training error. The deeper model performs worse, but it’s &lt;strong&gt;not caused by overfitting&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;Fact: Deep models have more representation power (more parameters) than shallower models.&lt;/li&gt;
      &lt;li&gt;Hypothesis: the problem is an optimization problem, &lt;strong&gt;deeper models are harder to optimize&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Solution: copying the learned layers from the shallower model and setting additional layers to identity mapping.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“Residual block”&lt;/em&gt;:
    &lt;ul&gt;
      &lt;li&gt;Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec9_3.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Full ResNet Architecture:
    &lt;ul&gt;
      &lt;li&gt;Stack residual blocks&lt;/li&gt;
      &lt;li&gt;Every residual block has two $3\times 3$ conv layers&lt;/li&gt;
      &lt;li&gt;Periodically, double number of filters and downsample spatially using stride &lt;em&gt;2&lt;/em&gt; (/2 in each dimension). Reduce the activation volume by half.&lt;/li&gt;
      &lt;li&gt;Additional conv layer at the beginning (7x7 conv in stem)&lt;/li&gt;
      &lt;li&gt;No FC layers at the end (only FC 1000 to output classes)&lt;/li&gt;
      &lt;li&gt;(In theory, you can train a ResNet with input image of variable sizes)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For deeper networks(ResNet-50+):
  use bottleneck layer to improve efficiency (similar to GoogLeNet)&lt;br /&gt;
  e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[(28x28x256 INPUT)-(1x1 CONV, 64)-(3x3 CONV, 64)-(1x1 CONV, 256)-(28x28x256 OUTPUT)]&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Training ResNet in practice:
    &lt;ul&gt;
      &lt;li&gt;Batch Normalization after every CONV layer&lt;/li&gt;
      &lt;li&gt;Xavier initialization from &lt;em&gt;He et al.&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;SGD + Momentum (&lt;em&gt;0.9&lt;/em&gt;)&lt;/li&gt;
      &lt;li&gt;Learning rate: &lt;em&gt;0.1&lt;/em&gt;, divided by &lt;em&gt;10&lt;/em&gt; when validation error plateaus&lt;/li&gt;
      &lt;li&gt;Mini-batch size &lt;em&gt;256&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Weight decay of &lt;em&gt;1e-5&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;No dropout used&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Experimental Results:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;He et al., 2015&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Able to train very deep networks without degrading (152 layers on ImageNet, 1202 on Cifar)&lt;/li&gt;
      &lt;li&gt;Deeper networks now achieve lower training error as expected&lt;/li&gt;
      &lt;li&gt;Swept 1st place in all ILSVRC and COCO 2015 competitions&lt;/li&gt;
      &lt;li&gt;ILSVRC 2015 classification winner (&lt;em&gt;3.6%&lt;/em&gt; top 5 error); better than human performance!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Details:
    &lt;ul&gt;
      &lt;li&gt;Very deep networks using residual connections&lt;/li&gt;
      &lt;li&gt;152-layer model for ImageNet&lt;/li&gt;
      &lt;li&gt;ILSVRC’15 classification winner(&lt;em&gt;3.57%&lt;/em&gt; top 5 error)&lt;/li&gt;
      &lt;li&gt;Swept all classification and detection competitions in ILSVRC’15 and COCO’15&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Review LeCun et al., 1998 $5\times 5$ Conv filters applied at stride 1 $2\times 2$ Subsampling (Pooling) layers applied at stride 2 i.e. architecture is [CONV-POOL-CONV-POOL-FC-FC] Stride: Downsample output activations Padding: Preserve input spatial dimensions in output activations Filter: Each conv filter outputs a “slice” in the activation Case Studies AlexNet: First CNN-based winner Architecture: [CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MaxPOOL3-FC6-FC7-FC8] Input: $227\times 227\times 3$ images First layer(CONV1): 96 $11\times 11$ filters applied at stride 4, pad 0 Output volume: $W’ = (W-F+2P)/S + 1 \rightarrow$ $55\times 55\times 96$ Parameters: $(11* 11* 3 +1)* 96 =$ 36K Second layer(POOL1): $3\times 3\times$ filters applied at stride 2 Output volume: $27\times 27\times 96$ Parameters: 0 $\vdots$ CONV2($27\times 27\times 256$): 256 $5\times 5$ filters applied at stride 1, pad 2 MAX POOL2($13\times 13\times 256): $3\times 3\times$ filters applied at stride 2 CONV3($13\times 13\times 384$): 384 $3\times 3$ filters applied at stride 1, pad 1 CONV4($13\times 13\times 384$): 384 $3\times 3$ filters applied at stride 1, pad 1 CONV5($13\times 13\times 256$): 256 $3\times 3$ filters applied at stride 1, pad 1 MAX POOL3($6\times 6\times 256$): $3\times 3\times$ filters applied at stride 2 FC6(4096): 4096 neurons FC7(4096): 4096 neurons FC8(1000): 1000 neurons (class scores) Historical note: Network spread across 2 GPUs, half the neurons (feature maps) on each GPU. CONV1, CONV2, CONV4, CONV5: Connections only with feature maps on same GPU CONV3, FC6, FC7, FC8: Connections with all feature maps in preceding layer, communication across GPUs Details/Retrospectives: Krizhevsky et al. 2012 first use of ReLU used Norm layers (not common anymore) heavy data augmentation dropout 0.5 batch size 128 SGD Momentum 0.9 Learning rate 1e-2, reduced by 10 manually when val accuracy plateaus L2 weight decay 5e-4 7 CNN ensemble: 18.2% $\rightarrow$ 15.4% ZFNet: Improved hyperparameters over AlexNet AlexNet but: CONV1: change from ($11\times 11$ stride 4) to ($7\times 7$ stride 2) CONV3,4,5: instead of 384, 384, 256 filters use 512, 1024, 512 ImageNet top 5 error: 16.4% -&amp;gt; 11.7% Zeiler and Fergus, 2013 VGGNet: Deeper Networks Small filters, Deeper networks 8 layers (AlexNet) $\rightarrow$ 16 - 19 layers (VGG16Net) Only $3\times 3$ CONV stride 1, pad 1 and $2\times 2$ MAX POOL with stride 2 11.7% top 5 error(ZFNet) $\rightarrow$ 7.3% top 5 error in ILSVRC’14 Why use smaller filters? :Stack of three $3\times 3$ conv (stride 1) layers has same effective receptive field as one $7\times 7$ conv layer, but with deeper, more non-linearities and fewer parameters TOTAL memory: 24M * 4 bytes ~= 96MB / image (for a forward pass) TOTAL params: 138M parameters Most memory is in early CONV, Most params are in late FC Details: Simonyan and Zisserman, 2014 ILSVRC’14 2nd in classification, 1st in localization Similar training procedure as Krizhevsky 2012 No Local Response Normalisation (LRN) Use VGG16 or VGG19 (VGG19 only slightly better, more memory) Use ensembles for best results FC7 features generalize well to other tasks GoogLeNet Inception module: design a good local network topology(network within a network) and then stack these modules on top of each other Apply parallel filter operations on the input from previous layer: Multiple receptive field sizes for convolution(1x1, 3x3, 5x5), Pooling(3x3) Concatenate all filter outputs together channel-wise “Bottlenect” layers to reduce computational complexity of inception: use 1x1 conv to reduce feature channel size; alternatively, interpret it as applying the same FC layer on each input pixel preserves spatial dimensions, reduces depth Full GoogLeNet Architecture: Stem Network: [Conv-POOL-2x CONV-POOL] Stack Inception modules: with dimension reduction on top of each other Classifier output: [(H*W*c)-Avg POOL-(1*1*c)-FC-Softmax] Global average pooling layer before final FC layer, avoids expensive FC layers Auxiliary classification layers: [AvgPool-1x1 Conv-FC-FC-Softmax] to inject additional gradient at lower layers Details: Deeper networks, with computational efficiency ILSVRC’14 classification winner (6.7% top 5 error) 22 layers Only 5 million parameters(12x less than AlexNet, 27x less than VGG-16) Efficient “Inception” module No FC layers ResNet From 2015, “Revolution of Depth”; more than 100 layers Stacking deeper layers on a “plain” convolutional neural network results in lower both test and training error. The deeper model performs worse, but it’s not caused by overfitting. Fact: Deep models have more representation power (more parameters) than shallower models. Hypothesis: the problem is an optimization problem, deeper models are harder to optimize Solution: copying the learned layers from the shallower model and setting additional layers to identity mapping. “Residual block”: Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping Full ResNet Architecture: Stack residual blocks Every residual block has two $3\times 3$ conv layers Periodically, double number of filters and downsample spatially using stride 2 (/2 in each dimension). Reduce the activation volume by half. Additional conv layer at the beginning (7x7 conv in stem) No FC layers at the end (only FC 1000 to output classes) (In theory, you can train a ResNet with input image of variable sizes) For deeper networks(ResNet-50+): use bottleneck layer to improve efficiency (similar to GoogLeNet) e.g. [(28x28x256 INPUT)-(1x1 CONV, 64)-(3x3 CONV, 64)-(1x1 CONV, 256)-(28x28x256 OUTPUT)] Training ResNet in practice: Batch Normalization after every CONV layer Xavier initialization from He et al. SGD + Momentum (0.9) Learning rate: 0.1, divided by 10 when validation error plateaus Mini-batch size 256 Weight decay of 1e-5 No dropout used Experimental Results: He et al., 2015 Able to train very deep networks without degrading (152 layers on ImageNet, 1202 on Cifar) Deeper networks now achieve lower training error as expected Swept 1st place in all ILSVRC and COCO 2015 competitions ILSVRC 2015 classification winner (3.6% top 5 error); better than human performance! Details: Very deep networks using residual connections 152-layer model for ImageNet ILSVRC’15 classification winner(3.57% top 5 error) Swept all classification and detection competitions in ILSVRC’15 and COCO’15</summary></entry><entry><title type="html">cs231n - Lecture 8. Training Neural Networks II</title><link href="http://0.0.0.0:4000/cs231n_lec8" rel="alternate" type="text/html" title="cs231n - Lecture 8. Training Neural Networks II" /><published>2021-12-28T00:00:00+09:00</published><updated>2021-12-28T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec8</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec8">&lt;h2 id=&quot;optimization&quot;&gt;Optimization&lt;/h2&gt;

&lt;h3 id=&quot;problems-with-sgd&quot;&gt;Problems with SGD&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;What if loss changes quickly in one direction and slowly in another? What does gradient descent do?
 Very slow progress along shallow dimension, jitter along steep direction&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What if the loss function has a local minima or saddle point?&lt;br /&gt;
 Zero gradient, gradient descent gets stuck(more common in high dimension)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradients come from minibatches can be noisy&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;sgd--momentum&quot;&gt;SGD + Momentum&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;To avoid local minima, combine gradient at current point with &lt;em&gt;velocity&lt;/em&gt; to get step used to update weights; continue moving in the general direction as the previous iterations&lt;br /&gt;
  \(v_{t+1}=\rho v_t + \nabla f(x_t)\)&lt;br /&gt;
  \(x_{t+1}=x_t - \alpha v_{t+1}\)&lt;br /&gt;
  with &lt;em&gt;rho&lt;/em&gt; giving “friction”; typically &lt;em&gt;0.9&lt;/em&gt; or &lt;em&gt;0.99&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rho&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;nesterov-momentum&quot;&gt;Nesterov Momentum&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;“Look ahead” to the point where updating using velocity would take us; compute gradient there and mix it with velocity to get actual update direction&lt;br /&gt;
  \(v_{t+1}=\rho v_t - \alpha\nabla f(x_t + \rho v_t)\)&lt;br /&gt;
  \(x_{t+1}=x_t + v_{t+1}\)&lt;br /&gt;
  rearrange with \(\tilde{x}_t = x_t + \rho v_t\),&lt;br /&gt;
  \(v_{t+1}=\rho v_t - \alpha\nabla f(\tilde{x}_t)\)&lt;br /&gt;
  \(\begin{align*}
  \tilde{x}_{t+1} &amp;amp;= \tilde{x}_t - \rho v_t + (1+\rho)v_{t+1}
                  &amp;amp;= \tilde{x}_t + v_{t+1} + \rho(v_{t+1}-v_t)
  \end{align*}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adagrad&quot;&gt;AdaGrad&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Added element-wise scaling of the gradient based on the historical sum of squares in each dimension&lt;br /&gt;
  “Per-parameter learning rates” or “adaptive learning rates”&lt;br /&gt;
  Progress along “steep” directions is damped and “flat” directions is accelerated&lt;br /&gt;
  Step size decays to zero over time&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rmsprop-leaky-adagrad&quot;&gt;RMSProp: “Leaky AdaGrad”&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decay_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decay_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;adam&quot;&gt;Adam&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# Momentum
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;first_unbias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;			&lt;span class=&quot;c1&quot;&gt;# Bias correction
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;second_unbias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first_unbias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second_unbias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# AdaGrad/ RMSProp
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Sort of like RMSProp with momentum
  Bias correction for the fact that first and second moment estimates start at zero&lt;br /&gt;
  Adam with &lt;em&gt;beta1 = 0.9, beta2 = 0.999, and learning_rate = 1e-3 or 5e-4&lt;/em&gt; is a great starting point&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learning-rate-schedules&quot;&gt;Learning rate schedules&lt;/h2&gt;

&lt;h3 id=&quot;learning-rate-decays-over-time&quot;&gt;Learning rate decays over time&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Reduce learning rate by a certain value at a few fixed points(after some epochs)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;learning-rate-decay&quot;&gt;Learning Rate Decay&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Reduce learning rate gradually, e.g.&lt;br /&gt;
  Cosine:  $\alpha_t = \frac{1}{2}\alpha_0(1+\mbox{cos}(t\pi / T))$&lt;br /&gt;
  Linear: $\alpha_t = \alpha_0(1-t/T)$&lt;br /&gt;
  Inverse sqrt: $\alpha_t = \alpha_0 / \sqrt{t}$&lt;br /&gt;
  while $\alpha_0$ is the initial learning rate, $\alpha_t$ is one at epoch &lt;em&gt;t&lt;/em&gt;, and &lt;em&gt;T&lt;/em&gt; is the total number of epochs&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Linear Warmup&lt;br /&gt;
  High initial learning rates can make loss explode; linearly increasing learning rate from &lt;em&gt;0&lt;/em&gt; over the first &lt;em&gt;~5000&lt;/em&gt; iterations can prevent this&lt;br /&gt;
  Empirical rule of thumb: If you increase the batch size by &lt;em&gt;N&lt;/em&gt;, also scale the initial learning rate by &lt;em&gt;N&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;first-order-optimization&quot;&gt;First-Order Optimization&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Use gradient from linear approximation&lt;/li&gt;
  &lt;li&gt;Step to minimize the approximation&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;second-order-optimization&quot;&gt;Second-Order Optimization&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Use gradient and &lt;strong&gt;Hessian&lt;/strong&gt; to form &lt;strong&gt;quadratic&lt;/strong&gt; approximation&lt;/li&gt;
  &lt;li&gt;Step to the &lt;strong&gt;minima&lt;/strong&gt; of the (quadratic) approximation&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;But Hessian has &lt;em&gt;O(N^2)&lt;/em&gt; elements and inverting takes &lt;em&gt;O(N^3)&lt;/em&gt;, &lt;em&gt;N&lt;/em&gt; is extremely large
    &lt;ul&gt;
      &lt;li&gt;Quasi-Newton methods (BGFS most popular):&lt;br /&gt;
  instead of inverting the Hessian, approximate inverse Hessian with rank 1 updates over time&lt;/li&gt;
      &lt;li&gt;L-BFGS (Limited memory BFGS):&lt;br /&gt;
  Does not form/store the full inverse Hessian. Usually works very well in full batch, deterministic mode, but does not transfer very well to mini-batch setting. Large-scale, stochastic setting is an active area of research.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Adam is a good default choice in many cases; even with constant learning rate&lt;/li&gt;
  &lt;li&gt;SGD+Momentum can outperform Adam but may equire more tuning of LR and schedule. Cosine schedule preferred, since it has very few hyperparameters.&lt;/li&gt;
  &lt;li&gt;L-BFGS is good if you can afford to do full batch updates.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;improve-test-error&quot;&gt;Improve test error&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Better optimization algorithms help reduce &lt;strong&gt;training&lt;/strong&gt; loss, but what we really care is about error on new data - how to reduce the gap?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;early-stopping-always-do-this&quot;&gt;Early Stopping: Always do this&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Stop training the model when accuracy on the validation set decreases. Or train for a long time, but always keep track of the model snapshot that worked best on val.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-ensembles&quot;&gt;Model Ensembles&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Train multiple independent models&lt;/li&gt;
  &lt;li&gt;At test time average their results&lt;br /&gt;
 (Take average of predicted probability distributions, then choose argmax)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;regularization&quot;&gt;Regularization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;To improve single-model performance, add terms to loss&lt;br /&gt;
  e.g. L1, L2, Elastic net.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;or, use Dropout:&lt;br /&gt;
  In each forward pass, randomly set some neurons to zero. Probability of dropping is a hyperparameter; 0.5 is common.&lt;br /&gt;
  It forces the network to have a redundant representation; Prevents co-adaptation of features. Dropout can be interpreted as training a large ensemble of models (that share parameters).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# dropout rate
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# drop in train time
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt;

	&lt;span class=&quot;c1&quot;&gt;# backward pass: compute gradients...
&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# perform parameter update...
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;			&lt;span class=&quot;c1&quot;&gt;# scale at test time
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;more common: “Inverted dropout”&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;U1 = (np.random.rand(*H1.shape) &amp;lt; p) / p&lt;/code&gt; in train time and no scaling in test time&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A common pattern of regularization&lt;br /&gt;
  Training: Add some kind of randomness&lt;br /&gt;
  $y = fw(x,z)$&lt;br /&gt;
  Testing: Average out randomness (sometimes approximate)&lt;br /&gt;
  \(y = f(x) = E_z[f(x,z)] = \int p(z)f(x,z)\, dz\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data Augmentation:&lt;br /&gt;
  Addes &lt;em&gt;transformed&lt;/em&gt; data to train model&lt;br /&gt;
  e.g. translation, rotation, stretching, shearing, lens distortions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DropConnect:&lt;br /&gt;
  Training: Drop connections between neurons (set weights to 0)&lt;br /&gt;
  Testing: Use all the connections&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fractional Pooling:&lt;br /&gt;
  Training: Use randomized pooling regions&lt;br /&gt;
  Testing: Average predictions from several regions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stochastic Depth:&lt;br /&gt;
  Training: Skip some layers in the network&lt;br /&gt;
  Testing: Use all the layer&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cutout:&lt;br /&gt;
  Training: Set random image regions to zero&lt;br /&gt;
  Testing: Use full image&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mixup:&lt;br /&gt;
  Training: Train on random blends of images&lt;br /&gt;
  Testing: Use original images&lt;br /&gt;
  e.g. Randomly blend the pixels of pairs of training images, say 40% cat and 60% dog, and set the target label as cat:0.4 and dog:0.6.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Summary
  Consider dropout for large fully-connected layers&lt;br /&gt;
  Batch normalization and data augmentation almost always a good idea&lt;br /&gt;
  Try cutout and mixup especially for small classification datasets&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;choosing-hyperparameters&quot;&gt;Choosing Hyperparameters&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Step 1: Check initial loss&lt;br /&gt;
  Turn off weight decay, sanity check loss at initialization&lt;br /&gt;
  e.g. &lt;em&gt;log(C)&lt;/em&gt; for softmax with &lt;em&gt;C&lt;/em&gt; classes&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 2: Overfit a small sample&lt;br /&gt;
  Try to train to 100% training accuracy on a small sample of training data (~5-10 minibatches); fiddle with architecture, learning rate, weight initialization&lt;br /&gt;
  If loss is not going down, LR too low or bad initialization. If loss explodes, then LR is too high or bad initialization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 3: Find LR that makes loss go down&lt;br /&gt;
  Use the architecture from the previous step, use all training data, turn on small weight decay, find a learning rate that makes the loss drop significantly within ~100 iterations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 4: Coarse grid, train for ~1-5 epochs&lt;br /&gt;
  Choose a few values of learning rate and weight decay around what worked from Step 3, train a few models for ~1-5 epochs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 5: Refine grid, train longer&lt;br /&gt;
  Pick best models from Step 4, train them for longer (~10-20 epochs) without learning rate decay&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 6: Look at loss and accuracy curves&lt;br /&gt;
  If accuracy still going up, you need to train longer. If it goes down, huge train / val gap means overfitting. You need to increase regularization or get more data. If there’s no gap between train / val, it means underfitting. Train longer or use a bigger model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Look at learning curves&lt;br /&gt;
  Losses may be noisy, use a scatter plot and also plot moving average to see trends better. Cross-validation is useful too.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 7: &lt;strong&gt;GO TO Step 5&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperparameters to play with:&lt;br /&gt;
  network architecture,&lt;br /&gt;
  learning rate, its decay schedule, update type,&lt;br /&gt;
  regularization (L2/Dropout strength)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;for Hyper-Parameter Optimization, consider both Random Search and Grid Search&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-1&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Improve your training error:
    &lt;ul&gt;
      &lt;li&gt;Optimizers&lt;/li&gt;
      &lt;li&gt;Learning rate schedules&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Improve your test error:
    &lt;ul&gt;
      &lt;li&gt;Regularization&lt;/li&gt;
      &lt;li&gt;Choosing Hyperparameters&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Optimization Problems with SGD What if loss changes quickly in one direction and slowly in another? What does gradient descent do? Very slow progress along shallow dimension, jitter along steep direction What if the loss function has a local minima or saddle point? Zero gradient, gradient descent gets stuck(more common in high dimension) Gradients come from minibatches can be noisy SGD + Momentum To avoid local minima, combine gradient at current point with velocity to get step used to update weights; continue moving in the general direction as the previous iterations \(v_{t+1}=\rho v_t + \nabla f(x_t)\) \(x_{t+1}=x_t - \alpha v_{t+1}\) with rho giving “friction”; typically 0.9 or 0.99 vx = 0 while True: dx = compute_gradient(x) vx = rho * vx + dx x -= learning_rate * vx Nesterov Momentum “Look ahead” to the point where updating using velocity would take us; compute gradient there and mix it with velocity to get actual update direction \(v_{t+1}=\rho v_t - \alpha\nabla f(x_t + \rho v_t)\) \(x_{t+1}=x_t + v_{t+1}\) rearrange with \(\tilde{x}_t = x_t + \rho v_t\), \(v_{t+1}=\rho v_t - \alpha\nabla f(\tilde{x}_t)\) \(\begin{align*} \tilde{x}_{t+1} &amp;amp;= \tilde{x}_t - \rho v_t + (1+\rho)v_{t+1} &amp;amp;= \tilde{x}_t + v_{t+1} + \rho(v_{t+1}-v_t) \end{align*}\) AdaGrad grad_squared = 0 while True: dx = compute_gradient(x) grad_squared += dx * dx x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7) Added element-wise scaling of the gradient based on the historical sum of squares in each dimension “Per-parameter learning rates” or “adaptive learning rates” Progress along “steep” directions is damped and “flat” directions is accelerated Step size decays to zero over time RMSProp: “Leaky AdaGrad” grad_squared = 0 while True: dx = compute_gradient(x) grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dx * dx x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7) Adam first_moment = 0 second_moment = 0 for t in range(1, num_iterations): dx = compute_gradient(x) first_moment = beta1 * first_moment + (1 - beta1) * dx # Momentum second_moment = beta2 * second_moment + (1 - beta2) * dx * dx first_unbias = first_moment / (1 - beta1 ** t) # Bias correction second_unbias = second_moment / (1 - beta2 ** t) x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + 1e-7) # AdaGrad/ RMSProp Sort of like RMSProp with momentum Bias correction for the fact that first and second moment estimates start at zero Adam with beta1 = 0.9, beta2 = 0.999, and learning_rate = 1e-3 or 5e-4 is a great starting point Learning rate schedules Learning rate decays over time Reduce learning rate by a certain value at a few fixed points(after some epochs) Learning Rate Decay Reduce learning rate gradually, e.g. Cosine: $\alpha_t = \frac{1}{2}\alpha_0(1+\mbox{cos}(t\pi / T))$ Linear: $\alpha_t = \alpha_0(1-t/T)$ Inverse sqrt: $\alpha_t = \alpha_0 / \sqrt{t}$ while $\alpha_0$ is the initial learning rate, $\alpha_t$ is one at epoch t, and T is the total number of epochs Linear Warmup High initial learning rates can make loss explode; linearly increasing learning rate from 0 over the first ~5000 iterations can prevent this Empirical rule of thumb: If you increase the batch size by N, also scale the initial learning rate by N First-Order Optimization Use gradient from linear approximation Step to minimize the approximation Second-Order Optimization Use gradient and Hessian to form quadratic approximation Step to the minima of the (quadratic) approximation But Hessian has O(N^2) elements and inverting takes O(N^3), N is extremely large Quasi-Newton methods (BGFS most popular): instead of inverting the Hessian, approximate inverse Hessian with rank 1 updates over time L-BFGS (Limited memory BFGS): Does not form/store the full inverse Hessian. Usually works very well in full batch, deterministic mode, but does not transfer very well to mini-batch setting. Large-scale, stochastic setting is an active area of research. Summary Adam is a good default choice in many cases; even with constant learning rate SGD+Momentum can outperform Adam but may equire more tuning of LR and schedule. Cosine schedule preferred, since it has very few hyperparameters. L-BFGS is good if you can afford to do full batch updates. Improve test error Better optimization algorithms help reduce training loss, but what we really care is about error on new data - how to reduce the gap? Early Stopping: Always do this Stop training the model when accuracy on the validation set decreases. Or train for a long time, but always keep track of the model snapshot that worked best on val. Model Ensembles Train multiple independent models At test time average their results (Take average of predicted probability distributions, then choose argmax) Regularization To improve single-model performance, add terms to loss e.g. L1, L2, Elastic net. or, use Dropout: In each forward pass, randomly set some neurons to zero. Probability of dropping is a hyperparameter; 0.5 is common. It forces the network to have a redundant representation; Prevents co-adaptation of features. Dropout can be interpreted as training a large ensemble of models (that share parameters). p = 0.5 # dropout rate def train_step(X): # drop in train time H1 = np.maximum(0, np.dot(W1, X) + b1) U1 = np.random.rand(*H1.shape) &amp;lt; p H1 *= U1 H2 = np.maximum(0, np.dot(W2, H1) + b2) U2 = np.random.rand(*H2.shape) &amp;lt; p H2 *= U2 out = np.dot(W3, H2) + b3 # backward pass: compute gradients... # perform parameter update... def predict(X): # scale at test time H1 = np.maximum(0, np.dot(W1, X) + b1) * p H2 = np.maximum(0, np.dot(W2, H1) + b2) * p out = np.dot(W3, H2) + b3 more common: “Inverted dropout” U1 = (np.random.rand(*H1.shape) &amp;lt; p) / p in train time and no scaling in test time A common pattern of regularization Training: Add some kind of randomness $y = fw(x,z)$ Testing: Average out randomness (sometimes approximate) \(y = f(x) = E_z[f(x,z)] = \int p(z)f(x,z)\, dz\) Data Augmentation: Addes transformed data to train model e.g. translation, rotation, stretching, shearing, lens distortions. DropConnect: Training: Drop connections between neurons (set weights to 0) Testing: Use all the connections Fractional Pooling: Training: Use randomized pooling regions Testing: Average predictions from several regions Stochastic Depth: Training: Skip some layers in the network Testing: Use all the layer Cutout: Training: Set random image regions to zero Testing: Use full image Mixup: Training: Train on random blends of images Testing: Use original images e.g. Randomly blend the pixels of pairs of training images, say 40% cat and 60% dog, and set the target label as cat:0.4 and dog:0.6. Summary Consider dropout for large fully-connected layers Batch normalization and data augmentation almost always a good idea Try cutout and mixup especially for small classification datasets Choosing Hyperparameters Step 1: Check initial loss Turn off weight decay, sanity check loss at initialization e.g. log(C) for softmax with C classes Step 2: Overfit a small sample Try to train to 100% training accuracy on a small sample of training data (~5-10 minibatches); fiddle with architecture, learning rate, weight initialization If loss is not going down, LR too low or bad initialization. If loss explodes, then LR is too high or bad initialization. Step 3: Find LR that makes loss go down Use the architecture from the previous step, use all training data, turn on small weight decay, find a learning rate that makes the loss drop significantly within ~100 iterations. Step 4: Coarse grid, train for ~1-5 epochs Choose a few values of learning rate and weight decay around what worked from Step 3, train a few models for ~1-5 epochs. Step 5: Refine grid, train longer Pick best models from Step 4, train them for longer (~10-20 epochs) without learning rate decay Step 6: Look at loss and accuracy curves If accuracy still going up, you need to train longer. If it goes down, huge train / val gap means overfitting. You need to increase regularization or get more data. If there’s no gap between train / val, it means underfitting. Train longer or use a bigger model. Look at learning curves Losses may be noisy, use a scatter plot and also plot moving average to see trends better. Cross-validation is useful too. Step 7: GO TO Step 5 Hyperparameters to play with: network architecture, learning rate, its decay schedule, update type, regularization (L2/Dropout strength) for Hyper-Parameter Optimization, consider both Random Search and Grid Search Summary Improve your training error: Optimizers Learning rate schedules Improve your test error: Regularization Choosing Hyperparameters</summary></entry><entry><title type="html">cs231n - Lecture 7. Training Neural Networks I</title><link href="http://0.0.0.0:4000/cs231n_lec7" rel="alternate" type="text/html" title="cs231n - Lecture 7. Training Neural Networks I" /><published>2021-12-27T00:00:00+09:00</published><updated>2021-12-27T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec7</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec7">&lt;h2 id=&quot;activation-functions&quot;&gt;Activation Functions&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec7_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;sigmoid&quot;&gt;Sigmoid&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;$\sigma(x)=1/(1+e^{-x})$
    &lt;ul&gt;
      &lt;li&gt;Squashes numbers to range [0,1]&lt;/li&gt;
      &lt;li&gt;Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Problem:
    &lt;ul&gt;
      &lt;li&gt;Gradient Vanishing: Saturated neurons “kill” the gradients; If all the gradients flowing back will be zero and weights will never change.&lt;/li&gt;
      &lt;li&gt;Sigmoid outputs are not zero-centered and always positive, so the gradients will be always all positive or all negative. Then the gradient update would follow a zig-zag path, resulting in bad efficiency.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;exp()&lt;/em&gt; is a bit compute expensive.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tanhx&quot;&gt;tanh(x)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Squashes numbers to range [-1,1]&lt;br /&gt;
  zero centered&lt;br /&gt;
  &lt;strong&gt;but still kills gradients when saturated&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;relurectified-linear-unit&quot;&gt;ReLU(Rectified Linear Unit)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(x) = \mbox{max}(0,x)\)&lt;br /&gt;
  Does not saturate (in &lt;em&gt;+&lt;/em&gt; region)&lt;br /&gt;
  Very computationally efficient&lt;br /&gt;
  Converges much faster than sigmoid/tanh&lt;br /&gt;
  &lt;strong&gt;but has not zero-centered output and weights will never be updated for negative &lt;em&gt;x&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;leaky-relu&quot;&gt;Leaky ReLU&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(x) = \mbox{max}(0.01x,x)\)&lt;br /&gt;
  (or &lt;em&gt;parametric&lt;/em&gt;, PReLU: \(f(x) = \mbox{max}(\alpha x, x)\))&lt;br /&gt;
  Not saturate&lt;br /&gt;
  Computationally efficient&lt;br /&gt;
  Converges much faster&lt;br /&gt;
  will not “die”&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;eluexponential-linear-units&quot;&gt;ELU(Exponential Linear Units)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(n)= \begin{cases} x &amp;amp; \mbox{if }x&amp;gt;0 \\
                      \alpha(\mbox{exp}(x)-1) &amp;amp; \mbox{if }x\le 0\end{cases}\)&lt;br /&gt;
  ($\scriptstyle{\alpha = 1}$)&lt;br /&gt;
  All benefits of ReLU&lt;br /&gt;
  Closer to zero mean outputs&lt;br /&gt;
  Negative saturation regime compared with Leaky ReLU adds some robustness to noise&lt;br /&gt;
  &lt;strong&gt;Computation requires exp()&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;selu-scaled-exponential-linear-units&quot;&gt;SELU (Scaled Exponential Linear Units)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(n)= \begin{cases} \lambda x &amp;amp; \mbox{if }x&amp;gt;0 \\
                      \lambda\alpha(e^x -1) &amp;amp; \mbox{otherwise}\end{cases}\)&lt;br /&gt;
  ($\scriptstyle{\alpha=1.6733, \lambda=1.0507}$)&lt;br /&gt;
  Scaled versionof ELU that works better for deep networks&lt;br /&gt;
  “Self-normalizing” property;&lt;br /&gt;
  Can train deep SELU networks without BatchNorm&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;maxout-neuron&quot;&gt;Maxout “Neuron”&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(\mbox{max}(w_1^T x + b_1, w_2^T x + b_2)\)&lt;br /&gt;
  Nonlinearity; does not have the basic form of dot product&lt;br /&gt;
  Generalizes ReLU and Leaky ReLU&lt;br /&gt;
  Linear Regime; does not saturate or die&lt;br /&gt;
  &lt;strong&gt;Complexity; Doubles the number of parameters/neuron&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;swish&quot;&gt;Swish&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(x)=x\sigma(\beta x)\)&lt;br /&gt;
  train a neural network to generate and test out different non-linearities&lt;br /&gt;
  outperformed all other options for &lt;em&gt;CIFAR-10&lt;/em&gt; accuracy&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Use &lt;em&gt;ReLU&lt;/em&gt; and be careful with learning rates&lt;br /&gt;
  Try out &lt;em&gt;Leaky ReLU / Maxout / ELU / SELU&lt;/em&gt; to squeeze out some marginal gains&lt;br /&gt;
  Don’t use sigmoid or tanh&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-preprocessing&quot;&gt;Data Preprocessing&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;We may have zero-centered, normalized, decorrelated(PCA) or whitened data&lt;/li&gt;
  &lt;li&gt;After normalization, it will be less sensitive to small changes in weights and easier to optimize&lt;/li&gt;
  &lt;li&gt;In practice for images, centering only used.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;weight-initialization&quot;&gt;Weight Initialization&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;First idea: Small random numbers&lt;br /&gt;
  (gaussian with zero mean and 1e-2 standard deviation)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;It works okay for small networks, but problems with deeper networks&lt;br /&gt;
  All activations and gradients tend to zero and no learning proceeded.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;xavier-initialization&quot;&gt;“Xavier” Initialization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;std = 1/sqrt(D_in)&lt;/em&gt;&lt;br /&gt;
  For conv layers, $\mbox{D_in}$ is $\mbox{filter_size}^2\times \mbox{input_channels}$&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Activations are nicely scaled for deeper layers&lt;br /&gt;
  works well especially in non-linear activation functions like sigmoid, tanh&lt;br /&gt;
  &lt;strong&gt;but cannot used in ReLU activation function; activations collapse to zero and no learning&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;kaiming--msra-initialization&quot;&gt;Kaiming / MSRA Initialization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ReLU correction: &lt;em&gt;std = sqrt(2/D_in)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;To make each dimension zero-mean unit-variance, apply:&lt;br /&gt;
  \(\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{\mbox{Var}[x^{(k)}]}}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Usually inserted after Fully Connected or Convolutional layers, and before nonlinearity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Makes deep networks much easier to train&lt;br /&gt;
  Improves gradient flow&lt;br /&gt;
  Allows higher learning rates, faster convergence&lt;br /&gt;
  Networks become more robust to initialization&lt;br /&gt;
  Acts as regularization during training&lt;br /&gt;
  Zero overhead at test-time: can be fused with conv&lt;br /&gt;
  &lt;strong&gt;Behaves differently during training and testing: can have bugs&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comparison of Normalization Layers&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec7_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;transfer-learning&quot;&gt;Transfer Learning&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Deep learning models are trained to capture characteristics of data, from general features at the first layer to specific features at the last layer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In transfer learning, we import pre-trained model and fine-tune to our cases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Strategies&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec7_2.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;50%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;E.g.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec7_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Transfer learning with CNNs is pervasive,&lt;br /&gt;
  for Object Detection(Fast R-CNN), Image Captioning(CNN + RNN), etc.&lt;br /&gt;
  but not always be necessary&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Activation Functions Sigmoid $\sigma(x)=1/(1+e^{-x})$ Squashes numbers to range [0,1] Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron. Problem: Gradient Vanishing: Saturated neurons “kill” the gradients; If all the gradients flowing back will be zero and weights will never change. Sigmoid outputs are not zero-centered and always positive, so the gradients will be always all positive or all negative. Then the gradient update would follow a zig-zag path, resulting in bad efficiency. exp() is a bit compute expensive. tanh(x) Squashes numbers to range [-1,1] zero centered but still kills gradients when saturated ReLU(Rectified Linear Unit) \(f(x) = \mbox{max}(0,x)\) Does not saturate (in + region) Very computationally efficient Converges much faster than sigmoid/tanh but has not zero-centered output and weights will never be updated for negative x Leaky ReLU \(f(x) = \mbox{max}(0.01x,x)\) (or parametric, PReLU: \(f(x) = \mbox{max}(\alpha x, x)\)) Not saturate Computationally efficient Converges much faster will not “die” ELU(Exponential Linear Units) \(f(n)= \begin{cases} x &amp;amp; \mbox{if }x&amp;gt;0 \\ \alpha(\mbox{exp}(x)-1) &amp;amp; \mbox{if }x\le 0\end{cases}\) ($\scriptstyle{\alpha = 1}$) All benefits of ReLU Closer to zero mean outputs Negative saturation regime compared with Leaky ReLU adds some robustness to noise Computation requires exp() SELU (Scaled Exponential Linear Units) \(f(n)= \begin{cases} \lambda x &amp;amp; \mbox{if }x&amp;gt;0 \\ \lambda\alpha(e^x -1) &amp;amp; \mbox{otherwise}\end{cases}\) ($\scriptstyle{\alpha=1.6733, \lambda=1.0507}$) Scaled versionof ELU that works better for deep networks “Self-normalizing” property; Can train deep SELU networks without BatchNorm Maxout “Neuron” \(\mbox{max}(w_1^T x + b_1, w_2^T x + b_2)\) Nonlinearity; does not have the basic form of dot product Generalizes ReLU and Leaky ReLU Linear Regime; does not saturate or die Complexity; Doubles the number of parameters/neuron Swish \(f(x)=x\sigma(\beta x)\) train a neural network to generate and test out different non-linearities outperformed all other options for CIFAR-10 accuracy Summary Use ReLU and be careful with learning rates Try out Leaky ReLU / Maxout / ELU / SELU to squeeze out some marginal gains Don’t use sigmoid or tanh Data Preprocessing We may have zero-centered, normalized, decorrelated(PCA) or whitened data After normalization, it will be less sensitive to small changes in weights and easier to optimize In practice for images, centering only used. Weight Initialization First idea: Small random numbers (gaussian with zero mean and 1e-2 standard deviation) W = 0.01 * np.random.randn(D_in, D_out) It works okay for small networks, but problems with deeper networks All activations and gradients tend to zero and no learning proceeded. “Xavier” Initialization std = 1/sqrt(D_in) For conv layers, $\mbox{D_in}$ is $\mbox{filter_size}^2\times \mbox{input_channels}$ W = np.random.randn(D_in, D_out) / np.sqrt(D_in) x = np.tanh(x.dot(W)) Activations are nicely scaled for deeper layers works well especially in non-linear activation functions like sigmoid, tanh but cannot used in ReLU activation function; activations collapse to zero and no learning Kaiming / MSRA Initialization ReLU correction: std = sqrt(2/D_in) W = np.random.randn(D_in, D_out) * np.sqrt(2/D_in) x = np.maximum(0, x.dot(W)) Batch Normalization To make each dimension zero-mean unit-variance, apply: \(\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{\mbox{Var}[x^{(k)}]}}\) Usually inserted after Fully Connected or Convolutional layers, and before nonlinearity. Makes deep networks much easier to train Improves gradient flow Allows higher learning rates, faster convergence Networks become more robust to initialization Acts as regularization during training Zero overhead at test-time: can be fused with conv Behaves differently during training and testing: can have bugs Comparison of Normalization Layers Transfer Learning Deep learning models are trained to capture characteristics of data, from general features at the first layer to specific features at the last layer. In transfer learning, we import pre-trained model and fine-tune to our cases. Strategies E.g. Transfer learning with CNNs is pervasive, for Object Detection(Fast R-CNN), Image Captioning(CNN + RNN), etc. but not always be necessary</summary></entry><entry><title type="html">cs231n - Lecture 5. Convolutional Neural Networks</title><link href="http://0.0.0.0:4000/cs231n_lec5" rel="alternate" type="text/html" title="cs231n - Lecture 5. Convolutional Neural Networks" /><published>2021-12-19T00:00:00+09:00</published><updated>2021-12-19T00:00:00+09:00</updated><id>http://0.0.0.0:4000/cs231n_lec5</id><content type="html" xml:base="http://0.0.0.0:4000/cs231n_lec5">&lt;h3 id=&quot;convolutional-neural-networks&quot;&gt;Convolutional Neural Networks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ConvNets are everywhere&lt;br /&gt;
  Classification, Retrieval, Detection, Segmentation, Image Captioning, etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Recap: Fully Connected Layer&lt;br /&gt;
  $32\times 32\times 3$ image $\rightarrow$ stretch to $3072\times 1$&lt;br /&gt;
  Then a dot product of $3072\times 1$ input &lt;em&gt;x&lt;/em&gt; and scoring weights &lt;em&gt;W&lt;/em&gt;, &lt;em&gt;Wx&lt;/em&gt; is in $10 \times 3072$. With some activation function, we can make classification scores in &lt;em&gt;10&lt;/em&gt; classes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;convolution-layer-preserve-spatial-structure&quot;&gt;Convolution Layer: preserve spatial structure&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Convolve the filter with the image, slide over the image spatially, computing dot products. Filters always extend the full depth of the input volume.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec5_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
Convolve(slide) over all spatial locations, we can make an activation map of size $28\times 28\times 1$ for each convolution filter. For example, if we had &lt;em&gt;6&lt;/em&gt; $5\times 5$ filters, we’ll get &lt;em&gt;6&lt;/em&gt; separate activation maps. We stack these up to get a “new image” of size $28\times 28\times 6$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ConvNet is a sequence of Convolution Layers, interspersed with activation functions.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec5_1.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;70%&quot; /&gt;&lt;br /&gt;
Input convolved repeatedly with filters shrinks volumes spatially. By each sequence, an image is processed from low-level features to high-level features. Shrinking too fast is not good, doesn’t work well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We call the layer convolutional because it is related to convolution of two signals: \(f[x,y]*g[x,y]=\sum_{n_1=-\infty}^\infty \sum_{n_2=-\infty}^\infty f[n_1,n_2]\cdot g[x-n_1,y-n_2]\); elementwise multiplication and sum of a filter and the signal (image)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Zero pad the border:&lt;br /&gt;
  The data on the border of an image will be convolved only once with each filter, while the others on the center of an image will be treated several times. Zero padding is introduced to solve this problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;General CONV layers:&lt;br /&gt;
  with $N\times N$ input, $F\times F$ filter, applied with stride &lt;em&gt;s&lt;/em&gt;, pad with &lt;em&gt;p&lt;/em&gt; pixel border, the output is &lt;strong&gt;$(N+2P-F)/s + 1$&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Example:&lt;br /&gt;
  Input volume &lt;strong&gt;$32\times 32\times 3$&lt;/strong&gt;&lt;br /&gt;
  &lt;strong&gt;&lt;em&gt;10&lt;/em&gt; $5\times 5$&lt;/strong&gt; filters with stride &lt;strong&gt;&lt;em&gt;1&lt;/em&gt;&lt;/strong&gt;, pad &lt;strong&gt;&lt;em&gt;2&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;$\rightarrow$ Output volume size: $(32+2*2-5)/1+1=32$ spatially, so $32\times 32\times 10$.&lt;/p&gt;

    &lt;p&gt;$\rightarrow$ Number of parameters in this layer: each filter has $5\times 5\times 3+1=76$ parameters(&lt;em&gt;+1&lt;/em&gt; for bias), thus for all &lt;em&gt;760&lt;/em&gt; params.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec5_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$1\times 1$ convolution layers used:&lt;br /&gt;
  To reduce the number of channels, so the number of parameters,&lt;br /&gt;
  Then we can perform a deeper layers(Bottleneck architecture).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pooling-layer&quot;&gt;Pooling layer&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Summarize the data in a partial space, into some representations&lt;br /&gt;
  Reducing output dimensions and the number of parameters&lt;br /&gt;
  Make it smaller and more manageable&lt;br /&gt;
  Operate over each activation map independently(downsampling)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;e.g. max pool with $2\times 2$ filters and stride &lt;em&gt;2&lt;/em&gt;, $4\times 4$ input reduced to $2\times 2$ output consisted of regional maximums.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fully-connected-layer-fc-layer&quot;&gt;Fully Connected Layer (FC layer)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Contains neurons that connect to the entire input volume, as in ordinary Neural
Networks. Stacked and followed by some activations, finally we make predictions or classifications.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ConvNets stack CONV,POOL,FC layers&lt;/li&gt;
  &lt;li&gt;Trend towards smaller filters and deeper architectures&lt;/li&gt;
  &lt;li&gt;Trend towards getting rid of POOL/FC layers (just CONV)&lt;/li&gt;
  &lt;li&gt;Historically architectures looked like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[(CONV-RELU)*N-POOL?]*M-(FC-RELU)*K,SOFTMAX&lt;/code&gt; where &lt;em&gt;N&lt;/em&gt; is usually up to &lt;em&gt;~5&lt;/em&gt;, &lt;em&gt;M&lt;/em&gt; is large, $0\le K \le 2$.&lt;/li&gt;
  &lt;li&gt;but recent advances such as ResNet/GoogLeNet have challenged this paradigm&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Darron Kwon</name></author><category term="cs231n" /><summary type="html">Convolutional Neural Networks ConvNets are everywhere Classification, Retrieval, Detection, Segmentation, Image Captioning, etc. Recap: Fully Connected Layer $32\times 32\times 3$ image $\rightarrow$ stretch to $3072\times 1$ Then a dot product of $3072\times 1$ input x and scoring weights W, Wx is in $10 \times 3072$. With some activation function, we can make classification scores in 10 classes. Convolution Layer: preserve spatial structure Convolve the filter with the image, slide over the image spatially, computing dot products. Filters always extend the full depth of the input volume. Convolve(slide) over all spatial locations, we can make an activation map of size $28\times 28\times 1$ for each convolution filter. For example, if we had 6 $5\times 5$ filters, we’ll get 6 separate activation maps. We stack these up to get a “new image” of size $28\times 28\times 6$. ConvNet is a sequence of Convolution Layers, interspersed with activation functions. Input convolved repeatedly with filters shrinks volumes spatially. By each sequence, an image is processed from low-level features to high-level features. Shrinking too fast is not good, doesn’t work well. We call the layer convolutional because it is related to convolution of two signals: \(f[x,y]*g[x,y]=\sum_{n_1=-\infty}^\infty \sum_{n_2=-\infty}^\infty f[n_1,n_2]\cdot g[x-n_1,y-n_2]\); elementwise multiplication and sum of a filter and the signal (image) Zero pad the border: The data on the border of an image will be convolved only once with each filter, while the others on the center of an image will be treated several times. Zero padding is introduced to solve this problem. General CONV layers: with $N\times N$ input, $F\times F$ filter, applied with stride s, pad with p pixel border, the output is $(N+2P-F)/s + 1$ Example: Input volume $32\times 32\times 3$ 10 $5\times 5$ filters with stride 1, pad 2 $\rightarrow$ Output volume size: $(32+2*2-5)/1+1=32$ spatially, so $32\times 32\times 10$. $\rightarrow$ Number of parameters in this layer: each filter has $5\times 5\times 3+1=76$ parameters(+1 for bias), thus for all 760 params. $1\times 1$ convolution layers used: To reduce the number of channels, so the number of parameters, Then we can perform a deeper layers(Bottleneck architecture). Pooling layer Summarize the data in a partial space, into some representations Reducing output dimensions and the number of parameters Make it smaller and more manageable Operate over each activation map independently(downsampling) e.g. max pool with $2\times 2$ filters and stride 2, $4\times 4$ input reduced to $2\times 2$ output consisted of regional maximums. Fully Connected Layer (FC layer) Contains neurons that connect to the entire input volume, as in ordinary Neural Networks. Stacked and followed by some activations, finally we make predictions or classifications. Summary ConvNets stack CONV,POOL,FC layers Trend towards smaller filters and deeper architectures Trend towards getting rid of POOL/FC layers (just CONV) Historically architectures looked like [(CONV-RELU)*N-POOL?]*M-(FC-RELU)*K,SOFTMAX where N is usually up to ~5, M is large, $0\le K \le 2$. but recent advances such as ResNet/GoogLeNet have challenged this paradigm</summary></entry></feed>