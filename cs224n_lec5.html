<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
	
	<!-- On Post front-matter YAML, set "use_math: true" to use LaTex -->
	
	  
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
        equationNumbers: {
        autoNumber: "AMS"
        }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$', '$$'], ["\\[","\\]"]  ],
    processEscapes: true,
    }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>cs224n - Lecture 5. Language Models and RNNs</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
	
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="" />
    <link rel="shortcut icon" href="http://0.0.0.0:4000/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="http://0.0.0.0:4000/cs224n_lec5" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Darron's Devlog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="cs224n - Lecture 5. Language Models and RNNs" />
    <meta property="og:description" content="How do we gain from a neural dependency parser? So far… Transition based dependency parsers were an efficient linear time method for giving the syntactic structure of natural language text. Worked pretty well before neural nets came along. $\color{red}{(-)}$ They worked with indicator features, specifying some condition and then checking" />
    <meta property="og:url" content="http://0.0.0.0:4000/cs224n_lec5" />
    <meta property="og:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2022-03-15T00:00:00+00:00" />
    <meta property="article:modified_time" content="2022-03-15T00:00:00+00:00" />
    <meta property="article:tag" content="cs224n" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="cs224n - Lecture 5. Language Models and RNNs" />
    <meta name="twitter:description" content="How do we gain from a neural dependency parser? So far… Transition based dependency parsers were an efficient linear time method for giving the syntactic structure of natural language text. Worked pretty well before neural nets came along. $\color{red}{(-)}$ They worked with indicator features, specifying some condition and then checking" />
    <meta name="twitter:url" content="http://0.0.0.0:4000/" />
    <meta name="twitter:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Darron's Devlog" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="cs224n" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Darron's Devlog",
        "logo": "http://0.0.0.0:4000/"
    },
    "url": "http://0.0.0.0:4000/cs224n_lec5",
    "image": {
        "@type": "ImageObject",
        "url": "http://0.0.0.0:4000/assets/built/images/blog-cover1.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://0.0.0.0:4000/cs224n_lec5"
    },
    "description": "How do we gain from a neural dependency parser? So far… Transition based dependency parsers were an efficient linear time method for giving the syntactic structure of natural language text. Worked pretty well before neural nets came along. $\color{red}{(-)}$ They worked with indicator features, specifying some condition and then checking"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="cs224n - Lecture 5. Language Models and RNNs" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="/">Darron's Devlog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-projects" role="menuitem"><a href="/tag/projects/">Projects</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/studies/">Studies</a></li>
	<li class="nav-blog" role="menuitem"><a href="/tag/blog/">Blog</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  tag-cs224n  no-image">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="15 March 2022">15 March 2022</time>
                    
                        <span class="date-divider">/</span>
                        
							
                            
                               <a href='/tag/cs224n/'>CS224N</a>
                            
                        
                    
                </section>
                <h1 class="post-full-title">cs224n - Lecture 5. Language Models and RNNs</h1>
            </header>
	<!--
            
	-->
            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <h3 id="how-do-we-gain-from-a-neural-dependency-parser">How do we gain from a neural dependency parser?</h3>
<ul>
  <li>So far…<br />
  Transition based dependency parsers were an efficient linear time method for giving the syntactic structure of natural language text.<br />
  Worked pretty well before neural nets came along.<br />
  $\color{red}{(-)}$ They worked with indicator features, specifying some condition and then checking whether it was true of a configuration. Problems of those features are:
    <ul>
      <li>Problem #1: sparse</li>
      <li>Problem #2: incomplete</li>
      <li>Problem #3: <strong>expensive computation</strong><br />
  More than 95% of parsing time is consumed by feature computation</li>
    </ul>
  </li>
  <li>Neural Approach:<br />
  Start with the same configuration of a stack and a buffer, run exactly the same transition sequence but with a dense vector.</li>
</ul>

<h2 id="a-neural-dependency-parserchen-and-manning-2014">A neural dependency parser(<em>Chen and Manning, 2014</em>)</h2>

<p><img src="/assets/images/cs224n/lec5_0.png" alt="png" width="60%&quot;, height=&quot;100%" /></p>

<ul>
  <li>Results on English parsing to Stanford Dependencies:
    <ul>
      <li>Unlabeled attachment score (UAS) = head</li>
      <li>Labeled attachment score (LAS) = head and label</li>
      <li>2% more accurate than the symbolic dependency parser</li>
      <li>noticeably faster</li>
    </ul>
  </li>
</ul>

<h3 id="first-win-distributed-representations">First win: Distributed Representations</h3>
<ul>
  <li>Represent each word as a <em>d</em>-dimensional dense vector (i.e., word embedding)
    <ul>
      <li>Similar words are expected to have close vectors.</li>
    </ul>
  </li>
  <li>Meanwhile, <strong>part-of-speech tags</strong>(POS) and <strong>dependency labels</strong> are also represented as <em>d</em>-dimensional vectors.
    <ul>
      <li>The smaller discrete sets also exhibit many semantical similarities.</li>
    </ul>
  </li>
  <li>Extracting Tokens &amp; vector representations from configuration
    <ul>
      <li>Extract a set of tokens based on the stack / buffer positions:</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/images/cs224n/lec5_1.png" alt="png" width="80%&quot;, height=&quot;100%" /></p>

<h3 id="second-win-deep-learning-classifiers-are-non-linear-classifiers">Second win: Deep Learning classifiers are non-linear classifiers</h3>
<ul>
  <li>
    <p>A <strong>softmax classifier</strong> assigns classes $y\in C$ based on inputs $x \in \mathbb{R}^d$ via the probability:<br />
  \(\begin{align*}
  p(y\mid x) = \frac{\exp(W_y . x)}{\sum_{c=1}^C \exp(W_c . x)}
  \end{align*}\)</p>
  </li>
  <li>We train the weight matrix $W \in \mathbb{R}^{C\times d}$ to minimize the neg. log loss: $\sum_i - \log p(y_i\mid x_i)$ (a.k.a. “<em>cross entropy loss</em>”)</li>
  <li>
    <p><strong>Traditional ML classifiers</strong> (including Naive Bayes, SVMs, logistic regression and softmax classifier) are not very powerful classifiers: they only <strong>give linear decision boundaries</strong>; limiting, unhelpful when a problem is complex.</p>
  </li>
  <li>Neural networks can learn much more complex functions with nonlinear decision boundaries.
    <ul>
      <li>Non-linear in the original space, linear for the softmax at the top of the neural network</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/images/cs224n/lec5_2.png" alt="png" width="80%&quot;, height=&quot;100%" /></p>

<h3 id="simple-feed-forward-neural-network-multi-class-classifier">Simple feed-forward neural network multi-class classifier</h3>
<p><img src="/assets/images/cs224n/lec5_3.png" alt="png" width="100%&quot;, height=&quot;100%" /></p>

<h3 id="neural-dependency-parser-model-architecture">Neural Dependency Parser Model Architecture</h3>
<p><img src="/assets/images/cs224n/lec5_4.png" alt="png" width="100%&quot;, height=&quot;100%" /></p>

<h3 id="dependency-parsing-for-sentence-structure">Dependency parsing for sentence structure</h3>
<ul>
  <li>C &amp; M 2014 showed that neural networks can accurately determine the structure of sentences, supporting meaning interpretation</li>
</ul>

<p><img src="/assets/images/cs224n/lec5_5.png" alt="png" width="100%&quot;, height=&quot;100%" /></p>

<ul>
  <li>It was the first simple, successful neural dependency parser</li>
  <li>The dense representations (and non-linear classifier) let it outperform other greedy parsers in both accuracy and speed</li>
</ul>

<h3 id="further-developments-in-transition-based-neural-dependency-parsing">Further developments in transition-based neural dependency parsing</h3>
<ul>
  <li>
    <p>Improvements
  Bigger, deeper networks with better tuned hyperparameters<br />
  Beam search<br />
  Global, conditional random field (CRF)-style inference over the decision sequence</p>
  </li>
  <li>
    <p>Leading to SyntaxNet and the Parsey McParseFace model(2016):<br />
  “The World’s Most Accurate Parser”<br />
  <a href="https://ai.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html">https://ai.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html</a></p>
  </li>
</ul>

<h3 id="graph-based-dependency-parsers">Graph-based dependency parsers</h3>
<ul>
  <li>Compute a score for every possible pair; dependency (choice of head) for each word
    <ul>
      <li>Doing this well requires more than just knowing the two words</li>
      <li>We need good “contextual” representations of each word token, which we will develop in the coming lectures</li>
    </ul>
  </li>
  <li>Repeat the same process for each other word; find the best parse (MST algorithm)</li>
</ul>

<p><img src="/assets/images/cs224n/lec5_7.png" alt="png" width="50%&quot;, height=&quot;100%" /></p>

<h3 id="a-neural-graph-based-dependency-parser">A Neural graph-based dependency parser</h3>
<ul>
  <li>
    <p><em>Dozat and Manning 2017; Dozat, Qi, and Manning 2017</em></p>
  </li>
  <li>This paper revived interest in graph-based dependency parsing in a neural world
    <ul>
      <li>Designed a biaffine scoring model for neural dependency parsing</li>
      <li>Also crucially uses a neural sequence model</li>
    </ul>
  </li>
  <li><strong>Great results</strong>, but slower than the simple neural transition-based parsers
    <ul>
      <li>There are $n^2$ possible dependencies in a sentence of length $n$</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/images/cs224n/lec5_6.png" alt="png" width="60%&quot;, height=&quot;100%" /></p>

<h2 id="a-bit-more-about-neural-networks">A bit more about neural networks</h2>

<h3 id="regularization">Regularization</h3>
<ul>
  <li>
    <p>A full loss function includes <strong>regularization</strong> over all parameters $\theta$, e.g., L2 regularization:<br />
  \(\begin{align*}
  J(\theta) = \frac{1}{N}\sum_{i=1}^N -\log \left( \frac{e^{f_{y_i}}}{\sum_{c=1}^C e^{f_c}} \right) + \lambda\sum_k \theta_k^2
  \end{align*}\)</p>
  </li>
  <li>
    <p>Classic view: Regularization works to prevent <strong>overfitting</strong> when we have a lot of features (or later a very powerful/deep model, etc.)</p>
  </li>
  <li>
    <p>Now: Regularization <strong>produces models that generalize well</strong> when we have a “big” model</p>
    <ul>
      <li>We <strong>do not care that our models overfit on the training data</strong>, even though they are hugely overfit</li>
    </ul>
  </li>
</ul>

<h3 id="dropout">Dropout</h3>
<ul>
  <li>
    <p><em>[Srivastava, Hinton, Krizhevsky, Sutskever, &amp; Salakhutdinov 2012/JMLR 2014]</em></p>
  </li>
  <li>
    <p>Preventing Feature Co-adaptation = Good Regularization Method</p>
    <ul>
      <li>Training time: at each instance(or batch) of evaluation (in online SGD-training), randomly set 50% of the inputs to each neuron to 0</li>
      <li>Test time: halve the model weights (because we now keep twice as many active neurons)<br />
  (Except usually only drop first layer inputs a little (~15%) or not at all)</li>
      <li>Prevents feature co-adaptation: A feature cannot only be useful in the presence of particular other features</li>
      <li>In a single layer: A kind of middle-ground between Naive Bayes (where all feature weights are set independently) and logistic regression models (where weights are set in the context of all others)</li>
      <li>Can be thought of as a form of model bagging (i.e., like an ensemble model)</li>
      <li>Nowadays usually thought of as <strong>strong, feature-dependent regularizer</strong><br />
  <em>[Wager, Wang, &amp; Liang 2013]</em></li>
    </ul>
  </li>
</ul>

<h3 id="vectorization">“Vectorization”</h3>
<ul>
  <li>E.g., looping over word vectors versus concatenating them all into one large matrix and then multiplying the softmax weights with that matrix:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">random</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1"># number of windows to classify
</span><span class="n">d</span> <span class="o">=</span> <span class="mi">300</span> <span class="c1"># dimensionality of each window
</span><span class="n">C</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># number of classes
</span><span class="n">W</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">wordvectors_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span>
<span class="n">wordvectors_one_matrix</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>

<span class="o">%</span><span class="n">timeit</span> <span class="p">[</span><span class="n">W</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">wordvectors_list</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">W</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">wordvectors_one_matrix</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Always try to use vectors and matrices rather than for loops; the speed gain goes from 1 to 2 orders of magnitude with GPUs.</li>
</ul>

<h3 id="non-linearities-old-and-new">Non-linearities, old and new</h3>
<ul>
  <li>logistic(“sigmoid”)<br />
  \(\begin{align*} f(z) = \frac{1}{1+\exp(-z)}\end{align*}\)</li>
  <li>tanh<br />
  \(\begin{align*} f(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\end{align*}\)<br />
  tanh is just a rescaled and shifted sigmoid(2x as steep, [-1, 1]):<br />
  $tanh(z) = 2 \text{logistic}(2z) - 1$</li>
  <li>hard tanh<br />
  \(\begin{align*} \text{PHardTanh}(x) = \begin{cases}
  -1 &amp; \mbox{ if } x &lt; -1 \\
  x &amp; \mbox{ if } -1 \le x \le 1 \\
  1 &amp; \mbox{ if } x &gt; 1 \end{cases}\end{align*}\)</li>
  <li>
    <p>ReLU(Rectrified Linear Unit)<br />
  $\text{rect}(z) = \text{max}(z, 0)$</p>
  </li>
  <li>
    <p>Others<br />
  Leaky ReLU / Parametric ReLU, Swish(<em>Ramachandran, Zoph &amp; Le 2017</em>)</p>
  </li>
  <li>Both logistic and tanh are still used in various places(e.g., to get a probability), but are no longer the defaults for making deep networks<br />
  For building a deep network, first try ReLU - it trains quickly and performs well due to good gradient backflow</li>
</ul>

<h3 id="parameter-initialization">Parameter Initialization</h3>
<ul>
  <li>Initialize weights to small random values (i.e., not zero matrices)<br />
  To avoid symmetries that prevent learning/specialization</li>
  <li>Initialize hidden layer biases to 0 and output (or reconstruction) biases to optimal value if weights were 0 (e.g., mean target or inverse sigmoid of mean target)</li>
  <li>Initialize <strong>all other weights</strong> $~ \text{Uniform}(–r, r)$, with $r$ chosen so numbers get neither too big or too small (later the need for this is removed with use of <em>layer normalization</em>)</li>
  <li>Xavier initialization has variance inversely proportional to <em>fan-in</em> $n_{in}$ (previous layer size) and <em>fan-out</em> $n_{out}$ (next layer size):<br />
  \(\begin{align*}
  Var(W_i) = \frac{2}{n_{in} + n_{out}}
  \end{align*}\)</li>
</ul>

<h3 id="optimizers">Optimizers</h3>
<ul>
  <li>SGD will work just fine, but getting good results will often require hand-tuning the learning rate</li>
  <li>Sophisticated “adaptive” optimizers that scale the parameter adjustment by an accumulated gradient; <em>Adam</em> is fairly good, safe place to start in many cases</li>
</ul>

<h3 id="learning-rates">Learning Rates</h3>
<ul>
  <li>A constant learning rate. Start around $lr = 0.001$?
    <ul>
      <li>It must be order of magnitude right – try powers of 10<br />
  Too big: model may diverge or not converge<br />
  Too small: model may not have trained by the assignment deadline</li>
    </ul>
  </li>
  <li>Better try <em>learning rate decay</em>
    <ul>
      <li>By hand: : halve the learning rate every <em>k</em> epochs</li>
      <li>By a formula: $lr = lr_0 e^{-kt}$, for epoch $t$</li>
      <li>There are fancier methods like cyclic learning rates (q.v.)</li>
    </ul>
  </li>
  <li>Fancier optimizers still use a learning rate but it may be an initial rate that the optimizer shrinks – so you may want to start with a higher learning rate</li>
</ul>

<h2 id="language-modeling">Language Modeling</h2>
<ul>
  <li><strong>Language Modeling</strong> is the task of predicting what word comes next</li>
  <li>
    <p>More formally: given a sequence of words $x^{(1)}, x^{(2)}, \ldots, x^{(t)}$, compute the probability distribution of the next words $x^{(t+1)}$:<br />
  $P(x^{(t+1)} \mid x^{(t)}, \ldots, x^{(1)})$<br />
  where $x^{(t+1)}$ can be any word in the vocabulary \(V = \left\{ w_1, \ldots, w_{\lvert V \rvert} \right\}\)</p>
  </li>
  <li>
    <p>You can also think of a Language Model as a system that <strong>assigns probability to a piece of text</strong></p>
  </li>
  <li>For example, if we have some text $x^{(1)}, x^{(2)}, \ldots, x^{(T)}$, then the probability of this text (according to the Language Model) is:<br />
  \(\begin{align*}
  P(x^{(1)}, \ldots, x^{(T)}) &amp;= P(x^{(1)}) \times P(x^{(2)}\mid x^{(1)}) \times \cdots \times P(x^{(T)}\mid x^{(T-1)}, \ldots, x^{(1)}) \\
  &amp;= \prod_{t=1}^T \underbrace{P(x^{(t)}\mid x^{(t-1)}, \ldots, x^{(1)})}_{\text{This is what our LM provides}}
  \end{align*}\)</li>
</ul>

<h3 id="n-gram-language-models">n-gram Language Models</h3>
<ul>
  <li>Question: How to learn a Language Model?</li>
  <li>Answer(traditional, pre- Deep Learning): learn an <strong>n-gram Language Model</strong></li>
  <li>Definition: A _n-gram__ is a chunk of <em>n</em> consecutive words.</li>
  <li>Idea: Collect statistics about how frequent different n-grams are and use these to predict next word.</li>
  <li>First we make a <strong>Markov assumption</strong>: $x^{(t+1)}$ depends only on the preceding <em>n-1</em> words;</li>
</ul>

\[\begin{align*}
P(x^{(t+1)}\mid x^{(t)}, \ldots, x^{(1)}) &amp;= \overbrace{P(x^{(t+1)}\mid x^{(t)}, \ldots, x^{(t-n+2)})}^{n-1 \text{words}} &amp;\text{(assumption)} \\
&amp;= \frac{\overbrace{P(x^{(t+1)}, x^{(t)}, \ldots, x^{(t-n+2)})}^{\text{prob of a n-gram}}}{\underbrace{P(x^{(t)}, \ldots, x^{(t-n+2)})}_{\text{prob of a (n-1)-gram}}} &amp;\text{(definition of conditional prob)}
\end{align*}\]

<ul>
  <li>Question: How do we get these <em>n</em>-gram and <em>(n-1)</em>-gram probabilities?</li>
  <li>Answer: By counting them in some large corpus of text</li>
</ul>

\[\begin{align*}
\approx \frac{\text{count}(x^{(t+1)}, x^{(t)}, \ldots, x^{(t-n+2)})}{\text{count}(x^{(t)}, \ldots, x^{(t-n+2)})} &amp;&amp;\text{(statistical approxtimation)}  
\end{align*}\]

<h3 id="n-gram-language-models-example">n-gram Language Models: Example</h3>
<p>Suppose we are learning a <strong>4-gram</strong> Language Model.</p>

<blockquote>
  <p><del>as the proctor started the clock</del>, the students opened their <strong>w</strong>(target)</p>
</blockquote>

<p>\(\begin{align*}
P(\mathbf{w}|\text{students opened their}) = \frac{\text{count}(\text{students opened their } \mathbf{w})}{\text{students opened their}}
\end{align*}\)</p>
<ul>
  <li>For example, suppose that in the corpus:
    <ul>
      <li>“students opened their” occurred 1000 times</li>
      <li>“students opened their <strong>books</strong>” occurred 400 times<br />
  $\rightarrow P(\text{books}|\text{students opened their}) = 0.4$</li>
      <li>“students opened their <strong>exams</strong>” occurred 100 times<br />
  $\rightarrow P(\text{exams}|\text{students opened their}) = 0.1$<br />
  Then, Should we have discarded the “proctor” context?</li>
    </ul>
  </li>
  <li>Naive Bayes: a class specific unigram language model, counting individual words</li>
</ul>

<h3 id="problems-with-n-gram-language-models">Problems with n-gram Language Models</h3>
<ul>
  <li>Sparsity:</li>
</ul>

<p><img src="/assets/images/cs224n/lec5_8.png" alt="png" width="80%&quot;, height=&quot;100%" /></p>

<ul>
  <li>Storage: Need to store count for all n-grams you saw in the corpus<br />
  Increasing <em>n</em> or increasing corpus increases model size</li>
</ul>

<h3 id="n-gram-language-models-in-practice">n-gram Language Models in practice</h3>
<ul>
  <li>You can build a simple trigram Language Model over a 1.7 million word corpus (Reuters) in a few <strong>seconds</strong> on your laptop</li>
</ul>

<p><img src="/assets/images/cs224n/lec5_9.png" alt="png" width="70%&quot;, height=&quot;100%" /></p>

<ul>
  <li>You can also use a Language Model to generate text
    <ul>
      <li>Get probability distribution, sample one. move forward and iterate.</li>
      <li>Results are <em>incoherent</em>. Need to consider more <em>n</em> words but increasing <em>n</em> worsens sparsity problem, and increases model size</li>
    </ul>
  </li>
</ul>

<h3 id="how-to-build-a-neural-language-model">How to build a neural Language Model?</h3>
<ul>
  <li>Recall the Language Modeling task:
    <ul>
      <li>Input: sequence of words $x^{(1)}, x^{(2)}, \ldots, x^{(t)}$</li>
      <li>Output: prob dist of the next word $P(x^{(t+1)} \mid x^{(t)}, \ldots, x^{(1)})$</li>
    </ul>
  </li>
  <li>Window-based neural model</li>
</ul>

<p><img src="/assets/images/cs224n/lec5_10.png" alt="png" width="70%&quot;, height=&quot;100%" /></p>

<h3 id="a-fixed-window-neural-language-model">A fixed-window neural Language Model</h3>
<p>Approximately: <em>Y. Bengio, et al. (2000/2003): A Neural Probabilistic Language Model</em><br />
<img src="/assets/images/cs224n/lec5_11.png" alt="png" width="100%&quot;, height=&quot;100%" /></p>

<ul>
  <li><span style="color:green">Improvements</span> over <em>n</em>-gram LM:
    <ul>
      <li>No sparsity problem</li>
      <li>Don’t need to store all observed <em>n</em>-grams</li>
    </ul>
  </li>
  <li>Remaining <span style="color:red">problems</span>:
    <ul>
      <li>Fixed window is <strong>too small</strong></li>
      <li>Enlarging window enlarges $W$</li>
      <li>Window can never be large enough</li>
      <li>$x^{(1)}$ and $x^{(2)}$ are multiplied by completely different weights in $W$. <strong>No symmetry</strong> in how the inputs are processed.<br />
$\rightarrow$ We need a neural architecture that can process <em>any length input</em></li>
    </ul>
  </li>
</ul>

<h3 id="recurrent-neural-networks-rnn">Recurrent Neural Networks (RNN)</h3>
<ul>
  <li>Core idea: Apply the same weights $W$ repeatedly</li>
</ul>

<p><img src="/assets/images/cs224n/lec5_12.png" alt="png" width="80%&quot;, height=&quot;100%" /><br />
<img src="/assets/images/cs224n/lec5_13.png" alt="png" width="100%&quot;, height=&quot;100%" /></p>

<ul>
  <li>RNN <span style="color:green">Advantages</span>:
    <ul>
      <li>Can process <strong>any length</strong> input</li>
      <li>Computation for step <em>t</em> can (in theory) use information from <strong>many steps back</strong></li>
      <li><strong>Model size doesn’t increase</strong> for longer input context</li>
      <li><strong>Symmetry</strong> input process; same weights applied on every timestep</li>
    </ul>
  </li>
  <li>RNN <span style="color:red">Disadvantages</span>:
    <ul>
      <li>Recurrent computation is <strong>slow</strong></li>
      <li>In practice, difficult to access information from <strong>many steps back</strong></li>
    </ul>
  </li>
</ul>


                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
	<!--
            
                <section class="subscribe-form">
                    <h3 class="subscribe-form-title">Subscribe to Darron's Devlog</h3>
                    <p>Get the latest posts delivered right to your inbox</p>
                    <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
                </section>
            
	-->
            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            var this_page_url = 'http://0.0.0.0:4000/cs224n_lec5';
                            var this_page_identifier = '/cs224n_lec5';
                            var this_page_title = 'cs224n - Lecture 5. Language Models and RNNs';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            


        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/built/images/blog-cover1.png)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; Darron's Devlog &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/cs224n/">Cs224n</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs224n_lec8">cs224n - Lecture 8. Attention (Cont.)</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs224n_lec7">cs224n - Lecture 7. Translation, Seq2Seq, Attention</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs224n_lec6">cs224n - Lecture 6. Simple and LSTM RNNs</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/cs224n/">
                                
                                    See all 7 posts  →
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                    <article class="post-card post-template no-image">
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/cs224n_lec6">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Cs224n</span>
                            
                        
                    

                    <h2 class="post-card-title">cs224n - Lecture 6. Simple and LSTM RNNs</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>The Simple RNN Language Model Training an RNN Language Model Get a big corpus of text which is a sequence of words $x^{(1)}, \ldots, x^{(T)}$ Feed into RNN-LM; compute output distribution $\hat{y}^{(t)}$ for</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
            </footer>
        </div>
    </article>

            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                    <article class="post-card post-template no-image">
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/cs224n_lec4">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Cs224n</span>
                            
                        
                    

                    <h2 class="post-card-title">cs224n - Lecture 4. Dependency Parsing</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>Two views of linguistic structure: Phrase structure Constituency = phrase structure grammar = context-free grammers(CFGs) Phrase structure organizes words into nested constituents Starting unit: words (noun, preposition, adjective, determiner, …) the, $\ $</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="/">
            
            <span>Darron's Devlog</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">cs224n - Lecture 5. Language Models and RNNs</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=cs224n+-+Lecture+5.+Language+Models+and+RNNs&amp;url=https://12kdh43.github.io/cs224n_lec5"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://12kdh43.github.io/cs224n_lec5"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Darron's Devlog</a> &copy; 2022</section>
                <!-- 
				<section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                -->
				<nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search Darron's Devlog</h1>
                <p class="subscribe-overlay-description">
				</p>
                <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

 </script>

	
    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
