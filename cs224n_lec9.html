<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
	
	<!-- On Post front-matter YAML, set "use_math: true" to use LaTex -->
	
	  
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
        equationNumbers: {
        autoNumber: "AMS"
        }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$', '$$'], ["\\[","\\]"]  ],
    processEscapes: true,
    }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>cs224n - Lecture 9. Self-Attention and Transformers</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
	
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="" />
    <link rel="shortcut icon" href="http://0.0.0.0:4000/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="http://0.0.0.0:4000/cs224n_lec9" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Darron's Devlog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="cs224n - Lecture 9. Self-Attention and Transformers" />
    <meta property="og:description" content="So far: recurrent models for (most) NLP - Circa 2016, the de facto strategy in NLP is to encode sentences with a bidirectional LSTM. (for example, the source sentence in a translation) - Define your output (parse, sentence, summary) as a sequence, and use an (unidirectional) LSTM to generate it." />
    <meta property="og:url" content="http://0.0.0.0:4000/cs224n_lec9" />
    <meta property="og:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2022-03-26T00:00:00+00:00" />
    <meta property="article:modified_time" content="2022-03-26T00:00:00+00:00" />
    <meta property="article:tag" content="cs224n" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="cs224n - Lecture 9. Self-Attention and Transformers" />
    <meta name="twitter:description" content="So far: recurrent models for (most) NLP - Circa 2016, the de facto strategy in NLP is to encode sentences with a bidirectional LSTM. (for example, the source sentence in a translation) - Define your output (parse, sentence, summary) as a sequence, and use an (unidirectional) LSTM to generate it." />
    <meta name="twitter:url" content="http://0.0.0.0:4000/" />
    <meta name="twitter:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Darron's Devlog" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="cs224n" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Darron's Devlog",
        "logo": "http://0.0.0.0:4000/"
    },
    "url": "http://0.0.0.0:4000/cs224n_lec9",
    "image": {
        "@type": "ImageObject",
        "url": "http://0.0.0.0:4000/assets/built/images/blog-cover1.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://0.0.0.0:4000/cs224n_lec9"
    },
    "description": "So far: recurrent models for (most) NLP - Circa 2016, the de facto strategy in NLP is to encode sentences with a bidirectional LSTM. (for example, the source sentence in a translation) - Define your output (parse, sentence, summary) as a sequence, and use an (unidirectional) LSTM to generate it."
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="cs224n - Lecture 9. Self-Attention and Transformers" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="/">Darron's Devlog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-projects" role="menuitem"><a href="/tag/projects/">Projects</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/studies/">Studies</a></li>
	<li class="nav-blog" role="menuitem"><a href="/tag/blog/">Blog</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  tag-cs224n  no-image">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="26 March 2022">26 March 2022</time>
                    
                        <span class="date-divider">/</span>
                        
							
                            
                               <a href='/tag/cs224n/'>CS224N</a>
                            
                        
                    
                </section>
                <h1 class="post-full-title">cs224n - Lecture 9. Self-Attention and Transformers</h1>
            </header>
	<!--
            
	-->
            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <h2 id="so-far-recurrent-models-for-most-nlp">So far: recurrent models for (most) NLP</h2>

<p>
	<img src="/assets/images/cs224n/lec9_0.png" alt="png" width="40%" style="float: left" />
	<br />
	- Circa 2016, the de facto strategy in NLP is to <strong>encode</strong>
	sentences with a bidirectional LSTM. <br />
	(for example, the source sentence in a translation) <br />  
	<br />
	<br />
	<br />
	- Define your output (parse, sentence, summary) as a sequence, and use an (unidirectional) LSTM to generate it. <br />   
	<br />
	<br />
	<br />
	<br />
	- Use attention to allow flexible access to memory.  
</p>

<ul>
  <li>
    <p>Seq2Seq models need to process variable-length inputs into fixed-length representations</p>
  </li>
  <li>
    <p>To deal with seq2seq problems, we learned end-to-end differentiable system using an encoder-decoder architecture.</p>
  </li>
  <li>
    <p>Instead of entirely new ways of looking at problems, we’re trying to find the best <strong>building blocks</strong> to plug into our models and enable broad progress.</p>
  </li>
</ul>

<p><img src="/assets/images/cs224n/lec9_1.png" alt="png" width="80%&quot;, height=&quot;100%" /></p>

<h3 id="issues-with-recurrent-models-linear-interaction-distance">Issues with recurrent models: Linear interaction distance</h3>
<ul>
  <li>RNNs are unrolled “left-to-right”:<br />
  This encodes linear locality: a useful heuristic
    <ul>
      <li>Nearby words often affect each other’s meanings</li>
    </ul>
  </li>
  <li><strong>Problem</strong>: RNNs take <strong>O(sequence length)</strong> steps for distant word pairs to interact.<br />
<img src="/assets/images/cs224n/lec9_2.png" alt="png" width="80%&quot;, height=&quot;100%" />
    <ul>
      <li>Hard to learn long-distance dependencies (because gradient problems)</li>
      <li>Linear order of words is “baked in”; linear order isn’t the right way to think about sentences</li>
    </ul>
  </li>
</ul>

<h3 id="issues-with-recurrent-models-lack-of-parallelizability">Issues with recurrent models: Lack of parallelizability</h3>
<ul>
  <li>Forward and backward passes have <strong>O(sequence length)</strong> unparallelizable operations
    <ul>
      <li>Future RNN hidden states can’t be computed in full before past RNN hidden states have been computed; not GPU friendly, inhibits training on very large datasets.</li>
    </ul>
  </li>
</ul>

<h3 id="alternatives-word-windows">Alternatives: Word windows</h3>
<ul>
  <li>Word window models aggregate local contexts (Also known as 1D convolution)
    <ul>
      <li>Number of unparallelizable operations does not increase sequence length<br />
  (<strong>O(1)</strong> dependence in time)<br />
  <img src="/assets/images/cs224n/lec9_3.png" alt="png" width="80%&quot;, height=&quot;100%" /></li>
    </ul>
  </li>
  <li>What about in long-distance dependencies?
    <ul>
      <li>Stacking word window layers allows interaction between farther words</li>
      <li>Maximum Interaction distance = <strong>sequence length / window size</strong><br />
  But if your sequences are too long, you’ll just ignore long-distance context
  <img src="/assets/images/cs224n/lec9_4.png" alt="png" width="100%&quot;, height=&quot;100%" /></li>
    </ul>
  </li>
</ul>

<h3 id="alternatives-attention">Alternatives: Attention</h3>
<ul>
  <li><strong>Attention</strong> treats each word’s representation as a <strong>query</strong> to access and
incorporate information from <strong>a set of values</strong>.
    <ul>
      <li>Out of the encoder-decoder structure; think about attention <strong>within a single sentence</strong>.</li>
      <li>Number of unparallelizable operations does not increase sequence length; not parallelizable in depth but parallelizable in time.</li>
      <li>Maximum interaction distance: <strong>O(1)</strong>, since all words interact at every layer<br />
  <img src="/assets/images/cs224n/lec9_5.png" alt="png" width="70%&quot;, height=&quot;100%" /></li>
    </ul>
  </li>
</ul>

<h2 id="self-attention">Self-Attention</h2>
<ul>
  <li>Recall: Attention operates on <strong>queries</strong>, <strong>keys</strong>, and <strong>values</strong>.
    <ul>
      <li>queries: $q_1, q_2, \ldots, q_T \in \mathbb{R}^d$</li>
      <li>keys: $k_1, k_2, \ldots, k_T \in \mathbb{R}d$</li>
      <li>values: $v_1, v_2, \ldots, v_T \in \mathbb{R}d$</li>
    </ul>
  </li>
  <li>
    <p>In <strong>self-attention</strong>, the queries, keys, and values are drawn from the <strong>same source</strong>.<br />
  For example, if the output of the previous layer is $x_1, \ldots, x_T$ (one vec per word), we could let $v_i = k_i = q_i = x_i$ (use the same vectors for all of them).</p>
  </li>
  <li>The (dot product) self-attention operation is as follows:
    <ul>
      <li>Compute <strong>key-query</strong> affinities: $e_{ij} = q_i^T k_j $</li>
      <li>Compute attention weights from affinities(softmax):<br />
  \(\begin{align*}
  \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{j^\prime} \exp(e_{ij^\prime})}
  \end{align*}\)</li>
      <li>Compute outputs as weighted sum of <strong>values</strong><br />
  \(\begin{align*}
  \text{output}_i = \sum_j \alpha_{ij}v_j
  \end{align*}\)</li>
    </ul>
  </li>
  <li>Q: FCN vs Self-Attention?</li>
</ul>

<h3 id="self-attention-as-an-nlp-building-block">Self-attention as an NLP building block</h3>
<p><img src="/assets/images/cs224n/lec9_6.png" alt="png" width="80%&quot;, height=&quot;100%" /></p>

<ul>
  <li>Can self-attention replace the recurrence? <strong>NO</strong>.</li>
</ul>

<h3 id="sequence-order">Sequence order</h3>
<ul>
  <li>Self-attention is an operation on <strong>sets</strong>; it has <strong>no inherent notion of order</strong>.
    <ul>
      <li>To fix this, encode the order of the sentence.</li>
    </ul>
  </li>
  <li>
    <p>Consider representing each <strong>sequence index</strong> as a <strong>vector</strong>;<br />
  $p_i \in \mathbb{R}^d$, for \(i \in \left\{ 1,2,\ldots, T \right\}\) are position vectors</p>
  </li>
  <li>Let $\tilde{v}_i, \tilde{k}_i, \tilde{q}_i$ be our old inputs, then add $p_i$ to inputs;<br />
  $v_i = \tilde{v}_i + p_i$<br />
  $q_i = \tilde{q}_i + p_i$<br />
  $k_i = \tilde{k}_i + p_i$
    <ul>
      <li>In deep self-attention networks, we do this at the first layer; you could concatenate them as well, but mostly, just add.</li>
    </ul>
  </li>
</ul>

<ol>
  <li>Position representation vectors through sinusoids
    <ul>
      <li><strong>Sinusoidal position representations</strong>: concatenate sinusoidal functions of varying periods<br />
 <img src="/assets/images/cs224n/lec9_7.png" alt="png" width="100%&quot;, height=&quot;100%" /></li>
      <li>Pros:
        <ul>
          <li>Periodicity indicates that maybe “absolute position” isn’t as important</li>
          <li>Maybe can extrapolate to longer sequences as periods restart!</li>
        </ul>
      </li>
      <li>Cons:
        <ul>
          <li>Not learnable; also the extrapolation doesn’t really work!</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Position representation vectors learned from scratch
    <ul>
      <li><strong>Learned absolute position representations</strong>: Let a matrix $p \in \mathbb{R}^{d \times T}$ and each learnable parameters $p_i$ be a column of that matrix; Most systems use this!</li>
      <li>Pros:
        <ul>
          <li>Flexibility: each position gets to be learned to fit the data</li>
        </ul>
      </li>
      <li>Cons:
        <ul>
          <li>Definitely can’t extrapolate to indices outside $1, \ldots, T$.</li>
        </ul>
      </li>
      <li>Some more flexible representations of position:
        <ul>
          <li>Relative linear position attention (<em>Shaw et al., 2018</em>)</li>
          <li>Dependency syntax-based position (<em>Wang et al., 2019</em>)</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h3 id="nonlinearities">Nonlinearities</h3>
<ul>
  <li>
    <p>There are no elementwise nonlinearities in self-attention; stacking more self-attention layers just re-averages <strong>value</strong> vectors.</p>
  </li>
  <li>
    <p>Easy fix: add a <strong>feed-forward network</strong> to post-process each output vector.<br />
  (<code class="language-plaintext highlighter-rouge">Self-Att. - FF - Self-Att. - FF - ...</code>)<br />
  \(\begin{align*}
  m_i &amp;= MLP(\text{output}_i) \\
      &amp;= W_2 \ast \text{ReLU}(W_1 \times \text{output}_i + b_1) + b_2
  \end{align*}\)</p>
  </li>
</ul>

<h3 id="masking-the-future">Masking the future</h3>
<ul>
  <li>
    <p>To use self-attention in <strong>decoders</strong>, we need to ensure we don’t “look at the future” when predicting a sequence.</p>
  </li>
  <li>
    <p>Easily: at every timestep, we could change the set of <strong>keys and queries</strong> to include only past words. But it’s inefficient dealing with tensors, not parallelizable.</p>
  </li>
  <li>
    <p>Instead, to enable parallelization, <strong>mask out attention</strong> to future words by setting attention scores to $-\infty$ (attention weights to 0).<br />
  $e_{ij} = \begin{cases} q_i^T k_j, &amp; j &lt; i <br />
                          -\infty, &amp; j \ge i \end{cases}$<br />
  <img src="/assets/images/cs224n/lec9_8.png" alt="png" width="80%&quot;, height=&quot;100%" /></p>
  </li>
</ul>

<h3 id="recap-necessities-for-a-self-attention-building-block">Recap: Necessities for a self-attention building block</h3>
<ul>
  <li><strong>Self-attention</strong>: the basis of the method</li>
  <li><strong>Position representations</strong>:<br />
  Specify the sequence order, since self-attention is an unordered function of its inputs.</li>
  <li><strong>Nonlinearities</strong>:<br />
  At the output of the self-attention block, frequently implemented as a simple feed-forward network.</li>
  <li><strong>Masking</strong>:<br />
  In order to parallelize operations while not looking at the future, keeps information about the future from “leaking” to the past.</li>
</ul>

<h2 id="transformer">Transformer</h2>
<ul>
  <li>The Transformer Encoder-Decodr (<em>Vaswani et al., 2017</em>)<br />
  At a high level look;</li>
</ul>

<p><img src="/assets/images/cs224n/lec9_9.png" alt="png" width="100%&quot;, height=&quot;100%" /></p>

<ul>
  <li>What’s left in a Transformer Encoder Block:
    <ol>
      <li><strong>Key-query-value attention</strong>: How do we get input vectors from a single word embedding?</li>
      <li>__Multi-headed attention: Attend to multiple places in a single layer</li>
      <li><strong>Tricks to help with training</strong>:
        <ul>
          <li>Residual connections</li>
          <li>Layer normalization</li>
          <li>Scaling the dot product
  These tricks <strong>don’t improve</strong> what the model is able to do; they help improve the training process. Both of these types of modeling improvements are very important.</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<h3 id="the-transformer-encoder-key-query-value-attention">The Transformer Encoder: Key-Query-Value Attention</h3>
<ul>
  <li>Let $x_1, \ldots, x_T \in \mathbb{R}^d$ be input vectors to the Transformer encoder. Then keys, queries, values are:
    <ul>
      <li>$k_i = Kx_i$, where $K \in \mathbb{R}^{d \times d}$ is the key matrix.</li>
      <li>$q_i = Qx_i$, where $Q \in \mathbb{R}^{d \times d}$ is the query matrix.</li>
      <li>$v_i = Vx_i$, where $V \in \mathbb{R}^{d \times d}$ is the value matrix.<br />
These matrices (of learnable parameters) allow <em>different aspects</em> of the $x$ vectors to be used/emphasized in each of the three roles.</li>
    </ul>
  </li>
  <li>Computed in matrices,
    <ul>
      <li>Let $X = \left[ x_1; \ldots; x_T \right] \in \mathbb{R}^{T\times d}$ be the concatenation of input vectors.</li>
      <li>First, note that $XK \in \mathbb{R}^{T\times d}$, $XQ \in \mathbb{R}^{T\times d}$, $XV \in \mathbb{R}^{T\times d}$.</li>
      <li>The output tensor is defined as $\text{output} = \text{softmax}(XQ(XK)^T) \times XV$.</li>
    </ul>
  </li>
</ul>

<ol>
  <li>Take the query-key dot products in one matrix multiplication: $XQ(XK)^T = XQK^T X^T \in \mathbb{T}^{T\times T} $ (All pairs of attention scores)</li>
  <li>Take softmax, and compute the weighted average with another matrix multiplication: $\text{output} \in \mathbb{T}^{T\times d}$</li>
</ol>

<h3 id="the-transformer-encoder-multi-headed-attention">The Transformer Encoder: Multi-headed attention</h3>
<ul>
  <li>What if we want to look in multiple places in the sentence at once?
    <ul>
      <li>For word $i$, self-attention “looks” where $x_i^T Q^T K x_j$ is high, but maybe we want to focus on different $j$ for different reasons?</li>
    </ul>
  </li>
  <li>Define <strong>multiple attention “heads”</strong> through multiple $Q, K, V$ matrices:
    <ul>
      <li>Let $Q_l, K_l, V_l \in \mathbb{R}^{d\times \frac{d}{h}}$, where $h$ is the number of attention heads, and $l$ ranges from $1$ to $h$.</li>
      <li>Each attention head performs attention independently:<br />
  $\text{output}_l = \text{softmax}(XQ_l K_l^T X^T) \ast X V_l \in \mathbb{R}^{d/h}$</li>
      <li>Then combine the all outputs from the heads:<br />
  $\text{output} = Y\left[ \text{output}_1; \ldots, \text{output}_h \right]$, where $Y \in \mathbb{R}^{d\times d}$.</li>
      <li>Each head gets to “look” at different things and construct value vectors differently.<br />
  <img src="/assets/images/cs224n/lec9_10.png" alt="png" width="100%&quot;, height=&quot;100%" /></li>
    </ul>
  </li>
</ul>

<h3 id="the-transformer-encoder-residual-connections-he-et-al-2016">The Transformer Encoder: Residual connections [He et al., 2016]</h3>
<ul>
  <li>
    <p><strong>Residual connections</strong> are a trick to help models train better.</p>
  </li>
  <li>
    <p>Instead of $X^{(i)} = \text{Layer}(X^{(i-1)})$ (where $i$ represents the layer)<br />
  We let $X^{(i)} = X^{(i-1)} + \text{Layer}(X^{(i-1)})$ (so we only have to learn “the residual” from the previous layer)</p>
  </li>
  <li>Solves vanishing gradient problem</li>
  <li>Residual connections are thought to make the <em>loss landscape</em> considerably smoother (thus easier training!)</li>
</ul>

<p><img src="/assets/images/cs224n/lec9_11.png" alt="png" width="40%&quot;, height=&quot;100%" /></p>

<h3 id="the-transformer-encoder-layer-normalization-ba-et-al-2016">The Transformer Encoder: Layer normalization [Ba et al., 2016]</h3>
<ul>
  <li><strong>Layer normalization</strong> is a trick to help models train faster.
    <ul>
      <li>Idea: cut down on uninformative variation in hidden vector values by normalizing to unit mean and standard deviation <strong>within each layer</strong>.</li>
      <li>LayerNorm’s success may be due to its normalizing gradients (<em>Xu et al., 2019</em>)</li>
    </ul>
  </li>
  <li>Let $x\in \mathbb{R}^d$ be an individual (word) vector in the model.
    <ul>
      <li>Let the mean $\mu = \sum_{j=1}^d x_j \in \mathbb{R}$</li>
      <li>Let the standard deviation $\sigma = \sqrt{\frac{1}{d}\sum_{j=1}^d (x_j-\mu)^2} \in \mathbb{R}$</li>
      <li>Let $\gamma \in \mathbb{R}^d$ and $\beta \in \mathbb{R}$ be learned “gain” and “bias” parameters (Can omit)</li>
      <li>Then layer normalization computes:<br />
  \(\begin{align*}\text{output} = \frac{x-\mu}{\sigma + \epsilon}\ast\gamma + \beta
  \end{align*}\);<br />
  (Normalize by scalar mean and variance, and modulate by learned elementwise gain and bias)</li>
    </ul>
  </li>
</ul>

<h3 id="the-transformer-encoder-scaled-dot-product-vaswani-et-al-2017">The Transformer Encoder: Scaled Dot Product [Vaswani et al., 2017]</h3>
<ul>
  <li>
    <p>When dimensionality $d$ becomes large, dot products between vectors tend to become large.<br />
  Because of this, inputs to the softmax function can be large, making the gradients small (leading to saturate region)</p>
  </li>
  <li>
    <p>Instead of the self-attention functiopn we’ve seen:<br />
  $\text{output}_l = \text{softmax}(XQ_l K_l^T X^T) \ast X V_l$<br />
  We divide the attention scores by $\sqrt{d/h}$, to stop the scores from becoming large just as a function of $d/h$ (The dimensionality divided by the number of heads.):<br />
  $\text{output}_l = \text{softmax}(\frac{XQ_l K_l^T X^T}{\sqrt{d/h}}) \ast X V_l$</p>
  </li>
</ul>

<h2 id="now-look-at-the-decoder-blocks">Now look at the Decoder Blocks</h2>
<p><img src="/assets/images/cs224n/lec9_12.png" alt="png" width="100%&quot;, height=&quot;100%" /></p>

<h3 id="the-transformer-decoder-cross-attention-details">The Transformer Decoder: Cross-attention (details)</h3>
<ul>
  <li>
    <p>Let $h_1, \ldots, h_T$ be <strong>output</strong> vecotrs <strong>from</strong> the Transformer <strong>encoder</strong> (the last block); $x_i \in \mathbb{R}^d$<br />
  Let $z_1, \ldots, z_T$ be input vectors from the Transformer <strong>decoder</strong>, $z_i\in \mathbb{R}^d$<br />
  Then keys and values are drawn from the <strong>encoder</strong> (like a memory); $k_i = Kh_i, v_i = Vh_i$<br />
  The queries are drawn from the <strong>decoder</strong>; $q_i = Qz_i$</p>
  </li>
  <li>
    <p>In matrices:</p>
    <ul>
      <li>Let $H = \left[ h_1; \ldots, h_T \right] \in \mathbb{R}^{T\times d}$ be the concatenation of encoder vecotrs.</li>
      <li>Let $Z = \left[ z_1; \ldots, z_T \right] \in \mathbb{R}^{T\times d}$ be the concatenation of decoder vectors.</li>
      <li>The output is defined as $\text{output} = \text{softmax}(ZQ(HK)^T)\times HV$.</li>
    </ul>
  </li>
</ul>

<h3 id="what-would-we-like-to-fix-about-the-transformer">What would we like to fix about the Transformer?</h3>
<ul>
  <li><strong>Quadratic compute in self-attention</strong>:
    <ul>
      <li>Computing all pairs of interactions means our computation grows quadratically with the sequence length.</li>
      <li>For recurrent models, it only grew linearly.</li>
    </ul>
  </li>
  <li><strong>Position representations</strong>:
    <ul>
      <li>Are simple absolute indices the best we can do to represent position?</li>
      <li>Relative linear position attention (<em>Shaw et al., 2018</em>)</li>
      <li>Dependency syntax-based position (<em>Wang et al., 2019</em>)</li>
    </ul>
  </li>
</ul>

<h3 id="quadratic-computation-as-a-function-of-sequence-length">Quadratic computation as a function of sequence length</h3>
<ul>
  <li>One of the benefits of self-attention over recurrence was that it’s highly parallelizable.<br />
  However, its total number of operations grows as $O(T^2 d)$, where $T$ is the sequence length, and $d$ is the dimensionality.</li>
</ul>

<p><img src="/assets/images/cs224n/lec9_13.png" alt="png" width="80%&quot;, height=&quot;100%" /></p>

<ul>
  <li>Think of $d$ as around <strong>1,000</strong>.<br />
  So, for a single (shortish) sentence, $T \le 30; T^2 \le$ <strong>900</strong>.<br />
  In practice, we set a bound like $T = 512$.<br />
  But what if we’d like <strong>$T\ge 10,000$</strong>? to work on long documents?</li>
</ul>

<h3 id="recent-work-on-improving-on-quadratic-self-attention-cost">Recent work on improving on quadratic self-attention cost</h3>
<ul>
  <li>
    <p>Considerable recent work has gone into the question,<br />
  <em>Can we build models like Transformers without paying the $O(T^2)$ all-pairs self-attention cost?</em></p>
  </li>
  <li><strong>Linformer</strong> (<em>Wang et al., 2020</em>)<br />
<img src="/assets/images/cs224n/lec9_14.png" alt="png" width="100%&quot;, height=&quot;100%" />
    <ul>
      <li>Key idea: map the sequence length dimension to a lower-dimensional space for values, keys.</li>
    </ul>
  </li>
  <li><strong>BigBird</strong> (<em>Zaheer et al., 2021</em>)<br />
<img src="/assets/images/cs224n/lec9_15.png" alt="png" width="100%&quot;, height=&quot;100%" />
    <ul>
      <li>Key idea: replace all-pairs interactions with a family of other interactions, like <strong>local windows</strong>, <strong>looking at everything</strong>, and <strong>random interactions</strong>.</li>
    </ul>
  </li>
</ul>


                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
	<!--
            
                <section class="subscribe-form">
                    <h3 class="subscribe-form-title">Subscribe to Darron's Devlog</h3>
                    <p>Get the latest posts delivered right to your inbox</p>
                    <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
                </section>
            
	-->
            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            var this_page_url = 'http://0.0.0.0:4000/cs224n_lec9';
                            var this_page_identifier = '/cs224n_lec9';
                            var this_page_title = 'cs224n - Lecture 9. Self-Attention and Transformers';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            


        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/built/images/blog-cover1.png)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; Darron's Devlog &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/cs224n/">Cs224n</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs224n_lec8">cs224n - Lecture 8. Attention (Cont.)</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs224n_lec7">cs224n - Lecture 7. Translation, Seq2Seq, Attention</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs224n_lec6">cs224n - Lecture 6. Simple and LSTM RNNs</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/cs224n/">
                                
                                    See all 8 posts  →
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                    <article class="post-card post-template no-image">
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/cs224n_lec8">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Cs224n</span>
                            
                        
                    

                    <h2 class="post-card-title">cs224n - Lecture 8. Attention (Cont.)</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>Attention Encoder hidden states $\mathbf{h}_1, \ldots, \mathbf{h}_N \in \mathbb{R}^h$ On timestep $t$, we have Decoder hidden state $\mathbf{s}_t \in \mathbb{R}^h$ Attention score $\mathbf{e}^t$ for this step: \(\mathbf{e}^t = \left[ \mathbf{s}_t^T \mathbf{h}_1, \ldots, \mathbf{s}_t^T</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="/">
            
            <span>Darron's Devlog</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">cs224n - Lecture 9. Self-Attention and Transformers</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=cs224n+-+Lecture+9.+Self-Attention+and+Transformers&amp;url=https://12kdh43.github.io/cs224n_lec9"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://12kdh43.github.io/cs224n_lec9"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Darron's Devlog</a> &copy; 2022</section>
                <!-- 
				<section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                -->
				<nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search Darron's Devlog</h1>
                <p class="subscribe-overlay-description">
				</p>
                <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

 </script>

	
    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
