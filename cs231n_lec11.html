<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
	
	<!-- On Post front-matter YAML, set "use_math: true" to use LaTex -->
	
	  
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
        equationNumbers: {
        autoNumber: "AMS"
        }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$', '$$'], ["\\[","\\]"]  ],
    processEscapes: true,
    }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>cs231n - Lecture 11. Attention and Transformers</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
	
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="" />
    <link rel="shortcut icon" href="http://0.0.0.0:4000/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="http://0.0.0.0:4000/cs231n_lec11" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Darron's Devlog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="cs231n - Lecture 11. Attention and Transformers" />
    <meta property="og:description" content="Attention with RNNs Image Captioning using spatial features Input: Image I Output: Sequence y $= y_1, y_2, \ldots, y_T$ Encoder: $h_0 = f_W(z)$, where z is spatial CNN features, $f_W(\cdot)$ is an MLP Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector c is often $c=h_0$ Problem: Input is “bottlenecked”" />
    <meta property="og:url" content="http://0.0.0.0:4000/cs231n_lec11" />
    <meta property="og:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2022-01-04T15:00:00+00:00" />
    <meta property="article:modified_time" content="2022-01-04T15:00:00+00:00" />
    <meta property="article:tag" content="cs231n" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="cs231n - Lecture 11. Attention and Transformers" />
    <meta name="twitter:description" content="Attention with RNNs Image Captioning using spatial features Input: Image I Output: Sequence y $= y_1, y_2, \ldots, y_T$ Encoder: $h_0 = f_W(z)$, where z is spatial CNN features, $f_W(\cdot)$ is an MLP Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector c is often $c=h_0$ Problem: Input is “bottlenecked”" />
    <meta name="twitter:url" content="http://0.0.0.0:4000/" />
    <meta name="twitter:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Darron's Devlog" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="cs231n" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Darron's Devlog",
        "logo": "http://0.0.0.0:4000/"
    },
    "url": "http://0.0.0.0:4000/cs231n_lec11",
    "image": {
        "@type": "ImageObject",
        "url": "http://0.0.0.0:4000/assets/built/images/blog-cover1.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://0.0.0.0:4000/cs231n_lec11"
    },
    "description": "Attention with RNNs Image Captioning using spatial features Input: Image I Output: Sequence y $= y_1, y_2, \ldots, y_T$ Encoder: $h_0 = f_W(z)$, where z is spatial CNN features, $f_W(\cdot)$ is an MLP Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector c is often $c=h_0$ Problem: Input is “bottlenecked”"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="cs231n - Lecture 11. Attention and Transformers" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="/">Darron's Devlog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-projects" role="menuitem"><a href="/tag/projects/">Projects</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/studies/">Studies</a></li>
	<li class="nav-blog" role="menuitem"><a href="/tag/blog/">Blog</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  tag-cs231n  no-image">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime=" 4 January 2022"> 4 January 2022</time>
                    
                        <span class="date-divider">/</span>
                        
							
                            
                               <a href='/tag/cs231n/'>CS231N</a>
                            
                        
                    
                </section>
                <h1 class="post-full-title">cs231n - Lecture 11. Attention and Transformers</h1>
            </header>
	<!--
            
	-->
            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <h2 id="attention-with-rnns">Attention with RNNs</h2>

<h3 id="image-captioning-using-spatial-features">Image Captioning using spatial features</h3>
<p>Input: Image <em>I</em><br />
Output: Sequence <strong>y</strong> $= y_1, y_2, \ldots, y_T$<br />
Encoder: $h_0 = f_W(z)$, where <em>z</em> is spatial CNN features, $f_W(\cdot)$ is an MLP<br />
Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector <em>c</em> is often $c=h_0$</p>

<p><img src="/assets/images/cs231n_lec11_0.png" alt="png" width="80%&quot;, height=&quot;80%" /><br />
Problem: Input is “bottlenecked” through <em>c</em>; especially in a long descriptions.  Model needs to encode everything it wants to say within <em>c</em></p>

<p>Attention idea: New context vector <em>c_t</em> at every time step<br />
Each context vector will attend to different image regions</p>
<ul>
  <li>Alignment scores(scalars): $H \times W$ matrix <strong><em>e</em></strong><br />
  $e_{t,i,j} = f_{\mbox{att}}(h_{t-1}, z_{i,j})$<br />
  where $f_{\mbox{att}}(\cdot)$ is an MLP</li>
  <li>Normalize to get attention weights:<br />
  $a_{t,:,:} = \mbox{softmax}(e_{t,:,:})$,<br />
  $0&lt;a_{t,i,j}&lt;1$, attention values sum to <em>1</em></li>
  <li>Compute context vector <em>c</em>: multiply <em>CNN features</em> and <em>Attention weights</em><br />
  $c_t = \sum_{i,j} a_{t,i,j} z_{t,i,j}$</li>
  <li>Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, \color{red}{c_t})$</li>
</ul>

<p><img src="/assets/images/cs231n_lec11_1.png" alt="png" width="50%&quot;, height=&quot;50%" /><br />
Each timestep of decode uses a different context vector that looks(attend) at different parts of the input image. This entire process is differentiable; model chooses its own attention weights. No attention supervision is required.</p>

<h3 id="similar-tasks-in-nlp---language-translation-example">Similar tasks in NLP - Language translation example</h3>
<p>Vanilla Encoder-Decoder setting:</p>
<ul>
  <li>Input: sequence <strong>x</strong> $= x_1, x_2, \ldots, x_T$</li>
  <li>Output: sequence <strong>y</strong> $= y_1, y_2, \ldots, y_T$</li>
  <li>Encoder: $h_0 = f_W(z)$, where $z_t = \mbox{RNN}(x_t, u_{t-1})$, $f_W(\cdot)$ is MLP, <em>u</em> is the hidden RNN state</li>
  <li>Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector <em>c</em> is often $c=h_0$</li>
</ul>

<p><img src="/assets/images/cs231n_lec11_2.png" alt="png" width="80%&quot;, height=&quot;80%" /><br />
Attention in NLP</p>
<ul>
  <li>Alignment scores(scalars):<br />
  $e_{t,i} = f_{\mbox{att}}(h_{t-1}, z_t)$, where $f_{\mbox{att}}(\cdot)$ is an MLP</li>
  <li>Normalize to get attention weights:<br />
  $a_{t,:} = \mbox{softmax}(e_{t,:})$,<br />
  $0&lt;a_{t,i}&lt;1$, attention values sum to <em>1</em></li>
  <li>Compute context vector <em>c</em>:<br />
  $c_t = \sum_i a_{t,i} z_{t,i}$</li>
  <li>Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, \color{red}c_t)$</li>
</ul>

<p><img src="/assets/images/cs231n_lec11_3.png" alt="png" width="80%&quot;, height=&quot;80%" /></p>

<p>Heatmap: visualization of attention weights; without any attention supervision, model learns different word orderings for different languages</p>

<h2 id="general-attention-layer">General Attention Layer</h2>
<p>Attention in image captioning before
<img src="/assets/images/cs231n_lec11_4.png" alt="png" width="60%&quot;, height=&quot;60%" /></p>

<h3 id="single-query-setting">Single query setting</h3>
<p><img src="/assets/images/cs231n_lec11_5.png" alt="png" width="40%&quot;, height=&quot;40%" /><br />
<strong>Inputs</strong></p>
<ul>
  <li>input vectors: <strong>x</strong>(shape: $N\times D$)<br />
  Attention operation is permutation invariant; produces the same output regardless of the order of elements(features) in the input vector. Stretch $H\times W = N$ into <em>N</em> vectors, transform $H\times W\times D$ features into $N\times D$ input vectors <strong>x</strong>(similar to attention in NLP).</li>
  <li>Query: <strong>h</strong>(shape: D)</li>
</ul>

<p><strong>Operations</strong></p>
<ul>
  <li>Alignment
    <ul>
      <li>Change $f_{\mbox{att}}(\cdot)$ to a simple dot product:<br />
  $e_i = h\cdot x_i$; only works well with key &amp; value transformation trick</li>
      <li>Change $f_{\mbox{att}}(\cdot)$ to a <strong>scaled</strong> dot product:<br />
  $e_i = h\cdot x_i / \sqrt{D}$;<br />
  Larger dimensions means more terms in the dot product sum. So, the variance of the logits is higher. Large magnitude(length) vectors will produce much higher logits. Then, the post-softmax distribution(<em>e</em>) has lower-entropy(high uncertainty) assuming logits are <em>I.I.D</em>. Ultimately, these large magnitude vectors will cause softmax to peak and assign very little weight to all others. To reduce this effect, divide by $sqrt{D}$.</li>
    </ul>
  </li>
  <li>Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$</li>
  <li>Output: $\mathbf{c} = \sum_i a_i x_i$</li>
</ul>

<p><strong>Outputs</strong><br />
	- context vector: <strong>c</strong>(shape: D)</p>

<h3 id="multiple-query-setting">Multiple query setting</h3>
<p><img src="/assets/images/cs231n_lec11_6.png" alt="png" width="40%&quot;, height=&quot;40%" /><br />
<strong>Inputs</strong></p>
<ul>
  <li>input vectors: <strong>x</strong>(shape: $N\times D$)</li>
  <li>Queries: <strong>q</strong>(shape: $M\times D$); multiple query vectors</li>
</ul>

<p><strong>Operations</strong></p>
<ul>
  <li>Alignment: $e_{i,j} = q_j\cdot x_i / \sqrt{D}$</li>
  <li>Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$</li>
  <li>Output: $y_j = \sum_i a_{i,j} x_i$</li>
</ul>

<p><strong>Outputs</strong></p>
<ul>
  <li>context vectors: <strong>y</strong>(shape: D);<br />
  each query creates a new output context vector</li>
</ul>

<h3 id="weight-layers-added">Weight layers added</h3>
<p><img src="/assets/images/cs231n_lec11_7.png" alt="png" width="45%&quot;, height=&quot;45%" /><br />
Notice that the input vectors <strong>x</strong> are used for both the alignment(<strong>e</strong>) and attention calculations(<strong>y</strong>); We can add more expressivity to the layer by adding a different FC layer before each of the two steps. The input and output dimensions can now change depending on the key and value FC layers.</p>

<p><strong>Inputs</strong></p>
<ul>
  <li>input vectors: <strong>x</strong>(shape: $N\times D$)</li>
  <li>Queries: <strong>q</strong>(shape: $M\times D_k$)</li>
</ul>

<p><strong>Operations</strong></p>
<ul>
  <li>Key vectors: $\mathbf{k} = \mathbf{x}W_k$</li>
  <li>Value vectors: $\mathbf{v} = \mathbf{x}W_v$</li>
  <li>Alignment: $e_{i,j} = q_j\cdot k_i / \sqrt{D}$</li>
  <li>Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$</li>
  <li>Output: $y_j = \sum_i a_{i,j} v_i$</li>
</ul>

<p><strong>Outputs</strong></p>
<ul>
  <li>context vectors: <strong>y</strong>(shape: $D_v$);</li>
</ul>

<h3 id="self-attention-layer">Self attention layer</h3>
<p><img src="/assets/images/cs231n_lec11_8.png" alt="png" width="45%&quot;, height=&quot;45%" /><br />
Recall that the query vector was a function of the input vectors; Encoder $h_0=f_W(z)$, where <em>z</em> is spatial CNN features, $f_W(\cdot)$ is an MLP. We can calculate the query vectors from the input vectors, defining a “self-attention” layer. No input query vectors anymore, instead query vectors are calculated using a FC layer.</p>

<p><strong>Inputs</strong></p>
<ul>
  <li>input vectors: <strong>x</strong>(shape: $N\times D$)</li>
</ul>

<p><strong>Operations</strong></p>
<ul>
  <li>Key vectors: $\mathbf{k} = \mathbf{x}W_k$</li>
  <li>Value vectors: $\mathbf{v} = \mathbf{x}W_v$</li>
  <li>Query vectors: $\mathbf{q} = \mathbf{x}W_q$</li>
  <li>Alignment: $e_{i,j} = q_j\cdot k_i / \sqrt{D}$</li>
  <li>Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$</li>
  <li>Output: $y_j = \sum_i a_{i,j} v_i$</li>
</ul>

<p><strong>Outputs</strong></p>
<ul>
  <li>context vectors: <strong>y</strong>(shape: $D_v$)</li>
</ul>

<h4 id="positional-encoding"><em>Positional encoding</em></h4>
<p><img src="/assets/images/cs231n_lec11_9.png" alt="png" width="35%&quot;, height=&quot;35%" /><br />
Self attention attends over sets of inputs; is permutation invariant. To encode the ordered sequences(e.g. language, image), concatenate special positional encoding $p_j$ to each input vector $x_j$.</p>

<p>$\mathit{pos}: N\rightarrow R^d$ to process the position <em>j</em> of the vector into a <em>d</em>-dimensional vector; $p_j = \mathit{pos}(j)$</p>

<p><strong>Desiderata</strong> of $\mathit{pos}(\cdot)$:</p>
<ol>
  <li>Should output a <strong>unique</strong> encoding for each time-step(word’s position in a sentence).</li>
  <li><strong>Distance</strong> between any two time-steps should be consistent across sentences with different lengths(variable inputs).</li>
  <li>Model should generalize to <strong>longer</strong> sentences without any efforts. Its values should be bounded.</li>
  <li>Must be <strong>deterministic</strong>.</li>
</ol>

<p><strong>Options</strong> for $\mathit{pos}(\cdot)$:</p>
<ol>
  <li>Learn a lookup table:
    <ul>
      <li>Learn parameters to use for $\mathit{pos}(t)$ for $t \in [0,T)$</li>
      <li>Lookup table contains $T\times d$ parameters</li>
    </ul>
  </li>
  <li>Design a fixed function with the desiderata<br />
<img src="/assets/images/cs231n_lec11_10.png" alt="png" width="40%&quot;, height=&quot;40%" /></li>
</ol>

<h3 id="masked-self-attention-layer">Masked self-attention layer</h3>
<p><img src="/assets/images/cs231n_lec11_11.png" alt="png" width="45%&quot;, height=&quot;45%" /><br />
Manually set alignment scores to $-\infty$, prevent vectors from looking at future vectors.</p>

<h3 id="multi-head-self-attention-layer">Multi-head self attention layer</h3>
<p><img src="/assets/images/cs231n_lec11_12.png" alt="png" width="60%&quot;, height=&quot;60%" /><br />
Multiple self-attention heads in parallel; similar to ensemble</p>

<h3 id="comparing-rnns-to-transformers">Comparing RNNs to Transformers</h3>
<p><strong>RNNs</strong><br />
<span style="color:green">(+) LSTMs work reasonably well for long sequences.</span><br />
<span style="color:red">(-) Expects an ordered sequences of inputs</span><br />
<span style="color:red">(-) Sequential computation: subsequent hidden states can only be computed after the previous ones are done.</span></p>

<p><strong>Transformers</strong><br />
<span style="color:green">(+) Good at long sequences. Each attention calculation looks at all inputs.</span><br />
<span style="color:green">(+) Can operate over unordered sets or ordered sequences with positional encodings.</span><br />
<span style="color:green">(+) Parallel computation: All alignment and attention scores for all inputs can be done in parallel.</span><br />
<span style="color:red">(-) Requires a lot of memory: N x M alignment and attention scalers need to be calculated and stored for a single self-attention head.</span></p>

<h2 id="transformers">Transformers</h2>
<h3 id="image-captioning-using-transformers">Image Captioning using transformers</h3>
<p><img src="/assets/images/cs231n_lec11_13.png" alt="png" width="80%&quot;, height=&quot;80%" /></p>
<ul>
  <li>No recurrence at all</li>
</ul>

<p>Input: Image <strong>I</strong><br />
Output: Sequence <strong>y</strong> $= y_1, y_2, \ldots, y_T$<br />
Encoder: $c = T_W(z)$, where <em>z</em> is spatial CNN features, $T_W(\cdot)$ is the transformer encoder<br />
Decoder: $y_t = T_D(y_{0:t-1}, c)$, where $T_D(\cdot)$ is the transformer decoder</p>

<h4 id="the-transformer-encoder-block">The Transformer encoder block</h4>
<p><img src="/assets/images/cs231n_lec11_14.png" alt="png" width="80%&quot;, height=&quot;80%" /><br />
Inputs: Set of vectors <strong>x</strong><br />
Outputs: Set of vectors <strong>y</strong></p>

<p>Self-attention is the only interaction between vectors; Layer norm and MLP operate independently per vector. Highly scalable, highly parallelizable, but high memory usage.</p>

<h4 id="the-transformer-decoder-block">The Transformer Decoder block</h4>
<p><img src="/assets/images/cs231n_lec11_15.png" alt="png" width="80%&quot;, height=&quot;80%" /><br />
Inputs: Set of vectors <strong>x</strong> and Set of context vector <strong>c</strong><br />
Outputs: Set of vectors <strong>y</strong></p>

<p>Masked Self-attention only interacts with past inputs(<em>x</em>, or previous output $y_{t-1}$). Multi-head attention block is NOT self-attention; it attends over the transformer encoder outputs. In this phase, we inject image features into the decoder. Highly scalable, highly parallelizable, but high memory usage.</p>

<h3 id="image-captioning-using-only-transformers">Image Captioning using ONLY transformers</h3>
<ul>
  <li>
    <p>Transformers from pixels to language<br />
  <em>Dosovitskiy et al, “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”, ArXiv 2020</em>  <a href="https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax.ipynb" target="_blank">colab notebook link</a></p>
  </li>
  <li>
    <p>Note: in Google Colab - TPU runtime setting</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># TPU initialization
</span><span class="n">resolver</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">distribute</span><span class="p">.</span><span class="n">cluster_resolver</span><span class="p">.</span><span class="n">TPUClusterResolver</span><span class="p">(</span><span class="n">tpu</span><span class="o">=</span><span class="s">'grpc://'</span> <span class="o">+</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'COLAB_TPU_ADDR'</span><span class="p">])</span>

<span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental_connect_to_cluster</span><span class="p">(</span><span class="n">resolver</span><span class="p">)</span>
<span class="n">tf</span><span class="p">.</span><span class="n">tpu</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">initialize_tpu_system</span><span class="p">(</span><span class="n">resolver</span><span class="p">)</span>

<span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">distribute</span><span class="p">.</span><span class="n">TPUStrategy</span><span class="p">(</span><span class="n">resolver</span><span class="p">)</span>

<span class="c1"># compile in strategy.scope
</span><span class="k">def</span> <span class="nf">create_model</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
      <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
       <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
       <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span>
       <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
       <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
       <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

<span class="k">with</span> <span class="n">strategy</span><span class="p">.</span><span class="n">scope</span><span class="p">():</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
  <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
                <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'sparse_categorical_accuracy'</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="summary">Summary</h3>
<ul>
  <li>Adding <strong>attention</strong> to RNNs allows them to “attend” to different parts of the input at every time step</li>
  <li>The <strong>general attention layer</strong> is a new type of layer that can be used to design new neural network architectures</li>
  <li><strong>Transformers</strong> are a type of layer that uses self-attention and layer norm.
    <ul>
      <li>It is highly scalable and highly parallelizable</li>
      <li>Faster training, larger models, better performance across vision and language tasks</li>
      <li>They are quickly replacing RNNs, LSTMs, and may even replace convolutions.</li>
    </ul>
  </li>
</ul>

                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
	<!--
            
                <section class="subscribe-form">
                    <h3 class="subscribe-form-title">Subscribe to Darron's Devlog</h3>
                    <p>Get the latest posts delivered right to your inbox</p>
                    <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
                </section>
            
	-->
            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            var this_page_url = 'http://0.0.0.0:4000/cs231n_lec11';
                            var this_page_identifier = '/cs231n_lec11';
                            var this_page_title = 'cs231n - Lecture 11. Attention and Transformers';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            


        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/built/images/blog-cover1.png)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; Darron's Devlog &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/cs231n/">Cs231n</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs231n_lec15">cs231n - Lecture 15. Detection and Segmentation</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs231n_lec14">cs231n - Lecture 14. Visualizing and Understanding</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs231n_lec13">cs231n - Lecture 13. Self-Supervised Learning</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/cs231n/">
                                
                                    See all 13 posts  →
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                    <article class="post-card post-template no-image">
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/cs231n_lec12">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Cs231n</span>
                            
                        
                    

                    <h2 class="post-card-title">cs231n - Lecture 12. Generative Models</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>Supervised vs. Unsupervised Supervised Learning: Data: $(x,y)$; y is label Goal: Learn a function to map $x\rightarrow y$ Unsupervised Learning: Data: x; no labels Goal: Learn some underlying hidden structure of the data</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
            </footer>
        </div>
    </article>

            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                    <article class="post-card post-template no-image">
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/cs231n_lec10">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Cs231n</span>
                            
                        
                    

                    <h2 class="post-card-title">cs231n - Lecture 10. Recurrent Neural Networks</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>RNN: Process Sequences one to one; vanilla neural networks one to many; e.g. Image Captioning(image to sequence of words) many to one; e.g. Action Prediction(video sequence to action class) many to many(1); e.g.</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="/">
            
            <span>Darron's Devlog</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">cs231n - Lecture 11. Attention and Transformers</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=cs231n+-+Lecture+11.+Attention+and+Transformers&amp;url=https://12kdh43.github.io/cs231n_lec11"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://12kdh43.github.io/cs231n_lec11"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Darron's Devlog</a> &copy; 2022</section>
                <!-- 
				<section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                -->
				<nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search Darron's Devlog</h1>
                <p class="subscribe-overlay-description">
				</p>
                <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

 </script>

	
    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
