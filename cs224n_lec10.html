<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
	
	<!-- On Post front-matter YAML, set "use_math: true" to use LaTex -->
	
	  
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
        equationNumbers: {
        autoNumber: "AMS"
        }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$', '$$'], ["\\[","\\]"]  ],
    processEscapes: true,
    }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>cs224n - Lecture 10. Transformers and Pretraining</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
	
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="" />
    <link rel="shortcut icon" href="http://0.0.0.0:4000/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="http://0.0.0.0:4000/cs224n_lec10" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Darron's Devlog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="cs224n - Lecture 10. Transformers and Pretraining" />
    <meta property="og:description" content="Word structure and subword models Assumpstions we’ve made: Fixed vocab of a number of words, built from the training set. All novel words seen at test time are mapped to a single UNK token. Finite vocabulary assumptions make even less sense in many languages. Many Languages exhibit complex morphology, or" />
    <meta property="og:url" content="http://0.0.0.0:4000/cs224n_lec10" />
    <meta property="og:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2022-08-15T00:00:00+00:00" />
    <meta property="article:modified_time" content="2022-08-15T00:00:00+00:00" />
    <meta property="article:tag" content="cs224n" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="cs224n - Lecture 10. Transformers and Pretraining" />
    <meta name="twitter:description" content="Word structure and subword models Assumpstions we’ve made: Fixed vocab of a number of words, built from the training set. All novel words seen at test time are mapped to a single UNK token. Finite vocabulary assumptions make even less sense in many languages. Many Languages exhibit complex morphology, or" />
    <meta name="twitter:url" content="http://0.0.0.0:4000/" />
    <meta name="twitter:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Darron's Devlog" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="cs224n" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Darron's Devlog",
        "logo": "http://0.0.0.0:4000/"
    },
    "url": "http://0.0.0.0:4000/cs224n_lec10",
    "image": {
        "@type": "ImageObject",
        "url": "http://0.0.0.0:4000/assets/built/images/blog-cover1.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://0.0.0.0:4000/cs224n_lec10"
    },
    "description": "Word structure and subword models Assumpstions we’ve made: Fixed vocab of a number of words, built from the training set. All novel words seen at test time are mapped to a single UNK token. Finite vocabulary assumptions make even less sense in many languages. Many Languages exhibit complex morphology, or"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="cs224n - Lecture 10. Transformers and Pretraining" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="/">Darron's Devlog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-projects" role="menuitem"><a href="/tag/projects/">Projects</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/studies/">Studies</a></li>
	<li class="nav-blog" role="menuitem"><a href="/tag/blog/">Blog</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  tag-cs224n  no-image">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="15 August 2022">15 August 2022</time>
                    
                        <span class="date-divider">/</span>
                        
							
                            
                               <a href='/tag/cs224n/'>CS224N</a>
                            
                        
                    
                </section>
                <h1 class="post-full-title">cs224n - Lecture 10. Transformers and Pretraining</h1>
            </header>
	<!--
            
	-->
            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <h2 id="word-structure-and-subword-models">Word structure and subword models</h2>
<ul>
  <li>Assumpstions we’ve made:
    <ul>
      <li>Fixed vocab of a number of words, built from the training set.</li>
      <li>All <em>novel</em> words seen at test time are mapped to a single <code class="language-plaintext highlighter-rouge">UNK</code> token.</li>
    </ul>
  </li>
  <li>Finite vocabulary assumptions make even <em>less</em> sense in many languages.
    <ul>
      <li>Many Languages exhibit complex <strong>morphology</strong>, or word structure.
        <ul>
          <li>The effect is more word types, each occuring fewer times.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="the-byte-pair-encoding-algorithm">The byte-pair encoding algorithm</h3>
<p>Subword modeling in NLP encompasses a wide range of methods for reasoning about structure below the word level. (Parts of words, characters, bytes.)</p>
<ul>
  <li>The dominant modern paradigm is to learn a vocabulary of <strong>parts of words (subword tokens)</strong>.</li>
  <li>At training and testing time, each word is split into a sequence of known subwords.</li>
</ul>

<p><strong>Byte-pair encoding</strong> is a simple, effective strategy for defining a subword vocabulary.</p>
<ol>
  <li>Start with a vocabulary containing only characters and an “end-of-word” symbol.</li>
  <li>Using a corpus of text, find the most common adjacent characters “a,b”; add “ab” as a subword.</li>
  <li>Replace instances of the character pair with the new subword; repeat until desired vocab size.</li>
  <li>Ex: starting characters {a, b, $\ldots$, z}. Encoding vocab: {a, $\ldots$, z, $\ldots$, apple, app#, #ly, $\ldots$}</li>
</ol>

<p>Originally used in NLP for machine translation; now a similar method (WordPiece) is used in pretrained models.</p>

<p>Common words end up being a part of the subword vocabulary, while rarer words are split into (sometimes intuitive, sometimes not) components. In the worst case, words are split into as many subwords as they have characters.<br />
<img src="/assets/images/cs224n/lec10_0.png" alt="png" width="80%&quot;, height=&quot;100%" /></p>

<ul>
  <li>Problem:<br />
  The model has no idea whether it’s dealing with words or subwords.<br />
  A rare(or weird) individual single word can be mapped to as many words as its characters, dominates vocabulary.</li>
</ul>

<h2 id="motivating-word-meaning-and-context">Motivating word meaning and context</h2>
<p>Recall the adage we mentioned at the beginning of the course:</p>
<blockquote>
  <p>“You shall know a word by the company it keeps” (J. R. Firth 1957: 11)</p>
</blockquote>

<p>This quote is a summary of <strong>distributional semantics</strong>, and motivated <strong>word2vec</strong>. But:</p>
<blockquote>
  <p>“… the complete meaning of a word is always contextual, and no study of meaning apart from a complete context can be taken seriously.” (J. R. Firth 1935)</p>
</blockquote>

<p>Consider I <em><strong>record</strong></em> the <em><strong>record</strong></em>: the two instances of <em><strong>record</strong></em> mean different things.</p>

<h3 id="where-we-were-pretrained-word-embeddings">Where we were: pretrained word embeddings</h3>
<p>
	<img src="/assets/images/cs224n/lec10_1.png" alt="png" width="40%" style="float: right" />
	Circa 2017:<br />
	- Start with pretrained word embeddings (no context)<br />
	- Learn how to incorporate context in an LSTM or Transformer while training on the task.<br />
	<br />
	Some issues to think about:<br />
	- The training data we have for our downstream task (like question answering) must be sufficient to teach all contextual aspects of language.<br />
	- Most of the parameters in our network are randomly initialized!
</p>

<h3 id="where-were-going-pretraining-whole-models">Where we’re going: pretraining whole models</h3>
<p>
	<img src="/assets/images/cs224n/lec10_2.png" alt="png" width="40%" style="float: right" />
	In modern NLP:<br />
	- All (or almost all) parameters in NLP networks are initialized via pretraining.<br />
	- Pretraining methods hide parts of the input from the model, and train the model to reconstruct those parts.<br />
	<br />
	This has been exceptionally effective at building strong:<br />
	- representations of language <br />
	- parameter initializations for strong NLP models. <br />
	- Probability distributions over language that we can sample from
</p>

<h3 id="pretraining-through-language-modeling-dai-and-le-2015">Pretraining through language modeling [<em>Dai and Le, 2015</em>]</h3>
<p>
	<img src="/assets/images/cs224n/lec10_3.png" alt="png" width="40%" style="float: right" />
	Recall the language modeling task:<br />
	- Model $p_\theta(w_t \vert w_{1:t-1})$, the probability distribution over words given their past contexts. <br />
	- There’s lots of data for this! (In English.) <br />
	<br />
	Pretraining through language modeling: <br />
	- Train a neural network to perform language modeling on a large amount of text. <br />
	- Save the network parameters. <br />
</p>

<h3 id="the-pretraining--finetuning-paradigm">The Pretraining / Finetuning Paradigm</h3>
<ul>
  <li>
    <p>Pretraining can improve NLP applications by serving as parameter initialization</p>
  </li>
  <li><strong>Step 1: Pretrain (on language modeling)</strong><br />
  Lots of text; learn general things</li>
  <li><strong>Step 2: Finetune (on your task)</strong><br />
  Not many labels; adapt to the task</li>
</ul>

<h3 id="stochastic-gradient-descent-and-pretrainfinetune">Stochastic gradient descent and pretrain/finetune</h3>
<p>Why should pretraining and finetuning help, from a “training neural nets” perspective?</p>

<ul>
  <li>Consider, provides parameters $\hat{\theta}$ by approximating \(\text{min}_{\theta} \mathcal{L}_{\text{pretrain}}(\theta)\) (the pretraining loss).</li>
  <li>Then, finetuning approximates \(\text{min}_{\theta } \mathcal{L}_{\text{finetune}}(\theta)\), starting at $\hat{\theta}$ (the finetuning loss).</li>
  <li>The pretraining may matter because stochastic gradient descent sticks (relatively) close to $\hat{\theta}$ during finetuning.
    <ul>
      <li>So, maybe the finetuning local minima near $\hat{\theta}$ tend to generalize well (but still a mystery)</li>
      <li>And/or, maybe the gradients of finetuning loss near $\hat{\theta}$ propagate nicely!</li>
    </ul>
  </li>
</ul>

<h2 id="pretraining-for-three-types-of-architectures">Pretraining for three types of architectures</h2>
<p>
	<img src="/assets/images/cs224n/lec10_4.png" alt="png" width="40%" style="float: left" />
	<br />
	- Language models! What we've seen so far. <br />
	- Nice to generate from; can’t condition on future words <br />
	<br />
	<br />
	<br />
	- Gets bidirectional context – can condition on future! <br />
	- Wait, how do we pretrain them? <br />
	  (if you try pretrain it as a language model, the loss will be 0 because you feed the future information)<br />
	<br />
	<br />
	- Good parts of decoders and encoders? <br />
	- What’s the best way to pretrain them? <br />
</p>

<h3 id="pretraining-decoders-1">Pretraining Decoders (1)</h3>
<ul>
  <li><strong>When pretraining a language model</strong>, we try to approximate the probability of a word given all of its previous words; $p_\theta(w_t \vert w_{1:t-1})$. <strong>When using language model pretrained decoders</strong>, we can ignore that they were trained to model $p$.</li>
</ul>

<p>
	<img src="/assets/images/cs224n/lec10_5.png" alt="png" width="40%" style="float: right" />
	- We can finetune them by training a classifier on the last word’s hidden state. <br />
	$$\begin{align*}
	h_1, \ldots, h_T &amp;= \text{Decoder}(w_1, \ldots, w_T) &amp;&amp; (w \text{is a subword}) \\
	&amp; y \sim Ah_t + b
	\end{align*}$$
	- Where $A, b$ are randomly initialized (not pretrained) and specified by the downstream task.<br />  
	- Gradients backpropagate through the wholde network, not just the linear layer.
</p>

<h3 id="pretraining-decoders-2">Pretraining Decoders (2)</h3>
<ul>
  <li>It’s natural to pretrain decoders as language models and then use them as generators, finetuning their $p_\theta(w_t \vert w_{1:t-1})$</li>
</ul>

<p>
	<img src="/assets/images/cs224n/lec10_6.png" alt="png" width="40%" style="float: right" />
	This is helpful in tasks where the output is a sequence with a vocabulary like that at pretraining time. <br />
	- Dialogue (context=dialogue history)<br />
	- Summarization (context=document)<br />
	$$\begin{align*}
	h_1, \ldots, h_T &amp;= \text{Decoder}(w_1, \ldots, w_T) \\
	&amp; w_t \sim Ah_{t-1} + b
	\end{align*}$$
	- Where $A, b$ were pretrained in the language model
</p>

<h3 id="generative-pretrained-transformer-gpt-radford-et-al-2018">Generative Pretrained Transformer (GPT) [<em>Radford et al., 2018</em>]</h3>
<ul>
  <li>2018’s GPT was a big success in pretraining a decoder!
    <ul>
      <li>Transformer decoder with 12 layers.</li>
      <li>768-dimensional hidden states, 3072-dimensional feed-forward hidden layers.</li>
      <li>Byte-pair encoding with 40,000 merges. (relatively small vocab)</li>
      <li>Trained on BooksCorpus: over 7000 unique books.
        <ul>
          <li>Contains long spans of contiguous text, for learning long-distance dependencies.</li>
        </ul>
      </li>
      <li>The acronym “GPT” never showed up in the original paper; it could stand for “Generative PreTraining” or “Generative Pretrained Transformer”</li>
    </ul>
  </li>
  <li>How do we format inputs to our decoder for <strong>finetuning tasks?</strong>
    <ul>
      <li>Evaluate on <strong>Natural Language Inference:</strong> Label pairs of sentences as <em>entailing/contradictory/neutral</em><br />
  e.g., sentence <strong>entailment</strong> in<br />
  Premise: <em>The man is in the doorway</em><br />
  Hyphothesis: <em>The person is near the door</em></li>
      <li>input format (roughly):<br />
  <code class="language-plaintext highlighter-rouge">[START]</code> <em>The man is in the doorway</em> <code class="language-plaintext highlighter-rouge">[DELIM]</code> <em>The person is near the door</em> <code class="language-plaintext highlighter-rouge">[EXTRACT]</code><br />
  The linear classifier is applied to the representation of the <code class="language-plaintext highlighter-rouge">[EXTRACT]</code> token.</li>
    </ul>
  </li>
  <li>The amount of task-specific human effort is very low!</li>
</ul>

<h3 id="increasingly-convinsing-generations-gpt2-radford-et-al-2018">Increasingly convinsing generations (GPT2) [<em>Radford et al., 2018</em>]</h3>
<ul>
  <li>We mentioned how pretrained decoders can be used <strong>in their capacities as language models</strong>.<br />
<strong>GPT-2</strong>, a larger version of GPT trained on more data, was shown to produce relatively convincing samples of natural language.</li>
</ul>

<p><img src="/assets/images/cs224n/lec10_7.png" alt="png" width="60%&quot;, height=&quot;100%" /></p>

<h3 id="pretraining-encoders-what-pretraining-objective-to-use">Pretraining Encoders: what pretraining objective to use?</h3>
<p><strong>Encoders get bidirectional context,</strong> so we can’t do language modeling!<br />
$\rightarrow$ <strong>Masked Language Modeling</strong></p>

<p>
	<img src="/assets/images/cs224n/lec10_8.png" alt="png" width="40%" style="float: right" />
	- Idea: replace some fraction of words in the input with a speical `[MASK]` token; predict these words.<br />
	$$\begin{align*}
	h_1, \ldots, h_T &amp;= \text{Encoder}(w_1, \ldots, w_T) \\
	&amp; y_i \sim Aw_i + b
	\end{align*}$$
	- Only add loss terms from words that are "masked out". If $\hat{x}$ is the masked version of $x$, we're learning $p_{\theta}(x\vert \hat{x})$.
</p>

<ul>
  <li>
    <p><em>Devlin et al., 2018</em> proposed the “Masked LM” objective and <strong>released the weights of a pretrained Transformer</strong>, a model they labeled <strong>BERT</strong>.</p>
  </li>
  <li>
    <p>Some more details about Masked LM for BERT:</p>
  </li>
</ul>

<p>
	<img src="/assets/images/cs224n/lec10_9.png" alt="png" width="40%" style="float: right" />
	Predict a random 15% of (sub)word tokens.<br />
	- Replace input word with [MASK] 80% of the time <br />
	- Replace input word with a random token 10% of the time <br />
	- Leave input word unchanged 10% of the time (but still predict it!) <br />
	(So we have three loss terms for example sentence)
</p>

<ul>
  <li>Why?: Doesn’t let the model get complacent and not build strong representations of non-masked words.<br />
  (No masks are seen at fine-tuning time!)</li>
</ul>

<h3 id="bert-bidirectional-encoder-representations-from-transformers">BERT: Bidirectional Encoder Representations from Transformers</h3>
<ul>
  <li>The pretraining input to BERT was two separate contiguous chunks of text:</li>
</ul>

<p><img src="/assets/images/cs224n/lec10_10.png" alt="png" width="100%&quot;, height=&quot;100%" /></p>

<ul>
  <li>Two chunks of text:<br />
  To better pretrain for objectives for downstream applications like QA, where you have two different pieces of text; sometimes actual chunk of text that directly follows the first chunk, or sometimes randomly sampled unrelated.<br />
  $\rightarrow$ BERT was trained to predict whether one chunk follows the other or is randomly sampled.
    <ul>
      <li>Later work has argued this “next sentence prediction” is not necessary.</li>
    </ul>
  </li>
  <li>Details about BERT
    <ul>
      <li>Two models were released:
        <ul>
          <li>BERT-base: 12 layers, 768-dim hidden states, 12 attention heads, 110 million params.</li>
          <li>BERT-large: 24 layers, 1024-dim hidden states, 16 attention heads, 340 million params.</li>
        </ul>
      </li>
      <li>Trained on:
        <ul>
          <li>BooksCorpus (800 million words)</li>
          <li>English Wikipedia (2,500 million words)</li>
        </ul>
      </li>
      <li>Pretraining is expensive and impractical on a single GPU.
        <ul>
          <li>BERT was pretrained with 64 TPU chips for a total of 4 days.</li>
        </ul>
      </li>
      <li>Finetuning is practical and common on a single GPU
        <ul>
          <li>“Pretrain once, finetune many times.”</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>BERT was massively popular and hugely versatile; finetuning BERT led to new state-of-the-art results on a broad range of tasks.</li>
</ul>

<p><img src="/assets/images/cs224n/lec10_11.png" alt="png" width="100%&quot;, height=&quot;100%" /></p>

<h3 id="limitations-of-pretrained-encoders">Limitations of pretrained encoders</h3>
<p>If your task involves generating sequences, consider using a pretrained decoder; BERT and other pretrained encoders don’t naturally lead to nice autoregressive (1-word-at-a-time) generation methods.<br />
<img src="/assets/images/cs224n/lec10_12.png" alt="png" width="100%&quot;, height=&quot;100%" /></p>

<h3 id="extensions-of-bert">Extensions of BERT</h3>
<ul>
  <li>Among a lot of BERT variants, some generally accepted improvements to the BERT pretraining formula:
    <ul>
      <li>RoBERTa: mainly just train BERT for longer and remove next sentence prediction <em>[Liu et al., 2019]</em><br />
  more compute, more data can improve pretraining even when not changing the underlying Transformer encoder.</li>
      <li>SpanBERT: masking contiguous spans of words makes a harder, more useful pretraining task <em>[Joshi et al., 2020]</em></li>
    </ul>
  </li>
</ul>

<p><img src="/assets/images/cs224n/lec10_13.png" alt="png" width="100%&quot;, height=&quot;100%" /></p>

<h3 id="pretraining-encoder-decoders-what-pretraining-objective-to-use">Pretraining Encoder-Decoders: what pretraining objective to use?</h3>
<ul>
  <li>For <strong>encoder-decoders</strong>, we could do something like <strong>language modeling</strong>, but where a prefix of every input is provided to the encoder and is not predicted.</li>
</ul>

<p>
	<img src="/assets/images/cs224n/lec10_14.png" alt="png" width="40%" style="float: right" />
	$$\begin{align*}
	h_1, \ldots, h_T &amp;= \text{Encoder}(w_1, \ldots, w_T) \\
	h_{T+1}, \ldots, h_{2T} &amp;= \text{Decoder}(w_1, \ldots, w_T, h_1, \ldots, h_T \\
	&amp; y_i \sim Aw_i + b, &amp;&amp; i &gt; T
	\end{align*}$$<br />
	The encoder portion benefits from bidirectional context; the decoder portion is used to train the whole model through language modeling.
</p>

<ul>
  <li>What <em>Raffel et al., 2018</em> found to work best was <strong>span corruption.</strong> Their model: <strong>T5.</strong></li>
</ul>

<p><img src="/assets/images/cs224n/lec10_15.png" alt="png" width="100%&quot;, height=&quot;100%" /></p>

<ul>
  <li>
    <p>Unlike BERT, this model doesn’t specify the length of missing token <code class="language-plaintext highlighter-rouge">&lt;X&gt;</code>.</p>
  </li>
  <li>
    <p><em>Raffel et al., 2018</em> found encoder-decoders to work better than decoders for their tasks, and span corruption (denoising) to work better than language modeling.</p>
  </li>
  <li>
    <p>A fascinating property of T5:<br />
  it can be finetuned to answer a wide range of questions, retrieving knowledge from its parameters.</p>
    <ul>
      <li>NQ: Natural Questions</li>
      <li>WQ: WebQuestions</li>
      <li>TQA: Trivia QA</li>
    </ul>

    <p>All “open-domain” versions</p>
  </li>
</ul>

<p><img src="/assets/images/cs224n/lec10_16.png" alt="png" width="100%&quot;, height=&quot;100%" /></p>

<h3 id="what-kinds-of-things-does-pretraining-learn">What kinds of things does pretraining learn?</h3>
<p>There’s increasing evidence that pretrained models learn a wide variety of things about the statistical properties of language. Taking our examples from the start of class:</p>
<ul>
  <li><em>Stanford University is located in <strong>__</strong>____, California.</em> [Trivia]</li>
  <li><em>I put ___ fork down on the table.</em> [syntax]</li>
  <li><em>The woman walked across the street, checking for traffic over ___ shoulder.</em> [coreference]</li>
  <li><em>I went to the ocean to see the fish, turtles, seals, and _____.</em> [lexical semantics/topic]</li>
  <li><em>Overall, the value I got from the two hours watching it was the sum total of the popcorn and the drink. The movie was ___.</em> [sentiment]</li>
  <li><em>Iroh went into the kitchen to make some tea. Standing next to Iroh, Zuko pondered his destiny. Zuko left the <strong>__</strong>.</em> [some reasoning – this is harder]</li>
  <li><em>I was thinking about the sequence that goes 1, 1, 2, 3, 5, 8, 13, 21, ____</em> [some basic arithmetic; they don’t learn the Fibonnaci sequence]</li>
  <li>Models also learn – and can exacerbate racism, sexism, all manner of bad biases.</li>
  <li>More on all this in the interpretability lecture!</li>
</ul>

<h3 id="gpt-3-in-context-learning-and-very-large-models">GPT-3, In-context learning, and very large models</h3>
<ul>
  <li>So far, we’ve interacted with pretrained models in two ways:
    <ul>
      <li>Sample from the distributions they define (maybe providing a prompt)</li>
      <li>Fine-tune them on a task we care about, and take their predictions.</li>
    </ul>
  </li>
  <li>Very large language models seem to perform some kind of learning <strong>without gradient steps</strong> simply from examples you provide within their contexts.<br />
  The in-context examples seem to specify the task to be performed, and the conditional distribution mocks performing the task to a certain extent.
    <ul>
      <li><strong>Input (prefix within a single Transformer decoder context):</strong><br />
  thanks -&gt; merci<br />
  hello -&gt; bonjour<br />
  mint -&gt; menthe<br />
  otter -&gt; ?</li>
      <li><strong>Output (conditional generations):</strong><br />
  loutre…
  <img src="/assets/images/cs224n/lec10_17.png" alt="png" width="100%&quot;, height=&quot;100%" /></li>
    </ul>
  </li>
  <li>
    <p>GPT-3 is the canonical example of this. The largest T5 model had 11 billion parameters but <strong>GPT-3 has 175 billion parameters.</strong></p>
  </li>
  <li>Remarks<br />
  These models are still not well-understood.<br />
  “Small” models like BERT have become general tools in a wide range of settings.</li>
</ul>


                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
	<!--
            
                <section class="subscribe-form">
                    <h3 class="subscribe-form-title">Subscribe to Darron's Devlog</h3>
                    <p>Get the latest posts delivered right to your inbox</p>
                    <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
                </section>
            
	-->
            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            var this_page_url = 'http://0.0.0.0:4000/cs224n_lec10';
                            var this_page_identifier = '/cs224n_lec10';
                            var this_page_title = 'cs224n - Lecture 10. Transformers and Pretraining';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            


        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/built/images/blog-cover1.png)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; Darron's Devlog &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/cs224n/">Cs224n</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs224n_lec9">cs224n - Lecture 9. Self-Attention and Transformers</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs224n_lec8">cs224n - Lecture 8. Attention (Contd.)</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs224n_lec7">cs224n - Lecture 7. Translation, Seq2Seq, Attention</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/cs224n/">
                                
                                    See all 9 posts  →
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                    <article class="post-card post-template no-image">
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/cs224n_lec9">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Cs224n</span>
                            
                        
                    

                    <h2 class="post-card-title">cs224n - Lecture 9. Self-Attention and Transformers</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>So far: recurrent models for (most) NLP - Circa 2016, the de facto strategy in NLP is to encode sentences with a bidirectional LSTM. (for example, the source sentence in a translation) -</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="/">
            
            <span>Darron's Devlog</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">cs224n - Lecture 10. Transformers and Pretraining</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=cs224n+-+Lecture+10.+Transformers+and+Pretraining&amp;url=https://12kdh43.github.io/cs224n_lec10"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://12kdh43.github.io/cs224n_lec10"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Darron's Devlog</a> &copy; 2022</section>
                <!-- 
				<section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                -->
				<nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search Darron's Devlog</h1>
                <p class="subscribe-overlay-description">
				</p>
                <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

 </script>

	
    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
