<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
	
	<!-- On Post front-matter YAML, set "use_math: true" to use LaTex -->
	
	  
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
        equationNumbers: {
        autoNumber: "AMS"
        }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$', '$$'], ["\\[","\\]"]  ],
    processEscapes: true,
    }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>ISLR - Chapter 4. Classification</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
	
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="" />
    <link rel="shortcut icon" href="http://0.0.0.0:4000/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="http://0.0.0.0:4000/islr_ch4" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Darron's Devlog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="ISLR - Chapter 4. Classification" />
    <meta property="og:description" content="Chapter 4. Classification 4.1. Overview of Classification 4.2. Why Not Linear Regression? 4.3. Logistic Regression 4.3.1. The Logistic Model 4.3.2. Estimating the Regression Coefficients: MLE 4.3.3. Multinomial Logistic Regression 4.4. Generative Models for Classification 4.4.1. LDA: Linear Discriminant Analysis for $p = 1$ 4.4.2. LDA: Linear Discriminant Analysis for $p" />
    <meta property="og:url" content="http://0.0.0.0:4000/islr_ch4" />
    <meta property="og:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2020-04-15T15:00:00+00:00" />
    <meta property="article:modified_time" content="2020-04-15T15:00:00+00:00" />
    <meta property="article:tag" content="ISLR" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="ISLR - Chapter 4. Classification" />
    <meta name="twitter:description" content="Chapter 4. Classification 4.1. Overview of Classification 4.2. Why Not Linear Regression? 4.3. Logistic Regression 4.3.1. The Logistic Model 4.3.2. Estimating the Regression Coefficients: MLE 4.3.3. Multinomial Logistic Regression 4.4. Generative Models for Classification 4.4.1. LDA: Linear Discriminant Analysis for $p = 1$ 4.4.2. LDA: Linear Discriminant Analysis for $p" />
    <meta name="twitter:url" content="http://0.0.0.0:4000/" />
    <meta name="twitter:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Darron's Devlog" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="ISLR" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Darron's Devlog",
        "logo": "http://0.0.0.0:4000/"
    },
    "url": "http://0.0.0.0:4000/islr_ch4",
    "image": {
        "@type": "ImageObject",
        "url": "http://0.0.0.0:4000/assets/built/images/blog-cover1.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://0.0.0.0:4000/islr_ch4"
    },
    "description": "Chapter 4. Classification 4.1. Overview of Classification 4.2. Why Not Linear Regression? 4.3. Logistic Regression 4.3.1. The Logistic Model 4.3.2. Estimating the Regression Coefficients: MLE 4.3.3. Multinomial Logistic Regression 4.4. Generative Models for Classification 4.4.1. LDA: Linear Discriminant Analysis for $p = 1$ 4.4.2. LDA: Linear Discriminant Analysis for $p"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="ISLR - Chapter 4. Classification" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="/">Darron's Devlog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-projects" role="menuitem"><a href="/tag/projects/">Projects</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/studies/">Studies</a></li>
	<li class="nav-blog" role="menuitem"><a href="/tag/blog/">Blog</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  tag-islr  no-image">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="15 April 2020">15 April 2020</time>
                    
                        <span class="date-divider">/</span>
                        
							
                            
                               <a href='/tag/islr/'>ISLR</a>
                            
                        
                    
                </section>
                <h1 class="post-full-title">ISLR - Chapter 4. Classification</h1>
            </header>
	<!--
            
	-->
            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <ul id="markdown-toc">
  <li><a href="#chapter-4-classification" id="markdown-toc-chapter-4-classification">Chapter 4. Classification</a></li>
  <li><a href="#41-overview-of-classification" id="markdown-toc-41-overview-of-classification">4.1. Overview of Classification</a></li>
  <li><a href="#42-why-not-linear-regression" id="markdown-toc-42-why-not-linear-regression">4.2. Why Not Linear Regression?</a></li>
  <li><a href="#43-logistic-regression" id="markdown-toc-43-logistic-regression">4.3. Logistic Regression</a>    <ul>
      <li><a href="#431-the-logistic-model" id="markdown-toc-431-the-logistic-model">4.3.1. The Logistic Model</a></li>
      <li><a href="#432-estimating-the-regression-coefficients-mle" id="markdown-toc-432-estimating-the-regression-coefficients-mle">4.3.2. Estimating the Regression Coefficients: MLE</a></li>
      <li><a href="#433-multinomial-logistic-regression" id="markdown-toc-433-multinomial-logistic-regression">4.3.3. Multinomial Logistic Regression</a></li>
    </ul>
  </li>
  <li><a href="#44-generative-models-for-classification" id="markdown-toc-44-generative-models-for-classification">4.4. Generative Models for Classification</a>    <ul>
      <li><a href="#441-lda-linear-discriminant-analysis-for-p--1" id="markdown-toc-441-lda-linear-discriminant-analysis-for-p--1">4.4.1. LDA: Linear Discriminant Analysis for $p = 1$</a></li>
      <li><a href="#442-lda-linear-discriminant-analysis-for-p--1" id="markdown-toc-442-lda-linear-discriminant-analysis-for-p--1">4.4.2. LDA: Linear Discriminant Analysis for $p &gt; 1$</a></li>
      <li><a href="#443-qda-quadratic-discriminant-analysis" id="markdown-toc-443-qda-quadratic-discriminant-analysis">4.4.3. QDA: Quadratic Discriminant Analysis</a></li>
      <li><a href="#444-naive-bayes" id="markdown-toc-444-naive-bayes">4.4.4. <em>Naive Bayes</em></a></li>
    </ul>
  </li>
  <li><a href="#45-a-comparison-of-classification-methods" id="markdown-toc-45-a-comparison-of-classification-methods">4.5. A Comparison of Classification Methods</a>    <ul>
      <li><a href="#451-an-analytical-comparison" id="markdown-toc-451-an-analytical-comparison">4.5.1. An Analytical Comparison</a></li>
      <li><a href="#452-an-empirical-comparison" id="markdown-toc-452-an-empirical-comparison">4.5.2. An Empirical Comparison</a></li>
    </ul>
  </li>
  <li><a href="#46-generalized-linear-models" id="markdown-toc-46-generalized-linear-models">4.6. Generalized Linear Models</a>    <ul>
      <li><a href="#461-linear-regression-problems" id="markdown-toc-461-linear-regression-problems">4.6.1. Linear Regression Problems</a></li>
      <li><a href="#462-poisson-regression" id="markdown-toc-462-poisson-regression">4.6.2. Poisson Regression</a></li>
      <li><a href="#463-generalized-linear-models-in-greater-generality" id="markdown-toc-463-generalized-linear-models-in-greater-generality">4.6.3. Generalized Linear Models in Greater Generality</a></li>
    </ul>
  </li>
</ul>

<p>2020-04-15</p>

<h2 id="chapter-4-classification">Chapter 4. Classification</h2>

<h2 id="41-overview-of-classification">4.1. Overview of Classification</h2>
<ul>
  <li>Response <em>Y</em> is <em>qualitative(categorical)</em> variable.<br />
Predicting a qualitative response, <em>classification</em></li>
</ul>

<h2 id="42-why-not-linear-regression">4.2. Why Not Linear Regression?</h2>
<ul>
  <li>Problem of linear regression in classification<br />
e.g. in binary response group: <em>Y</em> = 0 or 1<br />
Model prediction: $\hat y_i = x_i^T \hat\beta$; $\quad$ where $\beta$ is LSE<br />
Decision rule: if $ \hat y_i &gt; 0.5 $, classify <em>i</em>th obs. to group 1<br />
BUT, possible value of estimates outside the interval,<br />
$ \hat Y &gt;1 \text{ or } \hat Y &lt;0 $: do not fit for the range of <em>Y</em></li>
</ul>

<h2 id="43-logistic-regression">4.3. Logistic Regression</h2>

<h3 id="431-the-logistic-model">4.3.1. The Logistic Model</h3>
<ul>
  <li>
    <p>Supoose that <em>Y</em> has 2 classes where<br />
$p(X)=P(Y=1|X)$, $X=(1,X_1, \ldots, X_p)^T$, $\beta=(\beta_0,\beta_1,\ldots, \beta_p)^T$</p>
  </li>
  <li>
    <p>\(p(X)= \frac{e^{x^T\beta}}{1+e^{x^T\beta}}\): <em>logistic function</em><br />
\(0 &lt; \frac{p(X)}{1-p(X)} = e^{x^T\beta} &lt; \infty\): <em>odds</em> relative to range of $p(X)= [0,1]$<br />
\(\log\frac{p(X)}{1-p(X)} = X^T \beta\): <em>log odds</em> or <em>logit</em></p>
  </li>
</ul>

<h3 id="432-estimating-the-regression-coefficients-mle">4.3.2. Estimating the Regression Coefficients: MLE</h3>
<ul>
  <li>
    <p>as $\log\frac{p(X)}{1-p(X)} = X^T \beta $, $ p(X) = \frac{e^{x^T\beta}}{1+e^{x^T\beta}} $, $ e^{x^T\beta} = \frac{p(X)}{1-p(X)}$<br />
where $Y_i | X_i \sim$ Bernoulli($p(X_i)$),<br />
$\rightarrow \prod_{i=1}^N p(x_i)^{y_i} (1-p(x_i))^{1-y_i}$<br />
<em>likelihood function</em> L = $\prod_{i:y_i=1}p(x_i) \prod_{i:y_i=0}(1-p(x_i))$</p>
  </li>
  <li>
    <p><em>Maximum Likelihood Estimation</em> <strong>MLE</strong>:<br />
\(\hat\beta = \text{max}_\beta [\prod_{i:y_i=1} \frac{e^{x^T\beta}}{1+e^{x^T\beta}} \prod_{i:y_i=0} \frac{1}{1+e^{x^T\beta}}]\)<br />
take log and get <em>log-likelihood function</em> $l(\beta)$ and least squares $\hat\beta$</p>
  </li>
  <li>
    <p>But we cannot find analytic solution for $\frac{\partial l(\beta)}{\partial\beta} = 0$,<br />
use <em>Newton-Raphson method</em>, a numerical method to find $\hat\beta$.</p>
  </li>
  <li>
    <p>if $\hat\beta &gt;0$ , <em>p(X)</em> increases by a unit change in <em>X</em><br />
if $\hat\beta &lt;0$ , <em>p(X)</em> decreases by a unit change in <em>X</em><br />
(not a linear relationship, but a direction)</p>
  </li>
</ul>

<h3 id="433-multinomial-logistic-regression">4.3.3. Multinomial Logistic Regression</h3>
<ul>
  <li>
    <p>$P(Y=k | X=x) = \frac{e^{x^T\beta_k}}{1+\sum_{j=1}^{K-1} e^{x^T\beta_j}}$<br />
$P(Y=K | X=x)= \frac 1{1+\sum_{j=1}^{K-1} e^{x^T\beta_j}}$</p>
  </li>
  <li>
    <p>$\log\frac{P(Y=k | X=x)}{P(Y=K | X=x)} = x^T\beta_k $<br />
for $ k=1,\cdots, K-1$ with $\sum_{k=1}^K P(Y=k | X=x) = 1 $<br />
the choice of denominator the $K_{th}$ class is arbitary</p>
  </li>
  <li>
    <p>ML estimation (by numerical method):<br />
we can have <em>K-1</em> equations and <em>K-1</em> number of $\hat\beta_k$ (MLE)</p>
  </li>
</ul>

<h2 id="44-generative-models-for-classification">4.4. Generative Models for Classification</h2>
<ul>
  <li>
    <p>By logistic regression directly modeling $Pr(Y=K | X=x)$,<br />
in statistical jargon, we model the conditional distribution<br />
of the response <em>Y</em>, given the predictor(s) <em>X</em>.</p>
  </li>
  <li>then Why do we need another method?
    <ul>
      <li>when there is substantial separation between 2 classes and logistic model’s parameter estimates are unstable</li>
      <li>when <em>X</em> approximately follow normal distribution and the sample size is small</li>
      <li>in the case of more than two response classes</li>
    </ul>
  </li>
  <li>
    <p>In Generative approach, we model the distribution of the predictors <em>X</em> separately 
to the response class then use Bayes’ theorem to get $Pr(Y=K | X=x)$.<br />
if <em>X</em> for each <em>Y</em>s assumed to follow the normal distribution,<br />
model has very similar form to logistic regression.</p>
  </li>
  <li>
    <p>Suppose that we classify an obs. into one of <em>K</em>$\ge 2 $ classes<br />
$f_k(x)$; is the density funtion of <em>X</em> in class <strong><em>k</em></strong>,<br />
$\pi_k$; is the prior probability of class <strong><em>k</em></strong> with $\sum_{k=1}^K \pi_k =1$</p>
  </li>
  <li>Bayes’ Theorem<br />
$ Pr(Y=k | X=x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}$<br />
$ Pr(Y=k | X=x) $ is the <em>posterior</em> probability of obs. at <em>x</em> belongs to <em>k</em></li>
</ul>

<h3 id="441-lda-linear-discriminant-analysis-for-p--1">4.4.1. LDA: Linear Discriminant Analysis for $p = 1$</h3>
<ul>
  <li>Assumptions:<br />
a <em>Gaussian</em> or <em>normal</em> distribution $f_k(x)$
    <ul>
      <li>$f_k(x) = \frac{1}{\sqrt{2\pi}\sigma_k}exp\left(-\frac{1}{2\sigma_k^2}(x-\mu_k)^2\right)$</li>
      <li>$\mu_k$ is the mean for the <em>k</em>th class</li>
      <li>$\sigma_k^2$ is the shared variance across for all <em>K</em> classes(=$\sigma^2$)</li>
      <li>$p_k(x) = \frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{1}{2\sigma^2}(x-\mu_k)^2\right)}
              {\sum_{l=1}^K \pi_l \frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{1}{2\sigma^2}(x-\mu_l)^2\right)}$</li>
    </ul>
  </li>
  <li>
    <p>Bayes Classifier assigns an observation <em>X = x</em> to the class<br />
having lagest value of: \(\delta_k(x) = x\cdot\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)\)</p>
  </li>
  <li>
    <p>But in real-life situation,<br />
such assumptions are not enough to apply the Bayes classifier.<br />
have to estimate the parameters $\mu_1,\ldots,\mu_K,\ \pi_1,\ldots,\pi_K$ and $\sigma^2$<br />
$\rightarrow$ LDA method used to approximate the Bayes classifier.</p>
  </li>
  <li>
    <p>LDA estimates model parameters:<br />
$\hat\pi_k = n_k/n$<br />
$\hat\mu_k = \sum_{i:y_i=k}x_i / n_k $<br />
\(\begin{align*}\hat\sigma_k^2 = \sum_{k=1}^K\sum_{i:y_i=k}(x_i-\hat\mu_k) / (n-K) \end{align*}\)</p>
  </li>
  <li>LDA classifier assigns an observation <em>X = x</em> to the class<br />
having lagest value of <em>discriminant function</em>: \(\hat\delta_k(x) = x\cdot\frac{\hat\mu_k}{\hat\sigma^2} - \frac{\hat\mu_k^2}{2\hat\sigma^2} + \log(\hat\pi_k)\)</li>
</ul>

<h3 id="442-lda-linear-discriminant-analysis-for-p--1">4.4.2. LDA: Linear Discriminant Analysis for $p &gt; 1$</h3>
<ul>
  <li>Assumptions:<br />
<em>X</em> from a <em>multivariate Gaussian</em>(or <em>multivariate normal</em>) distribution,<br />
where each individual predictor follows a one-dimensional normal distribution,<br />
with a class-specific mean vector and a common covariance matrix
    <ul>
      <li>$f_k(x) ~ \text{MVN}(\mu_k,\Sigma_k)$</li>
      <li>$\mu_k$ is a $p \times 1$ vector, the mean for the <em>k</em>th class</li>
      <li>$\Sigma_k = \Sigma$ is a $p \times p$ covariance matrix for all <em>K</em> classes</li>
    </ul>
  </li>
  <li>
    <p>Find <strong><em>k</em></strong> maximizing \(\log P(Y=k|X=x)\):<br />
\(\begin{align*} \log P(Y=k|X=x) &amp;= \log\frac{f_k(x)\pi_k}{\sum_{j=1}^K f_j(x)\pi_j}\\
                    			   &amp;= \log(f_k(x)\pi_k) + C_1 \\
                                 &amp;= \cdots \\
                                 &amp;= \log\pi_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + x^T\Sigma^{-1}\mu_k + C_3\\
                                 &amp;= \delta_k(x) + C_3 \cdots \scriptstyle\text{ : Linear discriminant function (linear in X)} \\
              \Rightarrow \hat Y &amp;= \text{argmax}_k \delta_k(x)
\end{align*}\)</p>
  </li>
  <li>
    <p>Decision boundary is also in a linear form.<br />
when solving <em>x</em>: $\delta_k(x) = \delta_l(x)$, we can have linear form of decision boundary.</p>
  </li>
  <li>
    <p>LDA estimates model parameters:<br />
$\hat\pi_k = n_k/n$<br />
$\hat\mu_k = \sum_{i:y_i=k}x_i / n_k $<br />
\(\begin{align*}\hat\Sigma_k = \sum_{k=1}^K\sum_{y_i=k}(x_i-\hat\mu_k)(x_i-\hat\mu_k)^T / (n-K) \end{align*}\)</p>
  </li>
  <li>
    <p>ROC <em>curve</em>, <em>receiver operating characteristics</em><br />
to display two types of errors for all possible thresholds<br />
useful for comparing different classifiers<br />
<img src="/assets/images/ch4_roc_curve_0.png" alt="image.png" width="70%" height="70%" /></p>
  </li>
  <li>
    <p>AUC, <em>area under the (ROC) curve</em><br />
overall performance of a classifier summarized over all possible thresholds</p>
  </li>
  <li><em>sensitivity</em> and <em>specificity</em>, from <em>confusion matrix</em><br />
changes of true positive and false positive rate by varying the classifier threshold</li>
</ul>

<p><img src="/assets/images/ch4_confusion_matrix_0.png" alt="image.png" width="70%" height="70%" /><br />
<img src="/assets/images/ch4_confusion_matrix_1.png" alt="image.png" width="70%" height="70%" /></p>

<h3 id="443-qda-quadratic-discriminant-analysis">4.4.3. QDA: Quadratic Discriminant Analysis</h3>
<ul>
  <li>
    <p>Assumption: Same as LDA, but “$\Sigma_k \ne \Sigma$” for all <em>K</em> classes</p>
  </li>
  <li>
    <p>Quadratic discriminant function:<br />
$\delta_k(x)=  \log\pi_k - \frac{1}{2}\log| \Sigma_k | - \frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) $</p>
  </li>
  <li>
    <p>Decision boundary is in quadratic line.</p>
  </li>
  <li>
    <p>Estimation of parameters:<br />
$\hat\pi_k = n_k/n$<br />
$\hat\mu_k = \sum_{y_i=k}x_i / n_k $<br />
\(\begin{align*}\hat\Sigma_k =\sum_{y_i=k}(x_i-\hat\mu_k)(x_i-\hat\mu_k)^T / (n-1) \end{align*}\) (only obs. in $k_{th}$ group)</p>
  </li>
  <li>LDA vs. QDA: the bias-variance trade-off
    <ul>
      <li>LDA<br />
for <em>p</em> predictors, covariance matrix requires estimating $p(p+1)/2$ parameters.<br />
linear model in <em>x</em>, which means there are $Kp$ linear coefficients<br />
less flexible classifier having lower variance and potential of improved prediction performance<br />
but with assumption of shared variance, LDA can suffer from high bias<br />
when there are relatively few training observations and reducing variance is crucial</li>
      <li>QDA<br />
for <em>p</em> predictors, estimating seperate covariance matrix requires estimating $Kp(p+1)/2$ parameters.<br />
when there are very large training set, the variance of the classifier is not a major concern<br />
or there is no common covariance matrix for the K classes</li>
    </ul>
  </li>
  <li>LDA vs. Logistic Regression
    <ul>
      <li>LDA:<br />
MVN assumption needed<br />
Qualitative(categorical) inputs are not available</li>
      <li>Logistic:<br />
No assumption needed and <em>Y ~ Bernoulli</em> is evident; $\rightarrow$ robust model<br />
Qualitative are available<br />
can run F-test to choose more important features</li>
    </ul>
  </li>
</ul>

<h3 id="444-naive-bayes">4.4.4. <em>Naive Bayes</em></h3>
<ul>
  <li>
    <p>Bayes’ theorem expands posterior prob. $p_k(x) = Pr(Y=k|X=x)$ with two terms:<br />
$\pi_k$ is a prior prob. that an observation belongs to <em>k</em>th class<br />
$f_k(x)$ is a <em>p</em>-dim. density function for an observation in the <em>k</em>th class</p>
  </li>
  <li>
    <p>Estimations:<br />
$\hat\pi_k$ as the proportion of training observations belonging to the <em>k</em>th class.<br />
$\hat{f}_k(x)$ is more challenging, as we must consider both <em>marginal</em> and <em>joint
distribution</em> of predictors, in a multivariate normal distribution, the joint distribution
is summarized by the off-diagonal elements of the covariance matrix, but this association
is still hard to be characterized.</p>
  </li>
  <li>to simplifies task:
    <ul>
      <li>LDA assumes,<br />
MVN distribution $f_k(x)$ with class-specific mean and shared variance</li>
      <li>QDA assumes,<br />
MVN distribution $f_k(x)$ with class-specific mean and class-specific variance</li>
      <li>naive Bayes assumes,<br />
<em>Within the $k$th class, the $p$ predictors are independent</em>, or,<br />
$f_k(x) = f_{k1}(x_1) \times f_{k2}(x_2) \times \cdots \times f_{kp}(x_p)$ where<br />
$f_{kj}$ is the density function of the <em>j</em>th predictor among observations in the <em>k</em>th class.<br />
a simple assumption that there is <em>NO</em> association between the predictors</li>
    </ul>
  </li>
  <li>
    <p>the posterior probability $p_k(x)$<br />
$Pr(Y=k|X=x) = \frac{\pi_k\times f_{k1}(x_1)\times f_{k2}(x_2)\times\cdots\times f_{kp}(x_p)}
                    {\sum_{l=1}^K\pi_l}\times f_{l1}(x_1)\times f_{l2}(x_2)\times\cdots\times f_{lp}(x_p)$</p>
  </li>
  <li>To estimate the one-dimensional density function $f_{kj}$:
    <ul>
      <li>for quantitative $X_j$, we can assume that $X_j|Y = k \sim N(\mu_{jk}, \sigma_{jk}^2)$
within each class, the <em>j</em>th predictor $X_j$ is drawn from a normal distribution</li>
      <li>for quantitative $X_j$, or, we use a non-parametric estimate for $f_{kj}$
estimate $f_{kj}(x_j)$ as the fraction of the training observations in the <em>k</em>th class
that belong to the same histogram bin as $x_j$,
or we can use a <em>kernel density estimator</em> instead of histogram</li>
      <li>for qualitative $X_j$, we can simply count the proportion of training observations
for the <em>j</em>th predictor corresponding to each class.</li>
    </ul>
  </li>
</ul>

<h2 id="45-a-comparison-of-classification-methods">4.5. A Comparison of Classification Methods</h2>

<h3 id="451-an-analytical-comparison">4.5.1. An Analytical Comparison</h3>
<ul>
  <li><em>analytical</em> (or mathematical) comparison of LDA, QDA, naive Bayes, and logistic regression<br />
in a setting <em>K</em> as the <em>baseline</em> class and classify to the class that maximizes<br />
\(log\left(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}\right)\)
    <ul>
      <li>LDA:<br />
\(\begin{align*}
\log\left(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}\right)
&amp;= \log\left(\frac{\pi_k f_k(x)}{\pi_K f_K(x)}\right) \\
&amp;= \log\left(\frac{\pi_k exp(-\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k))}
                 {\pi_K exp(-\frac{1}{2}(x-\mu_K)^T\Sigma^{-1}(x-\mu_K))}\right) \\
&amp;= \log\left(\frac{\pi_k}{\pi_K}\right) - \frac{1}{2}(\mu_k+\mu_K)^T\Sigma^{-1}(\mu_k-\mu_K) \\
&amp;\quad + x^T\Sigma^(\mu_k-\mu_K)\\
&amp;= a_k + \sum_{j=1}^p b_{kj}x_j,
\end{align*}\)<br />
where $b_jk$ is the <em>j</em>th component of $\Sigma^{-1}(\mu_k-\mu_K)$</li>
      <li>QDA:<br />
\(\log\left(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}\right)
= a_k + \sum_{j=1}^p b_{kj}x_j + \sum_{j=1}^p\sum_{l=1}^p c_{kjl}x_j x_l\)</li>
      <li>naive Bayes:<br />
\(\begin{align*}
\log\left(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}\right)
&amp;= \log\left(\frac{\pi_k f_k(x)}{\pi_K f_K(x)}\right) \\
&amp;= \log\left(\frac{\pi_k\prod_{j=1}^p f_{kj}(x_j)}
                 {\pi_K\prod_{j=1}^p f_{Kj}(x_j)}\right) \\
&amp;= \log\left(\frac{\pi_k}{\pi_K}\right)
    + \sum_{j=1}^p\log\left(\frac{f_{kj}(x_j)}{f_{Kj}(x_j)}\right) \\
&amp;= a_k + \sum_{j=1}^p g_{kj}(x_j)
\end{align*}\)<br />
which takes the form of a <em>generalized additive model</em></li>
      <li>Logistic Regression:<br />
\(\log\left(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}\right)
= \beta_{k0} + \sum_{j=1}^p\beta_{kj}x_j\)</li>
    </ul>
  </li>
  <li>LDA is a special case of QDA with $c_{kjl}=0$ and including any classifier 
with linear decision boundary, is also a special case of naive Bayes with 
$g_{kj}(x_j) = b_{kj}(x_j)$,<br />
if in the naive Bayes classifier, we model $f_{kj}(x_j)$ using a one-dimensional 
Gaussian distribution $N(\mu_{kj},\sigma_j^2)$, we get $g_{kj}(x_j) = b_{kj}(x_j)$ 
where $b_{kj} = (\mu_{kj}-\mu_{Kj})/{\sigma_j^2}$. Then naive Bayes is a 
special case of LDA with variance restriced to be a diagonal matrix with 
<em>j</em>th diagonal element equal to $\sigma_j^2$.</li>
  <li>Neither QDA nor naive Bayes is a special case of the other.<br />
Navie Bayes is in a additive fit, a function of $x_j$ can be added but 
never multipied. By contrast, QDA takes terms of $c_{kjl}x_j x_l$.</li>
  <li>both LDA and Logistic are linear function of the predictors. Predictors 
in LDA follow a normal distribution, while the coefficients in Logistic 
are chosen to maximize the likelihood function. LDA outperforms when the 
normality assumption holds and vice versa for Logistic.</li>
  <li>KNN outperforms LDA and Logistic when the decision boundary is highly non-linear 
provided with large <em>n</em> obs. to small <em>p</em>. KNN tends to reduce bias while 
incurring a lot of variance.</li>
  <li>QDA may be preferrend to KNN when decision boundary is non-linear but <em>n</em> 
is only modest, or <em>p</em> is not very small. Taking a parametric form, it 
requires a smaller sample size for accurate classification relative to KNN.</li>
</ul>

<h3 id="452-an-empirical-comparison">4.5.2. An Empirical Comparison</h3>
<ul>
  <li>
    <p><em>empirical</em> (practical) comparison of classification performances,<br />
we can draw boxplots of test error rates.</p>
  </li>
  <li>
    <p>shortly, no one method will dominate the others in every situation.<br />
LDA and Logistic will tend to perform well when decision boundaries are 
linear. QDA or naive Bayes may be better in non-linear decision boundaries. 
KNN, non-parametric one is good for much more complicated decision boundaries.</p>
  </li>
  <li>
    <p>Like in regression using transformations of the predictors, including powers 
of predictors, we could create a more flexible logistic regression or LDA 
model. Adding quadratic terms and cross-products to LDA, we can have a model 
at somewhere between an LDA and a QDA.</p>
  </li>
</ul>

<h2 id="46-generalized-linear-models">4.6. Generalized Linear Models</h2>
<ul>
  <li>when <em>Y</em> is neither qualitative nor quantitative;<br />
a <em>discrete</em> variable <em>Y</em> takes on non-negative integer values, or <em>counts</em><br />
and treat predictors <em>X</em> as qualitative variables.</li>
</ul>

<h3 id="461-linear-regression-problems">4.6.1. Linear Regression Problems</h3>
<ul>
  <li>a linear regression model seems to provide reasonable and intuitive results on 
those <em>Y</em>, for example, compared to the clear weather as baseline, there should be 
negative coefficients to predict bike users on cloudy days and some lower values 
on rainy days.<br />
but there are some problems left in a linear model.</li>
</ul>

<ol>
  <li>
    <p>Negative fitted values relative to non-negative response <em>Y</em>. It raise concerns about 
 the accuracy of model; coefficient estimates, confidence intervals, and other outputs</p>
  </li>
  <li>
    <p>Mean-variance violates the assumptions of linear model. We assumed error term $\epsilon$ to 
 have zero mean and constant value of variance $\sigma^2$, but in practice, the variance of 
 $Y_i$ has some definite relationship to the expectation of $Y_i$. This problem is from the 
 heteroscedasticity of the data.</p>
  </li>
</ol>

<ul>
  <li>A solution: transformed response $\log(Y) = \sum_{j=1}^p X_j\beta_j + \epsilon $<br />
It can avoids the possibility of negative predictions and overcomes much of the heteroscedasticity. 
But there may be a challenge in interpretation and in the fact that log transformation cannot 
be applied to the responses with the value of zero.<br />
Thus, we introduce a <em>Poisson Regression</em>.</li>
</ul>

<h3 id="462-poisson-regression">4.6.2. Poisson Regression</h3>
<ul>
  <li>
    <p>For a random variable <em>Y</em> takes on nonnegative integer values,<br />
introduces $\lambda&gt;0, \lambda = E(Y) = Var(Y)$;<br />
Poisson distribution $Pr(Y=k) = \frac{e^{-\lambda}\lambda^k}{k!}$</p>
  </li>
  <li>
    <p>a function of $ E(Y) = \lambda $ of the covariates $X_j$:<br />
$\log(\lambda(X_1,\ldots,X_p)) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p$ or<br />
$\lambda(X_1,\ldots,X_p) = e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}$<br />
predicting <em>mean value of Y</em> in <em>particular conditions(values)</em> of predictor $X_j$s</p>
  </li>
  <li>
    <p>likelihood $l(\beta_0,\beta_1,\ldots,\beta_p)
  = \prod_{i=1}^n\frac{e^{-\lambda(x_i)}\lambda(x_i)^{y_i}}{y_i !}$<br />
where $\lambda(x_i) = e^{\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}}$<br />
By solving MLE in this likelihood function, we can have similar fit to the response,<br />
while we have more statistical significance of coefficients.</p>
  </li>
  <li>
    <p>Some important distinctions from the linear regression model.</p>
    <ol>
      <li>Interpretation: a one-unit change in $X_j$ is associated with the change by a factor of 
  $ exp(\beta_j) $, or $ e^{\beta_j} $ in E(Y) = $\lambda$.</li>
      <li>Mean-variance relationship: under the introduced term $\lambda = E(Y) = Var(Y)$, the 
  Poisson model is more adequate to the data with heteroscedasticity.</li>
      <li>No nonnegative fitted values</li>
    </ol>
  </li>
</ul>

<h3 id="463-generalized-linear-models-in-greater-generality">4.6.3. Generalized Linear Models in Greater Generality</h3>
<ul>
  <li>Common characteristics of Linear, Logistic, Poisson Regressions:
    <ol>
      <li>Use <em>p</em> predictors $X_j$ to predict a response <em>Y</em> and assume that conditional $P(Y|X)$ 
  belongs to a certain family of distributions(<em>Gaussian, Bernoulli, Poisson</em> distribution).<br />
  $*$which are in exponential family(with <em>exponential, Gamma, negative binomial distributions</em>)</li>
      <li>Modeling the mean of <em>Y</em>, or expected, $E(Y)$ as a function of predictors <em>X</em>.<br />
  $\text{in Linear: } E(Y|X_1,\ldots,X_p) = \beta_0 + \beta_1 X_1, + \cdots + \beta_p X_p$<br />
  \(\begin{align*} 
  \text{in Logistic: } E(Y|X_1,\ldots,X_p) &amp;= Pr(Y=1|X_1,\ldots,X_p) \\
                                        &amp;= \frac{e^{\beta_0 + \beta_1 X_1, + \cdots + \beta_p X_p}}
                                           {1+e^{\beta_0 + \beta_1 X_1, + \cdots + \beta_p X_p}}
  \end{align*}\)<br />
  \(\begin{align*}
  \text{in Poisson: } E(Y|X_1,\ldots,X_p) &amp;= \lambda(X_1,\ldots,X_p) \\
                                       &amp;= e^{\beta_0 + \beta_1 X_1, + \cdots + \beta_p X_p}
  \end{align*}\)<br />
  which can be expressed using a <em>link function</em>, $\eta$, applying a transformation that 
  $\eta(E(Y|X_1,\ldots,X_p)) = \beta_0 + \beta_1 X_1, + \cdots + \beta_p X_p$<br />
  and each $\eta(\mu)$ is $\mu, \log(\mu/(1-\mu)), \log(\mu)$; respectively to models.</li>
    </ol>
  </li>
  <li>GLM, a general recipe of regression approach:<br />
with the response <em>Y</em> from one of the exponential family, transform expected <em>Y</em>(or mean) as 
a linear function of the predictors, we can have a regression model.</li>
</ul>

                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
	<!--
            
                <section class="subscribe-form">
                    <h3 class="subscribe-form-title">Subscribe to Darron's Devlog</h3>
                    <p>Get the latest posts delivered right to your inbox</p>
                    <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
                </section>
            
	-->
            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            var this_page_url = 'http://0.0.0.0:4000/islr_ch4';
                            var this_page_identifier = '/islr_ch4';
                            var this_page_title = 'ISLR - Chapter 4. Classification';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            


        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/built/images/blog-cover1.png)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; Darron's Devlog &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/islr/">Islr</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/islr_ch10">ISLR - Chapter 10. Deep Learning</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/islr_ch9">ISLR - Chapter 9. Support Vector Machines</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/islr_ch8">ISLR - Chapter 8. Tree-Based Methods</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/islr/">
                                
                                    See all 8 posts  →
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                    <article class="post-card post-template no-image">
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/islr_ch5">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Islr</span>
                            
                        
                    

                    <h2 class="post-card-title">ISLR - Chapter 5. Resampling Methods</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>Chapter 5. Resampling Methods 5.1. Cross-Validation 5.1.1. The Validation Set Approach 5.1.2. Leave-One-Out Cross-Validation 5.1.3. k-Fold Cross-Validation 5.1.4. Bias-Variance Trade-Off for k-Fold Cross-Validation 5.1.5. Cross-Validation on Classification Problems 5.2. The Bootstrap Chapter 5.</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
            </footer>
        </div>
    </article>

            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                    <article class="post-card post-template no-image">
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/islr_ch3">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Islr</span>
                            
                        
                    

                    <h2 class="post-card-title">ISLR - Chapter 3. Linear Regression</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>
</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="/">
            
            <span>Darron's Devlog</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">ISLR - Chapter 4. Classification</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=ISLR+-+Chapter+4.+Classification&amp;url=https://12kdh43.github.io/islr_ch4"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://12kdh43.github.io/islr_ch4"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Darron's Devlog</a> &copy; 2022</section>
                <!-- 
				<section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                -->
				<nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search Darron's Devlog</h1>
                <p class="subscribe-overlay-description">
				</p>
                <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

 </script>

	
    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
