<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> - Articles</title>
    <description></description>
    <link>
    http://0.0.0.0:4000</link>
    
      
      <item>
        <title>GNN-based Fashion Coordinator</title>
        
          <description>
</description>
        
        <pubDate>Wed, 15 Sep 2021 00:00:00 +0900</pubDate>
        <link>
        http://0.0.0.0:4000/fashion_GNN</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/fashion_GNN</guid>
      </item>
      
    
      
      <item>
        <title>Stock Market Portfolio Modeling with R</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/assets/images/m_slide_full.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;
</description>
        
        <pubDate>Fri, 18 Jun 2021 00:00:00 +0900</pubDate>
        <link>
        http://0.0.0.0:4000/stock_portfolio</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/stock_portfolio</guid>
      </item>
      
    
      
      <item>
        <title>K-POP Fandom Data Analysis with networkX</title>
        
          <description>
</description>
        
        <pubDate>Wed, 02 Jun 2021 00:00:00 +0900</pubDate>
        <link>
        http://0.0.0.0:4000/kpop_fandom</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/kpop_fandom</guid>
      </item>
      
    
      
      <item>
        <title>Stock Market Cluster Analysis with NetworkX</title>
        
          <description>
</description>
        
        <pubDate>Fri, 21 May 2021 00:00:00 +0900</pubDate>
        <link>
        http://0.0.0.0:4000/stock_cluster</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/stock_cluster</guid>
      </item>
      
    
      
      <item>
        <title>R - Air Pollution Data Analysis</title>
        
          <description>
</description>
        
        <pubDate>Wed, 16 Dec 2020 00:00:00 +0900</pubDate>
        <link>
        http://0.0.0.0:4000/R_air_polution</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/R_air_polution</guid>
      </item>
      
    
      
      <item>
        <title>NLP - Korean Language Text Analysis with RNN</title>
        
          <description>
</description>
        
        <pubDate>Fri, 19 Jun 2020 00:00:00 +0900</pubDate>
        <link>
        http://0.0.0.0:4000/NLP_korean_RNN</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/NLP_korean_RNN</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 8. Tree-Based Methods</title>
        
          <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#chapter-8-tree-based-methods&quot; id=&quot;markdown-toc-chapter-8-tree-based-methods&quot;&gt;Chapter 8. Tree-Based Methods&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#81-the-basics-of-decision-trees&quot; id=&quot;markdown-toc-81-the-basics-of-decision-trees&quot;&gt;8.1. The Basics of Decision Trees&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#811-regression-trees&quot; id=&quot;markdown-toc-811-regression-trees&quot;&gt;8.1.1. Regression Trees&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#prediction-via-stratification-of-the-feature-space&quot; id=&quot;markdown-toc-prediction-via-stratification-of-the-feature-space&quot;&gt;Prediction via Stratification of the Feature Space&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#tree-pruning&quot; id=&quot;markdown-toc-tree-pruning&quot;&gt;Tree Pruning&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#812-classification-trees&quot; id=&quot;markdown-toc-812-classification-trees&quot;&gt;8.1.2. Classification Trees&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#813-trees-versus-linear-models&quot; id=&quot;markdown-toc-813-trees-versus-linear-models&quot;&gt;8.1.3. Trees Versus Linear Models&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#814-advantages-and-disadvantages-of-trees&quot; id=&quot;markdown-toc-814-advantages-and-disadvantages-of-trees&quot;&gt;8.1.4. Advantages and Disadvantages of Trees&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#82-bagging-random-forests-boosting-and-bayesian-additive-regression-trees&quot; id=&quot;markdown-toc-82-bagging-random-forests-boosting-and-bayesian-additive-regression-trees&quot;&gt;8.2. Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#821-bagging&quot; id=&quot;markdown-toc-821-bagging&quot;&gt;8.2.1. Bagging&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#out-of-bag-error-estimation&quot; id=&quot;markdown-toc-out-of-bag-error-estimation&quot;&gt;Out-of-Bag Error Estimation&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#variable-importance-measures&quot; id=&quot;markdown-toc-variable-importance-measures&quot;&gt;Variable Importance Measures&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#822-random-forests&quot; id=&quot;markdown-toc-822-random-forests&quot;&gt;8.2.2. Random Forests&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#permutation-importance&quot; id=&quot;markdown-toc-permutation-importance&quot;&gt;Permutation Importance&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#823-boosting&quot; id=&quot;markdown-toc-823-boosting&quot;&gt;8.2.3. Boosting&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#824-bayesian-additive-regression-trees&quot; id=&quot;markdown-toc-824-bayesian-additive-regression-trees&quot;&gt;8.2.4. Bayesian Additive Regression Trees&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#825-summary-of-tree-ensemble-methods&quot; id=&quot;markdown-toc-825-summary-of-tree-ensemble-methods&quot;&gt;8.2.5. Summary of Tree Ensemble Methods&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-8-tree-based-methods&quot;&gt;Chapter 8. Tree-Based Methods&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Background&lt;br /&gt;
  Tree by binary question, splitting variables with split points.&lt;br /&gt;
  Partitions of input space &amp;amp; fit a constant to each one.&lt;br /&gt;
  $X = (X_1,\ldots,X_p)$ to $R_1,\ldots,R_M$ regions, &lt;em&gt;p&lt;/em&gt;-dimension rectangles.&lt;br /&gt;
  where \(R_m\cap R_{m'}, m\ne m', \bigcup_{m=1}^M R_m = \mathbb{X}.\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model:&lt;br /&gt;
  \(f(X)=\sum_{m=1}^M C_m I(X\in R_m)\)&lt;br /&gt;
  where $C_m$ is a constant for each region &lt;em&gt;m&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;81-the-basics-of-decision-trees&quot;&gt;8.1. The Basics of Decision Trees&lt;/h2&gt;

&lt;h3 id=&quot;811-regression-trees&quot;&gt;8.1.1. Regression Trees&lt;/h3&gt;

&lt;h4 id=&quot;prediction-via-stratification-of-the-feature-space&quot;&gt;Prediction via Stratification of the Feature Space&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Two steps of building a regression tree:
    &lt;ol&gt;
      &lt;li&gt;Divide the predictor space; the set of possible values for $X_1, \ldots, X_p$ 
 into &lt;em&gt;J&lt;/em&gt; distinct and non-overlapping regions; $R_1, \ldots, R_J$.&lt;/li&gt;
      &lt;li&gt;For every observations that falls into the region $R_j$, we make the same 
 prediction, which is simply the mean of the response values for the training 
 observations in $R_j$.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To divide the predictor space into high-dimensional rectangles, or &lt;em&gt;boxes&lt;/em&gt;, our goal 
  is to find the boxes that minimize the RSS, given by:&lt;br /&gt;
  \(\sum_{j=1}^J\sum_{i\in R_j}(y_i-\hat{y}_{R_j})^2\)&lt;br /&gt;
  where $\hat{y}_{R_j}$ is the mean response for the training observations within 
  the &lt;em&gt;j&lt;/em&gt;th box.&lt;br /&gt;
  I.e., the Least Squares Criterion in given regions:&lt;br /&gt;
  \(\begin{align*}
  \text{min}_{C_m}\sum_{i=1}^N RSS &amp;amp;= \text{min}_{C_m}\sum_{i=1}^N(y_i-f(x_i))^2 \\
      &amp;amp;= \text{min}_{C_m}\sum_{i=1}^N\left[y_i - \sum_{m=1}^M C_m I(x_i\in R_m) \right]^2 \\
  \rightarrow \hat{C}_m &amp;amp;= \text{ave}(y_i|x_i\in R_m) = \bar{y}_m
  \end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;How to split into spaces?&lt;br /&gt;
  For its computational infeasibility, we take a &lt;em&gt;top-down&lt;/em&gt;, &lt;em&gt;greedy&lt;/em&gt; approach that 
  it known as &lt;em&gt;recursive binary splitting&lt;/em&gt;. To perform recursive binary splitting, 
  we first select the predictor $X_j$ and the cutpoint &lt;em&gt;s&lt;/em&gt; such that splitting the 
  predictor spaace in to the regions \(\{X|X_j&amp;lt;s\}\) and \(\{X|X_j\ge s\}\) leads to 
  the greatest possible reduction in RSS.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For splitting variable $X_j$ and split point &lt;em&gt;s&lt;/em&gt;,&lt;br /&gt;
  In two binary partitions \(R_1(j,s) = \{X|X_j&amp;lt;s\}\) and \(R_2(j,s) = \{X|X_j\ge s\}\),&lt;br /&gt;
  \(\text{min} \left[ \sum_{i:x_i\in R_1(j,s)}(y_i-\hat{y}_{R_1})^2 + \sum_{i:x_i\in R_2(j,s)}(y_i-\hat{y}_{R_2})^2 \right]\), or&lt;br /&gt;
  \(\text{min}_{C_1}\sum_{i:x_i\in R_1(j,s)}(y_i-C_1)^2 + \min_{C_2}\sum_{i:x_i\in R_2(j,s)}(y_i-C_2)^2\).&lt;br /&gt;
  \(\begin{align*}
  \rightarrow &amp;amp; \text{ave}(y_i|x_i\in R_1(j,s)) = \hat{C}_1 = \hat{y}_{R_1} \\
              &amp;amp; \text{ave}(y_i|x_i\in R_2(j,s)) = \hat{C}_2 = \hat{y}_{R_2}
  \end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;We repeat this process over the two previously identified regions, looking for the 
  best predictor and best cutpoint in order to split the data further so as the minimize 
  the RSS within each of the resulting regions. The process continues until a stopping 
  criterion is reached; for instance, until no region contains more than five observations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;tree-pruning&quot;&gt;Tree Pruning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tree size is based on the number of regions(&lt;em&gt;M&lt;/em&gt;) and the model complexity is on the
  number of parameters($C_m$). Since there is an extremely large number of possible 
  subtrees, estimating the CV error for every possible subtree would be too cumbersome. 
  Instead, to find the optimal &lt;em&gt;M&lt;/em&gt;, we use &lt;em&gt;Cost-Complexity Pruning&lt;/em&gt;, reducing from a 
  very large tree $T_0$ to a subtree that has the lowest test error rate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each value of a nonnegative tuning parameter $\alpha$ there corresponds a subtree 
  $T\in T_0$, such that minimizing&lt;br /&gt;
  \(\sum_{m=1}^{|T|}\sum_{i:x_i\in R_m}(y_i-\hat{y}_{R_m})^2 + \alpha|T|\).&lt;br /&gt;
  Here |&lt;em&gt;T&lt;/em&gt;| indicates the number of terminal nodes of the tree &lt;em&gt;T&lt;/em&gt;, $R_m$ is the 
  rectangle, or the subset of predictor space corresponding to the &lt;em&gt;m&lt;/em&gt;th terminal node, 
  and $\hat{y}_{R_m} = \hat{C}_m$ is the predicted response, the mean of the training 
  observations in $R_m$. When $\alpha=0$, then the subtree &lt;em&gt;T&lt;/em&gt; will simply equal $T_0$. 
  As $\alpha$ increases, there is a price to pay for having a tree with many terminal 
  nodes, and so the quantity will tend to be minimized for a smaller subtree.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Algorithm&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;Use recursive binary splitting to grow a large tree, stopping only when each 
 terminal node has fewer than some minimum number of observations, threshold.&lt;/li&gt;
      &lt;li&gt;Apply cost complexity pruning to obtain a sequence of best subtrees, as a function 
 of $\alpha$.&lt;/li&gt;
      &lt;li&gt;Use K-fold CV to choose $\alpha$:&lt;br /&gt;
 (a) Repeat Steps 1 and 2 on all but &lt;em&gt;k&lt;/em&gt;th fold of the training data.&lt;br /&gt;
 (b) Evaluate the mean squared prediction error on the data in the left-out &lt;em&gt;k&lt;/em&gt;th 
 fold, as a function of $\alpha$.&lt;br /&gt;
 Average the results for each value of $\alpha$, and pick a $\alpha$ to minimize 
 the average error.&lt;/li&gt;
      &lt;li&gt;Return the subtree from Step 2 that corresponds to the chosen value of $\alpha$.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;812-classification-trees&quot;&gt;8.1.2. Classification Trees&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of the mean response of the training observations that belong to the same 
  terminal node, a classification tree predict that each observation in the region 
  to the most commonly occurring class of training observations in the region to 
  which it belongs. In interpreting the result, we are often interested not only in 
  the class prediction, but also in the class proportions among the training observations 
  that fall into that region.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A natural alternative to RSS, the classification error rate; the fraction of the 
  training observations in that region that do not belong to the most common class:&lt;br /&gt;
  \(E=1-\text{max}_k(\hat{p}_{mk})\), where \(\hat{p}_{mk} = \frac{1}{N_m}\sum_{x_i\in R_m}I(y_i=k)\),&lt;br /&gt;
  represents the proportion of training observations in the &lt;em&gt;m&lt;/em&gt;th region that are from 
  the &lt;em&gt;k&lt;/em&gt;th class. By majority vote rule, \(k(m) = \text{argmax}_k \hat{p}_mk\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;However, the classification error is not sufficiently sensitive for tree-growing, 
  in practice we use two other measures preferable.
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Gini index&lt;/em&gt;:&lt;br /&gt;
  \(G=\sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})\),&lt;br /&gt;
  a measure of total variance across the &lt;em&gt;K&lt;/em&gt; classes. It is often referred to as 
  a measure of node &lt;em&gt;purity&lt;/em&gt;; a small value indicates that a node contains predominantly 
  observations from a single class.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Cross-entropy&lt;/em&gt; or deviance:&lt;br /&gt;
  \(D=-\sum_{k=1}^K\hat{p}_{mk}\log\hat{p}_{mk}\),&lt;br /&gt;
  a nonnegative value that take on a small value if the &lt;em&gt;m&lt;/em&gt;th node is pure.&lt;br /&gt;
  these measures are typically used to evaluate the quality of a particular split, 
  since they are more sensitive to node purity than is the classification error rate. 
  The classification error rate is preferable if prediction accuracy of the final 
  pruned tree is the goal.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Being performed to increase node purity, some of the splits yield two terminal nodes 
  that have the same predicted value. Even though such splits do not reduce the classification 
  error, it improves the Gini index and the entropy.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;813-trees-versus-linear-models&quot;&gt;8.1.3. Trees Versus Linear Models&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Linear regression model is:&lt;br /&gt;
  \(f(X) = \beta_0 + \sum_{j=1}^p X_j\beta_j\),&lt;br /&gt;
  whereas Regression tree model is:&lt;br /&gt;
  \(f(X) = \sum_{m=1}^M c_m\cdot 1_{(X\in R_m)}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If there is a highly non-linear and complex relationship between the features and 
  the response, then decision trees may outperform classical approaches. Also, trees 
  may be preffered for the sake of interpretability and visualization.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;814-advantages-and-disadvantages-of-trees&quot;&gt;8.1.4. Advantages and Disadvantages of Trees&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Pros
    &lt;ul&gt;
      &lt;li&gt;Good interpretability and easy to explain.&lt;/li&gt;
      &lt;li&gt;Closely mirror human decision-making than do some of the regression and classification 
  approaches.&lt;/li&gt;
      &lt;li&gt;Can be displayed graphically, and are easily interpreted even by a non-expert.&lt;/li&gt;
      &lt;li&gt;Can easily handle qualitative predictors without the need to create dummy variables.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cons
    &lt;ul&gt;
      &lt;li&gt;Do not have the same level of predictive accuracy as the regression and classification 
  approaches.&lt;/li&gt;
      &lt;li&gt;Non-robust, a small change in the data can cause a large change in the final 
  estimated tree(High variance).&lt;/li&gt;
      &lt;li&gt;Lack of smoothness, can be far from the true function.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By aggregating many decision trees, the predictive performance of trees can be 
	substantially improved.&lt;/p&gt;

&lt;h2 id=&quot;82-bagging-random-forests-boosting-and-bayesian-additive-regression-trees&quot;&gt;8.2. Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;An &lt;em&gt;ensemble&lt;/em&gt; method is an approach that combines many simple “building ensemble block” 
  models in order to obtain a single and potentially very powerful model. These simple 
  building block models are sometimes known as &lt;em&gt;weak learners&lt;/em&gt;, since they may lead 
  to mediocre predictions on their own.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;821-bagging&quot;&gt;8.2.1. Bagging&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Bootstrap aggregation&lt;/em&gt;, or &lt;em&gt;bagging&lt;/em&gt;, is a general-purpose procedure for reducing 
  the variance of a statistical learning method. Given a set of &lt;em&gt;n&lt;/em&gt; independent 
  observations $Z_1,\ldots,Z_n$, each with variance $\sigma^2$, the variance of the 
  mean $\bar{Z}$ of the observations is given by $\sigma^2/n$. In other words, 
  &lt;em&gt;averaging a set of observations reduces variance&lt;/em&gt;.&lt;br /&gt;
  Hence it is a natural way to reduce the variance and increase the test set accuracy 
  of a statistical learning method. We could calculate \(\hat{f}^1(x), \ldots, \hat{f}^B(x)\) 
  using &lt;em&gt;B&lt;/em&gt; seperate training sets, and average them in order to obtain a single 
  low-variance model, \(\hat{f}_{\text{avg}}(x) = \frac{1}{B}\sum_{b=1}^B\hat{f}^b(x)\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, we do not have access to multiple training sets. Instaed, we can bootstrap, 
  by taking repeated samples from the (single) training data set. The bagging model,&lt;br /&gt;
  \(\hat{f}_{\text{bag}}(x) = \frac{1}{B}\sum_{b=1}^B\hat{f}^{*b}(x)\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is particularly useful for decision trees. To apply begging to regression trees, 
  we simply construct &lt;em&gt;B&lt;/em&gt; regression trees using &lt;em&gt;B&lt;/em&gt; bootstrapped training sets, and 
  average the resulting predictions. These trees are grown deep but not pruned. Each 
  individual tree has high variance but low bias. Averaging these trees reduces the 
  variance. Bagging improves the accuracy by combining hundreds or even thousands of 
  trees into a single procedure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To a classification problem where &lt;em&gt;Y&lt;/em&gt; is qualitative, the simplest approach is the
  &lt;em&gt;majority vote&lt;/em&gt;; For a given test observation, record the class predicted by each 
  of the &lt;em&gt;B&lt;/em&gt; trees and the overall prediction is the most commonly occurring class 
  among those predictions.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;out-of-bag-error-estimation&quot;&gt;Out-of-Bag Error Estimation&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Without cross-validation or the validation set approach, there is a very straightforward 
  way to estimate the test error of a bagged model. The reamining observations not 
  used to fit a given bagged tree are referred to as the &lt;em&gt;out-of-bag&lt;/em&gt;(OOB) observations. 
  We can predict the response for the &lt;em&gt;i&lt;/em&gt;th observation using each of the trees in 
  which that observation was OOB. To obtain a single prediction for the &lt;em&gt;i&lt;/em&gt;th observation, 
  we average these predicted responses to a regression set, or take a majority vote 
  to a classification set. An OOB prediction can be obtained in this way for every 
  &lt;em&gt;n&lt;/em&gt; observations, from which the overall OOB MSE or classification error can be computed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sinse the response for each observation is predicted using only the trees that were 
  not fit using that observation, the resulting OOB error is a valid estimate of the 
  test error for the bagged model.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;variable-importance-measures&quot;&gt;Variable Importance Measures&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Bagging improves prediction accuracy at the expense of interpretability. One can obtain 
  an overall summary of the importance of each predictor using the RSS or the Gini 
  index. In the case of bagging regression trees, we can record the total amount that 
  the RSS is decreased due to splits over a given predictor, averaged over all &lt;em&gt;B&lt;/em&gt; 
  trees. A large value indicates an important predictor.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;822-random-forests&quot;&gt;8.2.2. Random Forests&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If there is one very strong predictor along with a number of other moderately strong 
  predictors, in bagging method, most trees will use this strong predictor in the top 
  split. Consequently, all of the bagged trees will look quite similar and the predictions 
  from the bagged trees will be highly correlated. Averaging highly correlated quantities 
  does not lead to a substantial reduction in variance over a single tree in this setting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Random forests overcome this problem by forcing each split to consider only a subset 
  of the predictors. As in bagging, we build a number of decision trees on bootstrapped 
  training samples. But when building these decision trees, each time a split in a 
  tree is considered, &lt;em&gt;a random sample of m predictors&lt;/em&gt; is chosen as split candidates 
  from the full set of &lt;em&gt;p&lt;/em&gt; predictors. Typically, the number of predictors considered 
  at each split &lt;em&gt;m&lt;/em&gt; is approximately equal to the square root of the total number of 
  predictors &lt;em&gt;p&lt;/em&gt;; $m\approx\sqrt{p}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On average $(p-m)/p$ of the splits will not even consider the strong predictor, and 
  so other predictors will have more of a chance. This process is &lt;em&gt;decorrelating&lt;/em&gt; 
  the trees, thereby making the average of the resulting trees less variable and more 
  reliable.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;permutation-importance&quot;&gt;Permutation Importance&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Calculate the estimated test error with the original OOB samples as validation data, 
  then randomly suffle; or &lt;em&gt;permute&lt;/em&gt; the order of the &lt;em&gt;j&lt;/em&gt;th variable on OOB sample 
  to break the relationship between $X_j$ and &lt;em&gt;Y&lt;/em&gt;. With this permuted OOB samples, 
  calculate the Permuation Measure. If the result of permuation measure is worse than 
  that of reference measure, we can consider $X_j$ is an important variable. For all 
  variables &lt;em&gt;X&lt;/em&gt;, we can make the rank of variable importance. However, permuation 
  importance can overestimates the importance of correlated predictors.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;823-boosting&quot;&gt;8.2.3. Boosting&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Bagging creates multiple copies of the original training data set using the bootstrap, 
  fits a separate decision tree to each copy and then combines all of the trees in 
  order to create a single predictive model. Each tree is built on a bootstrap data 
  set, independent of the other trees.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Boosting works in a similar way, except that the trees are grown &lt;em&gt;sequentially&lt;/em&gt;: each 
  each tree is grown using information from previously grown trees. This approach 
  does not involve bootstrap sampling; instead each tree is fit on a modified version 
  of the original data set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm
    &lt;ol&gt;
      &lt;li&gt;Set $\hat{f}(x)=0$ and $r_i=y_i$ for all &lt;em&gt;i&lt;/em&gt; in the training set.&lt;/li&gt;
      &lt;li&gt;For $b=1,2,\ldots,B$, repeat:&lt;br /&gt;
 (a) Fit a tree $\hat{f}^b$ with &lt;em&gt;d&lt;/em&gt; splits ($d+1$ terminal nodes) to the 
 training data (&lt;em&gt;X,r&lt;/em&gt;).&lt;br /&gt;
 (b) Update $\hat{f}$ by adding in a shrunken version of the new tree:&lt;br /&gt;
 \(\hat{f}(x)\leftarrow\hat{f}(x)+\lambda\hat{f}^b(x)\).&lt;br /&gt;
 (c) Update the residuals, removing the effect of &lt;em&gt;b&lt;/em&gt;th tree,&lt;br /&gt;
 Unexplained residuals \(r_i\leftarrow r_i - \lambda\hat{f}^b(x_i)\).&lt;/li&gt;
      &lt;li&gt;Output the boosted model,&lt;br /&gt;
 \(\hat{f}(x)=\sum_{i=1}^B\lambda\hat{f}^b(x)\).&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Boosting combines multiple weak learners(poor prediction models) and make better 
  prediction. In procedure, data(the weights of observations) are repeatedly modified 
  and the model &lt;em&gt;learns slowly&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Boosting tuning parameters
    &lt;ol&gt;
      &lt;li&gt;&lt;em&gt;B&lt;/em&gt; the number of trees. Unlike bagging and random forests, boosting can overfit 
 if &lt;em&gt;B&lt;/em&gt; is too large, although this overfitting tends to occur slowly if at all. 
 We use cross-validation to select &lt;em&gt;B&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;$\lambda$ the shrinkage parameter, a small positive number. This controls the 
 rate at which boosting learns. Typical values are 0.01 or 0.001. Very small 
 $\lambda$ can require using a very large value of &lt;em&gt;B&lt;/em&gt; to achieve good performance.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;d&lt;/em&gt; the number of splits in each tree, which controls the complexity of the boosted 
 ensemble. $d=1$ works well, in which case each tree is a &lt;em&gt;stump&lt;/em&gt;, consisting 
 of a single split. In this case, the boosted ensemble is fitting an additive 
 model, each term involves only a single variable. Generally &lt;em&gt;d&lt;/em&gt; is the 
 &lt;em&gt;interaction depth&lt;/em&gt;, and controls the interaction order of the boosted model.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;824-bayesian-additive-regression-trees&quot;&gt;8.2.4. Bayesian Additive Regression Trees&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Bagging and random forests make predictions from an average of independent trees, 
  while boosting uses a weighted sum of trees. BART is related to both approaches: 
  each tree is constructed in a random manner and each tree tries to caputre signal 
  not yet accounted for by the current model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;let &lt;em&gt;K&lt;/em&gt; denote the number of regression trees, and &lt;em&gt;B&lt;/em&gt; the number of iterations for 
  which the BART will be run. The notation $\hat{f}_k^b(x)$ represents the prediction 
  at &lt;em&gt;x&lt;/em&gt; for the &lt;em&gt;k&lt;/em&gt;th regression tree used in the &lt;em&gt;b&lt;/em&gt;th iteration. At the end of 
  each iteration, the &lt;em&gt;K&lt;/em&gt; trees from that iteration will be summed; 
  \(\hat{f}^b(x)=\sum_{k=1}^K\hat{f}_k^b(x)\) for $b=1,\ldots,B$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;First iteration, all trees are initialized to have a single root node, with 
 \(\hat{f}_k^1(x) = \frac{1}{nK}\sum_{i=1}^n y_i\), the mean of the response values 
 divided by the total number of trees. Thus, 
 \(\hat{f}^1(x)=\sum_{k=1}^K\hat{f}_k^1(x)=\frac{1}{n}\sum_{i=1}^n y_i = \bar{y}\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Next, updates each of the &lt;em&gt;K&lt;/em&gt; trees, one at a time.&lt;br /&gt;
 In the &lt;em&gt;b&lt;/em&gt;th iteration, we subtract from each response value the predictions from 
 all but the &lt;em&gt;k&lt;/em&gt;th tree to obtain a &lt;em&gt;partial residual&lt;/em&gt;&lt;br /&gt;
 \(r_i = y_i - \sum_{k'&amp;lt;k}\hat{f}_{k'}^b(x_i) - \sum_{k'&amp;gt;k}\hat{f}_{k'}^{b-1}(x_i)\)&lt;br /&gt;
 Rather than fitting a fresh tree to this partial residual, BART randomly chooses 
 a perturbation to the tree from the previous iteration from a set of possible 
 perturbations, favoring ones that improve the fit to the partial residual.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Two components to the perturbation:&lt;br /&gt;
  We may change the structure of the tree by adding or pruning branches.&lt;br /&gt;
  We may change the prediction in each terminal node of the tree.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Output of BART is a collection of prediction models,&lt;br /&gt;
 \(\hat{f}^b(x) = \sum_{k=1}^K\hat{f}_k^b(x)\)&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Typically, the first few of these prediction models in earlier iterations, known 
  as the &lt;em&gt;burn-in&lt;/em&gt; period, tend not to provide very good results. For the number of 
  burn-in iterations &lt;em&gt;L&lt;/em&gt;, for instance we might take &lt;em&gt;L&lt;/em&gt;=200, we simply remove and 
  take the average after &lt;em&gt;L&lt;/em&gt; iterations; \(\hat{f}(x)=\frac{1}{B-L}\sum_{b=L+1}^B\hat{f}^b(x)\). 
  Or we can compute quantities other than the average.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;825-summary-of-tree-ensemble-methods&quot;&gt;8.2.5. Summary of Tree Ensemble Methods&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Bagging: The trees are grown independently on random samples of the observations and 
  tend to be quite similar to each other. Thus, bagging can get caught in local optima 
  and fail to thoroughly explore the model space.&lt;/li&gt;
  &lt;li&gt;Random Forests: The trees are grown independently on random samples but each split 
  on each tree is performed using a random subset of the features, decorrelating the 
  trees and leading to a more thorough exploration of the model space.&lt;/li&gt;
  &lt;li&gt;Boosting: We only use the original data and do not draw any random samples. Trees 
  are grown successively, usiang slow learning approach: each new tree is fit to the 
  signal that is left over from the earlier trees, and shrunken down before it is 
  used.&lt;/li&gt;
  &lt;li&gt;BART: WE only use the original data and trees are grown successively. However, each 
  tree is perturbed in order to avoid local minima and achieve a more thorough exploration 
  of the model space.&lt;/li&gt;
&lt;/ul&gt;
</description>
        
        <pubDate>Thu, 14 May 2020 00:00:00 +0900</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch8</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch8</guid>
      </item>
      
    
      
      <item>
        <title>NLP - Text Analysis with ML algorithms</title>
        
          <description>
</description>
        
        <pubDate>Sun, 10 May 2020 16:00:00 +0900</pubDate>
        <link>
        http://0.0.0.0:4000/NLP_eng_ml</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/NLP_eng_ml</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 7. Moving Beyond Linearity</title>
        
          <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#chapter-7-moving-beyond-linearity&quot; id=&quot;markdown-toc-chapter-7-moving-beyond-linearity&quot;&gt;Chapter 7. Moving Beyond Linearity&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#71-polynomial-regression&quot; id=&quot;markdown-toc-71-polynomial-regression&quot;&gt;7.1. Polynomial Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#72-step-functions&quot; id=&quot;markdown-toc-72-step-functions&quot;&gt;7.2. Step Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#73-basis-functions&quot; id=&quot;markdown-toc-73-basis-functions&quot;&gt;7.3. Basis Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#74-regression-splines&quot; id=&quot;markdown-toc-74-regression-splines&quot;&gt;7.4. Regression Splines&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#741-piecewise-polynomials&quot; id=&quot;markdown-toc-741-piecewise-polynomials&quot;&gt;7.4.1. Piecewise Polynomials&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#742-constraints-and-splines&quot; id=&quot;markdown-toc-742-constraints-and-splines&quot;&gt;7.4.2. Constraints and Splines&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#743-the-spline-basis-representation&quot; id=&quot;markdown-toc-743-the-spline-basis-representation&quot;&gt;7.4.3. The Spline Basis Representation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#744-choosing-the-number-and-locations-of-the-knots&quot; id=&quot;markdown-toc-744-choosing-the-number-and-locations-of-the-knots&quot;&gt;7.4.4. Choosing the Number and Locations of the Knots&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#745-comparison-to-polynomial-regression&quot; id=&quot;markdown-toc-745-comparison-to-polynomial-regression&quot;&gt;7.4.5. Comparison to Polynomial Regression&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#75-smoothing-splines&quot; id=&quot;markdown-toc-75-smoothing-splines&quot;&gt;7.5. Smoothing Splines&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#751-an-overview-of-smoothing-splines&quot; id=&quot;markdown-toc-751-an-overview-of-smoothing-splines&quot;&gt;7.5.1. An Overview of Smoothing Splines&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#752-choosing-the-smoothing-parameter-lambda&quot; id=&quot;markdown-toc-752-choosing-the-smoothing-parameter-lambda&quot;&gt;7.5.2. Choosing the Smoothing Parameter $\lambda$&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#76-local-regression&quot; id=&quot;markdown-toc-76-local-regression&quot;&gt;7.6. Local Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#77-generalized-additive-models&quot; id=&quot;markdown-toc-77-generalized-additive-models&quot;&gt;7.7. Generalized Additive Models&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#771-gams-for-regression-problems&quot; id=&quot;markdown-toc-771-gams-for-regression-problems&quot;&gt;7.7.1. GAMs for Regression Problems&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#772-gams-for-classification-problems&quot; id=&quot;markdown-toc-772-gams-for-classification-problems&quot;&gt;7.7.2. GAMs for Classification Problems&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#pros-and-cons-of-gams&quot; id=&quot;markdown-toc-pros-and-cons-of-gams&quot;&gt;Pros and Cons of GAMs&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-7-moving-beyond-linearity&quot;&gt;Chapter 7. Moving Beyond Linearity&lt;/h2&gt;

&lt;h2 id=&quot;71-polynomial-regression&quot;&gt;7.1. Polynomial Regression&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;standard linear model $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$&lt;br /&gt;
  to a polynomial function $y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \cdots + \beta_d x_i^d + \epsilon_i$&lt;br /&gt;
  For large enough degree &lt;em&gt;d&lt;/em&gt;, a polynomial regression produces an extremely non-
  linear curve. The coefficients $\beta_d$’s can be easily estimated using least 
  squares linear regression.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;72-step-functions&quot;&gt;7.2. Step Functions&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Break the range of &lt;em&gt;X&lt;/em&gt; into &lt;em&gt;bins&lt;/em&gt;, and fit a different constant in each bin; 
  converting a continuous variables into an &lt;em&gt;ordered categorical variable&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;By cutpoints $c_1, c_2, \ldots, c_K$ in the range of &lt;em&gt;X&lt;/em&gt;, construct &lt;em&gt;K+1&lt;/em&gt; new variables&lt;br /&gt;
  \(\begin{align*}
  C_0(X) &amp;amp;= I(X &amp;lt; c_1), \\
  C_1(X) &amp;amp;= I(c_1 \le X &amp;lt; c_2), \\
  C_2(X) &amp;amp;= I(c_2 \le X &amp;lt; c_3), \\
         &amp;amp;\vdots \\
  C_{K-1}(X) &amp;amp;= I(c_{K-1} \le X &amp;lt; c_K), \\
  C_{K}(X) &amp;amp;= I(c_K \le X), \\
  \end{align*}\)&lt;br /&gt;
  where I($\cdot$) is an &lt;em&gt;indicator function&lt;/em&gt; returning value of 1 or 0. These are 
  sometimes called &lt;em&gt;dummy variables&lt;/em&gt;. For any value of &lt;em&gt;X&lt;/em&gt;, $\sum_{i=0}^K C_i(X) = 1$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The least squares model: $y_i = \beta_0 + \beta_1 C_1(x_i) + \cdots + \beta_K C_K(x_i) + \epsilon_i$&lt;br /&gt;
  When $X&amp;lt;c_1$, all of the predictors $C_1(X), C_2(X), \ldots, C_K(X)$ are zero, so 
  $\beta_0$ is the mean value of &lt;em&gt;Y&lt;/em&gt; or $X&amp;lt;c_1$. By comparison, the model predicts 
  a response of $\beta_0 + \beta_j$ for $c_j \le X &amp;lt; c_{j+1}$, so $\beta_j$ represents the 
  average increase in the response for &lt;em&gt;X&lt;/em&gt; in such range.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unless there are natural breakpoints in the predictors, piecewise-constant functions 
  can miss the action.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;73-basis-functions&quot;&gt;7.3. Basis Functions&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Polynomial and piecewise-constant regression models are in fact special cases of a 
  &lt;em&gt;basis function&lt;/em&gt; approach; with a family of functions or transformations applied 
  to a variable &lt;em&gt;X&lt;/em&gt;: $b_1(X), b_2(X), \ldots, b_K(X)$, fit a linear model&lt;br /&gt;
  $y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \cdots + \beta_K b_K(x_i) + \epsilon_i$.&lt;br /&gt;
  as a standard linear model with predictors $b_i(x_i), b_2(x_i), \ldots, b_K(x_i)$. 
  Hence, we can use least squares and all of the inference tools, such as std.err 
  for coefficient estimates and F-statistics for the model’s overall significance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that the basis functions $b_k(\cdot)$ are fixed and known(or, we choose the 
  functions before). E.g., for polynomial regression, the basis functions are $b_j(x_i)=x_i^j$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;74-regression-splines&quot;&gt;7.4. Regression Splines&lt;/h2&gt;

&lt;h3 id=&quot;741-piecewise-polynomials&quot;&gt;7.4.1. Piecewise Polynomials&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of fitting a high-degree polynomial over the entire range of &lt;em&gt;X&lt;/em&gt;, piecewise 
  polynomial regression fits separate low-degree polynomials over different regions. 
  Divide dimension &lt;em&gt;X&lt;/em&gt; by &lt;em&gt;knots&lt;/em&gt; and apply different polynomial model to each 
  region. If we place &lt;em&gt;K&lt;/em&gt; knots throughout the range of &lt;em&gt;X&lt;/em&gt;, then we fit &lt;em&gt;K+1&lt;/em&gt; different 
  models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;a cubic regression model; $y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$&lt;br /&gt;
  is a piecewise cubic with no knots with &lt;em&gt;d&lt;/em&gt; = 3.&lt;br /&gt;
  On the other hand,&lt;br /&gt;
  \(y_i = \begin{cases}
          \beta_{01}+\beta_{11}x_i + \beta_{21}x_i^2 + \beta_{31}x_i^3 + \epsilon_i &amp;amp; \mbox{if }x_i &amp;lt; c \\
          \beta_{02}+\beta_{12}x_i + \beta_{22}x_i^2 + \beta_{32}x_i^3 + \epsilon_i &amp;amp; \mbox{if }x_i \ge c. \\
          \end{cases}\)&lt;br /&gt;
  is with a single knot at a point c.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For other degrees &lt;em&gt;d&lt;/em&gt;:&lt;br /&gt;
  a piecewise-constant functions are piecewise polynomials of degree &lt;em&gt;0&lt;/em&gt;, a piecewise 
  linear functions are of degree &lt;em&gt;1&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;742-constraints-and-splines&quot;&gt;7.4.2. Constraints and Splines&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;To fix the discontinuity and overcomplexity, we add some additional constraints:&lt;br /&gt;
  in a degree-&lt;em&gt;d&lt;/em&gt; spline, or a piecewise degree-&lt;em&gt;d&lt;/em&gt; polynomial, it requires the 
  continuity in derivatives up to degree &lt;em&gt;d-1&lt;/em&gt; at each knot.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;743-the-spline-basis-representation&quot;&gt;7.4.3. The Spline Basis Representation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \cdots + \beta_{K+3} b_{K+3}(x_i) + \epsilon_i$&lt;br /&gt;
  is a cubic spline model with &lt;em&gt;K&lt;/em&gt; knots.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;with a &lt;em&gt;truncated power basis&lt;/em&gt; function per knot $\xi$, which is defined as 	
  \(h(x,\xi) = (x-\xi)_{+}^3 = \begin{cases} (x-\xi)^3 &amp;amp; \mbox{if }x&amp;gt;\xi \\
                                                  0	 &amp;amp; \mbox{otherwise,}
                               \end{cases}\)&lt;br /&gt;
  so, in fitting a cubic spline with &lt;em&gt;K&lt;/em&gt; knots, we perform least squares regression 
  with an intercept and &lt;em&gt;3+K&lt;/em&gt; predictors, of the form $X, X^2, X^3, h(X,\xi_1), h(X,\xi_2), \ldots, h(X,\xi_K)$.&lt;br /&gt;
  This amounts to estimating a total of &lt;em&gt;K+4&lt;/em&gt; regression coefficients; for this 
  regression, fitting a cubic spline with K knots uses &lt;em&gt;K+4&lt;/em&gt; degrees of freedom.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, splines can have high variance at the outer range of the predictors; 
  $X&amp;lt;c_1$ or $X\ge c_K$, when &lt;em&gt;X&lt;/em&gt; takes on either a very small or very large value. 
  We see that the confidence bands in the boundary region appear fairly wild.&lt;br /&gt;
  A &lt;em&gt;natural spline&lt;/em&gt; with additional &lt;em&gt;boundary constraints&lt;/em&gt; that the function is 
  required to be linear at the boundary, generally produce more stable estimates. In 
  this case, the corresponding confidence intervals are narrower.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;744-choosing-the-number-and-locations-of-the-knots&quot;&gt;7.4.4. Choosing the Number and Locations of the Knots&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The regression spline is most flexible in regions that contain a lot of knots, because 
  in those regions the polynomial coefficients can change rapidly. Hence, one option 
  is to place more knots in places where the function might vary most rapidly, and 
  to place fewer knots where it seems more stable. In practice, it is common to place 
  knots in a uniform fashion.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To determine the number of knots, or equivalently the degrees of freedom of the spline 
  contain, we use cross-validation method.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;745-comparison-to-polynomial-regression&quot;&gt;7.4.5. Comparison to Polynomial Regression&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Regression splines often give superior results to polynomial regression. This is 
  because unlike polynomials, which must use a high degree to produce flexible fits, 
  splines introduce flexibility by increasing the number of knots but keeping the 
  degree fixed. Generally, this approach produces more stable estimates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Splines can produce a reasonable fit at the boundaries and also allow us to place 
  more knots, or flexibility, over regions where the function &lt;em&gt;f&lt;/em&gt; seems to be changing 
  rapidly, and fewer knots where &lt;em&gt;f&lt;/em&gt; appears more stable.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;75-smoothing-splines&quot;&gt;7.5. Smoothing Splines&lt;/h2&gt;

&lt;h3 id=&quot;751-an-overview-of-smoothing-splines&quot;&gt;7.5.1. An Overview of Smoothing Splines&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In fitting a smooth curve to a set of data, some function $g(x)$, that fits the 
  observed data well: Minimizing $RSS = \sum_{i=1}^n(y_i-g(x_i))^2$. But if we don’t 
  put any constraints on &lt;em&gt;g&lt;/em&gt;, then we can always make &lt;em&gt;RSS&lt;/em&gt; zero simply by choosing 
  &lt;em&gt;g&lt;/em&gt; such that it &lt;em&gt;interpolates&lt;/em&gt; all of the $y_i$. Such a function would woefully 
  overfit the data, would be too flexible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Smoothing spline is the function &lt;em&gt;g&lt;/em&gt; that minimizes&lt;br /&gt;
  \(\sum_{i=1}^n(y_i - g(x_i))^2 + \lambda\int g''(t)^2, dt\)&lt;br /&gt;
  with a nonnegative tuning parameter $\lambda$, takes the “Loss+Penalty” formulation 
  like the ridge and lasso. The term $\sum_{i=1}^n(y_i - g(x_i))^2$ is a &lt;em&gt;loss function&lt;/em&gt; 
  that makes &lt;em&gt;g&lt;/em&gt; to fit the data well, and the term \(\lambda\int g''(t)^2, dt\) is a 
  &lt;em&gt;penalty term&lt;/em&gt; that reduces the variability in &lt;em&gt;g&lt;/em&gt;. \(g''(t)\) indicates the second 
  derivative of the function &lt;em&gt;g&lt;/em&gt;, corresponds to the amount by which the slope(the first 
  derivative) is changing. Broadly speaking, the second derivative is a measure of 
  its &lt;em&gt;roughness&lt;/em&gt;: large in absolute value if &lt;em&gt;g(t)&lt;/em&gt; is very wiggly near &lt;em&gt;t&lt;/em&gt;, close 
  to zero otherwise. It is zero as the derivative of a straight line, the function is 
  perfectly smooth. The integral is a summation over the range of &lt;em&gt;t&lt;/em&gt;, so \(\int g''(t)^2, dt\) 
  is a measure of the total change in the function $g’(t)$ over its entire range. If 
  &lt;em&gt;g&lt;/em&gt; is very smooth, then $g’(t)$ will be close to constant and \(\int g''(t)^2, dt\) will 
  take on a small value. Therefore, the penalty term encourages &lt;em&gt;g&lt;/em&gt; to be smooth. 
  The larger the $\lambda$, the smoother the &lt;em&gt;g&lt;/em&gt; will be.&lt;br /&gt;
  When $\lambda=0$, then there’s no penalty and the function &lt;em&gt;g&lt;/em&gt; will be very jumpy 
  and have perfect fit. When $\lambda\rightarrow\infty$, &lt;em&gt;g&lt;/em&gt; will be perfectly smooth; 
  a linear least squares line $g(x)=ax+b$. The $\lambda$ controls the bias-variance 
  trade-off of the smoothing spline.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Smoothing spline &lt;em&gt;g(x)&lt;/em&gt; have some special properties: it is a piecewise polynomial 
  with knots at the unique values of $x_1,\ldots,x_n$, and continuous first and second 
  derivatives at each knot. Furthermore, it is linear in the region outside of the 
  extreme knots.&lt;br /&gt;
  In other words, the Smoothing spline function is a nautral spline with knots at 
  $x_1,\ldots,x_n$; it is a shrunken version of such a natural spline, where the value 
  of the tuning parameter $\lambda$ controls the level of shrinkage.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;752-choosing-the-smoothing-parameter-lambda&quot;&gt;7.5.2. Choosing the Smoothing Parameter $\lambda$&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Have seen that a smoothing spline is simply a natural spline with knots at every unique 
  value of $x_i$, it might seem that a smoothing spline will have far too many degrees 
  of freedom, since a knot at each data point allows a great deal of flexibility. 
  But $\lambda$ controls the roughness of the smoothing spline, and hence the &lt;em&gt;effective&lt;/em&gt; 
  &lt;em&gt;degrees of freedom&lt;/em&gt;. We can show that as $\lambda$ increase from &lt;em&gt;0&lt;/em&gt; to $\infty$, 
  the effective degrees of freedom $df_\lambda$ decrease from &lt;em&gt;n&lt;/em&gt; to 2.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Usually degrees of freedom refer to the number of free parameters, such as the number 
  of coefficients fit in a polynomial or cubic spline. Though a smoothing spline has 
  &lt;em&gt;n&lt;/em&gt; parameters and &lt;em&gt;n&lt;/em&gt; nominal degrees of freedom, these &lt;em&gt;n&lt;/em&gt; parameters are heavily 
  constrainted or shrunk down. Thus $df_\lambda$ is a measure of the flexibility of 
  the smoothing spline, the higher it is, the more flexible the smoothing spline.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The definition of effective degrees of freedom, the measure of model complexity:&lt;br /&gt;
  \(\hat{\mathbf{g}}_\lambda = \mathbf{S}_\lambda\mathbf{y}\),&lt;br /&gt;
  where \(\hat{\mathbf{g}}_\lambda\) is the solution to the smoothing spline function 
  &lt;em&gt;g&lt;/em&gt; for a particular choice of $\lambda$, it is an &lt;em&gt;n&lt;/em&gt;-vector containing the fitted 
  values of the model at the training points $x_1,\ldots,x_n$.&lt;br /&gt;
  Then the effective degrees of freedom is defined to be&lt;br /&gt;
  \(df_\lambda = \text{trace}(\mathbf{S}_\lambda) = \sum_{i=1}^n\{ \mathbf{S}_\lambda \}_{ii}\),&lt;br /&gt;
  the sum of the diagonal elements of the matrix $\mathbf{S}_\lambda$.&lt;br /&gt;
  e.g.) for a linear regression model:&lt;br /&gt;
  \(\mathbb{H} = \mathbb{X}(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\),&lt;br /&gt;
  \(\text{trace}(\mathbb{H}) = p+1\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In fitting a smoothing spline, we do not need to select the number or location of the 
  knots; there will be a knot at each training observation. Instead, we need to choose 
  the value of $\lambda$. For this case, LOOCV can be computed very efficiently with 
  essentially the same cost as computing a single fit, using the following formula:&lt;br /&gt;
  \(RSS_{cv}(\lambda) = \sum_{i=1}^n(y_i - \hat{g}_\lambda^{(-i)}(x_i))^2 
  = \sum_{i=1}^n\left[\frac{y_i-\hat{g}_\lambda(x_i)}{1-\{\mathbf{S}_\lambda\}_{ii}}\right]^2\)&lt;br /&gt;
  where \(\hat{g}_\lambda^{(-i)}(x_i)\) indicates the fitted value for this smoothing 
  spline evaluated at $x_i$, where the fit uses all the training observations except 
  for the &lt;em&gt;i&lt;/em&gt;th observation. In contrast, \(\hat{g}_\lambda(x_i)\) indicates the smoothing 
  spline evaluated at $x_i$, where the function is fit to the full data. Thus, we can compute 
  each of LOOCV fits, by one-time computing of the original fit to all of the data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;By effective d.f, we can directly compare the model complexities of models discussed so 
  far, such as linear regression, ridge regression, smoothing splines, cubic splines, etc.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;76-local-regression&quot;&gt;7.6. Local Regression&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A different approach for fitting flexible non-linear functions, with the idea of KNN 
  but closer observations have more weights. This is sometimes referred to as a &lt;em&gt;memory-based&lt;/em&gt; 
  procedure, because we need all the training data each time we compute a prediction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm: Local Regression At $X = x_0$
    &lt;ol&gt;
      &lt;li&gt;Gather the faction $s = k/n$ of training points whose $x_i$ are closest to $x_0$.&lt;/li&gt;
      &lt;li&gt;Assign a weight $K_{i0} = K(x_i,x_0)$ to each point in this neighborhood, so that 
  the point furthest from $x_0$ has weight zero, and the closest has the highest weight. 
  All other points get weight zero.&lt;/li&gt;
      &lt;li&gt;Fit a &lt;em&gt;weighted least squares regression&lt;/em&gt; using the aforementioned weights;&lt;br /&gt;
  Objective function: \(\text{min}_{\beta_0,\beta_1}
                 \left[\sum_{i=1}^n K_{i0}(y_i - \beta_0 - \beta_1 x_i)^2 \right]\)&lt;/li&gt;
      &lt;li&gt;The fitted value at $x_0$ is given by $\hat{f}(x_0) = \hat\beta_0 + \hat\beta_1 x_0$.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In performing local regression, there are choices to be made, such as how to define 
  the weighting function &lt;em&gt;K&lt;/em&gt;, and which regression model to fit. The most important 
  choice is the &lt;em&gt;span s&lt;/em&gt;, which is the proportion of points used to compute the local 
  regression at $x_0$; “How many neighbors?”. It plays a role of the tuning parameter 
  $\lambda$, controls the flexibility of the fit. The smaller &lt;em&gt;s&lt;/em&gt;, the more &lt;em&gt;local&lt;/em&gt; and 
  wiggly the fit. We can use CV methods to choose &lt;em&gt;s&lt;/em&gt;, or we can specify it directly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea of local regression can be generalized in many different ways. In multivariate 
  settings, one very useful generalization involves fitting a multiple linear regression 
  model that is global in some variables, but local in another, such as time. Such 
  &lt;em&gt;varying coefficient models&lt;/em&gt; are a useful way of adapting a model to the most recently 
  gathered data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Local regression can perform poorly if the dimension &lt;em&gt;p&lt;/em&gt; is much larger, suffers the 
  dimensionality problem. Also, it has a boundary problem because there are less data 
  points for neighbors.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;77-generalized-additive-models&quot;&gt;7.7. Generalized Additive Models&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Approaches presented in sections above can be seen as extensions of simple linear 
  regression, flexibly predicting a response &lt;em&gt;Y&lt;/em&gt; on the basis of a single predictor &lt;em&gt;X&lt;/em&gt;. 
  GAMs provide a general framework for extending a standard linear model by allowing 
  non-linear functions of each of the variables, while maintaining &lt;em&gt;additivity&lt;/em&gt;. They 
  can be applied with both quantitative and qualitative responses.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;771-gams-for-regression-problems&quot;&gt;7.7.1. GAMs for Regression Problems&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;standard multiple linear regression model is&lt;br /&gt;
  \(y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \epsilon_i\).&lt;br /&gt;
  for non-linear relationships, replace each linear component \(\beta_j x_{ij}\) with an 
  unspecified (smoothing) non-linear function \(f_j(x_{ij})\), now the model is&lt;br /&gt;
  \(\begin{align*}
  y_i &amp;amp;= \beta_0 + \sum_{j=1}^p f_j(x_{ij}) + \epsilon_i \\
      &amp;amp;= \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \cdots + f_p(x_{ip}) + \epsilon_i.
  \end{align*}\)&lt;br /&gt;
  It is called an additive model because we calculate a separate &lt;em&gt;f&lt;/em&gt; for each &lt;em&gt;X&lt;/em&gt;, 
  then add together all of their contributions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case of using a smoothing spline to fit a GAM, it is not quite as simple as a 
  natural spline case, since the least squares cannot be used. However, Standard 
  softwares have some functions for GAMs using smoothing splines, via an approach 
  known as &lt;em&gt;backfitting&lt;/em&gt;; a method to fit a multivariate model by repeatedly updating 
  the fit for each predictor in turn, holding the others fixed. Each time we update 
  a function, we simply apply the fitting method for that variable to a partial residual.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;772-gams-for-classification-problems&quot;&gt;7.7.2. GAMs for Classification Problems&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;For qualitative response &lt;em&gt;Y&lt;/em&gt;, standard logistic regression model is&lt;br /&gt;
  \(\log\left(\frac{p(X)}{1-p(X)}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p.\)&lt;br /&gt;
  An extension allowing non-linear relationships; a logistic regression GAM is&lt;br /&gt;
  \(\log\left(\frac{p(X)}{1-p(X)}\right) = \beta_0 + f_1(X_1) + f_2(X_2) + \cdots + f_p(X_p).\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;pros-and-cons-of-gams&quot;&gt;Pros and Cons of GAMs&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Pros
    &lt;ol&gt;
      &lt;li&gt;GAMs allow us to fit a non-linear function to each variable, so that we can 
 automatically model non-linear relationships. This means we do not need to 
 manually try out many different transformations on each variable individually.&lt;/li&gt;
      &lt;li&gt;Potentially make more accurate predictions with non-linear fits.&lt;/li&gt;
      &lt;li&gt;Because the model is additive, we can examine the effect of each variable on the 
 response individually while holding all of the other variables fixed.&lt;/li&gt;
      &lt;li&gt;The smoothness of the function for the variable can be summarized via degrees 
 of freedom.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Cons
    &lt;ol&gt;
      &lt;li&gt;The model is restricted to be additive; with many variables, important interactions 
 can be missed. However, we can manually add interaction terms to the model by 
 including additional predictors of the form like $X_j \times X_k$.&lt;/li&gt;
      &lt;li&gt;The solution of the optimization is not unique; $\beta_0$ is not identifiable 
 because each $f_j$ model has its intercept term; a GAM has &lt;em&gt;p+1&lt;/em&gt; total intercepts 
 and they are not distinguishable. For this problem, we make a restriction that 
 every &lt;em&gt;j&lt;/em&gt;th &lt;em&gt;X&lt;/em&gt; variable to be centered; \(\sum_{i=1}^n f_j(x_{ij}) = 0\) and 
 $\hat\beta_0 = \bar{y}$.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        
        <pubDate>Thu, 07 May 2020 00:00:00 +0900</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch7</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch7</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 6. Linear Model Selection and Regularization</title>
        
          <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#chapter-6-linear-model-selection-and-regularization&quot; id=&quot;markdown-toc-chapter-6-linear-model-selection-and-regularization&quot;&gt;Chapter 6. Linear Model Selection and Regularization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#61-subset-selection&quot; id=&quot;markdown-toc-61-subset-selection&quot;&gt;6.1. Subset Selection&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#611-best-subset-selection&quot; id=&quot;markdown-toc-611-best-subset-selection&quot;&gt;6.1.1. Best Subset Selection&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#612-stepwise-selection&quot; id=&quot;markdown-toc-612-stepwise-selection&quot;&gt;6.1.2. Stepwise Selection&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#forward-stepwise-selection&quot; id=&quot;markdown-toc-forward-stepwise-selection&quot;&gt;Forward Stepwise Selection&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#backward-stepwise-selection&quot; id=&quot;markdown-toc-backward-stepwise-selection&quot;&gt;Backward Stepwise Selection&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#hybrid-approaches&quot; id=&quot;markdown-toc-hybrid-approaches&quot;&gt;Hybrid Approaches&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#613-choosing-the-optimal-model&quot; id=&quot;markdown-toc-613-choosing-the-optimal-model&quot;&gt;6.1.3. Choosing the Optimal Model&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#validation-and-cross-validation&quot; id=&quot;markdown-toc-validation-and-cross-validation&quot;&gt;Validation and Cross-Validation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#62-shrinkage-methods&quot; id=&quot;markdown-toc-62-shrinkage-methods&quot;&gt;6.2. Shrinkage Methods&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#621-ridge-regression&quot; id=&quot;markdown-toc-621-ridge-regression&quot;&gt;6.2.1. Ridge Regression&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#in-singular-value-decomposition&quot; id=&quot;markdown-toc-in-singular-value-decomposition&quot;&gt;in Singular Value Decomposition&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#622-the-lasso&quot; id=&quot;markdown-toc-622-the-lasso&quot;&gt;6.2.2. The Lasso&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#another-formulation-for-ridge-regression-and-the-lasso&quot; id=&quot;markdown-toc-another-formulation-for-ridge-regression-and-the-lasso&quot;&gt;Another Formulation for Ridge Regression and the Lasso&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#a-simple-special-case&quot; id=&quot;markdown-toc-a-simple-special-case&quot;&gt;A Simple Special Case&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#bayesian-interpretation&quot; id=&quot;markdown-toc-bayesian-interpretation&quot;&gt;Bayesian Interpretation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#63-dimension-reduction-methods&quot; id=&quot;markdown-toc-63-dimension-reduction-methods&quot;&gt;6.3. Dimension Reduction Methods&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#631--principal-components-regression&quot; id=&quot;markdown-toc-631--principal-components-regression&quot;&gt;6.3.1.  Principal Components Regression&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#principal-components-analysis&quot; id=&quot;markdown-toc-principal-components-analysis&quot;&gt;Principal Components Analysis&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-principal-components-regression-approach&quot; id=&quot;markdown-toc-the-principal-components-regression-approach&quot;&gt;The Principal Components Regression Approach&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#632-partial-least-squares&quot; id=&quot;markdown-toc-632-partial-least-squares&quot;&gt;6.3.2. Partial Least Squares&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#64-considerations-in-high-dimensions&quot; id=&quot;markdown-toc-64-considerations-in-high-dimensions&quot;&gt;6.4. Considerations in High Dimensions&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#641-high-dimensional-data&quot; id=&quot;markdown-toc-641-high-dimensional-data&quot;&gt;6.4.1. High-Dimensional Data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#642-what-goes-wrong-in-high-dimensions&quot; id=&quot;markdown-toc-642-what-goes-wrong-in-high-dimensions&quot;&gt;6.4.2. What Goes Wrong in High Dimensions?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#643-regression-in-high-dimensions&quot; id=&quot;markdown-toc-643-regression-in-high-dimensions&quot;&gt;6.4.3. Regression in High Dimensions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#644-interpreting-results-in-high-dimensions&quot; id=&quot;markdown-toc-644-interpreting-results-in-high-dimensions&quot;&gt;6.4.4. Interpreting Results in High Dimensions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-6-linear-model-selection-and-regularization&quot;&gt;Chapter 6. Linear Model Selection and Regularization&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Limitations of LSE
    &lt;ol&gt;
      &lt;li&gt;Prediction Accuracy:
        &lt;ul&gt;
          &lt;li&gt;if &lt;em&gt;n&lt;/em&gt; is not much larger than &lt;em&gt;p&lt;/em&gt;, the least squares fit can have a lot 
 of variability, results in overfitting and poor predictions to test data.&lt;/li&gt;
          &lt;li&gt;if &lt;em&gt;p&lt;/em&gt; &amp;gt; &lt;em&gt;n&lt;/em&gt;, there is no unique solution for the least squares coefficient 
 estimate; as $ Var(\hat\beta)=\infty$.&lt;/li&gt;
          &lt;li&gt;if &lt;em&gt;p&lt;/em&gt; is large, there can be correlations between &lt;em&gt;X&lt;/em&gt; variables. A model 
 having multicollinearity can have high variance.&lt;br /&gt;
&lt;em&gt;Constraining&lt;/em&gt; or &lt;em&gt;Shrinking&lt;/em&gt; the estimated coefficients can reduce the variance 
with negligible increase in bias, and improve in the accuracy to the test data.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Model Interpretability:
        &lt;ul&gt;
          &lt;li&gt;There are irrelevant variables $X_j$. Removing by setting coefficient estimates 
 $\beta_j = 0$, we can have more interpretability.&lt;br /&gt;
&lt;em&gt;Feature selection&lt;/em&gt; or &lt;em&gt;Variable selection&lt;/em&gt; can exclude irrelevant variables from a 
multiple regression model.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;61-subset-selection&quot;&gt;6.1. Subset Selection&lt;/h2&gt;

&lt;h3 id=&quot;611-best-subset-selection&quot;&gt;6.1.1. Best Subset Selection&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;fit a separate least squares regression for all $2^p$ possible models with combinations 
of the &lt;em&gt;p&lt;/em&gt; predictors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm
    &lt;ol&gt;
      &lt;li&gt;$\mathcal{M}_0$ as &lt;em&gt;null model&lt;/em&gt; (i.e., $ Y = \beta_0 + \epsilon $)&lt;/li&gt;
      &lt;li&gt;For $ k = 1, 2, \ldots, p $:&lt;br /&gt;
  (a) Fit all \({p \choose k}\) models with &lt;em&gt;k&lt;/em&gt; predictors&lt;br /&gt;
  (b) Pick the smallest RSS, (or largest $R^2$) = $ \mathcal{M}_k $&lt;/li&gt;
      &lt;li&gt;Select best model among $\mathcal{M}_0, \ldots,\mathcal{M}_p$ using cross-validated 
  prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Guarantees the best selection, while it suffers from computational limitations. Also, it 
only works for least squares linear regression.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;in the case of logistic regression, we use &lt;em&gt;deviance&lt;/em&gt;, $-2\log$MLE, instead of RSS in 
the 2nd step of algorithm upon.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;612-stepwise-selection&quot;&gt;6.1.2. Stepwise Selection&lt;/h3&gt;

&lt;h4 id=&quot;forward-stepwise-selection&quot;&gt;Forward Stepwise Selection&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Algorithm
    &lt;ol&gt;
      &lt;li&gt;$\mathcal{M}_0$ as &lt;em&gt;null model&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;For $ k = 1, 2, \ldots, p $:&lt;br /&gt;
  (a) Fit all &lt;em&gt;p - k&lt;/em&gt; models in \(\mathcal{M}_k\) with one additional predictor&lt;br /&gt;
  (b) Pick the smallest RSS among &lt;em&gt;p - k&lt;/em&gt; models, $\mathcal{M}_{k+1}$&lt;/li&gt;
      &lt;li&gt;Select best model among $\mathcal{M}_0, \ldots,\mathcal{M}_p$ with CV scores&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Total $\frac{p(p+1)}{2}+1$ possible models. No guarantee but available for the case of 
high dimensional data($n&amp;lt;p$).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;backward-stepwise-selection&quot;&gt;Backward Stepwise Selection&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Algorithm
    &lt;ol&gt;
      &lt;li&gt;$\mathcal{M}_p$ as &lt;em&gt;full model&lt;/em&gt;, contains all &lt;em&gt;p&lt;/em&gt; predictors&lt;/li&gt;
      &lt;li&gt;For $ k = p, p-1, \ldots, 1 $:&lt;br /&gt;
  (a) Fit all &lt;em&gt;k - 1&lt;/em&gt; models contain all but one of the predictors in \(\mathcal{M}_k\)&lt;br /&gt;
  (b) Pick the smallest RSS among &lt;em&gt;k - 1&lt;/em&gt; models, $\mathcal{M}_{k-1}$&lt;/li&gt;
      &lt;li&gt;Select best model among $\mathcal{M}_0, ldots,\mathcal{M}_p$ with CV scores&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Total $\frac{p(p+1)}{2}+1$ possible models. No guarantee and not for &lt;em&gt;n &amp;lt; p&lt;/em&gt; case.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;hybrid-approaches&quot;&gt;Hybrid Approaches&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;add then remove one predictors in each step.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;613-choosing-the-optimal-model&quot;&gt;6.1.3. Choosing the Optimal Model&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A model containing all of the predictors will always have the smallest RSS and the largest 
$R^2$, since these quantities are related to the training error. Instead, we need a model with a 
low test error.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;$C_p = \frac{1}{n}(RSS + 2 d \hat\sigma^2)$&lt;br /&gt;
 For a fitted least squares model, with &lt;em&gt;d&lt;/em&gt; as the number of predictors and $\hat\sigma^2$ as 
 an estimate of the variance of the error. Typically $\hat\sigma^2$ is estimated using the full 
 model containing all predictors. Adding a penalty to the training RSS is to adjust its 
 underestimation to the test error. As the number of predictors increase, the penalty increase. 
 If there is a proof of $\hat\sigma^2$ is an unbiased estimate of $\sigma^2$, $C_p$ is an unbiased 
 estimate of test MSE. Then, a model with the lowest $C_p$ is the best model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AIC $= \frac{1}{n}(RSS + 2 d \hat\sigma^2)$&lt;br /&gt;
 For a models fit by maximum likelihood(MLE), given by omitted irrelevant constants. $C_p$ and 
 AIC are proportional to each other.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BIC $= \frac{1}{n}(RSS + \log(n)d\hat\sigma^2)$&lt;br /&gt;
 From a Bayesian point of view, for a fitted least squares model. Also given by omitted 
 irrelevant constants. BIC has heavier penalty then $C_p$ or AIC, results in selecting smaller 
 models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adjusted $R^2 = 1 - \frac{RSS/(n-d-1)}{TSS/(n-1)}$&lt;br /&gt;
 Since the usual $R^2$ is defined as $1 - RSS/TSS$, it always increases as more variables added. 
 Adjusted $R^2$ gives penalty of &lt;em&gt;d&lt;/em&gt;, the number of predictors in the denominator. Unlike other 
 statistics, a large value of adjusted $R^2$ indicates a small test error.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;validation-and-cross-validation&quot;&gt;Validation and Cross-Validation&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;one-standard-error rule&lt;/em&gt;&lt;br /&gt;
First calculate the standard error of the estimated test MSE for each model size, then select the 
smallest model for which the estimated test error is within one standard error of the lowest point 
on the curve.&lt;br /&gt;
If a set of models appear to be more or less equally good, then we might as well choose the simplest 
model; the model with the smallest number of predictors.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;62-shrinkage-methods&quot;&gt;6.2. Shrinkage Methods&lt;/h2&gt;

&lt;h3 id=&quot;621-ridge-regression&quot;&gt;6.2.1. Ridge Regression&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ridge regression coefficient estimates&lt;br /&gt;
\(\begin{align*}
\hat\beta^R &amp;amp;= \text{min}_{\beta}\left[
                  \underbrace{\sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p \beta_j x_{ij})}_{RSS}
                  + \lambda\sum_{j=1}^p \beta_j^2 \right] \\
            &amp;amp;= (X^TX + \lambda I)^{-1} X^T\underline{y}	
\end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\lambda \ge 0 $ is a &lt;em&gt;tuning parameter&lt;/em&gt;, $\lambda\sum_{j=1}^p \beta_j^2$ is a &lt;em&gt;shrinkage penalty&lt;/em&gt;. 
The penalty is small when the coefficients are close to zero, and so it has the effect of &lt;em&gt;shrinking&lt;/em&gt; 
the estimates of $\beta_j$ towards zero. Ridge regression will produce a different set of coefficient 
estimates $\beta_{\lambda}^R$, for each value of $\lambda$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We do not want to shrink the intercept $\beta_0$, which is simply a measure of the mean value of 
the response when $x_{i1}=x_{i2}=\ldots=x_{ip}=0$. If the variables, the columns of the data matrix
&lt;strong&gt;$X$&lt;/strong&gt;, have been centered to have mean zero before ridge regression is performed, then the estiamted 
intercept will take the form $\hat\beta_0 = \bar{y} = \sum_{i=1}^n y_i/n$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The standard least squares coefficient estimates are &lt;em&gt;scale equivariant&lt;/em&gt;; multiplying $X_j$ by a constant 
&lt;em&gt;c&lt;/em&gt; leads to a scaling of the least squares coefficient estimates by a factor of 1/&lt;em&gt;c&lt;/em&gt;. I.e., regardless 
of how the &lt;em&gt;j&lt;/em&gt;th predictor is scaled, $X_j\hat\beta_j$ will remain the same.&lt;br /&gt;
In contrast, the ridge regression coefficient estimates can change substantially when multiplying a 
given predictor by a constant. The value of $X_j\hat\beta_{j,\lambda}^R$ may depend on the scaling of 
the other predictors. Thus, before applying ridge regression, the variables need to be standardized to 
have a standard deviation of one.&lt;br /&gt;
The formula: \(\tilde{x}_{ij}=\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2}}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ridge regression overperforms the standard least squares when the number of variables &lt;em&gt;p&lt;/em&gt; is almost 
as large as the number of observations &lt;em&gt;n&lt;/em&gt;, or even when $p &amp;gt; n$. Also it has computational advantages 
over best subset selection, which requires searching through $2^p$ models. Ridge regression only fits a 
single model for any fixed value of $\lambda$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;in-singular-value-decomposition&quot;&gt;in Singular Value Decomposition&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;where $ X = \mathbb{UDV}^T$,&lt;br /&gt;
\(\begin{align*} X\hat\beta^{\text{LSE}} &amp;amp;= X(X^TX)^{-1}X^T\underline{y} \\
                                         &amp;amp;= \mathbb{UU}^T\underline{y} \\
                            X\hat\beta^R &amp;amp;= UD(D^2 + \lambda I)^{-1}DU^T\underline{y} \\
                                         &amp;amp;= \sum_{j=1}^p\underline{u}_j\frac{d_{ij}^2}{d_{ij}^2+\lambda}\underline{u}_j^T\underline{y}
\end{align*}\)&lt;br /&gt;
\(\begin{align*}
\rightarrow \partial f(\lambda) &amp;amp;= tr[X(X^TX + \lambda I)^{-1} X^T] \\
                                &amp;amp;= tr(\mathbb{H}_{\lambda})  \\
                                &amp;amp;= \sum_{j=1}^p\frac{d_{ij}^2}{d_{ij}^2+\lambda}
\end{align*}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;622-the-lasso&quot;&gt;6.2.2. The Lasso&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ridge regression estimates shrink towards zero but will not set nay of them exactly to zero(unless 
$\lambda = \infty$). This may not be a problem for prediction accuracy, but it can be a challenge in 
model interpretation when &lt;em&gt;p&lt;/em&gt; is quite large.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;lasso&lt;/em&gt;&lt;br /&gt;
\(\hat\beta^L_{\lambda} = \text{min}_{\beta}\left[RSS+\lambda\sum_{j=1}^p|\beta_j|\right]\)&lt;br /&gt;
Instead of $\mathcal{l}_2$ penalty in Ridge, the lasso uses an $\mathcal{l}_1$ penalty. 
The $\mathcal{l}_1$ norm of a coefficient vector $\beta$ is given by $\lVert \beta \rVert_1 = 
\sum |\beta_j|$. This penalty has the effect of forcing some of the coefficient estimates to be 
exactly equal to zero when the tuning parameter is sufficiently large. Hence, the lasso performs 
&lt;em&gt;variable selection&lt;/em&gt;, these &lt;em&gt;sparse&lt;/em&gt; models with the lasso are much easier to interpret than those 
with ridge.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;another-formulation-for-ridge-regression-and-the-lasso&quot;&gt;Another Formulation for Ridge Regression and the Lasso&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ridge:
\(\text{min}_{\beta}\left\{ \sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p\beta_j x_{ij})^2 
                      \right\}\) subject to $\sum_{j=1}^p\beta_j^2 \le s $&lt;br /&gt;
Lasso:
\(\text{min}_{\beta}\left\{ \sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p\beta_j x_{ij})^2 
                      \right\}\) subject to $\sum_{j=1}^p|\beta_j| \le s $&lt;br /&gt;
where the &lt;em&gt;budget s&lt;/em&gt; as the regularization parameter ($\lambda\uparrow \equiv s\downarrow$).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;when $p = 2$, then the ridge regression estimates have the smallest RSS out of all points that lie 
within the circle defined by $\beta_1^2 + \beta_2^2 \le s$, while the lasso estimates have within 
the diamond defined by $|\beta_1|+|\beta_2| \le s$. when $p = 3$, he constraint region for ridge 
becomes a sphere, for lasso becomes a polyhedron. For larger &lt;em&gt;p&lt;/em&gt;, it becomes a hypersphere and a 
polytope each. The lasso leads to feature selection due to the sharp corners of its constraint region.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;the number of predictors that is related to the response is never known a &lt;em&gt;priori&lt;/em&gt; for real data sets. 
A technique such as cross-validation can be used in order to determine which approach is better on a 
particular data set.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;a-simple-special-case&quot;&gt;A Simple Special Case&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;An analytical method(solution) for the case when $n = p$, and &lt;strong&gt;&lt;em&gt;X&lt;/em&gt;&lt;/strong&gt; a diagonal matrix with 1’s on 
the diagonal and 0’s in all off-diagonal elements. I.e., the columns of &lt;strong&gt;&lt;em&gt;X&lt;/em&gt;&lt;/strong&gt; are orthogonal. Also, 
assume that we are performing regression without an intercept(or standardized).&lt;br /&gt;
(c.f. in real world cases, we need to use numerical methods.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The usual least squares, $\hat\beta$ is that minimizes; $\sum_{j=1}^p(y_j-\beta_j)^2$.&lt;br /&gt;
and for the ridge, minimizing $\sum_{j=1}^p(y_j-\beta_j)^2+\lambda\sum_{j=1}^p\beta_j^2$.&lt;br /&gt;
and for the lasso, minimizing $\sum_{j=1}^p(y_j-\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j|$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The ridge regression estiamtes $\hat\beta_j^R = y_j/(1+\lambda)$ and&lt;br /&gt;
\(\text{the lasso estimates} \begin{align*}
\hat\beta_j^L &amp;amp;= \text{sign}(\hat\beta_j)(|\hat\beta_j|-\lambda)_{+}, \\
    \text{or} &amp;amp;= \begin{cases}
                  y_j - \lambda/2, &amp;amp; \mbox{if }y_j &amp;gt; \lambda/2; \\
                  y_j + \lambda/2, &amp;amp; \mbox{if }y_j &amp;lt; -\lambda/2; \\
                  0				 &amp;amp; \mbox{if }|y_j| \le \lambda/2.
                  \end{cases}
\end{align*}\)&lt;br /&gt;
&lt;img src=&quot;/assets/images/ch6_ridge_lasso_effect_0.png&quot; alt=&quot;png&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;&lt;br /&gt;
Ridge shrinks all coefficients towards zero by the same &lt;em&gt;“proportion”&lt;/em&gt;,&lt;br /&gt;
Lasso shrinks all coefficients towards zero by the same &lt;em&gt;“amount”&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;bayesian-interpretation&quot;&gt;Bayesian Interpretation&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;$p(\beta|X,Y)\propto f(Y|X,\beta)p(\beta|X) = f(Y|X,\beta)p(\beta)$&lt;br /&gt;
with assumption of $p(\beta)=\prod_{j=1}^p g(\beta_j)$ for some density function &lt;em&gt;g&lt;/em&gt;.&lt;br /&gt;
Two special cases of &lt;em&gt;g&lt;/em&gt;:
    &lt;ul&gt;
      &lt;li&gt;If &lt;em&gt;g&lt;/em&gt; is a Gaussian distribution with mean zero and standard deviation a function of $\lambda$, 
it follows that the &lt;em&gt;posterior mode&lt;/em&gt; for $\beta$, is given by the ridge regression solution. Also, 
the solution is equal to posterior mean.&lt;/li&gt;
      &lt;li&gt;If &lt;em&gt;g&lt;/em&gt; is a double-exponential(Laplace) distribution with mean zero and scale parameter a function 
of $\lambda$, it follows that the posterior mode for $\beta$ is the lasso soultion(which is not the 
posteriror mean in this case).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hence, the lasso expects a priori that many of the coefficients are (exactly) zero, while ridge 
assumes the coefficients are randomly distributed about zero.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;63-dimension-reduction-methods&quot;&gt;6.3. Dimension Reduction Methods&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;p&lt;/em&gt; predictors to &lt;em&gt;M&lt;/em&gt; new transformed variables.&lt;br /&gt;
Let $Z_m = \sum_{j=1}^p\phi_{jm}X_j$ represent &lt;em&gt;M &amp;lt; p linear combinations&lt;/em&gt; of original &lt;em&gt;p&lt;/em&gt; predictors. 
Then fit the linear regression model $y_i = \theta_0 + \sum_{m=1}^M\theta_m z_{im} + \epsilon_i, \quad i = 1, \ldots, n$, 
using least squares. If the constants $\phi_{1m}, \ldots, \phi_{pm}$ are chosen wisely, dimension 
reduction approaches can outperform least squares regression. I.e., using least squares, fitting 
reduced model can lead to better results than fitting the standard linear model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(\sum_{m=1}^M\theta_m z_{im} = \sum_{m=1}^M\theta_m\sum_{j=1}^p\phi_{jm}x_{ij} = 
  \sum_{j=1}^p\sum_{m=1}^M\theta_m\phi_{jm}x_{ij} = \sum_{j=1}^p\beta_j x_{ij},\)&lt;br /&gt;
  where \(\beta_j = \sum_{m=1}^M\theta_m\phi_{jm}\).&lt;br /&gt;
  Hence, this model can be a special case of the standard linear regression model. In situations where 
  &lt;em&gt;p&lt;/em&gt; is large relative to &lt;em&gt;n&lt;/em&gt;, demension reduction methods can significantly reduce the variance of the 
  fitted coefficients. If $M = p$, and all the $Z_m$ are linearly independent, then there are no constraints 
  and the model is equivalent to the standard linear model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All dimension reduction methods work in two steps. First, the transformed predictors $Z_m$ are obtained. 
  Second, the model is fit using these &lt;em&gt;M&lt;/em&gt; predictors. The choice of $Z_m$, which is, the selection of the 
  $\phi_{jm}$’s can be achieved in different ways.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;631--principal-components-regression&quot;&gt;6.3.1.  Principal Components Regression&lt;/h3&gt;

&lt;h4 id=&quot;principal-components-analysis&quot;&gt;Principal Components Analysis&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Goal of PCA:&lt;br /&gt;
  PCA is a technique for reducing the dimension of an &lt;em&gt;n by p&lt;/em&gt; data matrix &lt;strong&gt;&lt;em&gt;X&lt;/em&gt;&lt;/strong&gt;, finding small number 
  of dimensions &lt;em&gt;M&lt;/em&gt;, which have simillar amount of information to original &lt;em&gt;p&lt;/em&gt; predictors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;principal component&lt;/em&gt; direction of the data is that along which the observations &lt;em&gt;vary the most&lt;/em&gt;; 
  with the largest variance of the observations projected onto. The principal component vector $Z_m$ 
  defines the line that is &lt;em&gt;as close as possible&lt;/em&gt; to the data, minimizing the sum of the squared 
  perpendicular distances between each point and the line. In other word, the principal component appears 
  to capture most of the information contained in two variables.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;e.g. in the first principal component,&lt;br /&gt;
  &lt;img src=&quot;/assets/images/ch6_pca_0.png&quot; alt=&quot;png&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;&lt;br /&gt;
  total variance keeped: $Var(X_1)+Var(X_2) = Var(PC_1)+Var(PC_2)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;where $X_s$ is $n \times p$ standardized matrix,&lt;em&gt;j&lt;/em&gt;th Principal Component Vector of $X_s$: $z_j = X_s v_j$, 
  $\quad j=1,\ldots,p$ is that satisfying \(\text{max}_{\alpha}Var(X_s\alpha)\) subject to \(\lVert\alpha\rVert=1\). 
  Here, the values of $z_{1j}, \ldots, z_{nj}$ are known as the &lt;em&gt;principal component scores&lt;/em&gt;.&lt;br /&gt;
  $v_j$ is $p \times 1$ size eigenvector of $X_s^T X_s$ corresponding to the &lt;em&gt;j&lt;/em&gt;th largest eigenvalue, 
  and $\alpha$ is $v_j$’s orthogonality to $v_1,\ldots,v_{j-1}$ ($\alpha^T S v_k = 0$, where S is the 
  sample covariance matrix of $X_s$, or $X_s^T X_s$, and $k = 1, \cdots, j-1$).&lt;br /&gt;
  Then $z_1 = X_s v_1$, $z_2\bot z_1$, $z_3\bot z_1,z_2$, $\cdots$, $z_p\bot z_1,\ldots,z_{p-1}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;derivation&lt;/em&gt;&lt;br /&gt;
  Since $X_s$ is standardized matrix,&lt;br /&gt;
  \(Var(X_s\alpha) = \alpha^T X_s^T X_s\alpha\)&lt;br /&gt;
  &lt;em&gt;by Lagrangian form&lt;/em&gt;,&lt;br /&gt;
  \(\begin{align*}
  \text{max}_{\alpha}Q(X_s,\lambda) &amp;amp;= \text{max}_{alpha}\left[\alpha^T X_s^T X_s\alpha
                                                              -\lambda\alpha^T\alpha \right] \\
  \rightarrow \frac{\partial Q}{\partial\alpha} &amp;amp;= 2X_s^T X\alpha - 2\lambda\alpha \\
  \text{for } \hat\alpha, X_s^T X\alpha &amp;amp;= \lambda\alpha
  \end{align*}\)&lt;br /&gt;
  &lt;em&gt;note that&lt;/em&gt; $\mathbb{A}_v = ev$, the combination of eigenvalue and eigenvector of $\mathbb{A}$.&lt;br /&gt;
  Thus, $\alpha = v_j$, the &lt;em&gt;j&lt;/em&gt;th eigenvector of $X_s^T X_s$, that is, the constraint of orthogonality 
  is satisfied.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since PCA has no single solution &lt;em&gt;M&lt;/em&gt;;&lt;br /&gt;
  the proportion of variance explained by &lt;em&gt;m&lt;/em&gt;th PC($Z_m$) used:&lt;br /&gt;
  \(PVE_m = \frac{Var(Z_m)}{\sum_{j=1}^p(Var(Z_j))}\)&lt;br /&gt;
  (\(\sum_{j=1}^p(Var(Z_j)) = \sum Var(X_j) =\) total variance)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;in &lt;em&gt;SVD&lt;/em&gt; of covariance matrix $X^T X$,&lt;br /&gt;
  \(\begin{align*}
  X^T X &amp;amp;= \mathbb{VDU}^T\mathbb{UDV}^T \\
        &amp;amp;= \mathbb{VD^2 V}^T
  \end{align*}\)&lt;br /&gt;
  in this eigen decomposition,&lt;br /&gt;
  \(\mathbb{V} = (v_1,\ldots,v_p)\) the eigen vectors of $X^T X$&lt;br /&gt;
  \(\mathbb{D}^2 = \begin{bmatrix}
                      d_1^2 &amp;amp; \cdots &amp;amp; 0 \\
                      \vdots &amp;amp; \ddots &amp;amp; \vdots \\
                      0 &amp;amp; \cdots &amp;amp; d_p^2
                      \end{bmatrix}\)
                      $d_j^2 = e_j$, &lt;em&gt;j&lt;/em&gt;th eigenvalue of $X^T X$&lt;br /&gt;
  thus,&lt;br /&gt;
  \(\begin{align*}
  Var(Z_m) &amp;amp;= \frac{1}{n}(Z_m^T Z_m) \\
           &amp;amp;= \frac{1}{n}(v_m^T X_s^T X_s v_m) \\
           &amp;amp;= \frac{1}{n}(v_m^T\mathbb{VD}^2\mathbb{V}^T v_m) \\
           &amp;amp;= \frac{1}{n}d_m^2 = \frac{1}{n}e_m
  \end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Therefore,&lt;br /&gt;
  \(PVE_m = \frac{Var(Z_m)}{\sum_{j=1}^p(Var(Z_j))} = \frac{e_m}{\sum_{j=1}^p e_j}\)&lt;br /&gt;
  we can draw a &lt;em&gt;scree plot&lt;/em&gt; on the value of $PVE_m$ over the value of &lt;em&gt;m&lt;/em&gt; to find optimal “M”.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;the-principal-components-regression-approach&quot;&gt;The Principal Components Regression Approach&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The key idea is that a small number of principal components can explain most of the variability in the 
  data, as well as the relationship with the response. Under this assumption, fitting a least squares model 
  to $Z_1, \ldots, Z_M$ will lead to better results than fitting a least squares model to $X_1, \ldots, X_p$, 
  since most or all of the information in the data is contained in $Z_m$ and there are smaller number of 
  coefficients, we can mitigate overfitting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that PCR is not a feature selection method; is a linear combination of all &lt;em&gt;p&lt;/em&gt; of the original features. 
  In this sense, PCR is more closely related to ridge regression than to the lasso.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deciding “M”:&lt;br /&gt;
  full model is \(\hat{Y} = \hat{\theta}_0 + \hat{\theta}_1 Z_1 + \cdots + \hat{\theta}_p Z_p\)&lt;br /&gt;
  when $Z_1,\ldots,Z_m$ is from standardized $X_s$ and \(\hat{y}_0 = \bar{y}\),&lt;br /&gt;
  as $Z_j$’s are orthogonal, adding variable $Z_{j+1}$ does not affect the coefficients. Thus, $\theta_j$’s are 
  not changed by feature selection; that is,&lt;br /&gt;
  \(\hat{Y} = \hat{\theta}_0 + \hat{\theta}_1 Z_1 \\
  \hat{Y} = \hat{\theta}_0 + \hat{\theta}_1 Z_1 + \hat{\theta}_2 Z_2 \\
  \vdots \\
  \hat{Y} = \hat{\theta}_0 + \hat{\theta}_1 Z_1 +\cdots + \hat{\theta}_p Z_p\) the value of $\theta_k$ is the same.&lt;br /&gt;
  Then we can use CV methods over these models to get optimal &lt;em&gt;M&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;632-partial-least-squares&quot;&gt;6.3.2. Partial Least Squares&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The PCR approach identifies linear combinations, or &lt;em&gt;directions&lt;/em&gt;, that best represent the predictors. 
  These directions are identified in an &lt;em&gt;unsupervised&lt;/em&gt; way, since the response &lt;em&gt;Y&lt;/em&gt; is not used to help 
  determine the principal component directions. There, PCR suffers from a drawback: there is no guarantee 
  that the directions that best explain the predictors will also be the best directions to use for 
  predicting the response.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;PLS is a &lt;em&gt;supervised&lt;/em&gt; alternative to PCR; finding PLS directions $Z_1,\ldots,Z_m$ that 
  $Cov(Y,Z_1)\ge Cov(Y,Z_2)\ge\cdots\ge Cov(Y,Z_M)$ instead of $Var(Z_1)\ge\cdots\ge Var(Z_M)$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;First PLS direction is computed, after standardizing predictors, by setting each $\phi_{j1}$ equal to 
  the coefficient from the simple linear regression $Y$ onto $X_j$. As $Z_1 = \sum_{j=1}^p\phi_{j1}X_j$, 
  PLS places the highest weight on the variables that are most strongly related to the response.&lt;br /&gt;
  To find second PLS direction, we adjust each of the variables for $Z_1$, by regressing each variable 
  on $Z_1$ and taking &lt;em&gt;residuals&lt;/em&gt;. The residuals can be interpreted as the remaining information that has 
  not been explained by the first PLS direction. Then we compute $Z_2$ using this orthogonalized data by 
  the same way of computing $Z_1$. This predecure repeated &lt;em&gt;M&lt;/em&gt; times.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;in Simple Regression case,&lt;br /&gt;
  $\hat X_j^s$ is a projection of original data $X_j^s$ to a vector $Z_1$; $X_j^s = \alpha Z_1$.&lt;br /&gt;
  the residual vector $r_j = \hat X_j^s - X_j^s$ and $r_j\bot Z_1$.&lt;br /&gt;
  Then, $r_j = X_j^{(2)}$ is the orthogonalized data for computing the next $Z_2$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;m&lt;/em&gt;th PLS direction:&lt;br /&gt;
  \(\text{max}_{\phi} Cov(y,X_s\phi)\) subject to $\lVert\phi\rVert = 1$, $\phi^T S v_l = 0$&lt;br /&gt;
  for $\phi$ as orthogonal directions, sample covariance matrix &lt;em&gt;S&lt;/em&gt;, and $v_l$ as &lt;em&gt;l&lt;/em&gt; th PLS direction.&lt;br /&gt;
  \(\text{max}_{\phi}[E(\phi^T X_s^T y)-E(y)E(\phi^T X_s)]\), as standardized, $E(X_s) = 0$,&lt;br /&gt;
  \(\equiv \text{max}_{\phi}\phi^T \dot X_s^T y\) is maximization of dot product of 2 vectors.&lt;br /&gt;
  note that, when two vectors are in the same direction, dot product is maximized.&lt;br /&gt;
  $\therefore \phi=X_s^T y$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ch6_pls_algorithm_0.png&quot; alt=&quot;png&quot; width=&quot;80%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;64-considerations-in-high-dimensions&quot;&gt;6.4. Considerations in High Dimensions&lt;/h2&gt;

&lt;h3 id=&quot;641-high-dimensional-data&quot;&gt;6.4.1. High-Dimensional Data&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Data sets that containing more features than observations, $p &amp;gt; n$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;642-what-goes-wrong-in-high-dimensions&quot;&gt;6.4.2. What Goes Wrong in High Dimensions?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Standard least squares cannot be performed. Regardless of the true relationship between features and response, 
  least squares will result in a perfect fit to the data, lead to overfitting of the data and poor 
  predictions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;643-regression-in-high-dimensions&quot;&gt;6.4.3. Regression in High Dimensions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;new technologies that allow for the collection of measurements for thousands or millions of features 
  are a double-edged sword: they can lead to improved predictive models if these features are in fact 
  relevant to the problem at hand, but will lead to worse results if the features are not relevant. 
  Even if they are relevant, the variance incurred in fitting their coefficients may outweigh the 
  reduction in bias that they bring.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;644-interpreting-results-in-high-dimensions&quot;&gt;6.4.4. Interpreting Results in High Dimensions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In high-dimensional setting, the multicollinearity problem is extreme:&lt;br /&gt;
  any variable in the model is a linear combination of all of the other variables in the model. This means 
  we can never know exactly which variables truly are predictive of the outcome, and we can never identify 
  the best coefficients for use in the regression.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When $p &amp;gt; n$, it is easy to obtain a a useless model that has zero residuals. Therefore, we should never 
  use sum of squared errors, p-values, $R^2$ statistics, or other traditional measures of model fit on the 
  training data as evidence of a good model fit. Instead we report results on an independent test set, or 
  cross-validation errors.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        
        <pubDate>Thu, 30 Apr 2020 00:00:00 +0900</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch6</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch6</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 5. Resampling Methods</title>
        
          <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#chapter-5-resampling-methods&quot; id=&quot;markdown-toc-chapter-5-resampling-methods&quot;&gt;Chapter 5. Resampling Methods&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#51--cross-validation&quot; id=&quot;markdown-toc-51--cross-validation&quot;&gt;5.1.  Cross-Validation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#511-the-validation-set-approach&quot; id=&quot;markdown-toc-511-the-validation-set-approach&quot;&gt;5.1.1. The Validation Set Approach&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#512-leave-one-out-cross-validation&quot; id=&quot;markdown-toc-512-leave-one-out-cross-validation&quot;&gt;5.1.2. Leave-One-Out Cross-Validation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#513-k-fold-cross-validation&quot; id=&quot;markdown-toc-513-k-fold-cross-validation&quot;&gt;5.1.3. k-Fold Cross-Validation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#514-bias-variance-trade-off-for-k-fold-cross-validation&quot; id=&quot;markdown-toc-514-bias-variance-trade-off-for-k-fold-cross-validation&quot;&gt;5.1.4. Bias-Variance Trade-Off for k-Fold Cross-Validation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#515-cross-validation-on-classification-problems&quot; id=&quot;markdown-toc-515-cross-validation-on-classification-problems&quot;&gt;5.1.5. Cross-Validation on Classification Problems&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#52-the-bootstrap&quot; id=&quot;markdown-toc-52-the-bootstrap&quot;&gt;5.2. The Bootstrap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-5-resampling-methods&quot;&gt;Chapter 5. Resampling Methods&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;repeatedly drawing samples from a training set and refitting a model of interest on 
each sample in order to obtain additional information about the fitted model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To optain information that would not be available from fitting the model only once 
using the original training sample.&lt;br /&gt;
e.g. to estimate the variability of a model fit, draw different samples and fit it to 
each new sample, then examine the extent to which the resulting fits differ.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;51--cross-validation&quot;&gt;5.1.  Cross-Validation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;In the absence of a very large designated test set that can be used to directly estimate 
the test error rate, a class of methods that estimate the test error rate by &lt;em&gt;holding out&lt;/em&gt; 
a subset of the training observations from the fitting process, then applying the statistical 
learning method to those held out observations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;511-the-validation-set-approach&quot;&gt;5.1.1. The Validation Set Approach&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;randomly dividing the available set of observations into two parts;&lt;br /&gt;
a &lt;em&gt;training set&lt;/em&gt; and a &lt;em&gt;validation set&lt;/em&gt; (or hold-out set)&lt;br /&gt;
model is fit on the training set, and the fitted model is used to predict the responses for the 
observations in the validation set. The validation set error rate estimates the test error rate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Repeating this predecure, we have different estimate for the test MSE over random splits of the 
observations and there are two issues:&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;The validation estimate of the test error rate can be highly variable, depending on which observations 
 are included in the training set or the validation test.&lt;/li&gt;
      &lt;li&gt;Only a subset of the observations are used to fit the model. Trained on fewer observations, the 
 validation set error rate may &lt;em&gt;overestimate&lt;/em&gt; the test error rate for the model fit on the entire 
 data set.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;512-leave-one-out-cross-validation&quot;&gt;5.1.2. Leave-One-Out Cross-Validation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;each single observation is used for the validation set, and the remaining observations are for the 
training set. The statistical learning method is fit on the &lt;em&gt;n-1&lt;/em&gt; training obs. The prediction is made 
for the excluded observation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LOOCV estiamte for the test MSE:&lt;br /&gt;
\(CV_{(n)} = \frac{1}{n}\sum_{i=1}^n MSE_i\)&lt;br /&gt;
No overestimation on the test error, No variance of test MSE, but Expansive.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;a shortcut of LOOCV on Least Squares(regression):&lt;br /&gt;
\(\begin{align*}
CV_{(n)} = \frac{1}{n} \sum_{i=1}^n \left(\frac{y_i - \hat y_i}{1-h_i}\right)^2
\end{align*}\)&lt;br /&gt;
where $\hat y_i$ is the fitted value from the original least squares fit, one-time build of a full 
model and set a leverage $h_i = \frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum_{i^\prime=1}^n(x_{i^\prime}-\bar{x})^2}$. 
The levearge lies between &lt;em&gt;1/n&lt;/em&gt; and &lt;em&gt;1&lt;/em&gt;, reflects the amount that an observation influences its own fit.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;513-k-fold-cross-validation&quot;&gt;5.1.3. k-Fold Cross-Validation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;random division into &lt;em&gt;k&lt;/em&gt; groups, or &lt;em&gt;folds&lt;/em&gt;, of approximately equal size. A fold is used for the 
validation set, and the method is fit on the remaining &lt;em&gt;k-1&lt;/em&gt; folds. The MSE is computed on the 
observations in the held-out fold and the procedure is repeated &lt;em&gt;k&lt;/em&gt; times.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;k-Fold CV estimate for the test MSE:&lt;br /&gt;
\(CV_{(k)} = \frac{1}{n}\sum_{i=1}^k MSE_i\)&lt;br /&gt;
when &lt;em&gt;k=n&lt;/em&gt;, LOOCV is a special case of k-Fold. Using smaller &lt;em&gt;k&lt;/em&gt;, k-fold CV has a computational 
advantage to LOOCV.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We perform CV to:&lt;br /&gt;
To determine how well a given model can be expected to perform on independent data.&lt;br /&gt;
To identify a model results in the lowest test error, over different models or different levels of 
flexibility.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;514-bias-variance-trade-off-for-k-fold-cross-validation&quot;&gt;5.1.4. Bias-Variance Trade-Off for k-Fold Cross-Validation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Besides the computational advantage, k-fold CV often gives more accurate estimates of the test error 
rate than does LOOCV.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LOOCV will give approximately unbiased estiamtes of the test error, containing &lt;em&gt;n-1&lt;/em&gt;, almost as many as 
the number of observations in the full data set. By contrast, k-fold CV will lead to an intermediate 
level of bias, containing &lt;em&gt;(k-1)n/k&lt;/em&gt; observations. Clearly, LOOCV is to be preferred in the perspective 
of bias reduction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;But, in LOOCV, averaging the outputs of &lt;em&gt;n&lt;/em&gt; fitted models, which are trained on an almost identical set 
of observations, these outputs are highly correlated with each other. This high correlation results in 
higher variance of test error estimate from LOOCV than from k-fold CV.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;515-cross-validation-on-classification-problems&quot;&gt;5.1.5. Cross-Validation on Classification Problems&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;LOOCV on the classification:&lt;br /&gt;
\(CV_{(n)} = \frac{1}{n}\sum_{i=1}^n Err_i\),  where \(Err_i = I(y_i \ne \hat{y}_i)\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;k-fold CV on the classification:&lt;br /&gt;
\(\frac{1}{n}\sum_{i=1}^k MCR_i\).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;52-the-bootstrap&quot;&gt;5.2. The Bootstrap&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Sampling with &lt;em&gt;replacement&lt;/em&gt; on:&lt;br /&gt;
Dataset $Z = (z_1, \ldots, z_n)$, $ z_i = (x_i,y_i)$&lt;br /&gt;
Sample $Z^{*b}$, where $ b = 1, \ldots, B$ samples&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;for Any statistic term $S(Z)$ computed from full dataset &lt;em&gt;Z&lt;/em&gt;,&lt;br /&gt;
and $S(Z^{*b})$ from bootstrap samples,&lt;br /&gt;
\(\begin{align*}
Var(\hat{S(Z)}) = \frac{1}{B-1}\sum_{b=1}^B(S(Z^{*b})-\bar{S}^*)^2
\end{align*}\)&lt;br /&gt;
$\cdots \bar{S}^{*} = \frac{1}{B}\sum_{b=1}^B S(Z^{*b})$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        
        <pubDate>Thu, 23 Apr 2020 00:00:00 +0900</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch5</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch5</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 4. Classification</title>
        
          <description>
</description>
        
        <pubDate>Thu, 16 Apr 2020 00:00:00 +0900</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch4</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch4</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 3. Linear Regression</title>
        
          <description>
</description>
        
        <pubDate>Fri, 03 Apr 2020 00:00:00 +0900</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch3</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch3</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 2. Statistical Learning</title>
        
          <description>
</description>
        
        <pubDate>Mon, 16 Mar 2020 00:00:00 +0900</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch2</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch2</guid>
      </item>
      
    
  </channel>
</rss>
