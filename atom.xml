<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> - Articles</title>
    <description></description>
    <link>
    http://0.0.0.0:4000</link>
    
      
      <item>
        <title>DevEnv Setup</title>
        
          <description>&lt;ul&gt;
  &lt;li&gt;On purpose of setting local development environment on a new SSD storage, followed instructions below. Post for later use.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Document &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl&quot;&gt;Enable NVIDIA CUDA on WSL&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Install stable version of Windows 11&lt;/li&gt;
      &lt;li&gt;Enable WSL, install Ubuntu(20.04.3 LTS)&lt;br /&gt;
  On Windows &lt;strong&gt;Settings&lt;/strong&gt; app, select &lt;strong&gt;Check for updates&lt;/strong&gt; in the &lt;strong&gt;Windows Update&lt;/strong&gt; section and get the latest kernel(5.10.43.3 or higher)&lt;br /&gt;
  To check the version, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wsl cat /proc/version&lt;/code&gt; command in &lt;strong&gt;Powershell&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Install the GPU driver&lt;br /&gt;
  Download and install the NVIDIA CUDA enabled driver for WSL&lt;br /&gt;
  (Studio version: 511.65-desktop-win10-win11-64bit-international-nsd-dch-whql)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Install Docker Desktop app on Windows
    &lt;ul&gt;
      &lt;li&gt;Run:&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Result:
        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Windowed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Simulation&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Single&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;floating&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simulation&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Devices&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;used&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simulation&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;GPU&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Device&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Ampere&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capability&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;8.6&lt;/span&gt;

  &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Compute&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;8.6&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CUDA&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NVIDIA&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GeForce&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RTX&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3070&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;mi&quot;&gt;47104&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bodies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;40.275&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ms&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;550.910&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;billion&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interactions&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;11018.199&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;single&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GFLOP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flops&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interaction&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Setting Docker image for TensorFlow-GPU
    &lt;ul&gt;
      &lt;li&gt;Pull the latest TensorFlow-GPU image
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker run -it --gpus all tensorflow/tensorflow:latest-gpu&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Install Anaconda on user:root(ref: &lt;a href=&quot;https://omhdydy.tistory.com/6&quot;&gt;blog&lt;/a&gt;)
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  # update and install prerequisites
  apt-get update
  apt-get install wget
  # get proper version of anaconda3
  wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh 
  sh Anaconda3-2021.11-Linux-x86_64.sh
  exec bash
  # create anaconda environment and install libraries(for stability)
  conda create -n !env_name pip python=3.7
  conda activate !env_name
  pip install tensorflow-gpu
  pip install ipykernel
  python -m ipykernel install --user --name !env_name --display-name !dispaly_name
  pip install jupyter
  # escape with Ctrl + p, Ctrl + q
  docker commit -m &quot;!message&quot; !container_id !image_name:tag
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;(Optional) Install TensorFlow Object Detection API
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  apt-get install git
  git clone --depth 1 https://github.com/tensorflow/models
  cd models/research/
  apt install -y protobuf-compiler
  # found a symlink err, fixed with running:
  # ln -s /usr/lib/x86_64-linux-gnu/libcuda.so.1 /usr/local/cuda/lib64/libcuda.so
  # and rerun: apt install -y protobuf-compiler
  protoc object_detection/protos/*.proto --python_out=.
  cd models/research/
  # install Object Detection API
  cp object_detection/packages/tf2/setup.py .
  python -m pip install --use-feature=2020-resolver .
  # run test
  python object_detection/builders/model_builder_tf2_test.py
  # rm -rf models (if desired)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;(Optional) Install JupyterLab Extensions and enable TensorBoard within Jupyterlab-Docker container&lt;br /&gt;
  Stable versions worked on my local environment
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  curl -sL https://deb.nodesource.com/setup_12.x | bash -
  apt-get install -y nodejs
  node --version # check: v12.22.10
  npm --version # check: 6.14.16
  pip install jupyterlab==2.3.2 
  pip install git+https://github.com/cliffwoolley/jupyter_tensorboard.git 
  pip install tensorboard==2.2
  jupyter labextension install jupyterlab_tensorboard
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
        &lt;p&gt;Commit and run container with any open port for JupyterLab&lt;br /&gt;
  e.g.&lt;/p&gt;
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  docker run --rm -it --gpus all -p 4000:4000 !image_name:tag
  conda activate !env_name
  jupyter lab --ip='0.0.0.0' --port=4000 --no-browser --allow-root
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
        &lt;p&gt;On your &lt;strong&gt;Windows&lt;/strong&gt;, open &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost:4000&lt;/code&gt; with browser&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Setting Docker image for Jekyll blog
    &lt;ul&gt;
      &lt;li&gt;Get latest Ubuntu image and install packages
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  docker run --rm -it -p 4000:4000 ubuntu
  apt-get update
  apt-get install git
  apt-get install vim ruby-full build-essential zlib1g-dev -y

  echo '# Install Ruby Gems to ~/gems' &amp;gt;&amp;gt; ~/.bashrc
  echo 'export GEM_HOME=&quot;$HOME/gems&quot;' &amp;gt;&amp;gt; ~/.bashrc
  echo 'export PATH=&quot;$HOME/gems/bin:$PATH&quot;' &amp;gt;&amp;gt; ~/.bashrc
  source ~/.bashrc
  gem install jekyll bundler
		
  jekyll -v # 4.2.1
  mkdir -p /root/blog_home
  echo 'export BLOG_HOME=&quot;/root/blog_home&quot;' &amp;gt;&amp;gt; ~/.bashrc
  echo '# Start jekyll' &amp;gt;&amp;gt; ~/.bashrc
  source ~/.bashrc
  cd $BLOG_HOME # Get any jekyll blog template here
  rm Gemfile.lock # if needed
  bundle install
  bundle exec jekyll serve --host 0.0.0.0 -p 4000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        
        <pubDate>Wed, 02 Mar 2022 00:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/DevEnv_Setup</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/DevEnv_Setup</guid>
      </item>
      
    
      
      <item>
        <title>Starfish detection w/ TF Object Detection API</title>
        
          <description>&lt;h2 id=&quot;tensorflow---help-protect-the-great-barrier-reef&quot;&gt;TensorFlow - Help Protect the Great Barrier Reef&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Worked in Feb. 2022. to study object detection model&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Task:&lt;br /&gt;
  Underwater + Small object detection&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Score(IoU=0.50:0.95):&lt;br /&gt;
  &lt;em&gt;mAP@100: 0.364686 / AR@100: 0.491768 / Expected F2: 0.459727&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Direct link: &lt;a href=&quot;https://www.kaggle.com/kwondalhyeon/starfish-detection-w-tf-object-detection-api?scriptVersionId=87885389&quot;&gt;kaggle notebook&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;iframe src=&quot;https://www.kaggle.com/embed/kwondalhyeon/starfish-detection-w-tf-object-detection-api?kernelSessionId=87885389&quot; height=&quot;1200&quot; style=&quot;margin: 0 auto; width: 100%; max-width: 100%;&quot; frameborder=&quot;0&quot; scrolling=&quot;auto&quot; title=&quot;Starfish detection w/ TF Object Detection API&quot;&gt;&lt;/iframe&gt;
</description>
        
        <pubDate>Mon, 14 Feb 2022 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/starfish_detection</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/starfish_detection</guid>
      </item>
      
    
      
      <item>
        <title>cs231n - Lecture 15. Detection and Segmentation</title>
        
          <description>&lt;h3 id=&quot;computer-vision-tasks&quot;&gt;Computer Vision Tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Image Classification: No spatial extent&lt;/li&gt;
  &lt;li&gt;Semantic Segmentation: No objects, just pixels&lt;/li&gt;
  &lt;li&gt;Object Detection/ Instance Segmentation: Multiple objects&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;semantic-segmentation&quot;&gt;Semantic Segmentation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Paired training data:&lt;br /&gt;
  For each training image, &lt;strong&gt;each pixel is labeled&lt;/strong&gt; with a semantic category.&lt;/li&gt;
  &lt;li&gt;At test time, classify each pixel of a new image.&lt;/li&gt;
  &lt;li&gt;Problem:&lt;br /&gt;
  Classifying with only single pixel does not include context information.&lt;/li&gt;
  &lt;li&gt;Idea:
    &lt;ul&gt;
      &lt;li&gt;Sliding Window&lt;br /&gt;
  Extract patch from full image, classify center pixel with CNN.&lt;br /&gt;
  $\color{red}{(-)}$ Very inefficient, not reusing shared features between overlapping patches.&lt;/li&gt;
      &lt;li&gt;Convolution&lt;br /&gt;
  Encode the entire image with conv net, and do semantic segmentation on top.&lt;br /&gt;
  $\color{red}{(-)}$ CNN architectures often change the spatial sizes, but semantic segmentation requires the output size to be same as input size.&lt;/li&gt;
      &lt;li&gt;Fully Convolutional&lt;br /&gt;
  Design a network with &lt;strong&gt;only&lt;/strong&gt; convolutional layers without downsampling operators&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
  $\textcolor{red}{(-)}$ convolutions at original image resolution is very expensive&lt;br /&gt;
  $\rightarrow$ Design convolutional network with &lt;strong&gt;downsampling and upsampling&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Downsampling: Pooling, strided convolution&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In-Network Upsampling: Unpooling, strided transpose convolution&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unpooling:&lt;br /&gt;
  Nearest Neighbor: copy-paste to extended region&lt;br /&gt;
  “Bed of Nails”: no positional argument, pad with zeros&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Max Unpooling: use positions from poolying layer ahead, pad with zeros&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learnable Downsampling: Strided convolution&lt;br /&gt;
  Output is a dot product between filter and input&lt;br /&gt;
  Stride gives ratio between movement in input and output&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learnable Upsampling: Transposed convolution&lt;br /&gt;
  Input gives weight for filter&lt;br /&gt;
  Output contains copies of the filter weighted by the input, summing at where at overlaps in the output&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Summary&lt;br /&gt;
  Label each pixel in the image with a category label&lt;br /&gt;
  Don’t differentiate instances, only care about pixels&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;object-detection&quot;&gt;Object Detection&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multiple Objects:&lt;br /&gt;
  Each image needs a different number of outputs;&lt;br /&gt;
  $\rightarrow$ Apply a CNN to many different crops of the image, CNN classifies each crop as object or background.&lt;br /&gt;
  $\color{red}{(-)}$ Need to apply CNN to huge number of locations, scales, and aspect ratios, very computationally expensive.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;r-cnn&quot;&gt;R-CNN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_4.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Girshick et al, “Rich feature hierarchies for accurate object detection and
semantic segmentation”, CVPR 2014&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;2-stage Detector: Region Proposal + Region Classification
    &lt;ol&gt;
      &lt;li&gt;Image as input&lt;/li&gt;
      &lt;li&gt;Crop bounding boxes with Selective Search&lt;br /&gt;
 Warp into same size pixels for CNN model&lt;/li&gt;
      &lt;li&gt;Input Warped images into CNN&lt;/li&gt;
      &lt;li&gt;Run classification on each&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;ol&gt;
      &lt;li&gt;Region Proposals: Selective Search&lt;br /&gt;
 Find “blobby” image regions that are likely to contain objects.&lt;br /&gt;
 Relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU.&lt;/li&gt;
      &lt;li&gt;CNN:&lt;br /&gt;
 For a pre-trained CNN architecture, change the number of classes on the last classification layer(detection classes &lt;em&gt;N&lt;/em&gt; + background &lt;em&gt;1&lt;/em&gt;), fine-tune with dataset for Object Detection. From the region proposal input, outputs a fixed-length feature vector.&lt;/li&gt;
      &lt;li&gt;SVM: Category-Specific Linear SVMs&lt;br /&gt;
 Positive: ground-truth boxes&lt;br /&gt;
 Negative: IoU under 0.3&lt;br /&gt;
 Scores each feature vector for classes, classifies whether each one is positive/negative(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_object&lt;/code&gt;).&lt;/li&gt;
      &lt;li&gt;Non-Maximum Suppression: with concept of &lt;strong&gt;IoU&lt;/strong&gt;&lt;br /&gt;
 Intersection over Union; area of intersection divided by area of union&lt;br /&gt;
 If there are two boxes with IoU over 0.5, consider them proposed on the same object, leave one with the highest score.&lt;/li&gt;
      &lt;li&gt;Bounding Box Regression: adjust boxes from Selective Search
        &lt;ul&gt;
          &lt;li&gt;Algorithm:&lt;br /&gt;
 Assume a bounding box $P^i = (P_x^i, P_y^i, P_w^i, P_h^i)$,&lt;br /&gt;
 Ground-truth box $G = (G_x, G_y, G_w, G_h)$.&lt;br /&gt;
 Define a function $d$, mapping $P$ close to $G$;&lt;br /&gt;
 \(\hat{G}_x = P_w d_x(P) + P_x\)&lt;br /&gt;
 \(\hat{G}_y = P_h d_y(P) + P_y\)&lt;br /&gt;
 \(\hat{G}_w = P_w \mbox{exp}(d_w(P))\)&lt;br /&gt;
 \(\hat{G}_h = P_h \mbox{exp}(d_h(P))\)&lt;br /&gt;
 where $d_{\star}(P) = w_{\star}^T \phi_5(P)$, is modeled as a linear function(learnable weight vector &lt;em&gt;w&lt;/em&gt;) of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POOL5&lt;/code&gt; features of proposal &lt;em&gt;P&lt;/em&gt;($\phi_5(P)$). We learn $w_{\star}$ by optimizing the regularized least squares objective(Ridge regression)&lt;br /&gt;
  &lt;em&gt;Learnable parameters on: 2, 3, 5&lt;/em&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Summary:&lt;br /&gt;
  Score: 53.7% on Pascal VOC 2010&lt;br /&gt;
  Problem:&lt;br /&gt;
      1. Low Performance; Warping images into 224x224 size for AlexNet&lt;br /&gt;
      2. Slow; Using all candidates from Selective Search&lt;br /&gt;
      3. Not GPU-optimized; Using Selective Search and SVM&lt;br /&gt;
      4. No Back Propagation; Not sharing computations&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fast-r-cnn&quot;&gt;Fast R-CNN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_5.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Girshick, “Fast R-CNN”, ICCV 2015&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;idea:&lt;br /&gt;
  Pass the image through convnet &lt;strong&gt;before&lt;/strong&gt; cropping. Crop the conv feature instead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;ol&gt;
      &lt;li&gt;Pass the full image through pre-trained CNN and extract feature maps.&lt;/li&gt;
      &lt;li&gt;Get RoIs from a proposal method(Selective Search) and crop by RoI Pooling, get fixed size feature vectors.&lt;/li&gt;
      &lt;li&gt;With RoI feature vectors, pass some fully connected layers and split into two branches.&lt;/li&gt;
      &lt;li&gt;1) pass softmax and classify the class of RoI. no SVM used. 2) Run bounding box regression.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Cropping Features: RoI Pool &lt;!--[](href) link to terminologies --&gt;
    &lt;ol&gt;
      &lt;li&gt;Project RoI proposals(on input image) onto CNN image features.&lt;/li&gt;
      &lt;li&gt;Divide into subregions.&lt;/li&gt;
      &lt;li&gt;Run pooling(Max-pool) within each subregion.&lt;br /&gt;
  $\rightarrow$ Region features always be the same size regardless of input region size&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;faster-r-cnn&quot;&gt;Faster R-CNN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_6.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Ren et al, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”, NIPS 2015&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;idea:&lt;br /&gt;
  Fast R-CNN is not GPU-optimized; runtime dominated by region proposals.&lt;br /&gt;
  By inserting Region Proposal Network(RPN), implemented end-to-end architecture.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm:
    &lt;ol&gt;
      &lt;li&gt;Pass the full image through pre-trained CNN and extract feature maps.&lt;/li&gt;
      &lt;li&gt;RPN:&lt;br /&gt;
 For &lt;em&gt;K&lt;/em&gt; different anchor boxes of different size and scale at each point in the feature map, predict whether it contains an object(binary classification), and also predict a corrections from the anchor to the ground-truth box(regress 4 numbers per pixel).&lt;br /&gt;
 &lt;img src=&quot;/assets/images/cs231n_lec15_7.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;Jointly train with 4 losses:&lt;br /&gt;
 1) RPN classify object / not object&lt;br /&gt;
 2) RPN regress box coordinates&lt;br /&gt;
 3) Final classification score (object classes)&lt;br /&gt;
 4) Final box coordinates&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Glossing over many details:
    &lt;ul&gt;
      &lt;li&gt;Ignore overlapping proposals with non-max suppression&lt;/li&gt;
      &lt;li&gt;How are anchors determined?&lt;/li&gt;
      &lt;li&gt;How do we sample positive / negative samples for training the RPN?&lt;/li&gt;
      &lt;li&gt;How to parameterize bounding box regression?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Two-stage object detector:
    &lt;ul&gt;
      &lt;li&gt;First stage: Run once per image
        &lt;ul&gt;
          &lt;li&gt;Backbone network&lt;/li&gt;
          &lt;li&gt;Region proposal network(RPN)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Second stage: Run once per region
        &lt;ul&gt;
          &lt;li&gt;Crop features: RoI pool/ align&lt;/li&gt;
          &lt;li&gt;Predict object class&lt;/li&gt;
          &lt;li&gt;Prediction bbox offset&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;single-stage-object-detectors-yolo--ssd--retinanet&quot;&gt;Single-Stage Object Detectors: YOLO / SSD / RetinaNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Algorithm:
    &lt;ol&gt;
      &lt;li&gt;Divide input imgae into grid&lt;/li&gt;
      &lt;li&gt;Image a set of &lt;strong&gt;base boxes&lt;/strong&gt; centered at each grid cell&lt;/li&gt;
      &lt;li&gt;Within each grid cell:
        &lt;ul&gt;
          &lt;li&gt;Regress from each of the &lt;em&gt;B&lt;/em&gt; base boxes to a final box with 5 numbers(dx, dy, dh, dw, confidence)&lt;/li&gt;
          &lt;li&gt;Predict scores for each of &lt;em&gt;C&lt;/em&gt; classes(including background as a class)&lt;/li&gt;
          &lt;li&gt;Looks a lot like RPN, but category-specific&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_8.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;instance-segmentation-mask-r-cnn&quot;&gt;Instance Segmentation: Mask R-CNN&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;He et al, “Mask R-CNN”, ICCV 2017&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_9.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_10.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_11.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_12.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_13.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;open-source-frameworks&quot;&gt;Open Source Frameworks&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/object_detection&quot;&gt;TensorFlow Detection API&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;&gt;Detectron2(Pytorch)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;beyond-2d-object-detection&quot;&gt;Beyond 2D Object Detection&lt;/h2&gt;

&lt;h3 id=&quot;object-detection--captioning-dense-captioning&quot;&gt;Object Detection + Captioning: Dense Captioning&lt;/h3&gt;

&lt;h3 id=&quot;dense-video-captioning-timestep-t&quot;&gt;Dense Video Captioning: timestep “T”&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_14.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;objects--relationships-scene-graphs&quot;&gt;Objects + Relationships: Scene Graphs&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_15.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec15_16.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3d-object-detection&quot;&gt;3D Object Detection&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec15_17.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;2D bounding box: (x, y, w, h)&lt;br /&gt;
  $\rightarrow$ 3D oriented bounding box: (x, y, z, w, h, l, r, p, y)&lt;br /&gt;
  $\rightarrow$ Simplified bbox: no roll &amp;amp; pitch&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Simple Camera Model:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_18.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
  A point on the image plane corresponds to a &lt;strong&gt;ray&lt;/strong&gt; in the 3D space&lt;br /&gt;
  A 2D bounding box on an image is a &lt;strong&gt;frustrum&lt;/strong&gt; in the 3D space&lt;br /&gt;
  Localize an object in 3D: The object can be anywhere in the &lt;strong&gt;camera viewing frustrum&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Monocular Camera:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec15_19.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;80%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;Same idea as Faster RCNN, but proposals are in 3D&lt;/li&gt;
      &lt;li&gt;3D bounding box proposal, regress 3D box parameters + class score&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;3D Shape Prediction: Mesh R-CNN
&lt;img src=&quot;/assets/images/cs231n_lec15_20.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
참고
https://www.youtube.com/watch?v=jqNCdjOB15s
https://towardsdatascience.com/understanding-region-of-interest-part-1-roi-pooling-e4f5dd65bb44
https://yeomko.tistory.com/13 갈아먹는 object detection
https://nuggy875.tistory.com/33 object detection 시리즈
https://ropiens.tistory.com/73 
--&gt;
</description>
        
        <pubDate>Mon, 07 Feb 2022 00:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/cs231n_lec15</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/cs231n_lec15</guid>
      </item>
      
    
      
      <item>
        <title>cs231n - Lecture 14. Visualizing and Understanding</title>
        
          <description>&lt;h2 id=&quot;whats-going-on-inside-convnets&quot;&gt;What’s going on inside ConvNets?&lt;/h2&gt;

&lt;h3 id=&quot;visualizing-what-models-have-learned&quot;&gt;Visualizing what models have learned&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;First layer: Visualize Filters&lt;br /&gt;
  At the first layer, we can visualize the raw weights and see gabor-like features. While the higher layers are about the weights to the activations from the layer before, it is not very interpretable.&lt;/li&gt;
  &lt;li&gt;Last layer: Visualize Representations(feature vector)
    &lt;ul&gt;
      &lt;li&gt;Nearest neighbors in feature space&lt;/li&gt;
      &lt;li&gt;Dimensionality reduction: clustering with similarity; using simple algorithm(PCA) or more complex one(t-SNE)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Visualizing Activations:&lt;br /&gt;
  &lt;em&gt;Yosinski et al, “Understanding Neural Networks Through Deep Visualization”, ICML DL Workshop 2014&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;understanding-input-pixels&quot;&gt;Understanding input pixels&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Maximally Activating Patches&lt;br /&gt;
  Run many images, record values of chosen channel(layer or neuron) and visualize image patches that correspond to maximal activations.&lt;/li&gt;
  &lt;li&gt;Saliency via Occlusion: &lt;em&gt;Zeiler and Fergus, “Visualizing and Understanding Convolutional Networks”, ECCV 2014&lt;/em&gt;.&lt;br /&gt;
  Mask part of the image before feeding to CNN, slide the occluder and check how much predicted probabilities change. Found that when there are multiple objects, the classification performance improved as the false class object masked.&lt;/li&gt;
  &lt;li&gt;Which pixels matter: Saliency via Backprop
    &lt;ul&gt;
      &lt;li&gt;Visualize the data gradient
        &lt;ol&gt;
          &lt;li&gt;foward an image&lt;/li&gt;
          &lt;li&gt;set &lt;em&gt;activations&lt;/em&gt; in layer of interest to all zero, except for a 1.0 for a neuron of interest&lt;/li&gt;
          &lt;li&gt;backprop to image input&lt;/li&gt;
          &lt;li&gt;squish the channels of gradients to get 1-dimensional activation map(or we can run segmentation using grabcut on this heatmap).&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;Can also find biases; to see what false classifier actually see&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Intermediate Features via (guided) backprop
    &lt;ul&gt;
      &lt;li&gt;Deconvolution-based approach:
        &lt;ol&gt;
          &lt;li&gt;Feed image into net&lt;/li&gt;
          &lt;li&gt;Pick a layer, set the gradient there to be all zero except for one 1 for some neuron of interest&lt;/li&gt;
          &lt;li&gt;Backprop to image(use guided backprop to pass the positive influence; activation with positive values, using modified relu or deconvnet)&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Visualizing CNN features: Gradient Ascent
  (Guided) backprop: Find the part of an image that a neuron responds to.&lt;br /&gt;
  Gradient ascent: Generate a synthetic image that maximally activates a neuron.&lt;br /&gt;
  \(I^{\ast} = \mbox{argmax}_I f(I) + R(I)\); (neuron value + regularizer)
    &lt;ul&gt;
      &lt;li&gt;Optimization-based approach: freeze/fix the weights and run “image update” to find images that maximize the score of chosen class.&lt;br /&gt;
  &lt;em&gt;Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, “Deep Inside Convolutional Networks: Visualizing Image Classification Models and Saliency Maps”, Workshop at ICLR, 2014&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;Find images that maximize some class score: \(\mbox{argmax}_I S_c(I) - \lambda {\lVert I \rVert}^2_2\)
        &lt;ol&gt;
          &lt;li&gt;initialize image to zeros&lt;/li&gt;
          &lt;li&gt;forward image to compute current scores&lt;/li&gt;
          &lt;li&gt;set the &lt;em&gt;gradient&lt;/em&gt; of the scores vector to be &lt;em&gt;I&lt;/em&gt;, then backprop to image&lt;/li&gt;
          &lt;li&gt;make a small update to the image&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;Instead of using L2 norm, we can use better regularizer(Gaussian blur image, Clip pixels with small values/gradients to 0, …)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimize in FC6 latent space instead of pixel space:&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec14_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec14_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adversarial-perturbations&quot;&gt;Adversarial perturbations&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Fooling Images / Adversarial Examples
    &lt;ol&gt;
      &lt;li&gt;Start from an arbitrary image&lt;/li&gt;
      &lt;li&gt;Pick an arbitrary class&lt;/li&gt;
      &lt;li&gt;Modify the image to maximize the class&lt;/li&gt;
      &lt;li&gt;Repeat until network is fooled&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Ian Goodfellow, “Explaining and Harnessing Adversarial Examples”, 2014&lt;/em&gt;&lt;br /&gt;
  Classifier is vulnerable to adversarical perturbation because of its linear nature. Check &lt;a href=&quot;https://www.youtube.com/watch?v=CIfsB_EYsVI&quot;&gt;Ian Goodfellow’s lecture&lt;/a&gt; from 2017&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Moosavi-Dezfooli, Seyed-Mohsen, et al. “Universal adversarial perturbations”, IEEE, 2017.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;style-transfer&quot;&gt;Style Transfer&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;DeepDream: Amplify existing features&lt;br /&gt;
  Rather than synthesizing an image to maximize a specific neuron, instead try to &lt;strong&gt;amplify&lt;/strong&gt; the neuron activations at some layer in the network.&lt;/li&gt;
  &lt;li&gt;Choose an image and a layer in a CNN; repeat:
    &lt;ol&gt;
      &lt;li&gt;Forward: compute activations at chosen layer&lt;/li&gt;
      &lt;li&gt;Set gradient of chosen layer &lt;em&gt;equal to its activation&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Backward: Compute gradient on image&lt;/li&gt;
      &lt;li&gt;Update image&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec14_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feature Inversion&lt;br /&gt;
  &lt;em&gt;Mahendran and Vedaldi, “Understanding Deep Image Representations by Inverting Them”, CVPR 2015&lt;/em&gt;&lt;br /&gt;
  Given a CNN feature vector for an image, find a new image that:
    &lt;ul&gt;
      &lt;li&gt;Matches the given feature vector&lt;/li&gt;
      &lt;li&gt;“looks natural” (image prior regularization)&lt;br /&gt;
  \(\begin{align*}
  x^{\ast} &amp;amp;= \underset{x\in\mathbb{R}^{H \times W \times C}}{\mbox{argmin}} l(\Phi(x), \Phi_0) + \lambda \mathcal{R}(x) \\
      &amp;amp; \mbox{where loss } l = {\lVert \Phi(x) - \Phi_0 \rVert}^2
  \end{align*}\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Texture Synthesis: Nearest Neighbor&lt;br /&gt;
  Given a sample patch of some texture, generate a bigger image of the same texture&lt;br /&gt;
  Generate pixels one at a time in scanline order; form neighborhood of already generated pixels and copy nearest neighbor from input&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Neural Texture Synthesis: Gram Matrix&lt;br /&gt;
  a pair-wise statistics; interpret given &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CxHxW&lt;/code&gt; features as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HxW&lt;/code&gt; grid of C-dimensional vectors. By computing outer product and sum up for all spacial locations($G=V^T V$), it gives &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CxC&lt;/code&gt; matrix measuring co-occurrence.
    &lt;ol&gt;
      &lt;li&gt;Pretrain a CNN on ImageNet (VGG-19)&lt;/li&gt;
      &lt;li&gt;Run input texture forward through CNN, record activations on every layer&lt;/li&gt;
      &lt;li&gt;At each layer compute the Gram matrix&lt;/li&gt;
      &lt;li&gt;Initialize generated image from random noise&lt;/li&gt;
      &lt;li&gt;Pass generated image through CNN, compute Gram matrix on each layer&lt;/li&gt;
      &lt;li&gt;Compute loss: weighted sum of L2 distance between Gram matrices&lt;/li&gt;
      &lt;li&gt;Backprop to get gradient on image&lt;/li&gt;
      &lt;li&gt;Make gradient step on image&lt;/li&gt;
      &lt;li&gt;GOTO 5
&lt;img src=&quot;/assets/images/cs231n_lec14_3.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Neural Style Transfer: Feature + Gram Reconstruction
    &lt;ol&gt;
      &lt;li&gt;Extract content targets (ConvNet activations of all layers for the given image)&lt;/li&gt;
      &lt;li&gt;Extract style targets (Gram matrices of ConvNet activations of all layers)&lt;/li&gt;
      &lt;li&gt;Optimize over image to have:
        &lt;ul&gt;
          &lt;li&gt;The content of the content image(activations match content)&lt;/li&gt;
          &lt;li&gt;The style of the stlye image(Gram matrices of activations match style)&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec14_4.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;Problem: requires many forward, backward passes; VGG is very slow&lt;/li&gt;
          &lt;li&gt;Solution: Train another neural network; &lt;strong&gt;Fast Style Transfer&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Fast Style Transfer
  &lt;img src=&quot;/assets/images/cs231n_lec14_5.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        
        <pubDate>Wed, 02 Feb 2022 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/cs231n_lec14</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/cs231n_lec14</guid>
      </item>
      
    
      
      <item>
        <title>Unsupervised Representation Learning by Predicting Image Rotations</title>
        
          <description>&lt;h2 id=&quot;unsupervised-representation-learning-by-predicting-image-rotations&quot;&gt;Unsupervised Representation Learning by Predicting Image Rotations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Gidaris et al. 2018&lt;/em&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://github.com/gidariss/FeatureLearningRotNet&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ConvNet:&lt;br /&gt;
  (+) Unparalleled capacity to learn high level semantic image features&lt;br /&gt;
  (-) Require massive amounts of manually labeled data, expensive and impractical to scale&lt;br /&gt;
  $\rightarrow$ &lt;em&gt;Unsupervised Learning&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Unsupervised semantic feature learning:&lt;br /&gt;
  Learn image features by training ConvNets to recognize the 2d rotated images as input. With apparently simple task, provides a very powerful supervisory signal for semantic feature learning(Conv). Evaluated in various unsupervised feature learning benchmarks, exceeds SotA performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;featurelearningrotnet&quot;&gt;FeatureLearningRotNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;How To:&lt;br /&gt;
  First define a small set of discrete geometric transformations, then each of those transformations are applied to each image on the dataset and produced transformed images are fed to ConvNet model that is trained to recognize the transformation of each image.
    &lt;ul&gt;
      &lt;li&gt;Set of geometric transformations define the classification pretext task that the ConvNet has to learn; to achieve unsupervised semantic feature learning, it is important to properly choose those geometric transformations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Purpose: to define the geometric transformations as rotations of 4 different degrees, ConvNet trained on the 4-way image classification task of recognizing one of the four Maximizing prob. $F^y(x^{y^{*}})$, probability of transformation &lt;em&gt;y&lt;/em&gt; predicted by &lt;em&gt;F&lt;/em&gt;, when given X is transformed by the transformation $y^{*}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;With idea: In order a ConvNet model to be able recognize the rotation transformations, it will require to understand the concept of the objects depicted in the image	such as their location, type, and pose.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;define a set of &lt;em&gt;K&lt;/em&gt; discrete geometric transformations \(G = \{g(\cdot\vert y)\}_{y=1}^K\), where $g(.\vert y)$ applies to input X, transformed image $X^y = g(X\vert y)$&lt;/li&gt;
  &lt;li&gt;ConvNet model F(.) gets as input an image $X^{y^{\ast}}$, to recognize unknown $y^{\ast}$ yields as output a probability distribution over all possible transformations	\(F(X^{y^{\ast}}\vert\theta) = \{ F^y(X^{y^{\ast}}\vert\theta) \}_{y=1}^K\), output &lt;em&gt;F&lt;/em&gt; returns probs for all classes $y$.&lt;/li&gt;
  &lt;li&gt;Therefore, &lt;em&gt;N&lt;/em&gt; training images \(D = \{ X_i \}_{i=0}^N\), the self-supervised training objective that ConvNet must learn to solve is:&lt;br /&gt;
 \(\mbox{min}_{\theta}\frac{1}{N}\sum_{i=1}^N \mbox{loss}(X_i,\theta)\),&lt;br /&gt;
 where the loss function is defined as:&lt;br /&gt;
 \(\mbox{loss}(X_i,\theta) = -\frac{1}{K}\sum_{y=1}^K \log(F^y(g(X_i|y)|\theta))\)&lt;br /&gt;
 (negative sum of log probs &lt;em&gt;F&lt;/em&gt; for all classes &lt;em&gt;y&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;2d image rotations:&lt;br /&gt;
  $Rot(X, \phi)$, operator that rotates image X by $\phi$ degrees&lt;br /&gt;
  In this case 0, 90, 180, 270; K=4 for G, where $g(X|y)=Rot(X,(y-1)90)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;forcing-the-learning-of-semantic-features&quot;&gt;Forcing the learning of semantic features&lt;/h3&gt;
&lt;p&gt;Fact that it is essentially impossible for a ConvNet model to effectively perform the above rotation recognition task, unless it has first learnt to recognize and detect classes of objects as well as their &lt;strong&gt;semantic parts&lt;/strong&gt; in images.&lt;br /&gt;
$\rightarrow$ &lt;strong&gt;ATTENTION MAPS&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/papers_rotation_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;By comparing the attention maps from two models trained on supervised and unsupervised way, we observe that both models seem to focus on roughly &lt;strong&gt;the same image regions&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/papers_rotation_1.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Also, trained on the proposed rotation recognition task, &lt;strong&gt;visualized layer filters&lt;/strong&gt; learnt appear to have a big variety of edge filters on multiple orientations and multiple frequencies, then the filters learnt by the supervised task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$*$ Activation-based Attention Maps from &lt;em&gt;“Paying More Attention to Attention”, Zagoruyko et al., 2017&lt;/em&gt; - &lt;a href=&quot;https://arxiv.org/abs/1612.03928&quot;&gt;LINK&lt;/a&gt;&lt;br /&gt;
  Activation tensor of a conv. layer: $A\in R^{C\times H\times W}$ consists of &lt;em&gt;C&lt;/em&gt; feautre planes with spatial dimensions &lt;em&gt;H&lt;/em&gt;x&lt;em&gt;W&lt;/em&gt;&lt;br /&gt;
  Activation-based mapping function &lt;em&gt;F&lt;/em&gt; w.r.t that layer: $\mathcal{F}: R^{C\times H\times W} \rightarrow R^{H\times W}$&lt;br /&gt;
  With implicit assumption: Absolute value of a hidden neuron activation(that results when the network is evaluated on given input) can be used as an indication about the importance of that neuron w.r.t. the specific input.&lt;br /&gt;
  By considering, therefore, the absolute values of the elements of tensor A,	we construct a spatial attention map by computing statistics of these values	across the channel dimension(&lt;em&gt;C&lt;/em&gt;)&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;sum of abs: $F_{sum}(A)=\sum_{i=1}^C\vert A_i\vert$&lt;/li&gt;
      &lt;li&gt;sum of abs, raised to the power of p(&amp;gt;1): $F_{sum}^p(A) = \sum_{i=1}^C\vert A_i\vert^p$&lt;/li&gt;
      &lt;li&gt;max of abs, raised to the pwoer of p(&amp;gt;1): $F_{max}^p(A) = \mbox{max}_{i=1,C}\vert A_i\vert^p$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;transfer-learning&quot;&gt;Transfer Learning&lt;/h3&gt;
&lt;p&gt;With a model trained on proposed rotation recognition task with unlabeled data, freeze its early conv. layers and attach the layers from a supervised model, evaluate on a supervised task with a subset of labeled data.&lt;/p&gt;
</description>
        
        <pubDate>Mon, 24 Jan 2022 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/Rotation</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/Rotation</guid>
      </item>
      
    
      
      <item>
        <title>cs231n - Lecture 13. Self-Supervised Learning</title>
        
          <description>&lt;h2 id=&quot;self-supervised-learning&quot;&gt;Self-Supervised Learning&lt;/h2&gt;

&lt;h3 id=&quot;generative-vs-self-supervised-learning&quot;&gt;Generative vs. Self-supervised Learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Both aim to learn from data without manual label annotation&lt;/li&gt;
  &lt;li&gt;Generative learning aims to model &lt;strong&gt;data distribution&lt;/strong&gt; $p_{data}(x)$,&lt;br /&gt;
  e.g., generating realistic images.&lt;/li&gt;
  &lt;li&gt;Self-supervised learning methods solve “pretext” tasks that produce &lt;strong&gt;good features&lt;/strong&gt; for downstream tasks.
    &lt;ul&gt;
      &lt;li&gt;Learn with supervised learning objectives, e.g., classification, regression.&lt;/li&gt;
      &lt;li&gt;Labels of these pretext tasks are generated &lt;em&gt;automatically&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;self-supervised-pretext-tasks&quot;&gt;Self-supervised pretext tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Example: learn to predict image transformations / complete corrupted images;&lt;br /&gt;
  e.g. image completion, rotation prediction, “jigsaw puzzle”, coloriztaion.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Solving the pretext tasks allow the model to learn good features.&lt;/li&gt;
  &lt;li&gt;We can automatically generate labels for the pretext tasks.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Learning to generate pixel-level details is often unnecessary; learn high-level semantic features with pretext tasks instead(only encode high-level features sufficient enough to distinguish different objects, Contrastive Methods): &lt;a href=&quot;https://aeon.co/essays/your-brain-does-not-process-information-and-it-is-not-a-computer&quot; target=&quot;_blank&quot;&gt;Epstein, 2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-evaluate-a-self-supervised-learning-method&quot;&gt;How to evaluate a self-supervised learning method?&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Self-supervised learning:&lt;br /&gt;
 With lots of unlabeled data, learn good feature extractors from self-supervised pretext tasks, e.g., predicting image rotations.&lt;/li&gt;
  &lt;li&gt;Supervised Learning:&lt;br /&gt;
 With small amount of labeled data on the target task, attach a shallow network on the feature extractor; train the shallow network and evaluate on the target task, e.g., classification, detection.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;pretext-tasks-from-image-transformations&quot;&gt;Pretext tasks from image transformations&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Predict Rotations&lt;br /&gt;
  &lt;em&gt;Gidaris et al., 2018&lt;/em&gt; - &lt;a href=&quot;/Rotation&quot;&gt;(Paper Review)&lt;/a&gt; 
  &lt;img src=&quot;/assets/images/cs231n_lec13_17.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Predict Relative Patch Locations&lt;br /&gt;
  &lt;em&gt;Doersch et al., 2015&lt;/em&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Solving “jigsaw puzzles”; shuffled patches &lt;br /&gt;
  &lt;em&gt;Noroozi &amp;amp; Favaro, 2016&lt;/em&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_1.png&quot; alt=&quot;png&quot; width=&quot;65%&amp;quot;, height=&amp;quot;65%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Predict Missing Pixels(Inpainting); encoder-decoder&lt;br /&gt;
  &lt;em&gt;Pathak et al., 2016&lt;/em&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_2.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Image Coloring; Split-brain Autoencoder&lt;br /&gt;
  &lt;em&gt;Richard Zhang/ Phillip Isola&lt;/em&gt;&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_3.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Video Coloring; from t=0 reference frame to the later frames&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_4.png&quot; alt=&quot;png&quot; width=&quot;65%&amp;quot;, height=&amp;quot;65%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-pretext-tasks&quot;&gt;Summary: Pretext tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Pretext tasks focus on “visual common sense”; by image transformations, can learn without supervision(big labeled data).&lt;/li&gt;
  &lt;li&gt;The models are forced learn good features about natural images, e.g., semantic representation of an object category, in order to solve the pretext tasks.&lt;/li&gt;
  &lt;li&gt;We don’t care about the performance of these pretext tasks, but rather how useful the learned features are for downstream tasks.&lt;/li&gt;
  &lt;li&gt;$\color{red}{Problems}$: 1) coming up with individual pretext tasks is tedious, and 2) the learned representations may not be general; tied to a specific pretext task.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;contrastive-representation-learning&quot;&gt;Contrastive Representation Learning&lt;/h2&gt;
&lt;p&gt;For a more general pretext task,&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_5.png&quot; alt=&quot;png&quot; width=&quot;75%&amp;quot;, height=&amp;quot;75%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;a-formulation-of-contrastive-learning&quot;&gt;A formulation of contrastive learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;What we want:&lt;br /&gt;
  $\mbox{score}(f(x), f(x^+)) » score(f(x), f(x^-))$&lt;br /&gt;
  &lt;em&gt;x&lt;/em&gt;: reference sample, &lt;em&gt;x+: positive sample&lt;/em&gt;, &lt;em&gt;x-&lt;/em&gt;: negative sample&lt;br /&gt;
  Given a chosen score function, we aim to learn an &lt;strong&gt;encoder function&lt;/strong&gt; &lt;em&gt;f&lt;/em&gt; that yields high score for positive pairs and low scores for negative pairs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loss function given &lt;em&gt;1&lt;/em&gt; positive sample and &lt;em&gt;N-1&lt;/em&gt; negative samples:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec13_6.png&quot; alt=&quot;png&quot; width=&quot;65%&amp;quot;, height=&amp;quot;65%&quot; /&gt;&lt;br /&gt;
  seems familiar with &lt;strong&gt;Cross entropy loss&lt;/strong&gt; for a N-way softmax classifier!&lt;br /&gt;
  i.e., learn to find the positive sample from the N samples&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Commonly known as the InfoNCE loss(&lt;em&gt;van den Oord et al., 2018&lt;/em&gt;)&lt;br /&gt;
  A &lt;em&gt;lower bound&lt;/em&gt; on the mutual information between $f(x)$ and $f(x^+)$&lt;br /&gt;
  \(\rightarrow MI[f(x), f(x^+)] - \log(N) \ge -L\)&lt;br /&gt;
  The larger the negative sample size(N), the tighter the bound&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;simclr-a-simple-framework-for-contrastive-learning&quot;&gt;SimCLR: A Simple Framework for Contrastive Learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Chen et al., 2020&lt;/em&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_7.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Cosine similarity as the score function:&lt;br /&gt;
  \(s(u, v) = \frac{u^T v}{\lVert u \rVert \lVert v \rVert}\)&lt;/li&gt;
  &lt;li&gt;Use a projection network &lt;strong&gt;&lt;em&gt;h(.)&lt;/em&gt;&lt;/strong&gt; to project features to a space where contrastive learning is applied.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generate positive samples through data augmentation:&lt;br /&gt;
  random cropping, random color distortion, and random blur.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_8.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_9.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Evaluate: Freeze feature encoder, train(finetune) on a supervised downstream task&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- simCLR v1 &amp; v2 리뷰: https://rauleun.github.io/SimCLR--&gt;
&lt;h4 id=&quot;simclr-design-choices-projection-headzg&quot;&gt;SimCLR design choices: Projection head($z=g(.)$)&lt;/h4&gt;
&lt;p&gt;Linear / non-linear projection heads improve representation learning.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A possible explanation:
    &lt;ul&gt;
      &lt;li&gt;contrastive learning objective may discard useful information for downstream tasks.&lt;/li&gt;
      &lt;li&gt;representation space &lt;strong&gt;z&lt;/strong&gt; is trained to be invariant to data transformation.&lt;/li&gt;
      &lt;li&gt;by leveraging the projection head &lt;strong&gt;g(.)&lt;/strong&gt;, more information can be preserved in the &lt;strong&gt;h&lt;/strong&gt; representation space&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;simclr-design-choices-large-batch-size&quot;&gt;SimCLR design choices: Large batch size&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec13_10.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
Large training batch size is crucial for SimCLR, but it causes large memory footprint during backpropagation; requires distributed training on TPUs.&lt;/p&gt;

&lt;h3 id=&quot;momentum-contrastive-learning-moco&quot;&gt;Momentum Contrastive Learning (MoCo)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;He et al., 2020&lt;/em&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_11.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Key differences to SimCLR:
    &lt;ul&gt;
      &lt;li&gt;Keep a running queue of keys (negative samples).&lt;/li&gt;
      &lt;li&gt;Compute gradients and update the encoder only through the queries.&lt;/li&gt;
      &lt;li&gt;Decouple min-batch size with the number of keys: can support a large number of negative samples.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec13_12.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;moco-v2&quot;&gt;MoCo V2&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Chen et al., 2020&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;A hybrid of ideas from SimCLR and MoCo:&lt;br /&gt;
  From SimCLR: non-linear projection head and strong data augmentation.&lt;br /&gt;
  From MoCo: momentum-updated queues that allow training on a large number of negative samples (no TPU required).&lt;/li&gt;
  &lt;li&gt;Key takeaways(vs. SimCLR, MoCo V1):
    &lt;ul&gt;
      &lt;li&gt;Non-linear projection head and strong data augmentation are crucial for contrastive learning.&lt;/li&gt;
      &lt;li&gt;Decoupling mini-batch size with negative sample size allows MoCo-V2 to outperform SimCLR with smaller batch size (256 vs. 8192).&lt;/li&gt;
      &lt;li&gt;Achieved with much smaller memory footprint.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;instance-vs-sequence-contrastive-learning&quot;&gt;Instance vs. Sequence Contrastive Learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Instance-level contrastive learning:&lt;br /&gt;
  Based on positive &amp;amp; negative instances.&lt;br /&gt;
  E.g., SimCLR, MoCo&lt;/li&gt;
  &lt;li&gt;Sequence-level contrastive learning:&lt;br /&gt;
  Based on sequential / temporal orders.&lt;br /&gt;
  E.g., Contrastive Predictive Coding (CPC)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;contrastive-predictive-coding-cpc&quot;&gt;Contrastive Predictive Coding (CPC)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;van den Oord et al., 2018&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Contrastive: contrast between “right” and “wrong” sequences using contrastive learning.&lt;/li&gt;
  &lt;li&gt;Predictive: the model has to predict future patterns given the current context.&lt;/li&gt;
  &lt;li&gt;Coding: the model learns useful feature vectors, or “code”, for downstream tasks, similar to other context self-supervised methods.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec13_13.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Encode all samples in a sequence into vectors $z_t = g_{\mbox{enc}}(x_t)$&lt;/li&gt;
  &lt;li&gt;Summarize context (e.g., half of a sequence) into a context code $c_t$ using an auto-regressive model ($g_{\mbox{ar}}$). The original paper uses GRU-RNN here.&lt;/li&gt;
  &lt;li&gt;Compute InfoNCE loss between the context $c_t$ and future code $z_{t+k}$ using the following time-dependent score funtion: $s_k(z_{t+k}, c_t) = z_{t+k}^T W_k c_t$, where $W_k$ is a trainable matrix.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Summary(CPC):&lt;br /&gt;
  Contrast “right” sequence with “wrong” sequence.&lt;br /&gt;
  InfoNCE loss with a time-dependent score function.&lt;br /&gt;
  Can be applied to a variety of learning problems, but not as effective in learning image representations compared to instance-level methods.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-examples&quot;&gt;Other examples&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec13_14.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_15.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec13_16.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
</description>
        
        <pubDate>Wed, 19 Jan 2022 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/cs231n_lec13</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/cs231n_lec13</guid>
      </item>
      
    
      
      <item>
        <title>cs231n - Lecture 12. Generative Models</title>
        
          <description>&lt;h3 id=&quot;supervised-vs-unsupervised&quot;&gt;Supervised vs. Unsupervised&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Supervised Learning:&lt;br /&gt;
  Data: $(x,y)$; &lt;em&gt;y&lt;/em&gt; is label&lt;br /&gt;
  Goal: Learn a function to map $x\rightarrow y$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unsupervised Learning:&lt;br /&gt;
  Data: &lt;em&gt;x&lt;/em&gt;; no labels&lt;br /&gt;
  Goal: Learn some underlying hidden structure of the data&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;generative-modeling&quot;&gt;Generative Modeling&lt;/h3&gt;
&lt;p&gt;Given training data, generate new samples from same distribution&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Objectives:
    &lt;ol&gt;
      &lt;li&gt;Learn $p_{\scriptstyle\text{model}}(x)$ that approximates $p_{\scriptstyle\text{data}}(x)$&lt;/li&gt;
      &lt;li&gt;Sampling new &lt;em&gt;x&lt;/em&gt; from $p_{\scriptstyle\text{model}}(x)$&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Formulate as density estimation problems:
    &lt;ul&gt;
      &lt;li&gt;Explicit density estimation: explicitly define and solve for $p_{\scriptstyle\text{model}}(x)$.&lt;/li&gt;
      &lt;li&gt;Implicit density estimation: learn model that can sample from $p_{\scriptstyle\text{model}}(x)$ without explicitly defining it.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Why Generative Models?&lt;br /&gt;
  Realistic samples for artwork, super-resolution, colorization, etc.&lt;br /&gt;
  Learn useful features for downstream tasks such as classification.&lt;br /&gt;
  Getting insights from high-dimensional data (physics, medical imaging, etc.)&lt;br /&gt;
  Modeling physical world for simulation and planning (robotics and reinforcement learning applications)&lt;br /&gt;
  …&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pixelrnn-and-pixelcnn-a-brief-overview&quot;&gt;PixelRNN and PixelCNN; a brief overview&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Fully visible belief network (FVBN)&lt;br /&gt;
  is an explicit density model, defines tractable density function using chain rule to decompose the likelihood of an image &lt;em&gt;x&lt;/em&gt; into product of &lt;em&gt;1&lt;/em&gt;-d distributions:&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_0.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
  Then maximize likelihood of training data. It is a complex distribution over pixel values, express using a neural network.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pixelrnn-van-der-oord-et-al-2016&quot;&gt;PixelRNN, &lt;em&gt;van der Oord et al., 2016&lt;/em&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Generate image pixels starting from corner, dependency on previous pixels modeled using an RNN(LSTM). 
  &lt;img src=&quot;/assets/images/cs231n_lec12_1.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;br /&gt;
  Drawback: sequential generation is slow in both training and inference&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pixelcnn-van-der-oord-et-al-2016&quot;&gt;PixelCNN, &lt;em&gt;van der Oord et al., 2016&lt;/em&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generate image pixels starting from corner, ependency on previous pixels modeled using a CNN over context region(&lt;strong&gt;masked convolution&lt;/strong&gt;)&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_2.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;br /&gt;
  Training is faster than PixelRNN: it can parallelize convolutions since context region values known from training images.&lt;br /&gt;
  Generation is still slow: for a 32x32 image, we need to do forward passes of the network &lt;em&gt;1024&lt;/em&gt; times for a single image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Improving PixelCNN performance&lt;br /&gt;
  Gated convolutional layers, Short-cut connections, Discretized logistic loss, Multi-scale, Training tricks, etc.&lt;br /&gt;
  See also PixelCNN++, &lt;em&gt;Salimans et al., 2017&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Pros:&lt;br /&gt;
  Can explicitly compute likelihood &lt;em&gt;p(x)&lt;/em&gt;&lt;br /&gt;
  Easy to optimize&lt;br /&gt;
  Good samples&lt;/li&gt;
  &lt;li&gt;Cons:&lt;br /&gt;
  Sequential generation is slow&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variational-autoencodervae&quot;&gt;Variational Autoencoder(VAE)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;VAE is an explicit density model, defines intractable(approximate) density function with latent &lt;strong&gt;z&lt;/strong&gt;:&lt;br /&gt;
  $p_\theta(x) = \int p_\theta(z)p_\theta(x|z)\, dz$&lt;br /&gt;
  No dependencies among pixels, can generate all pixels at the same time. But cannot optimize directly, derive and optimize lower bound on likelihood instead&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background-autoencoders&quot;&gt;Background: Autoencoders&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_3.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;z&lt;/strong&gt; usually smaller than &lt;strong&gt;x&lt;/strong&gt;: with dimensionality reduction to capture meaningful factors of variation in data. Train such that features can be used to reconstruct original data($\hat{x}$)&lt;/li&gt;
      &lt;li&gt;“Autoencoding”; encoding input itself(&lt;em&gt;L2&lt;/em&gt; loss)&lt;/li&gt;
      &lt;li&gt;After training, throw away decoder and adjust to the final task&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder can be used to initialize a supervised model;
  Transfer from large, unlabeled dataset(Autoencoder) to small, labeled dataset and fine-tune; train for final task.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_4.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;But we can’t generate new images from an autoencoder because we don’t know the space of &lt;strong&gt;z&lt;/strong&gt;. $\rightarrow$ Variational Autoencoders for a generative model.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;variational-autoencoders-probabilistic-spin-on-autoencoders&quot;&gt;Variational Autoencoders: Probabilistic spin on autoencoders&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Assume training data \(\left\{ x^{(i)}\right\} _{i=1}^N\) is generated from the distribution of unobserved (latent) representation &lt;strong&gt;z&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Intuition from autoencoders: &lt;strong&gt;x&lt;/strong&gt; is an image, &lt;strong&gt;z&lt;/strong&gt; is latent factors used to generate &lt;strong&gt;x&lt;/strong&gt;: attributes, orientation, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_5.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We want to estimate the true parameters $\theta^*$ of this generative model given training data &lt;em&gt;x&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Model representation:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;p(z)&lt;/em&gt;: Choose prior to be simple, e.g. Gaussian.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;z&lt;/strong&gt;: Reasonable for latent attributes, e.g. pose, how much smile.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;p(x|z)&lt;/em&gt;: Generating images, conditional probability is complex&lt;br /&gt;
  $\rightarrow$ represent with neural network&lt;/li&gt;
      &lt;li&gt;$p_\theta(x)$: Learn model parameters to maximize likelihood of training data&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;variational-autoencoders-intractability&quot;&gt;Variational Autoencoders: Intractability&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Data likelihood:&lt;br /&gt;
  $p_\theta(x) = \int p_\theta(z)p_\theta(x|z)\, dz$&lt;br /&gt;
  where $p_\theta(z)$ is a Simple Gaussian prior and $p_\theta(x|z)$ is a decoder neural network, it is intractable to compute &lt;em&gt;p(x|z)&lt;/em&gt; for every &lt;em&gt;z&lt;/em&gt;.&lt;br /&gt;
  while &lt;em&gt;Monte Carlo estimation&lt;/em&gt;-$\log p(x) \approx \log\frac{1}{k}\sum_{i=1}^k p(x|z^{(i)})$, where $z^{(i)}\sim p(z)$- is too high variance.&lt;/li&gt;
  &lt;li&gt;divided by intractable $p_\theta(x)$, Posterior density also intractable:&lt;br /&gt;
  $p_\theta(z|x) = p_\theta(x|z)p_\theta(z)/p_\theta(x)$&lt;/li&gt;
  &lt;li&gt;Solution:&lt;br /&gt;
  In addition to modeling $p_\theta(x|z)$, learn $q_\phi(z|x)$ that approximates the true posterior $p_\theta(z|x)$. $q_\phi$, approximate posterior allows us to derive a lower bound on the data likelihood that is tractable, which can be optimized.&lt;br /&gt;
  &lt;strong&gt;Variational inference&lt;/strong&gt; is to approximate the unknown posterior distribution from only the observed data &lt;em&gt;x&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

\[\begin{align*}
\log p_\theta(x^{(i)})
&amp;amp;= \mathbf{E}_{z~q_\phi(z|x^{(i)})}\left[ \log p_\theta(x^{(i)}) \right] \quad \textit(p_\theta(x^{(i)})\ does\ not\ depend\ on\ z) \\
&amp;amp;= \mathbf{E}_z \left[
	\log\frac{p_\theta(x^{(i)}|z)p_\theta(z)}{p_\theta(z|x^{(i)})} \right] \quad \textit(Bayes'\ Rule) \\
&amp;amp;= \mathbf{E}_z \left[ 
	\log\frac{p_\theta(x^{(i)}|z)p_\theta(z)}{p_\theta(z|x^{(i)})}
		\frac{q_\phi(z|x^{(i)})}{q_\phi(z|x^{(i)})} \right] \quad \textit(Multiply\ by\ constant)\\
&amp;amp;= \mathbf{E}_z \left[\log p_\theta(x^{(i)}|z) \right]
	- \mathbf{E}_z \left[ \log\frac{q_\phi(z|x^{(i)})}{p_\theta(z)}\right]
	+ \mathbf{E}_z \left[ \log\frac{q_\phi(z|x^{(i)})}{p_\theta(z|x^{(i)})}\right] \quad \textit(Logarithms) \\
&amp;amp;= \mathbf{E}_z \left[\log p_\theta(x^{(i)}|z) \right]
	- D_{KL}(q_\phi(z|x^{(i)})|p_\theta(z)) + D_{KL}(q_\phi(z|x^{(i)})|p_\theta(z|x^{(i)}))
\end{align*}\]

&lt;ul&gt;
  &lt;li&gt;With taking expectation with respect to &lt;em&gt;z&lt;/em&gt;(using encoder network) let us write nice &lt;em&gt;KL&lt;/em&gt; terms;
    &lt;ul&gt;
      &lt;li&gt;\(\mathbf{E}_z \left[\log p_\theta(x^{(i)}\vert z) \right]\): &lt;strong&gt;Decoder&lt;/strong&gt; network gives $p_\theta(x\vert z)$, can compute estimate of this term through sampling(need some trick to differentiate through sampling). It reconstruct the input data.&lt;/li&gt;
      &lt;li&gt;\(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z))\): KL term between Gaussian for encoder and &lt;em&gt;z&lt;/em&gt; prior has nice closed-form solution. &lt;strong&gt;Encoder&lt;/strong&gt; makes approximate posterior distribution close to prior.&lt;/li&gt;
      &lt;li&gt;\(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z\vert x^{(i)}))\): is intractable, we can’t compute this term; but we know KL divergence always greater than &lt;em&gt;0&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To maximize the data likelihood, we can rewrite&lt;br /&gt;
\(\begin{align*}
\log p_\theta (x^{(i)}) &amp;amp;= \mathbf{E}_z \left[ \log p _\theta (x^{(i)}\vert z) \right]
                      - D_{KL}(q_\phi (z\vert x^{(i)})\vert p_\theta (z)) + D_{KL}(q_\phi (z\vert x^{(i)})\vert p_\theta (z\vert x^{(i)})) \\
                      &amp;amp;= \mathcal{L}(x^{(i)},\theta ,\phi ) + (C\ge 0)
\end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;\(\mathcal{L}(x^{(i)},\theta,\phi)\): &lt;em&gt;Decoder - Encoder&lt;/em&gt;&lt;br /&gt;
  &lt;strong&gt;Tractable lower bound&lt;/strong&gt; which we can take gradient of and optimize. Maximizing this &lt;em&gt;evidence lower bound(ELBO)&lt;/em&gt;, we can maximize $\log p_\theta(x)$. Later, we take minus on this term for the loss function of a neural network.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_6.png&quot; alt=&quot;png&quot; width=&quot;45%&amp;quot;, height=&amp;quot;45%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Encoder part; \(D_{KL}(q_\phi(z\vert x^{(i)})\vert p_\theta(z))\)&lt;br /&gt;
  We choose &lt;em&gt;q(z)&lt;/em&gt; as a Gaussian distribution, $q(z\vert x) = N(\mu_{z\vert x}, \Sigma_{z\vert x})$. Computing the KL divergence, \(D_{KL}(N(\mu_{z\vert x}, \Sigma_{z\vert x}))\vert N(0,I))\), having analytical solution.&lt;/li&gt;
  &lt;li&gt;Reparameterization trick &lt;em&gt;z&lt;/em&gt;:&lt;br /&gt;
  to make sampling differentiable, input sample $\epsilon\sim N(0,I)$ to the graph $z = \mu_{z\vert x} + \epsilon\sigma_{z\vert x}$; where $\mu, \sigma$ are the part of computation graph.&lt;/li&gt;
  &lt;li&gt;Decoder part;&lt;br /&gt;
  Maximize likelihood of original input being reconstructed, $\hat{x}-x$.&lt;/li&gt;
  &lt;li&gt;For every minibatch of input data, compute $\mathcal{L}$ graph forward pass and backprop.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_7.png&quot; alt=&quot;png&quot; width=&quot;75%&amp;quot;, height=&amp;quot;75%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;variational-autoencoders-generating-data&quot;&gt;Variational Autoencoders: Generating Data&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_8.png&quot; alt=&quot;png&quot; width=&quot;55%&amp;quot;, height=&amp;quot;55%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Diagonal prior on &lt;strong&gt;z&lt;/strong&gt; for independent latent variables&lt;/li&gt;
  &lt;li&gt;Different dimensions of &lt;strong&gt;z&lt;/strong&gt; encode interpretable factors of variation;&lt;br /&gt;
  Also good feature representation taht can be computed using $q_\phi(z\vert x)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-1&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Probabilistic spin to traditional autoencoders, allows generating data&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Defines an intractable density; derive and optimize a (variational) lower bound&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Pros:&lt;br /&gt;
  Principled approach to generative models&lt;br /&gt;
  Interpretable latent space&lt;br /&gt;
  Allows inference of $q(z\vert x)$, can be useful feature representation for other tasks  - Cons:&lt;br /&gt;
  Maximizes lower bound of likelihood: not as good evaluation as tractable model&lt;br /&gt;
  Samples &lt;em&gt;mean&lt;/em&gt;; blurrier and lower quality compared to state-of-the-art (GANs)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;generative-adversarial-networksgans&quot;&gt;Generative Adversarial Networks(GANs)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_9.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
idea: Use a discriminator network to tell whether the generate image is within data distribution (“real”) or not&lt;/p&gt;

&lt;h3 id=&quot;training-gans-two-player-game&quot;&gt;Training GANs: Two-player game&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_10.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
Discriminator network: try to distinguish between real and fake images&lt;br /&gt;
Generator network: try to fool discriminator by generating real-looking images&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Train jointly in &lt;strong&gt;minimax game&lt;/strong&gt;;&lt;br /&gt;
  Minimax objective function:&lt;br /&gt;
  \(\mbox{min}_{\theta_g} \mbox{max}_{\theta_d}\left[\mathbb{E}_{x\sim {p_{data}}}\log D_{\theta_d}(x) + \mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z))) \right]\)&lt;br /&gt;
  where $\theta_g$ is an objective for the generator objective and $\theta_d$ for the discriminator
    &lt;ul&gt;
      &lt;li&gt;$D_{\theta_d}(x)$: Discriminator outputs likelihood in &lt;em&gt;(0,1)&lt;/em&gt; of real image&lt;/li&gt;
      &lt;li&gt;$D_{\theta_d}(G_{\theta_g}(z))$: Discriminator output for generated fake data &lt;em&gt;G(z)&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Discriminator($\theta_d$) wants to &lt;strong&gt;maximize objective&lt;/strong&gt; such that &lt;em&gt;D(x)&lt;/em&gt; is close to &lt;em&gt;1&lt;/em&gt;(real) and &lt;em&gt;D(G(z))&lt;/em&gt; is close to &lt;em&gt;0&lt;/em&gt;(fake)&lt;/li&gt;
  &lt;li&gt;Generator($\theta_g$) wants to &lt;em&gt;minimize objective&lt;/em&gt; such that &lt;em&gt;D(G(z))&lt;/em&gt; is close to &lt;em&gt;1&lt;/em&gt;(to fool discriminator)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We alternate the minimax objection function with:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Gradient ascent&lt;/strong&gt; on discriminator&lt;br /&gt;
 \(\mbox{max}_{\theta_d}\left[\mathbb{E}_{x\sim p_{data}}\log D_{\theta_d}(x) + \mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z))) \right]\)&lt;/li&gt;
  &lt;li&gt;1) &lt;strong&gt;Gradient descent&lt;/strong&gt; on generator&lt;br /&gt;
 \(\mbox{min}_{\theta_g}\mathbb{E}_{z\sim p(z)}\log(1-D_{\theta_d}(G_{\theta_g}(z)))\)
    &lt;ul&gt;
      &lt;li&gt;In practice, optimizing this generator objective does not work well;&lt;br /&gt;
  When sample is likely fake, want to learn from it to improve generator (move to the right on &lt;em&gt;X&lt;/em&gt; axis), but gradient near &lt;em&gt;0&lt;/em&gt; in &lt;em&gt;X&lt;/em&gt; axis is relatively flat; Gradient signal is dominated by region where sample is already good(near &lt;em&gt;1&lt;/em&gt;).&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_11.png&quot; alt=&quot;png&quot; width=&quot;30%&amp;quot;, height=&amp;quot;30%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;2) &lt;strong&gt;Instead: Gradient ascent&lt;/strong&gt; on generator, different objective&lt;br /&gt;
 \(\mbox{max}_{\theta_d}\mathbb{E}_{z\sim p(z)}\log(D_{\theta_d}(G_{\theta_g}(z)))\)&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Rather than minimizing likelihood of discriminator being correct, maximize likelihood of discriminator being wrong. Same objective of fooling discriminator, but now higher gradient signal for bad samples.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_12.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GAN training Algorithm&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec12_13.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;70%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After training, use generator network to generate new images&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gan-convolutional-architectures&quot;&gt;GAN: Convolutional Architectures&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generator is an upsampling network with fractionally-strided convolutions&lt;br /&gt;
  Discriminator is a convolutional network&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Architecture guidelines for stable Deep Conv GANs&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Replace any pooling layers with strided convolutions(discriminator) and fractional-strided convolutions(generator).&lt;/li&gt;
      &lt;li&gt;Use batchnorm in both network.&lt;/li&gt;
      &lt;li&gt;Remove fully connected hidden layers for deeper architecture.&lt;/li&gt;
      &lt;li&gt;Use ReLU activation in generator for all layers except for the output, which uses Tanh.&lt;/li&gt;
      &lt;li&gt;Use LeakyReLU activation in the discriminator for all layers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gan-interpretable-vector-math&quot;&gt;GAN: Interpretable Vector Math&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_14.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;works similar to a language model&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2017-explosion-of-gans&quot;&gt;2017: Explosion of GANs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;“The GAN Zoo”, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://github.com/hindupuravinash/the-gan-zoo&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;check &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://github.com/soumith/ganhacks&lt;/code&gt; for tips and tricks for training GANs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;scene-graphs-to-gans&quot;&gt;Scene graphs to GANs&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec12_15.png&quot; alt=&quot;png&quot; width=&quot;30%&amp;quot;, height=&amp;quot;30%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Specifying exactly what kind of image you want to generate. The explicit structure in scene graphs provides better image generation for complex scenes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-gans&quot;&gt;Summary: GANs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Don’t work with an explicit density function&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Take game-theoretic approach: learn to generate from training distribution through 2-player game&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Pros:
    &lt;ul&gt;
      &lt;li&gt;Beautiful, state-of-the-art samples&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cons:
    &lt;ul&gt;
      &lt;li&gt;Trickier / more unstable to train&lt;/li&gt;
      &lt;li&gt;Can’t solve inference queries such as $p(x)$, $p(z\vert x)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Active areas of research:
    &lt;ul&gt;
      &lt;li&gt;Better loss functions, more stable training (Wasserstein GAN, LSGAN, many others)&lt;/li&gt;
      &lt;li&gt;Conditional GANs, GANs for all kinds of applications&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Useful Resources on Generative Models&lt;br /&gt;
  CS236: Deep Generative Models (Stanford)&lt;br /&gt;
  CS 294-158 Deep Unsupervised Learning (Berkeley)&lt;/li&gt;
&lt;/ul&gt;
</description>
        
        <pubDate>Sun, 09 Jan 2022 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/cs231n_lec12</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/cs231n_lec12</guid>
      </item>
      
    
      
      <item>
        <title>cs231n - Lecture 11. Attention and Transformers</title>
        
          <description>&lt;h2 id=&quot;attention-with-rnns&quot;&gt;Attention with RNNs&lt;/h2&gt;

&lt;h3 id=&quot;image-captioning-using-spatial-features&quot;&gt;Image Captioning using spatial features&lt;/h3&gt;
&lt;p&gt;Input: Image &lt;em&gt;I&lt;/em&gt;&lt;br /&gt;
Output: Sequence &lt;strong&gt;y&lt;/strong&gt; $= y_1, y_2, \ldots, y_T$&lt;br /&gt;
Encoder: $h_0 = f_W(z)$, where &lt;em&gt;z&lt;/em&gt; is spatial CNN features, $f_W(\cdot)$ is an MLP&lt;br /&gt;
Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector &lt;em&gt;c&lt;/em&gt; is often $c=h_0$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
Problem: Input is “bottlenecked” through &lt;em&gt;c&lt;/em&gt;; especially in a long descriptions.  Model needs to encode everything it wants to say within &lt;em&gt;c&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Attention idea: New context vector &lt;em&gt;c_t&lt;/em&gt; at every time step&lt;br /&gt;
Each context vector will attend to different image regions&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alignment scores(scalars): $H \times W$ matrix &lt;strong&gt;&lt;em&gt;e&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
  $e_{t,i,j} = f_{\mbox{att}}(h_{t-1}, z_{i,j})$&lt;br /&gt;
  where $f_{\mbox{att}}(\cdot)$ is an MLP&lt;/li&gt;
  &lt;li&gt;Normalize to get attention weights:&lt;br /&gt;
  $a_{t,:,:} = \mbox{softmax}(e_{t,:,:})$,&lt;br /&gt;
  $0&amp;lt;a_{t,i,j}&amp;lt;1$, attention values sum to &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Compute context vector &lt;em&gt;c&lt;/em&gt;: multiply &lt;em&gt;CNN features&lt;/em&gt; and &lt;em&gt;Attention weights&lt;/em&gt;&lt;br /&gt;
  $c_t = \sum_{i,j} a_{t,i,j} z_{t,i,j}$&lt;/li&gt;
  &lt;li&gt;Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, \color{red}{c_t})$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_1.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;50%&quot; /&gt;&lt;br /&gt;
Each timestep of decode uses a different context vector that looks(attend) at different parts of the input image. This entire process is differentiable; model chooses its own attention weights. No attention supervision is required.&lt;/p&gt;

&lt;h3 id=&quot;similar-tasks-in-nlp---language-translation-example&quot;&gt;Similar tasks in NLP - Language translation example&lt;/h3&gt;
&lt;p&gt;Vanilla Encoder-Decoder setting:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Input: sequence &lt;strong&gt;x&lt;/strong&gt; $= x_1, x_2, \ldots, x_T$&lt;/li&gt;
  &lt;li&gt;Output: sequence &lt;strong&gt;y&lt;/strong&gt; $= y_1, y_2, \ldots, y_T$&lt;/li&gt;
  &lt;li&gt;Encoder: $h_0 = f_W(z)$, where $z_t = \mbox{RNN}(x_t, u_{t-1})$, $f_W(\cdot)$ is MLP, &lt;em&gt;u&lt;/em&gt; is the hidden RNN state&lt;/li&gt;
  &lt;li&gt;Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, c)$, where context vector &lt;em&gt;c&lt;/em&gt; is often $c=h_0$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
Attention in NLP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alignment scores(scalars):&lt;br /&gt;
  $e_{t,i} = f_{\mbox{att}}(h_{t-1}, z_t)$, where $f_{\mbox{att}}(\cdot)$ is an MLP&lt;/li&gt;
  &lt;li&gt;Normalize to get attention weights:&lt;br /&gt;
  $a_{t,:} = \mbox{softmax}(e_{t,:})$,&lt;br /&gt;
  $0&amp;lt;a_{t,i}&amp;lt;1$, attention values sum to &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Compute context vector &lt;em&gt;c&lt;/em&gt;:&lt;br /&gt;
  $c_t = \sum_i a_{t,i} z_{t,i}$&lt;/li&gt;
  &lt;li&gt;Decoder: $y_t = g_v(y_{t-1}, h_{t-1}, \color{red}c_t)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Heatmap: visualization of attention weights; without any attention supervision, model learns different word orderings for different languages&lt;/p&gt;

&lt;h2 id=&quot;general-attention-layer&quot;&gt;General Attention Layer&lt;/h2&gt;
&lt;p&gt;Attention in image captioning before
&lt;img src=&quot;/assets/images/cs231n_lec11_4.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;single-query-setting&quot;&gt;Single query setting&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_5.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
&lt;strong&gt;Inputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;input vectors: &lt;strong&gt;x&lt;/strong&gt;(shape: $N\times D$)&lt;br /&gt;
  Attention operation is permutation invariant; produces the same output regardless of the order of elements(features) in the input vector. Stretch $H\times W = N$ into &lt;em&gt;N&lt;/em&gt; vectors, transform $H\times W\times D$ features into $N\times D$ input vectors &lt;strong&gt;x&lt;/strong&gt;(similar to attention in NLP).&lt;/li&gt;
  &lt;li&gt;Query: &lt;strong&gt;h&lt;/strong&gt;(shape: D)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alignment
    &lt;ul&gt;
      &lt;li&gt;Change $f_{\mbox{att}}(\cdot)$ to a simple dot product:&lt;br /&gt;
  $e_i = h\cdot x_i$; only works well with key &amp;amp; value transformation trick&lt;/li&gt;
      &lt;li&gt;Change $f_{\mbox{att}}(\cdot)$ to a &lt;strong&gt;scaled&lt;/strong&gt; dot product:&lt;br /&gt;
  $e_i = h\cdot x_i / \sqrt{D}$;&lt;br /&gt;
  Larger dimensions means more terms in the dot product sum. So, the variance of the logits is higher. Large magnitude(length) vectors will produce much higher logits. Then, the post-softmax distribution(&lt;em&gt;e&lt;/em&gt;) has lower-entropy(high uncertainty) assuming logits are &lt;em&gt;I.I.D&lt;/em&gt;. Ultimately, these large magnitude vectors will cause softmax to peak and assign very little weight to all others. To reduce this effect, divide by $sqrt{D}$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$&lt;/li&gt;
  &lt;li&gt;Output: $\mathbf{c} = \sum_i a_i x_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;br /&gt;
	- context vector: &lt;strong&gt;c&lt;/strong&gt;(shape: D)&lt;/p&gt;

&lt;h3 id=&quot;multiple-query-setting&quot;&gt;Multiple query setting&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_6.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;br /&gt;
&lt;strong&gt;Inputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;input vectors: &lt;strong&gt;x&lt;/strong&gt;(shape: $N\times D$)&lt;/li&gt;
  &lt;li&gt;Queries: &lt;strong&gt;q&lt;/strong&gt;(shape: $M\times D$); multiple query vectors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Alignment: $e_{i,j} = q_j\cdot x_i / \sqrt{D}$&lt;/li&gt;
  &lt;li&gt;Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$&lt;/li&gt;
  &lt;li&gt;Output: $y_j = \sum_i a_{i,j} x_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;context vectors: &lt;strong&gt;y&lt;/strong&gt;(shape: D);&lt;br /&gt;
  each query creates a new output context vector&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;weight-layers-added&quot;&gt;Weight layers added&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_7.png&quot; alt=&quot;png&quot; width=&quot;45%&amp;quot;, height=&amp;quot;45%&quot; /&gt;&lt;br /&gt;
Notice that the input vectors &lt;strong&gt;x&lt;/strong&gt; are used for both the alignment(&lt;strong&gt;e&lt;/strong&gt;) and attention calculations(&lt;strong&gt;y&lt;/strong&gt;); We can add more expressivity to the layer by adding a different FC layer before each of the two steps. The input and output dimensions can now change depending on the key and value FC layers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;input vectors: &lt;strong&gt;x&lt;/strong&gt;(shape: $N\times D$)&lt;/li&gt;
  &lt;li&gt;Queries: &lt;strong&gt;q&lt;/strong&gt;(shape: $M\times D_k$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Key vectors: $\mathbf{k} = \mathbf{x}W_k$&lt;/li&gt;
  &lt;li&gt;Value vectors: $\mathbf{v} = \mathbf{x}W_v$&lt;/li&gt;
  &lt;li&gt;Alignment: $e_{i,j} = q_j\cdot k_i / \sqrt{D}$&lt;/li&gt;
  &lt;li&gt;Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$&lt;/li&gt;
  &lt;li&gt;Output: $y_j = \sum_i a_{i,j} v_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;context vectors: &lt;strong&gt;y&lt;/strong&gt;(shape: $D_v$);&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;self-attention-layer&quot;&gt;Self attention layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_8.png&quot; alt=&quot;png&quot; width=&quot;45%&amp;quot;, height=&amp;quot;45%&quot; /&gt;&lt;br /&gt;
Recall that the query vector was a function of the input vectors; Encoder $h_0=f_W(z)$, where &lt;em&gt;z&lt;/em&gt; is spatial CNN features, $f_W(\cdot)$ is an MLP. We can calculate the query vectors from the input vectors, defining a “self-attention” layer. No input query vectors anymore, instead query vectors are calculated using a FC layer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;input vectors: &lt;strong&gt;x&lt;/strong&gt;(shape: $N\times D$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Key vectors: $\mathbf{k} = \mathbf{x}W_k$&lt;/li&gt;
  &lt;li&gt;Value vectors: $\mathbf{v} = \mathbf{x}W_v$&lt;/li&gt;
  &lt;li&gt;Query vectors: $\mathbf{q} = \mathbf{x}W_q$&lt;/li&gt;
  &lt;li&gt;Alignment: $e_{i,j} = q_j\cdot k_i / \sqrt{D}$&lt;/li&gt;
  &lt;li&gt;Attention: $\mathbf{a} = \mbox{softmax}(\mathbf{e})$&lt;/li&gt;
  &lt;li&gt;Output: $y_j = \sum_i a_{i,j} v_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;context vectors: &lt;strong&gt;y&lt;/strong&gt;(shape: $D_v$)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;positional-encoding&quot;&gt;&lt;em&gt;Positional encoding&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_9.png&quot; alt=&quot;png&quot; width=&quot;35%&amp;quot;, height=&amp;quot;35%&quot; /&gt;&lt;br /&gt;
Self attention attends over sets of inputs; is permutation invariant. To encode the ordered sequences(e.g. language, image), concatenate special positional encoding $p_j$ to each input vector $x_j$.&lt;/p&gt;

&lt;p&gt;$\mathit{pos}: N\rightarrow R^d$ to process the position &lt;em&gt;j&lt;/em&gt; of the vector into a &lt;em&gt;d&lt;/em&gt;-dimensional vector; $p_j = \mathit{pos}(j)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Desiderata&lt;/strong&gt; of $\mathit{pos}(\cdot)$:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Should output a &lt;strong&gt;unique&lt;/strong&gt; encoding for each time-step(word’s position in a sentence).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Distance&lt;/strong&gt; between any two time-steps should be consistent across sentences with different lengths(variable inputs).&lt;/li&gt;
  &lt;li&gt;Model should generalize to &lt;strong&gt;longer&lt;/strong&gt; sentences without any efforts. Its values should be bounded.&lt;/li&gt;
  &lt;li&gt;Must be &lt;strong&gt;deterministic&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Options&lt;/strong&gt; for $\mathit{pos}(\cdot)$:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Learn a lookup table:
    &lt;ul&gt;
      &lt;li&gt;Learn parameters to use for $\mathit{pos}(t)$ for $t \in [0,T)$&lt;/li&gt;
      &lt;li&gt;Lookup table contains $T\times d$ parameters&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Design a fixed function with the desiderata&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec11_10.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;masked-self-attention-layer&quot;&gt;Masked self-attention layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_11.png&quot; alt=&quot;png&quot; width=&quot;45%&amp;quot;, height=&amp;quot;45%&quot; /&gt;&lt;br /&gt;
Manually set alignment scores to $-\infty$, prevent vectors from looking at future vectors.&lt;/p&gt;

&lt;h3 id=&quot;multi-head-self-attention-layer&quot;&gt;Multi-head self attention layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_12.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
Multiple self-attention heads in parallel; similar to ensemble&lt;/p&gt;

&lt;h3 id=&quot;comparing-rnns-to-transformers&quot;&gt;Comparing RNNs to Transformers&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;RNNs&lt;/strong&gt;&lt;br /&gt;
&lt;span style=&quot;color:green&quot;&gt;(+) LSTMs work reasonably well for long sequences.&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:red&quot;&gt;(-) Expects an ordered sequences of inputs&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:red&quot;&gt;(-) Sequential computation: subsequent hidden states can only be computed after the previous ones are done.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Transformers&lt;/strong&gt;&lt;br /&gt;
&lt;span style=&quot;color:green&quot;&gt;(+) Good at long sequences. Each attention calculation looks at all inputs.&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:green&quot;&gt;(+) Can operate over unordered sets or ordered sequences with positional encodings.&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:green&quot;&gt;(+) Parallel computation: All alignment and attention scores for all inputs can be done in parallel.&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;color:red&quot;&gt;(-) Requires a lot of memory: N x M alignment and attention scalers need to be calculated and stored for a single self-attention head.&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;transformers&quot;&gt;Transformers&lt;/h2&gt;
&lt;h3 id=&quot;image-captioning-using-transformers&quot;&gt;Image Captioning using transformers&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_13.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;No recurrence at all&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Input: Image &lt;strong&gt;I&lt;/strong&gt;&lt;br /&gt;
Output: Sequence &lt;strong&gt;y&lt;/strong&gt; $= y_1, y_2, \ldots, y_T$&lt;br /&gt;
Encoder: $c = T_W(z)$, where &lt;em&gt;z&lt;/em&gt; is spatial CNN features, $T_W(\cdot)$ is the transformer encoder&lt;br /&gt;
Decoder: $y_t = T_D(y_{0:t-1}, c)$, where $T_D(\cdot)$ is the transformer decoder&lt;/p&gt;

&lt;h4 id=&quot;the-transformer-encoder-block&quot;&gt;The Transformer encoder block&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_14.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
Inputs: Set of vectors &lt;strong&gt;x&lt;/strong&gt;&lt;br /&gt;
Outputs: Set of vectors &lt;strong&gt;y&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Self-attention is the only interaction between vectors; Layer norm and MLP operate independently per vector. Highly scalable, highly parallelizable, but high memory usage.&lt;/p&gt;

&lt;h4 id=&quot;the-transformer-decoder-block&quot;&gt;The Transformer Decoder block&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec11_15.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
Inputs: Set of vectors &lt;strong&gt;x&lt;/strong&gt; and Set of context vector &lt;strong&gt;c&lt;/strong&gt;&lt;br /&gt;
Outputs: Set of vectors &lt;strong&gt;y&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Masked Self-attention only interacts with past inputs(&lt;em&gt;x&lt;/em&gt;, or previous output $y_{t-1}$). Multi-head attention block is NOT self-attention; it attends over the transformer encoder outputs. In this phase, we inject image features into the decoder. Highly scalable, highly parallelizable, but high memory usage.&lt;/p&gt;

&lt;h3 id=&quot;image-captioning-using-only-transformers&quot;&gt;Image Captioning using ONLY transformers&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Transformers from pixels to language&lt;br /&gt;
  &lt;em&gt;Dosovitskiy et al, “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”, ArXiv 2020&lt;/em&gt;  &lt;a href=&quot;https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax.ipynb&quot; target=&quot;_blank&quot;&gt;colab notebook link&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note: in Google Colab - TPU runtime setting&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# TPU initialization
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolver&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distribute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cluster_resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TPUClusterResolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'grpc://'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'COLAB_TPU_ADDR'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experimental_connect_to_cluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experimental&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initialize_tpu_system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;strategy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distribute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TPUStrategy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# compile in strategy.scope
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strategy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'adam'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SparseCategoricalCrossentropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_logits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sparse_categorical_accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Adding &lt;strong&gt;attention&lt;/strong&gt; to RNNs allows them to “attend” to different parts of the input at every time step&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;general attention layer&lt;/strong&gt; is a new type of layer that can be used to design new neural network architectures&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transformers&lt;/strong&gt; are a type of layer that uses self-attention and layer norm.
    &lt;ul&gt;
      &lt;li&gt;It is highly scalable and highly parallelizable&lt;/li&gt;
      &lt;li&gt;Faster training, larger models, better performance across vision and language tasks&lt;/li&gt;
      &lt;li&gt;They are quickly replacing RNNs, LSTMs, and may even replace convolutions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        
        <pubDate>Tue, 04 Jan 2022 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/cs231n_lec11</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/cs231n_lec11</guid>
      </item>
      
    
      
      <item>
        <title>cs231n - Lecture 10. Recurrent Neural Networks</title>
        
          <description>&lt;h2 id=&quot;rnn-process-sequences&quot;&gt;RNN: Process Sequences&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;one to one; vanilla neural networks&lt;/li&gt;
  &lt;li&gt;one to many; e.g. Image Captioning(image to sequence of words)&lt;/li&gt;
  &lt;li&gt;many to one; e.g. Action Prediction(video sequence to action class)&lt;/li&gt;
  &lt;li&gt;many to many(1); e.g. Video Captioning(video sequence to caption)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;many to many(2); e.g. Video Classification on frame level&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Why existing convnets are insufficient?:&lt;br /&gt;
  Variable sequence length inputs and outputs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_1.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;70%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Key idea: RNNs have an “internal state” that is updated as a sequence is processed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN hidden state update:&lt;br /&gt;
  \(h_t = f_W(h_{t-1}, x_t)\)&lt;br /&gt;
  The same function and the same set of parameters are used at every time step.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RNN output generation: \(y_t = f_{W_hy}(h_t)\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Simple(Vanilla) RNN: The state consists of a single hidden vector &lt;em&gt;h&lt;/em&gt;&lt;br /&gt;
  $h_t = \mbox{tanh}(W_hh h_{t-1} + W_{xh}x_t)$&lt;br /&gt;
  $y_t = W_{hy}h_t$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sequence-to-sequenceseq2seq-many-to-one--one-to-many&quot;&gt;Sequence to Sequence(Seq2Seq): Many-to-One + One-to-Many&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Many-to-One: Encode input sequence in a single vector&lt;br /&gt;
  One-to-Many: Produce output sequence from single input vector&lt;br /&gt;
  Encoder produces the last hidden state $h_T$ and decoder uses it as a default $h_0$. Weights($W_1, W_2$) are re-used for each procedure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Example: Character-level Language Model Sampling&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_2.png&quot; alt=&quot;png&quot; width=&quot;40%&amp;quot;, height=&amp;quot;40%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;backpropagation&quot;&gt;Backpropagation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Backpropagation through time: Computationally Expensive&lt;br /&gt;
  Forward through entire sequence to compute loss, then backward through entire sequence to compute gradient.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Truncated&lt;/strong&gt; Backpropagation through time:&lt;br /&gt;
  Run forward and backward through &lt;strong&gt;chunks of the sequence&lt;/strong&gt; instead of whole sequence. Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rnn-tradeoffs&quot;&gt;RNN tradeoffs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RNN Advantages:
    &lt;ul&gt;
      &lt;li&gt;Can process any length input&lt;/li&gt;
      &lt;li&gt;Computation for step t can (in theory) use information from many steps back&lt;/li&gt;
      &lt;li&gt;Model size doesn’t increase for longer input&lt;/li&gt;
      &lt;li&gt;Same weights applied on every timestep, so there is symmetry in how inputs are processed.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN Disadvantages:
    &lt;ul&gt;
      &lt;li&gt;Recurrent computation is slow&lt;/li&gt;
      &lt;li&gt;In practice, difficult to access information from many steps back&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;image-captioning-cnn--rnn&quot;&gt;Image Captioning: CNN + RNN&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Instead of the final FC layer and the classifier in CNN, use FC output &lt;em&gt;v&lt;/em&gt;(say 4096 length vector) to formulate the default hidden state $h_0$ in RNN.
    &lt;ul&gt;
      &lt;li&gt;before: $h = \mbox{tanh}(W_{xh}\ast x+W_{hh}\ast h)$&lt;/li&gt;
      &lt;li&gt;now: $h=\mbox{tanh}(W_{xh}\ast x + W_{hh}\ast h + W_{ih}\ast v)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN for Image Captioning&lt;br /&gt;
  Re-sample the previous output $y_{t-1}$ as the next input $x_t$, iterate untill $y_t$ sample takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;END&amp;gt;&lt;/code&gt; token.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;visual-question-answering-rnns-with-attention&quot;&gt;Visual Question Answering: RNNs with Attention&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;other-tasks&quot;&gt;Other tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Visual Dialog: Conversations about images&lt;/li&gt;
  &lt;li&gt;Visual Language Navigation: Go to the living room&lt;br /&gt;
  Agent encodes instructions in language and uses an RNN to generate a series of movements as the visual input changes after each move.&lt;/li&gt;
  &lt;li&gt;Visual Question Answering: Dataset Bias&lt;br /&gt;
  With different types(Image + Question + Answer) of data used, model performances are better.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;long-short-term-memory-lstm&quot;&gt;Long Short Term Memory (LSTM)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Vanilla RNN&lt;br /&gt;
  \(h_t = \mbox{tanh}(W_{hh}h_{t-1} + W_{xh}x_t) \\
      = \mbox{tanh}\left(
          (W_{hh} \ W_{hx}) {\begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}}
                  \right) \\
      = \mbox{tanh}\left(
          W {\begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}}
                  \right)\)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(\frac{\partial h_t}{\partial h_{t-1}} = \mbox{tanh}' (W_{hh}h_{t-1} + W_{xh}x_t)W_{hh}\)&lt;br /&gt;
  $\frac{\partial L}{\partial W} = \sum_{t=1}^T \frac{\partial L_t}{\partial W}$&lt;/p&gt;

\[\begin{align*}
  \frac{\partial L_T}{\partial W} &amp;amp;= \frac{\partial L_T}{\partial h_T}
                                      \frac{\partial h_t}{\partial h_{t-1}}\cdots
                                      \frac{\partial h_1}{\partial W} \\
                                   &amp;amp;= \frac{\partial L_T}{\partial h_T}(\prod_{t=2}^T \frac{\partial h_t}{\partial h_{t-1}})\frac{\partial h_1}{\partial W} \\
                                  &amp;amp;= \frac{\partial L_T}{\partial h_T}(\prod_{t=2}^T \mbox{tanh}'(W_{hh}h_{t-1} + W_{xh}x_t))W_{hh}^{T-1} \frac{\partial h_1}{\partial W}
  \end{align*}\]
  &lt;/li&gt;
  &lt;li&gt;Problem&lt;br /&gt;
  As the output of &lt;em&gt;tanh&lt;/em&gt; function are in range of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[-1,1]&lt;/code&gt; and almost smaller than 1, vanilla RNN has &lt;strong&gt;&lt;em&gt;vanishing gradients&lt;/em&gt;&lt;/strong&gt;. If we assume no non-linearity, the gradient will be \(\frac{\partial L_T}{\partial W} = \frac{\partial L_T}{\partial h_T}W_{hh}^{T-1}\frac{\partial h_1}{\partial W}\). In this case, when the largest singular value is greater than 1, we have exploding gradients, while the value is smaller than 1, we have vanishing gradients.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# dimensionality of hidden state
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# number of time steps
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Whh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# forward pass of an RNN (ignoring inputs x)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Whh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	
&lt;span class=&quot;c1&quot;&gt;# backward pass of the RNN
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dhs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dhs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#start off the chain with random gradient
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reversed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dhs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# backprop through the nonlinearity
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;dhs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Whh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# backprop into previous hidden state
&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# &quot;Whh.T&quot; multiplied by &quot;T&quot; times!
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For exploding gradients: control with gradient clipping.&lt;br /&gt;
  For vanishing gradients: change the architecture, LSTM introduced.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LSTM:&lt;br /&gt;
  \(\begin{pmatrix} i \\ f \\ o \\ g \end{pmatrix} =
  \begin{pmatrix} \sigma \\ \sigma \\ \sigma \\ \mbox{tanh}\end{pmatrix} W \begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}\)&lt;br /&gt;
  \(c_t = f \odot c_{t-1} + i \odot g\), &lt;em&gt;memory cell update&lt;/em&gt;&lt;br /&gt;
  \(h_t = o \odot \mbox{tanh}(c_t)\), &lt;em&gt;hidden state update&lt;/em&gt;&lt;br /&gt;
  where &lt;em&gt;W&lt;/em&gt; is a stack of $W_h$ and $W_x$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_4.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
i: Input gate, whether to write to cell&lt;br /&gt;
f: Forget gate, Whether to erase cell&lt;br /&gt;
o: Output gate, How much to reveal cell&lt;br /&gt;
g: Gate gate, How much to write to cell&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec10_5.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Backpropagation from $c_t$ to $c_{t-1}$ only elementwise multiplication by &lt;em&gt;f&lt;/em&gt;, no matrix multiply by &lt;em&gt;W&lt;/em&gt;. Notice that the gradient contains the &lt;em&gt;f&lt;/em&gt; gate’s vector of activations; it allows better control of gradients values, using suitable parameter updates of the forget gate. Also notice that are added through the &lt;em&gt;f, i, g,&lt;/em&gt; and &lt;em&gt;o&lt;/em&gt; gates, we can have better balancing of gradient values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Recall: “PlainNets” vs. ResNets&lt;br /&gt;
  ResNet is to PlainNet what LSTM is to RNN, kind of.&lt;br /&gt;
  &lt;em&gt;Additive skip connections&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Do LSTMs solve the vanishing gradient problem?:&lt;br /&gt;
  The LSTM architecture makes it easier for the RNN to preserve information over many timesteps. e.g. If $f=1$ and $i=0$, then the information of that cell is preserved indefinitely. By contrast, it’s harder for vanilla RNN to learn a recurrent weight matrix $W_h$ that preserves information in hidden state.&lt;br /&gt;
  LSTM doesn’t guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;in between: Highway Networks, &lt;em&gt;Srivastava et al, 2015, [arXiv:1505.00387v2]&lt;/em&gt;&lt;br /&gt;
  A new architecture designed to ease gradient-based training of very deep networks. To regulate the flow of information and enlarge the possibility of studying extremely deep and efficient architectures.&lt;br /&gt;
  $g = T(x, W_T)$, $y = g \odot H(x, W_H) + (1-g)\odot x$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-rnn-variants&quot;&gt;Other RNN Variants&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Neural Architecture Search(NAS) with Reinforcement Learning, &lt;em&gt;Zoph et Le, 2017&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;RNN to design model; idea that we can represent the model architecture with a variable-length string.&lt;/li&gt;
      &lt;li&gt;Apply reinforcement learning on a neural network to maximize the accuracy(as a reward) on validation set, find a good architecture.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GRU; smaller LSTM, &lt;em&gt;“Learning phrase representations using rnn encoder-decoder for statistical machine translation”, Cho et al., 2014&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“An Empirical Exploration of Recurrent Network Architectures”, Jozefowicz et al., 2015&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;LSTM: A Search Space Odyssey, Greff et al., 2015&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;recurrence-for-vision&quot;&gt;Recurrence for Vision&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;LSTM wer a good default choice until this year&lt;/li&gt;
  &lt;li&gt;Use variants like GRU if you want faster compute and less parameters&lt;/li&gt;
  &lt;li&gt;Use transformers (next lecture) as they are dominating NLP models&lt;/li&gt;
  &lt;li&gt;almost everyday there is a new vision transformer model&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RNNs allow a lot of flexibility in architecture design&lt;/li&gt;
  &lt;li&gt;Vanilla RNNs are simple but don’t work very well&lt;/li&gt;
  &lt;li&gt;Common to use LSTM or GRU: their additive interactions improve gradient flow&lt;/li&gt;
  &lt;li&gt;Backward flow of gradients in RNN can explode or vanish. Exploding is controlled with gradient clipping. Vanishing is controlled with additive interactions (LSTM)&lt;/li&gt;
  &lt;li&gt;Better/simpler architectures are a hot topic of current research, as well as new paradigms for reasoning over sequences&lt;/li&gt;
  &lt;li&gt;Better understanding (both theoretical and empirical) is needed.&lt;/li&gt;
&lt;/ul&gt;
</description>
        
        <pubDate>Mon, 03 Jan 2022 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/cs231n_lec10</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/cs231n_lec10</guid>
      </item>
      
    
      
      <item>
        <title>cs231n - Lecture 9. CNN Architectures</title>
        
          <description>&lt;h2 id=&quot;review&quot;&gt;Review&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;LeCun et al., 1998&lt;/em&gt;&lt;br /&gt;
  $5\times 5$ Conv filters applied at stride &lt;em&gt;1&lt;/em&gt;&lt;br /&gt;
  $2\times 2$ Subsampling (Pooling) layers applied at stride &lt;em&gt;2&lt;/em&gt;&lt;br /&gt;
  i.e. architecture is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[CONV-POOL-CONV-POOL-FC-FC]&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stride: Downsample output activations&lt;br /&gt;
  Padding: Preserve input spatial dimensions in output activations&lt;br /&gt;
  Filter: Each conv filter outputs a “slice” in the activation&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;case-studies&quot;&gt;Case Studies&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec9_0.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;alexnet-first-cnn-based-winner&quot;&gt;AlexNet: First CNN-based winner&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Architecture: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MaxPOOL3-FC6-FC7-FC8]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;Input: $227\times 227\times 3$ images&lt;/li&gt;
      &lt;li&gt;First layer(CONV1):&lt;br /&gt;
  &lt;em&gt;96&lt;/em&gt; $11\times 11$ filters applied at stride &lt;em&gt;4&lt;/em&gt;, pad &lt;em&gt;0&lt;/em&gt;&lt;br /&gt;
  Output volume: $W’ = (W-F+2P)/S + 1 \rightarrow$ $55\times 55\times 96$&lt;br /&gt;
  Parameters: $(11* 11* 3 +1)* 96 =$ &lt;strong&gt;36K&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Second layer(POOL1):&lt;br /&gt;
  $3\times 3\times$ filters applied at stride &lt;em&gt;2&lt;/em&gt;&lt;br /&gt;
  Output volume: $27\times 27\times 96$&lt;br /&gt;
  Parameters: 0&lt;br /&gt;
  $\vdots$&lt;/li&gt;
      &lt;li&gt;CONV2($27\times 27\times 256$):&lt;br /&gt;
  &lt;em&gt;256&lt;/em&gt; $5\times 5$ filters applied at stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;2&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;MAX POOL2($13\times 13\times 256):&lt;br /&gt;
  $3\times 3\times$ filters applied at stride &lt;em&gt;2&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;CONV3($13\times 13\times 384$):&lt;br /&gt;
  &lt;em&gt;384&lt;/em&gt; $3\times 3$ filters applied at stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;CONV4($13\times 13\times 384$):&lt;br /&gt;
  &lt;em&gt;384&lt;/em&gt; $3\times 3$ filters applied at stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;CONV5($13\times 13\times 256$):&lt;br /&gt;
  &lt;em&gt;256&lt;/em&gt; $3\times 3$ filters applied at stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;1&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;MAX POOL3($6\times 6\times 256$):&lt;br /&gt;
  $3\times 3\times$ filters applied at stride &lt;em&gt;2&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;FC6(4096): &lt;em&gt;4096&lt;/em&gt; neurons&lt;/li&gt;
      &lt;li&gt;FC7(4096): &lt;em&gt;4096&lt;/em&gt; neurons&lt;/li&gt;
      &lt;li&gt;FC8(1000): &lt;em&gt;1000&lt;/em&gt; neurons (class scores)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Historical note:
    &lt;ul&gt;
      &lt;li&gt;Network spread across &lt;em&gt;2&lt;/em&gt; GPUs, half the neurons (feature maps) on each GPU.&lt;/li&gt;
      &lt;li&gt;CONV1, CONV2, CONV4, CONV5: Connections only with feature maps on same GPU&lt;/li&gt;
      &lt;li&gt;CONV3, FC6, FC7, FC8: Connections with all feature maps in preceding layer, communication across GPUs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Details/Retrospectives:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Krizhevsky et al. 2012&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;first use of ReLU&lt;/li&gt;
      &lt;li&gt;used Norm layers (not common anymore)&lt;/li&gt;
      &lt;li&gt;heavy data augmentation&lt;/li&gt;
      &lt;li&gt;dropout &lt;em&gt;0.5&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;batch size &lt;em&gt;128&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;SGD Momentum &lt;em&gt;0.9&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Learning rate &lt;em&gt;1e-2&lt;/em&gt;, reduced by &lt;em&gt;10&lt;/em&gt; manually when val accuracy plateaus&lt;/li&gt;
      &lt;li&gt;L2 weight decay &lt;em&gt;5e-4&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;7 CNN ensemble: &lt;em&gt;18.2%&lt;/em&gt; $\rightarrow$ &lt;em&gt;15.4%&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;zfnet-improved-hyperparameters-over-alexnet&quot;&gt;ZFNet: Improved hyperparameters over AlexNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;AlexNet but:
    &lt;ul&gt;
      &lt;li&gt;CONV1: change from ($11\times 11$ stride &lt;em&gt;4&lt;/em&gt;) to ($7\times 7$ stride &lt;em&gt;2&lt;/em&gt;)&lt;/li&gt;
      &lt;li&gt;CONV3,4,5: instead of &lt;em&gt;384&lt;/em&gt;, &lt;em&gt;384&lt;/em&gt;, &lt;em&gt;256&lt;/em&gt; filters use &lt;em&gt;512&lt;/em&gt;, &lt;em&gt;1024&lt;/em&gt;, &lt;em&gt;512&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;ImageNet top 5 error: &lt;em&gt;16.4%&lt;/em&gt; -&amp;gt; &lt;em&gt;11.7%&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Zeiler and Fergus, 2013&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vggnet-deeper-networks&quot;&gt;VGGNet: Deeper Networks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Small filters, Deeper networks
    &lt;ul&gt;
      &lt;li&gt;8 layers (AlexNet) $\rightarrow$ 16 - 19 layers (VGG16Net)&lt;/li&gt;
      &lt;li&gt;Only $3\times 3$ CONV stride &lt;em&gt;1&lt;/em&gt;, pad &lt;em&gt;1&lt;/em&gt; and $2\times 2$ MAX POOL with stride &lt;em&gt;2&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;11.7%&lt;/em&gt; top 5 error(ZFNet) $\rightarrow$ &lt;em&gt;7.3%&lt;/em&gt; top 5 error in ILSVRC’14&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Why use smaller filters?&lt;br /&gt;
  :Stack of three $3\times 3$ conv (stride &lt;em&gt;1&lt;/em&gt;) layers has same effective receptive field as one $7\times 7$ conv layer, but with deeper, more non-linearities and fewer parameters&lt;br /&gt;
  &lt;img src=&quot;/assets/images/cs231n_lec9_1.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TOTAL memory: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;24M * 4 bytes ~= 96MB&lt;/code&gt; / image (for a forward pass)&lt;br /&gt;
  TOTAL params: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;138M&lt;/code&gt; parameters&lt;br /&gt;
  Most memory is in early CONV, Most params are in late FC&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Details:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Simonyan and Zisserman, 2014&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;ILSVRC’14 2nd in classification, 1st in localization&lt;/li&gt;
      &lt;li&gt;Similar training procedure as &lt;em&gt;Krizhevsky 2012&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;No Local Response Normalisation (LRN)&lt;/li&gt;
      &lt;li&gt;Use VGG16 or VGG19 (VGG19 only slightly better, more memory)&lt;/li&gt;
      &lt;li&gt;Use ensembles for best results&lt;/li&gt;
      &lt;li&gt;FC7 features generalize well to other	tasks&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;googlenet&quot;&gt;GoogLeNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Inception module&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;design a good local network topology(network within a network) and then stack these modules on top of each other&lt;/li&gt;
      &lt;li&gt;Apply parallel filter operations on the input from previous layer: Multiple receptive field sizes for convolution(1x1, 3x3, 5x5), Pooling(3x3)&lt;/li&gt;
      &lt;li&gt;Concatenate all filter outputs together channel-wise&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“Bottlenect”&lt;/em&gt; layers to reduce computational complexity of inception:
    &lt;ul&gt;
      &lt;li&gt;use 1x1 conv to reduce feature channel size; alternatively, interpret it as applying the same FC layer on each input pixel&lt;/li&gt;
      &lt;li&gt;preserves spatial dimensions, reduces depth&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec9_2.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Full GoogLeNet Architecture:
    &lt;ul&gt;
      &lt;li&gt;Stem Network: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[Conv-POOL-2x CONV-POOL]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Stack Inception modules: with dimension reduction on top of each other&lt;/li&gt;
      &lt;li&gt;Classifier output: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[(H*W*c)-Avg POOL-(1*1*c)-FC-Softmax]&lt;/code&gt;&lt;br /&gt;
  Global average pooling layer before final FC layer, avoids expensive FC layers&lt;/li&gt;
      &lt;li&gt;Auxiliary classification layers: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[AvgPool-1x1 Conv-FC-FC-Softmax]&lt;/code&gt;&lt;br /&gt;
  to inject additional gradient at lower layers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Details:
    &lt;ul&gt;
      &lt;li&gt;Deeper networks, with computational efficiency&lt;/li&gt;
      &lt;li&gt;ILSVRC’14 classification winner (&lt;em&gt;6.7%&lt;/em&gt; top 5 error)&lt;/li&gt;
      &lt;li&gt;22 layers&lt;/li&gt;
      &lt;li&gt;Only 5 million parameters(12x less than AlexNet, 27x less than VGG-16)&lt;/li&gt;
      &lt;li&gt;Efficient “Inception” module&lt;/li&gt;
      &lt;li&gt;No FC layers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;resnet&quot;&gt;ResNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;From 2015, “Revolution of Depth”; more than 100 layers&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Stacking deeper layers on a “plain” convolutional neural network results in lower both test and training error. The deeper model performs worse, but it’s &lt;strong&gt;not caused by overfitting&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;Fact: Deep models have more representation power (more parameters) than shallower models.&lt;/li&gt;
      &lt;li&gt;Hypothesis: the problem is an optimization problem, &lt;strong&gt;deeper models are harder to optimize&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Solution: copying the learned layers from the shallower model and setting additional layers to identity mapping.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“Residual block”&lt;/em&gt;:
    &lt;ul&gt;
      &lt;li&gt;Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec9_3.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Full ResNet Architecture:
    &lt;ul&gt;
      &lt;li&gt;Stack residual blocks&lt;/li&gt;
      &lt;li&gt;Every residual block has two $3\times 3$ conv layers&lt;/li&gt;
      &lt;li&gt;Periodically, double number of filters and downsample spatially using stride &lt;em&gt;2&lt;/em&gt; (/2 in each dimension). Reduce the activation volume by half.&lt;/li&gt;
      &lt;li&gt;Additional conv layer at the beginning (7x7 conv in stem)&lt;/li&gt;
      &lt;li&gt;No FC layers at the end (only FC 1000 to output classes)&lt;/li&gt;
      &lt;li&gt;(In theory, you can train a ResNet with input image of variable sizes)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For deeper networks(ResNet-50+):
  use bottleneck layer to improve efficiency (similar to GoogLeNet)&lt;br /&gt;
  e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[(28x28x256 INPUT)-(1x1 CONV, 64)-(3x3 CONV, 64)-(1x1 CONV, 256)-(28x28x256 OUTPUT)]&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Training ResNet in practice:
    &lt;ul&gt;
      &lt;li&gt;Batch Normalization after every CONV layer&lt;/li&gt;
      &lt;li&gt;Xavier initialization from &lt;em&gt;He et al.&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;SGD + Momentum (&lt;em&gt;0.9&lt;/em&gt;)&lt;/li&gt;
      &lt;li&gt;Learning rate: &lt;em&gt;0.1&lt;/em&gt;, divided by &lt;em&gt;10&lt;/em&gt; when validation error plateaus&lt;/li&gt;
      &lt;li&gt;Mini-batch size &lt;em&gt;256&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Weight decay of &lt;em&gt;1e-5&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;No dropout used&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Experimental Results:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;He et al., 2015&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Able to train very deep networks without degrading (152 layers on ImageNet, 1202 on Cifar)&lt;/li&gt;
      &lt;li&gt;Deeper networks now achieve lower training error as expected&lt;/li&gt;
      &lt;li&gt;Swept 1st place in all ILSVRC and COCO 2015 competitions&lt;/li&gt;
      &lt;li&gt;ILSVRC 2015 classification winner (&lt;em&gt;3.6%&lt;/em&gt; top 5 error); better than human performance!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Details:
    &lt;ul&gt;
      &lt;li&gt;Very deep networks using residual connections&lt;/li&gt;
      &lt;li&gt;152-layer model for ImageNet&lt;/li&gt;
      &lt;li&gt;ILSVRC’15 classification winner(&lt;em&gt;3.57%&lt;/em&gt; top 5 error)&lt;/li&gt;
      &lt;li&gt;Swept all classification and detection competitions in ILSVRC’15 and COCO’15&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        
        <pubDate>Sun, 02 Jan 2022 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/cs231n_lec9</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/cs231n_lec9</guid>
      </item>
      
    
      
      <item>
        <title>cs231n - Lecture 8. Training Neural Networks II</title>
        
          <description>&lt;h2 id=&quot;optimization&quot;&gt;Optimization&lt;/h2&gt;

&lt;h3 id=&quot;problems-with-sgd&quot;&gt;Problems with SGD&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;What if loss changes quickly in one direction and slowly in another? What does gradient descent do?
 Very slow progress along shallow dimension, jitter along steep direction&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What if the loss function has a local minima or saddle point?&lt;br /&gt;
 Zero gradient, gradient descent gets stuck(more common in high dimension)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradients come from minibatches can be noisy&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;sgd--momentum&quot;&gt;SGD + Momentum&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;To avoid local minima, combine gradient at current point with &lt;em&gt;velocity&lt;/em&gt; to get step used to update weights; continue moving in the general direction as the previous iterations&lt;br /&gt;
  \(v_{t+1}=\rho v_t + \nabla f(x_t)\)&lt;br /&gt;
  \(x_{t+1}=x_t - \alpha v_{t+1}\)&lt;br /&gt;
  with &lt;em&gt;rho&lt;/em&gt; giving “friction”; typically &lt;em&gt;0.9&lt;/em&gt; or &lt;em&gt;0.99&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rho&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;nesterov-momentum&quot;&gt;Nesterov Momentum&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;“Look ahead” to the point where updating using velocity would take us; compute gradient there and mix it with velocity to get actual update direction&lt;br /&gt;
  \(v_{t+1}=\rho v_t - \alpha\nabla f(x_t + \rho v_t)\)&lt;br /&gt;
  \(x_{t+1}=x_t + v_{t+1}\)&lt;br /&gt;
  rearrange with \(\tilde{x}_t = x_t + \rho v_t\),&lt;br /&gt;
  \(v_{t+1}=\rho v_t - \alpha\nabla f(\tilde{x}_t)\)&lt;br /&gt;
  \(\begin{align*}
  \tilde{x}_{t+1} &amp;amp;= \tilde{x}_t - \rho v_t + (1+\rho)v_{t+1}
                  &amp;amp;= \tilde{x}_t + v_{t+1} + \rho(v_{t+1}-v_t)
  \end{align*}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adagrad&quot;&gt;AdaGrad&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Added element-wise scaling of the gradient based on the historical sum of squares in each dimension&lt;br /&gt;
  “Per-parameter learning rates” or “adaptive learning rates”&lt;br /&gt;
  Progress along “steep” directions is damped and “flat” directions is accelerated&lt;br /&gt;
  Step size decays to zero over time&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rmsprop-leaky-adagrad&quot;&gt;RMSProp: “Leaky AdaGrad”&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decay_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decay_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_squared&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;adam&quot;&gt;Adam&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# Momentum
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;first_unbias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;			&lt;span class=&quot;c1&quot;&gt;# Bias correction
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;second_unbias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second_moment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first_unbias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second_unbias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# AdaGrad/ RMSProp
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Sort of like RMSProp with momentum
  Bias correction for the fact that first and second moment estimates start at zero&lt;br /&gt;
  Adam with &lt;em&gt;beta1 = 0.9, beta2 = 0.999, and learning_rate = 1e-3 or 5e-4&lt;/em&gt; is a great starting point&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learning-rate-schedules&quot;&gt;Learning rate schedules&lt;/h2&gt;

&lt;h3 id=&quot;learning-rate-decays-over-time&quot;&gt;Learning rate decays over time&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Reduce learning rate by a certain value at a few fixed points(after some epochs)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;learning-rate-decay&quot;&gt;Learning Rate Decay&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Reduce learning rate gradually, e.g.&lt;br /&gt;
  Cosine:  $\alpha_t = \frac{1}{2}\alpha_0(1+\mbox{cos}(t\pi / T))$&lt;br /&gt;
  Linear: $\alpha_t = \alpha_0(1-t/T)$&lt;br /&gt;
  Inverse sqrt: $\alpha_t = \alpha_0 / \sqrt{t}$&lt;br /&gt;
  while $\alpha_0$ is the initial learning rate, $\alpha_t$ is one at epoch &lt;em&gt;t&lt;/em&gt;, and &lt;em&gt;T&lt;/em&gt; is the total number of epochs&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Linear Warmup&lt;br /&gt;
  High initial learning rates can make loss explode; linearly increasing learning rate from &lt;em&gt;0&lt;/em&gt; over the first &lt;em&gt;~5000&lt;/em&gt; iterations can prevent this&lt;br /&gt;
  Empirical rule of thumb: If you increase the batch size by &lt;em&gt;N&lt;/em&gt;, also scale the initial learning rate by &lt;em&gt;N&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;first-order-optimization&quot;&gt;First-Order Optimization&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Use gradient from linear approximation&lt;/li&gt;
  &lt;li&gt;Step to minimize the approximation&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;second-order-optimization&quot;&gt;Second-Order Optimization&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Use gradient and &lt;strong&gt;Hessian&lt;/strong&gt; to form &lt;strong&gt;quadratic&lt;/strong&gt; approximation&lt;/li&gt;
  &lt;li&gt;Step to the &lt;strong&gt;minima&lt;/strong&gt; of the (quadratic) approximation&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;But Hessian has &lt;em&gt;O(N^2)&lt;/em&gt; elements and inverting takes &lt;em&gt;O(N^3)&lt;/em&gt;, &lt;em&gt;N&lt;/em&gt; is extremely large
    &lt;ul&gt;
      &lt;li&gt;Quasi-Newton methods (BGFS most popular):&lt;br /&gt;
  instead of inverting the Hessian, approximate inverse Hessian with rank 1 updates over time&lt;/li&gt;
      &lt;li&gt;L-BFGS (Limited memory BFGS):&lt;br /&gt;
  Does not form/store the full inverse Hessian. Usually works very well in full batch, deterministic mode, but does not transfer very well to mini-batch setting. Large-scale, stochastic setting is an active area of research.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Adam is a good default choice in many cases; even with constant learning rate&lt;/li&gt;
  &lt;li&gt;SGD+Momentum can outperform Adam but may equire more tuning of LR and schedule. Cosine schedule preferred, since it has very few hyperparameters.&lt;/li&gt;
  &lt;li&gt;L-BFGS is good if you can afford to do full batch updates.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;improve-test-error&quot;&gt;Improve test error&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Better optimization algorithms help reduce &lt;strong&gt;training&lt;/strong&gt; loss, but what we really care is about error on new data - how to reduce the gap?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;early-stopping-always-do-this&quot;&gt;Early Stopping: Always do this&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Stop training the model when accuracy on the validation set decreases. Or train for a long time, but always keep track of the model snapshot that worked best on val.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-ensembles&quot;&gt;Model Ensembles&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Train multiple independent models&lt;/li&gt;
  &lt;li&gt;At test time average their results&lt;br /&gt;
 (Take average of predicted probability distributions, then choose argmax)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;regularization&quot;&gt;Regularization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;To improve single-model performance, add terms to loss&lt;br /&gt;
  e.g. L1, L2, Elastic net.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;or, use Dropout:&lt;br /&gt;
  In each forward pass, randomly set some neurons to zero. Probability of dropping is a hyperparameter; 0.5 is common.&lt;br /&gt;
  It forces the network to have a redundant representation; Prevents co-adaptation of features. Dropout can be interpreted as training a large ensemble of models (that share parameters).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# dropout rate
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# drop in train time
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt;

	&lt;span class=&quot;c1&quot;&gt;# backward pass: compute gradients...
&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# perform parameter update...
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;			&lt;span class=&quot;c1&quot;&gt;# scale at test time
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;more common: “Inverted dropout”&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;U1 = (np.random.rand(*H1.shape) &amp;lt; p) / p&lt;/code&gt; in train time and no scaling in test time&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A common pattern of regularization&lt;br /&gt;
  Training: Add some kind of randomness&lt;br /&gt;
  $y = fw(x,z)$&lt;br /&gt;
  Testing: Average out randomness (sometimes approximate)&lt;br /&gt;
  \(y = f(x) = E_z[f(x,z)] = \int p(z)f(x,z)\, dz\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data Augmentation:&lt;br /&gt;
  Addes &lt;em&gt;transformed&lt;/em&gt; data to train model&lt;br /&gt;
  e.g. translation, rotation, stretching, shearing, lens distortions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DropConnect:&lt;br /&gt;
  Training: Drop connections between neurons (set weights to 0)&lt;br /&gt;
  Testing: Use all the connections&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fractional Pooling:&lt;br /&gt;
  Training: Use randomized pooling regions&lt;br /&gt;
  Testing: Average predictions from several regions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stochastic Depth:&lt;br /&gt;
  Training: Skip some layers in the network&lt;br /&gt;
  Testing: Use all the layer&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cutout:&lt;br /&gt;
  Training: Set random image regions to zero&lt;br /&gt;
  Testing: Use full image&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mixup:&lt;br /&gt;
  Training: Train on random blends of images&lt;br /&gt;
  Testing: Use original images&lt;br /&gt;
  e.g. Randomly blend the pixels of pairs of training images, say 40% cat and 60% dog, and set the target label as cat:0.4 and dog:0.6.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Summary
  Consider dropout for large fully-connected layers&lt;br /&gt;
  Batch normalization and data augmentation almost always a good idea&lt;br /&gt;
  Try cutout and mixup especially for small classification datasets&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;choosing-hyperparameters&quot;&gt;Choosing Hyperparameters&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Step 1: Check initial loss&lt;br /&gt;
  Turn off weight decay, sanity check loss at initialization&lt;br /&gt;
  e.g. &lt;em&gt;log(C)&lt;/em&gt; for softmax with &lt;em&gt;C&lt;/em&gt; classes&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 2: Overfit a small sample&lt;br /&gt;
  Try to train to 100% training accuracy on a small sample of training data (~5-10 minibatches); fiddle with architecture, learning rate, weight initialization&lt;br /&gt;
  If loss is not going down, LR too low or bad initialization. If loss explodes, then LR is too high or bad initialization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 3: Find LR that makes loss go down&lt;br /&gt;
  Use the architecture from the previous step, use all training data, turn on small weight decay, find a learning rate that makes the loss drop significantly within ~100 iterations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 4: Coarse grid, train for ~1-5 epochs&lt;br /&gt;
  Choose a few values of learning rate and weight decay around what worked from Step 3, train a few models for ~1-5 epochs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 5: Refine grid, train longer&lt;br /&gt;
  Pick best models from Step 4, train them for longer (~10-20 epochs) without learning rate decay&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 6: Look at loss and accuracy curves&lt;br /&gt;
  If accuracy still going up, you need to train longer. If it goes down, huge train / val gap means overfitting. You need to increase regularization or get more data. If there’s no gap between train / val, it means underfitting. Train longer or use a bigger model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Look at learning curves&lt;br /&gt;
  Losses may be noisy, use a scatter plot and also plot moving average to see trends better. Cross-validation is useful too.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 7: &lt;strong&gt;GO TO Step 5&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperparameters to play with:&lt;br /&gt;
  network architecture,&lt;br /&gt;
  learning rate, its decay schedule, update type,&lt;br /&gt;
  regularization (L2/Dropout strength)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;for Hyper-Parameter Optimization, consider both Random Search and Grid Search&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-1&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Improve your training error:
    &lt;ul&gt;
      &lt;li&gt;Optimizers&lt;/li&gt;
      &lt;li&gt;Learning rate schedules&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Improve your test error:
    &lt;ul&gt;
      &lt;li&gt;Regularization&lt;/li&gt;
      &lt;li&gt;Choosing Hyperparameters&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        
        <pubDate>Mon, 27 Dec 2021 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/cs231n_lec8</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/cs231n_lec8</guid>
      </item>
      
    
      
      <item>
        <title>cs231n - Lecture 7. Training Neural Networks I</title>
        
          <description>&lt;h2 id=&quot;activation-functions&quot;&gt;Activation Functions&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec7_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;sigmoid&quot;&gt;Sigmoid&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;$\sigma(x)=1/(1+e^{-x})$
    &lt;ul&gt;
      &lt;li&gt;Squashes numbers to range [0,1]&lt;/li&gt;
      &lt;li&gt;Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Problem:
    &lt;ul&gt;
      &lt;li&gt;Gradient Vanishing: Saturated neurons “kill” the gradients; If all the gradients flowing back will be zero and weights will never change.&lt;/li&gt;
      &lt;li&gt;Sigmoid outputs are not zero-centered and always positive, so the gradients will be always all positive or all negative. Then the gradient update would follow a zig-zag path, resulting in bad efficiency.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;exp()&lt;/em&gt; is a bit compute expensive.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tanhx&quot;&gt;tanh(x)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Squashes numbers to range [-1,1]&lt;br /&gt;
  zero centered&lt;br /&gt;
  &lt;strong&gt;but still kills gradients when saturated&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;relurectified-linear-unit&quot;&gt;ReLU(Rectified Linear Unit)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(x) = \mbox{max}(0,x)\)&lt;br /&gt;
  Does not saturate (in &lt;em&gt;+&lt;/em&gt; region)&lt;br /&gt;
  Very computationally efficient&lt;br /&gt;
  Converges much faster than sigmoid/tanh&lt;br /&gt;
  &lt;strong&gt;but has not zero-centered output and weights will never be updated for negative &lt;em&gt;x&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;leaky-relu&quot;&gt;Leaky ReLU&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(x) = \mbox{max}(0.01x,x)\)&lt;br /&gt;
  (or &lt;em&gt;parametric&lt;/em&gt;, PReLU: \(f(x) = \mbox{max}(\alpha x, x)\))&lt;br /&gt;
  Not saturate&lt;br /&gt;
  Computationally efficient&lt;br /&gt;
  Converges much faster&lt;br /&gt;
  will not “die”&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;eluexponential-linear-units&quot;&gt;ELU(Exponential Linear Units)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(n)= \begin{cases} x &amp;amp; \mbox{if }x&amp;gt;0 \\
                      \alpha(\mbox{exp}(x)-1) &amp;amp; \mbox{if }x\le 0\end{cases}\)&lt;br /&gt;
  ($\scriptstyle{\alpha = 1}$)&lt;br /&gt;
  All benefits of ReLU&lt;br /&gt;
  Closer to zero mean outputs&lt;br /&gt;
  Negative saturation regime compared with Leaky ReLU adds some robustness to noise&lt;br /&gt;
  &lt;strong&gt;Computation requires exp()&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;selu-scaled-exponential-linear-units&quot;&gt;SELU (Scaled Exponential Linear Units)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(n)= \begin{cases} \lambda x &amp;amp; \mbox{if }x&amp;gt;0 \\
                      \lambda\alpha(e^x -1) &amp;amp; \mbox{otherwise}\end{cases}\)&lt;br /&gt;
  ($\scriptstyle{\alpha=1.6733, \lambda=1.0507}$)&lt;br /&gt;
  Scaled versionof ELU that works better for deep networks&lt;br /&gt;
  “Self-normalizing” property;&lt;br /&gt;
  Can train deep SELU networks without BatchNorm&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;maxout-neuron&quot;&gt;Maxout “Neuron”&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(\mbox{max}(w_1^T x + b_1, w_2^T x + b_2)\)&lt;br /&gt;
  Nonlinearity; does not have the basic form of dot product&lt;br /&gt;
  Generalizes ReLU and Leaky ReLU&lt;br /&gt;
  Linear Regime; does not saturate or die&lt;br /&gt;
  &lt;strong&gt;Complexity; Doubles the number of parameters/neuron&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;swish&quot;&gt;Swish&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\(f(x)=x\sigma(\beta x)\)&lt;br /&gt;
  train a neural network to generate and test out different non-linearities&lt;br /&gt;
  outperformed all other options for &lt;em&gt;CIFAR-10&lt;/em&gt; accuracy&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Use &lt;em&gt;ReLU&lt;/em&gt; and be careful with learning rates&lt;br /&gt;
  Try out &lt;em&gt;Leaky ReLU / Maxout / ELU / SELU&lt;/em&gt; to squeeze out some marginal gains&lt;br /&gt;
  Don’t use sigmoid or tanh&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-preprocessing&quot;&gt;Data Preprocessing&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;We may have zero-centered, normalized, decorrelated(PCA) or whitened data&lt;/li&gt;
  &lt;li&gt;After normalization, it will be less sensitive to small changes in weights and easier to optimize&lt;/li&gt;
  &lt;li&gt;In practice for images, centering only used.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;weight-initialization&quot;&gt;Weight Initialization&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;First idea: Small random numbers&lt;br /&gt;
  (gaussian with zero mean and 1e-2 standard deviation)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;It works okay for small networks, but problems with deeper networks&lt;br /&gt;
  All activations and gradients tend to zero and no learning proceeded.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;xavier-initialization&quot;&gt;“Xavier” Initialization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;std = 1/sqrt(D_in)&lt;/em&gt;&lt;br /&gt;
  For conv layers, $\mbox{D_in}$ is $\mbox{filter_size}^2\times \mbox{input_channels}$&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Activations are nicely scaled for deeper layers&lt;br /&gt;
  works well especially in non-linear activation functions like sigmoid, tanh&lt;br /&gt;
  &lt;strong&gt;but cannot used in ReLU activation function; activations collapse to zero and no learning&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;kaiming--msra-initialization&quot;&gt;Kaiming / MSRA Initialization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ReLU correction: &lt;em&gt;std = sqrt(2/D_in)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;To make each dimension zero-mean unit-variance, apply:&lt;br /&gt;
  \(\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{\mbox{Var}[x^{(k)}]}}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Usually inserted after Fully Connected or Convolutional layers, and before nonlinearity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Makes deep networks much easier to train&lt;br /&gt;
  Improves gradient flow&lt;br /&gt;
  Allows higher learning rates, faster convergence&lt;br /&gt;
  Networks become more robust to initialization&lt;br /&gt;
  Acts as regularization during training&lt;br /&gt;
  Zero overhead at test-time: can be fused with conv&lt;br /&gt;
  &lt;strong&gt;Behaves differently during training and testing: can have bugs&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comparison of Normalization Layers&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec7_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;transfer-learning&quot;&gt;Transfer Learning&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Deep learning models are trained to capture characteristics of data, from general features at the first layer to specific features at the last layer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In transfer learning, we import pre-trained model and fine-tune to our cases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Strategies&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec7_2.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;50%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;E.g.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec7_3.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Transfer learning with CNNs is pervasive,&lt;br /&gt;
  for Object Detection(Fast R-CNN), Image Captioning(CNN + RNN), etc.&lt;br /&gt;
  but not always be necessary&lt;/li&gt;
&lt;/ul&gt;

</description>
        
        <pubDate>Sun, 26 Dec 2021 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/cs231n_lec7</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/cs231n_lec7</guid>
      </item>
      
    
      
      <item>
        <title>cs231n - Lecture 6. Hardware and Software</title>
        
          <description>&lt;h3 id=&quot;deeplearning-software&quot;&gt;Deeplearning Software&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The point of deep learning frameworks&lt;br /&gt;
  (1) Quick to develop and test new ideas&lt;br /&gt;
  (2) Automatically compute gradients&lt;br /&gt;
  (3) Run it all efficiently on GPU (wrap cuDNN, cuBLAS, OpenCL, etc)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;computational-graph-example&quot;&gt;Computational graph example&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;grad_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grad_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grad_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grad_z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grad_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grad_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;in Numpy&lt;br /&gt;
  Good: Clean API, easy to write numeric code&lt;br /&gt;
  Bad: Have to compute our own gradients and can’t run on GPU&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'cuda:0'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;in PyTorch&lt;br /&gt;
  PyTorch handles gradients for us&lt;br /&gt;
  Can run on GPU&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-fundamental-concepts&quot;&gt;PyTorch: Fundamental Concepts&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.Tensor&lt;/code&gt;: Like a numpy array, but can run on GPU&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.autograd&lt;/code&gt;: Package for building computational graphs out of Tensors, and automatically computing gradients&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.Module&lt;/code&gt;: A neural network layer; may store state or learnable weights&lt;/li&gt;
  &lt;li&gt;we are using PyTorch version &lt;em&gt;1.7&lt;/em&gt; here&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-autograd&quot;&gt;PyTorch: Autograd&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Running example: Train a two-layer ReLU network on random data with L2 loss
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# enables autograd
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;							&lt;span class=&quot;c1&quot;&gt;# Forward pass
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;					&lt;span class=&quot;c1&quot;&gt;# no need to track intermediate values
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;					&lt;span class=&quot;c1&quot;&gt;# = x.mm(w1).clamp(min=0).mm(w2)
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;loss_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;							&lt;span class=&quot;c1&quot;&gt;# Compute gradient of loss
&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;					&lt;span class=&quot;c1&quot;&gt;# Gradient descent
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;			
		&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;or-you-can-define-your-own&quot;&gt;Or you can define your own&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;staticmethod&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;#Use ctx object to “cache” values for the backward pass
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_for_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;staticmethod&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;saved_tensors&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;grad_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_input&lt;/span&gt;
		
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;my_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# a helper function to make it easy to use the new function
&lt;/span&gt;	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Now we can replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_pred = x.mm(w1).clamp(min=0).mm(w2)&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_pred = my_relu(x.mm(w1)).mm(w2)&lt;/code&gt;. In practice, do it only when you need custom backward.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-nn&quot;&gt;PyTorch: nn&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Higher-level wrapper for working with neural nets
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-2&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	
	&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;pytorch-optim&quot;&gt;PyTorch: optim&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# different update rules
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;pytorch-define-new-modules&quot;&gt;PyTorch: Define new Modules&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TwoLayerNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# init sets up two children
&lt;/span&gt;		&lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TwoLayerNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TwoLayerNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;pytorch-pretrained-models&quot;&gt;PyTorch: Pretrained Models&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;alexnet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alexnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vgg16&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vgg16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;resnet101&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resnet101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;tensorflow-24&quot;&gt;TensorFlow 2.4&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Default dynamic graph, optionally static.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# weights
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# weights
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientTape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;					&lt;span class=&quot;c1&quot;&gt;# build dynamic graph
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;			&lt;span class=&quot;c1&quot;&gt;# forward pass
&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# backward pass
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# gradient descent
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;keras-high-level-wrapper&quot;&gt;Keras: High-level Wrapper&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientTape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MeanSquaredError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainable_variables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainable_variables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We can make use of different update rules with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.optimizers.{}&lt;/code&gt; and predefined loss functions as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Keras can handle the training loop;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MeanSquaredError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
			 &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;tensorflow-compile-static-graph&quot;&gt;TensorFlow: compile static graph&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;model_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MeanSquaredError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@tf.function&lt;/code&gt; decorator (implicitly) compiles python functions to static graph for better performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dynamic-vs-static&quot;&gt;Dynamic vs. Static&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Dynamic Computation Graphs: 
  Building the graph and computing the graph happen at the same time.&lt;br /&gt;
  Graph building and execution are intertwined, so always need to keep code around&lt;br /&gt;
  Inefficient, especially if we are building the same graph over and over again.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Static Computation Graphs:&lt;br /&gt;
  Build computational graph describing our computation(including finding paths for backprop)&lt;br /&gt;
  Reuse the same graph on every iteration&lt;br /&gt;
  Once graph is built, can serialize it and run it without the code that built the graph&lt;br /&gt;
  Framework can optimize the graph before it runs&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-vs-tensorflow&quot;&gt;PyTorch vs. TensorFlow&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;PyTorch&lt;br /&gt;
  Dynamic Graphs as default set&lt;br /&gt;
  Static: ONNX, TorchScript&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TensorFlow&lt;br /&gt;
  Dynamic: Eager set&lt;br /&gt;
  Static: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@tf.function&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-parallel-vs-data-parallel&quot;&gt;Model Parallel vs. Data Parallel&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Model parallelism:&lt;br /&gt;
  split computation graph into parts and distribute to GPUs/nodes&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data parallelism:&lt;br /&gt;
  split minibatch into chunks and distribute to GPUs/ nodes&lt;br /&gt;
  PyTorch: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.DataParallel&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.DistributedDataParallel&lt;/code&gt;&lt;br /&gt;
  TensorFlow: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.distributed.Strategy&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        
        <pubDate>Sat, 18 Dec 2021 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/cs231n_lec6</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/cs231n_lec6</guid>
      </item>
      
    
      
      <item>
        <title>cs231n - Lecture 5. Convolutional Neural Networks</title>
        
          <description>&lt;h3 id=&quot;convolutional-neural-networks&quot;&gt;Convolutional Neural Networks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ConvNets are everywhere&lt;br /&gt;
  Classification, Retrieval, Detection, Segmentation, Image Captioning, etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Recap: Fully Connected Layer&lt;br /&gt;
  $32\times 32\times 3$ image $\rightarrow$ stretch to $3072\times 1$&lt;br /&gt;
  Then a dot product of $3072\times 1$ input &lt;em&gt;x&lt;/em&gt; and scoring weights &lt;em&gt;W&lt;/em&gt;, &lt;em&gt;Wx&lt;/em&gt; is in $10 \times 3072$. With some activation function, we can make classification scores in &lt;em&gt;10&lt;/em&gt; classes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;convolution-layer-preserve-spatial-structure&quot;&gt;Convolution Layer: preserve spatial structure&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Convolve the filter with the image, slide over the image spatially, computing dot products. Filters always extend the full depth of the input volume.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec5_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;br /&gt;
Convolve(slide) over all spatial locations, we can make an activation map of size $28\times 28\times 1$ for each convolution filter. For example, if we had &lt;em&gt;6&lt;/em&gt; $5\times 5$ filters, we’ll get &lt;em&gt;6&lt;/em&gt; separate activation maps. We stack these up to get a “new image” of size $28\times 28\times 6$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ConvNet is a sequence of Convolution Layers, interspersed with activation functions.&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec5_1.png&quot; alt=&quot;png&quot; width=&quot;70%&amp;quot;, height=&amp;quot;70%&quot; /&gt;&lt;br /&gt;
Input convolved repeatedly with filters shrinks volumes spatially. By each sequence, an image is processed from low-level features to high-level features. Shrinking too fast is not good, doesn’t work well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We call the layer convolutional because it is related to convolution of two signals: \(f[x,y]*g[x,y]=\sum_{n_1=-\infty}^\infty \sum_{n_2=-\infty}^\infty f[n_1,n_2]\cdot g[x-n_1,y-n_2]\); elementwise multiplication and sum of a filter and the signal (image)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Zero pad the border:&lt;br /&gt;
  The data on the border of an image will be convolved only once with each filter, while the others on the center of an image will be treated several times. Zero padding is introduced to solve this problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;General CONV layers:&lt;br /&gt;
  with $N\times N$ input, $F\times F$ filter, applied with stride &lt;em&gt;s&lt;/em&gt;, pad with &lt;em&gt;p&lt;/em&gt; pixel border, the output is &lt;strong&gt;$(N+2P-F)/s + 1$&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Example:&lt;br /&gt;
  Input volume &lt;strong&gt;$32\times 32\times 3$&lt;/strong&gt;&lt;br /&gt;
  &lt;strong&gt;&lt;em&gt;10&lt;/em&gt; $5\times 5$&lt;/strong&gt; filters with stride &lt;strong&gt;&lt;em&gt;1&lt;/em&gt;&lt;/strong&gt;, pad &lt;strong&gt;&lt;em&gt;2&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;$\rightarrow$ Output volume size: $(32+2*2-5)/1+1=32$ spatially, so $32\times 32\times 10$.&lt;/p&gt;

    &lt;p&gt;$\rightarrow$ Number of parameters in this layer: each filter has $5\times 5\times 3+1=76$ parameters(&lt;em&gt;+1&lt;/em&gt; for bias), thus for all &lt;em&gt;760&lt;/em&gt; params.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec5_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$1\times 1$ convolution layers used:&lt;br /&gt;
  To reduce the number of channels, so the number of parameters,&lt;br /&gt;
  Then we can perform a deeper layers(Bottleneck architecture).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pooling-layer&quot;&gt;Pooling layer&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Summarize the data in a partial space, into some representations&lt;br /&gt;
  Reducing output dimensions and the number of parameters&lt;br /&gt;
  Make it smaller and more manageable&lt;br /&gt;
  Operate over each activation map independently(downsampling)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;e.g. max pool with $2\times 2$ filters and stride &lt;em&gt;2&lt;/em&gt;, $4\times 4$ input reduced to $2\times 2$ output consisted of regional maximums.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fully-connected-layer-fc-layer&quot;&gt;Fully Connected Layer (FC layer)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Contains neurons that connect to the entire input volume, as in ordinary Neural
Networks. Stacked and followed by some activations, finally we make predictions or classifications.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ConvNets stack CONV,POOL,FC layers&lt;/li&gt;
  &lt;li&gt;Trend towards smaller filters and deeper architectures&lt;/li&gt;
  &lt;li&gt;Trend towards getting rid of POOL/FC layers (just CONV)&lt;/li&gt;
  &lt;li&gt;Historically architectures looked like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[(CONV-RELU)*N-POOL?]*M-(FC-RELU)*K,SOFTMAX&lt;/code&gt; where &lt;em&gt;N&lt;/em&gt; is usually up to &lt;em&gt;~5&lt;/em&gt;, &lt;em&gt;M&lt;/em&gt; is large, $0\le K \le 2$.&lt;/li&gt;
  &lt;li&gt;but recent advances such as ResNet/GoogLeNet have challenged this paradigm&lt;/li&gt;
&lt;/ul&gt;
</description>
        
        <pubDate>Sat, 18 Dec 2021 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/cs231n_lec5</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/cs231n_lec5</guid>
      </item>
      
    
      
      <item>
        <title>cs231n - Lecture 4. Neural Networks and Backpropagation</title>
        
          <description>&lt;h3 id=&quot;image-features&quot;&gt;Image Features&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Problem: Linear Classifiers are not very powerful
    &lt;ul&gt;
      &lt;li&gt;Visual Viewpoint: Linear classifiers learn one template per class&lt;/li&gt;
      &lt;li&gt;Geometric Viewpoint: Linear classifiers can only draw linear decision boundaries&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Image Features: Motivation&lt;br /&gt;
After applying feature transform, points can be separated by linear classifier&lt;br /&gt;
$f(x,y) = (r(x,y), \theta(x,y))$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Image Features vs. ConvNets&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec4_0.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;neural-networks&quot;&gt;Neural Networks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Neural networks, also called &lt;em&gt;Fully connected networks&lt;/em&gt;(FCN) or sometimes &lt;em&gt;multi-layer perceptrons&lt;/em&gt;(MLP)&lt;br /&gt;
(Before) Linear score function:&lt;br /&gt;
\(\begin{align*}&amp;amp; f=Wx \\
&amp;amp; x\in\mathbb{R}^D, W\in\mathbb{R}^{C\times D}
\end{align*}\)&lt;br /&gt;
$\rightarrow$ &lt;em&gt;2&lt;/em&gt;-layer Neural Network:&lt;br /&gt;
\(\begin{align*}&amp;amp; f=W_2 \mbox{max}(0,W_1 x) \\
&amp;amp; x\in\mathbb{R}^D, W_1\in\mathbb{R}^{H\times D}, 
W_2\in\mathbb{R}^{C\times H}
\end{align*}\)&lt;br /&gt;
$\rightarrow$ or &lt;em&gt;3&lt;/em&gt;-layer Neural Network:&lt;br /&gt;
\(f=W_3\mbox{max}(0,W_2 \mbox{max}(0,W_1 x)) \\ 
\vdots\)&lt;br /&gt;
(In practice we will usually add a learnable bias at each layer as well)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Neural networks: hierarchical computation&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec4_1.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;50%&quot; /&gt;&lt;br /&gt;
Learning 100s of templates instead of 10 and share templates between classes&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Why is &lt;em&gt;max&lt;/em&gt; operator important?&lt;br /&gt;
The function $\mbox{max}(0,z)$ is called the activation function.&lt;br /&gt;
Q: What if we try to build a neural network without one?&lt;br /&gt;
A: We end up with a linear classifier again!&lt;br /&gt;
$f=W_2 W_1 x, W_3=W_1 W_2, f = W_3 x$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Activation functions&lt;br /&gt;
  ReLU($\mbox{max}(0,z)$) is a good default choice for most problems&lt;br /&gt;
  Others: Sigmoid, tanh, Leaky ReLU, Maxout, ELU, etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Neural networks: Architectures&lt;br /&gt;
Example feed-forward computation of a neural network&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec4_2.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;50%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# forward-pass of a 3-layer neural network:
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# activation function (use sigmoid)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# random input vector of three numbers (3x1)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# calculate first hidden layer activations (4x1)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#calculate second hidden layer activations (4x1)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#output neuron (1x1)
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Full implementation of training a &lt;em&gt;2&lt;/em&gt;-layer Neural Network:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy.random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# Define the network
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;					&lt;span class=&quot;c1&quot;&gt;# Forward pass
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;	&lt;span class=&quot;c1&quot;&gt;# Calculate the analytical gradients
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;grad_w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grad_h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grad_w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_w1&lt;/span&gt;				&lt;span class=&quot;c1&quot;&gt;# Gradient descent
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_w2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Plugging in neural networks with loss functions&lt;br /&gt;
$s = f(x;W_1,W_2) = W_2\mbox{max}(0,W_1 x)$ Nonlinear score function&lt;br /&gt;
$L_i = \sum_{j\ne y_i}\mbox{max}(0,s_j-s_{y_i}+1)$ SVM Loss on predictions&lt;br /&gt;
$R(W)=\sum_k W_k^2$ Regularization&lt;br /&gt;
$L=\frac{1}{N}\sum_{i=1}^N L_i + \lambda R(W_1) + \lambda R(W_2)$ Total loss: data loss + regularization&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Problem: How to compute gradients?&lt;br /&gt;
If we can compute partial derivaties, then we can learn $W_1$ and $W_2$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;backpropagation&quot;&gt;Backpropagation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Chain rule:&lt;br /&gt;
\(\begin{align*}
\frac{\partial f}{\partial y}=\frac{\partial f}{\partial q} \frac{\partial q}{\partial y} \mbox{Upstream gradient} \times \mbox{Local gradient}
\end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Patterns in gradient flow&lt;br /&gt;
&lt;img src=&quot;/assets/images/cs231n_lec4_3.png&quot; alt=&quot;png&quot; width=&quot;60%&amp;quot;, height=&amp;quot;60%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        
        <pubDate>Wed, 15 Dec 2021 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/cs231n_lec4</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/cs231n_lec4</guid>
      </item>
      
    
      
      <item>
        <title>cs231n - Lecture 3. Loss Functions and Optimization</title>
        
          <description>&lt;h3 id=&quot;linear-classifier-cont&quot;&gt;Linear Classifier (cont.)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Todo:
    &lt;ol&gt;
      &lt;li&gt;Define a loss function: how good the classifier is&lt;/li&gt;
      &lt;li&gt;Optimization: efficient way of finding the parameters that minimize the loss function&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loss function&lt;br /&gt;
given a dataset of examples \(\left\{ (x_i,y_i) \right\}_{n=1}^N\)&lt;br /&gt;
where $x_i$ is image and $y_i$ is (integer) label&lt;br /&gt;
Average of loss over examples: \(L=\frac{1}{N}\sum_i L_i(f(x_i,W),y_i)\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Multiclass SVM loss:&lt;br /&gt;
using the shorthand for the scores vector: $s = f(x_i,W)$&lt;br /&gt;
\(L_i=\sum_{j\ne y_i}\mbox{max}(0,s_j-s_{y_i}+1)\)&lt;br /&gt;
where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+1&lt;/code&gt; is a safety margin&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec3_0.png&quot; alt=&quot;png&quot; width=&quot;50%&amp;quot;, height=&amp;quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Q1: What happens to loss if car scores decrease by &lt;em&gt;0.5&lt;/em&gt; for this training example?&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L = max(0,0.8-4.4+1) + max(0,1.5-4.4+1) = 0&lt;/code&gt;&lt;br /&gt;
Result will not be changed; SVM hinge loss is robust to small change of scores.&lt;/p&gt;

&lt;p&gt;Q2: what is the min/max possible SVM loss $L_i$?&lt;br /&gt;
The possible minimum value of SVM loss is &lt;em&gt;0&lt;/em&gt; and maximum value is $\infty$.&lt;/p&gt;

&lt;p&gt;Q3: At initialization &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W&lt;/code&gt; is small so all &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s ≈ 0&lt;/code&gt;. What is the loss $L_i$, assuming &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt; examples and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; classes?&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C-1&lt;/code&gt; the number of classes minus &lt;em&gt;1&lt;/em&gt;.&lt;br /&gt;
(&lt;em&gt;Sanity check for weight initialization&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;Q4: What if the sum was over all classes? (including $j = y_i$)&lt;br /&gt;
All losses increased by &lt;em&gt;1&lt;/em&gt;, so as the average loss over full dataset. In this case, the minimum value of loss will be &lt;em&gt;1&lt;/em&gt;. What we want is to minimize the loss function and that’s why we remove the class $s_j=s_{y_i}$ to set it &lt;em&gt;0&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Q5: What if we used mean instead of sum?&lt;br /&gt;
Because a loss function is to find optimizing parameters, rescaling loss has no effect to the result.&lt;/p&gt;

&lt;p&gt;Q6: What if we used squared function of
\(L_i=\sum_{j\ne y_i}\mbox{max}(0,s_j-s_{y_i}+1)^2\)&lt;br /&gt;
A squared hinge loss can be better at finding optimizing parameters &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W&lt;/code&gt;. If the observation is close to the answer, it will reflect the loss much smaller than original hinge loss.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;L_i_vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
	&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
	&lt;span class=&quot;n&quot;&gt;margins&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;margins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;loss_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;margins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_i&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;$f(x,W) = Wx$&lt;br /&gt;
\(L=\frac{1}{N}\sum_{i=1}^N \mbox{max}(0,f(x_i;W)_j -f(x_i;W)y_i +1)\)&lt;/p&gt;

&lt;p&gt;Q7: Suppose that we found a W such that $L = 0$. Is this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W&lt;/code&gt; unique?&lt;br /&gt;
No. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2W&lt;/code&gt; is also has $L=0$. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W&lt;/code&gt; not a unique solution.&lt;br /&gt;
$\rightarrow$ How do we choose between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2W&lt;/code&gt;?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Regularization: introduce penalty term $\lambda R(W)$ on L(W)&lt;br /&gt;
L2 regularization: $R(W)=\sum_k\sum_l W_{k,l}^2$; likes to spread out the weights&lt;br /&gt;
L1 regularization: $R(W)=\sum_k\sum_l |W_{k,l}|$&lt;br /&gt;
Elastic net (L1 + L2): $R(W) = \sum_k\sum_l\beta W_{k,l}^2 + |W_{k,l}|$&lt;br /&gt;
Dropout, Batch normalization, Stochastic depth, fractional pooling, etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Why regularize?&lt;br /&gt;
Express preferences over weights&lt;br /&gt;
Make the model simple so it works on test data&lt;br /&gt;
Improve optimization by adding curvature&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;softmax-classifiermultinomial-logistic-regression&quot;&gt;Softmax classifier(Multinomial Logistic Regression)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cs231n_lec3_1.png&quot; alt=&quot;png&quot; width=&quot;80%&amp;quot;, height=&amp;quot;80%&quot; /&gt;&lt;br /&gt;
The &lt;em&gt;information content&lt;/em&gt;(also called the &lt;em&gt;surprisal&lt;/em&gt; or &lt;em&gt;self-information&lt;/em&gt;) is described as $h(E) = -\log P(E)$. For a discrete random variable &lt;em&gt;X&lt;/em&gt;, its &lt;em&gt;entropy&lt;/em&gt;(or expected; mean information) $H(X) = -\sum_{i=1}^N p_i \log p_i$. &lt;em&gt;Cross-entropy&lt;/em&gt; is an approximaton of &lt;em&gt;q&lt;/em&gt; to &lt;em&gt;p&lt;/em&gt; is to minimizing &lt;em&gt;KL(p|q)&lt;/em&gt;, while the &lt;em&gt;Kullback–Leibler divergence&lt;/em&gt; as $D_{KL}(P||Q)=\sum_y P(y)\log\frac{P(y)}{Q(y)} = \sum_y P(y)\log P(y) - \sum_y P(y)\log Q(y)$. Thus, cross entropy loss $L = -\sum_y P(y)\log Q(y)$.&lt;/p&gt;

&lt;p&gt;Q1: What is the min/max possible softmax loss $L_i$?&lt;br /&gt;
min &lt;em&gt;0&lt;/em&gt; to max $\infty$&lt;/p&gt;

&lt;p&gt;Q2: At initialization all $s_j$ will be approximately equal; what is the softmax loss $L_i$, assuming &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; classes?&lt;br /&gt;
$-\log(1/C)=\log C$&lt;/p&gt;

&lt;h3 id=&quot;recap&quot;&gt;Recap&lt;/h3&gt;
&lt;p&gt;For some dataset of $(x,y)$, a score function $s = f(x;W)$, a loss function $L_i$,&lt;br /&gt;
Full loss \(L=\frac{1}{N}\sum_{i=1}^N L_i + R(W)\)&lt;/p&gt;

&lt;h3 id=&quot;optimization&quot;&gt;Optimization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Strategy: Follow the slope&lt;br /&gt;
In &lt;em&gt;1&lt;/em&gt;-dimension, the derivative of a function:&lt;br /&gt;
\(\frac{df(x)}{dx}=\lim_{n\to 0}\frac{f(x+h)-f(x)}{h}\)&lt;br /&gt;
In multiple dimensions, the gradient is the vector of (partial derivatives) along each dimension.&lt;br /&gt;
The slope in any direction is the dot product of the direction with the gradient. The direction of steepest descent is the negative gradient.&lt;br /&gt;
The loss is just a function of &lt;em&gt;W&lt;/em&gt;, what we want is $\nabla_w L$&lt;br /&gt;
In practice: Always use analytic gradient, but check implementation with numerical gradient. This is called a gradient check.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stochastic Gradient Descent(SGD)&lt;br /&gt;
Full sum expensive when &lt;em&gt;N&lt;/em&gt; is large. Approximate su using a minibatch of examples
(32 / 64 / 128 common)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Vanilla Minibatch Gradient Descent
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;data_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
	&lt;span class=&quot;n&quot;&gt;weights_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_fun&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights_grad&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        
        <pubDate>Tue, 14 Dec 2021 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/cs231n_lec3</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/cs231n_lec3</guid>
      </item>
      
    
      
      <item>
        <title>cs231n - Lecture 2. Image Classification</title>
        
          <description>&lt;h3 id=&quot;image-classification-a-core-task-in-computer-vision&quot;&gt;Image Classification: A Core Task in Computer Vision&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The Problem: Semantic Gap&lt;br /&gt;
considering image as a tensor of integers between [0,255] with 3 channels RGB&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Challenges:&lt;br /&gt;
  Viewpoint variation&lt;br /&gt;
  Background Clutter
  Illumination&lt;br /&gt;
  Occlusion&lt;br /&gt;
  Deformation&lt;br /&gt;
  Intraclass variation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An image classifier&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;classify_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
&lt;span class=&quot;c1&quot;&gt;# Some magic here?  
&lt;/span&gt;	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_label&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;ML: Data-Driven Approach
    &lt;ol&gt;
      &lt;li&gt;Collect a dataset of images and labels&lt;/li&gt;
      &lt;li&gt;Use ML algorithms to train a classifier&lt;/li&gt;
      &lt;li&gt;Evaluate the classifier on new images&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
	&lt;span class=&quot;c1&quot;&gt;# Machine Learning!  
&lt;/span&gt;	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;  

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
	&lt;span class=&quot;c1&quot;&gt;# Use model to predict labels  
&lt;/span&gt;	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_labels&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Nearest Neighbor Classifier&lt;br /&gt;
Predict the label of the most similar training imgae&lt;br /&gt;
Training data with labels x $\leftrightarrow$ query data \(x^*\)&lt;br /&gt;
distance metric \(|x,x^*| \rightarrow R\)&lt;br /&gt;
&lt;em&gt;L1&lt;/em&gt; distance \(d_1(I_1,I_2) = \sum_p |I_1^p-I_2^p|\)&lt;br /&gt;
pixel-wise absolute value differences $\rightarrow$ sum for scoring&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NearestNeighbor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
		&lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;### Memorize training data
&lt;/span&gt;		&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; X is N x D for n example. Y is 1-dim of size N&quot;&quot;&quot;&lt;/span&gt;  
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xtr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xf&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ytr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;  

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
		&lt;span class=&quot;n&quot;&gt;num_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  
		&lt;span class=&quot;n&quot;&gt;Ypred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ytr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;c1&quot;&gt;### find closest train image for each test image, predict label of its
&lt;/span&gt;			&lt;span class=&quot;n&quot;&gt;distances&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xtr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;min_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
			&lt;span class=&quot;n&quot;&gt;Ypred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ytr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ypred&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Q: With &lt;em&gt;N&lt;/em&gt; examples, how fast are training and prediction?&lt;br /&gt;
Answer: Train &lt;strong&gt;O(1)&lt;/strong&gt;, predict &lt;strong&gt;O(N)&lt;/strong&gt;&lt;br /&gt;
$\rightarrow$ Bad: we want fast at prediction; slow for training is ok.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;KNN with majority vote&lt;br /&gt;
Distance metric: &lt;em&gt;L1(Manhattan), L2(Euclidean)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperparameters&lt;br /&gt;
To find best value of &lt;em&gt;k&lt;/em&gt; and best distance(metric) to use, use train-val-test approach&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, pixel distances are not informative for KNN&lt;br /&gt;
very slow at test time &lt;em&gt;&amp;amp;&lt;/em&gt; curse of dimensionality&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;linear-classifier&quot;&gt;Linear Classifier&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Parametric Approach&lt;br /&gt;
$f(x,W)=Wx+b$; &lt;em&gt;W&lt;/em&gt; for parameters or weights&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interpreting a linear classifier: Geometric Viewpoint&lt;br /&gt;
hard cases in non-linearity&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        
        <pubDate>Mon, 13 Dec 2021 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/cs231n_lec2</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/cs231n_lec2</guid>
      </item>
      
    
      
      <item>
        <title>GNN-based Fashion Coordinator</title>
        
          <description>
</description>
        
        <pubDate>Tue, 14 Sep 2021 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/fashion_GNN</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/fashion_GNN</guid>
      </item>
      
    
      
      <item>
        <title>Stock Market Portfolio Modeling with R</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/assets/images/m_slide_full.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;
</description>
        
        <pubDate>Thu, 17 Jun 2021 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/stock_portfolio</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/stock_portfolio</guid>
      </item>
      
    
      
      <item>
        <title>K-POP Fandom Data Analysis with networkX</title>
        
          <description>
</description>
        
        <pubDate>Tue, 01 Jun 2021 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/kpop_fandom</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/kpop_fandom</guid>
      </item>
      
    
      
      <item>
        <title>Stock Market Cluster Analysis with NetworkX</title>
        
          <description>
</description>
        
        <pubDate>Thu, 20 May 2021 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/stock_cluster</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/stock_cluster</guid>
      </item>
      
    
      
      <item>
        <title>R - Air Pollution Data Analysis</title>
        
          <description>
</description>
        
        <pubDate>Tue, 15 Dec 2020 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/R_air_polution</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/R_air_polution</guid>
      </item>
      
    
      
      <item>
        <title>NLP - Korean Language Text Analysis with RNN</title>
        
          <description>
</description>
        
        <pubDate>Thu, 18 Jun 2020 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/NLP_korean_RNN</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/NLP_korean_RNN</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 10. Deep Learning</title>
        
          <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#chapter-10-deep-learning&quot; id=&quot;markdown-toc-chapter-10-deep-learning&quot;&gt;Chapter 10. Deep Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#101-single-layer-neural-networks&quot; id=&quot;markdown-toc-101-single-layer-neural-networks&quot;&gt;10.1. Single Layer Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#102-multilayer-neural-networks&quot; id=&quot;markdown-toc-102-multilayer-neural-networks&quot;&gt;10.2. Multilayer Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#103-convolutional-neural-networks&quot; id=&quot;markdown-toc-103-convolutional-neural-networks&quot;&gt;10.3. Convolutional Neural Networks&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1031-convolution-layers&quot; id=&quot;markdown-toc-1031-convolution-layers&quot;&gt;10.3.1. Convolution Layers&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#1032-pooling-layers&quot; id=&quot;markdown-toc-1032-pooling-layers&quot;&gt;10.3.2. Pooling Layers&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#1033-architecture-of-a-convolutional-neural-network&quot; id=&quot;markdown-toc-1033-architecture-of-a-convolutional-neural-network&quot;&gt;10.3.3. Architecture of a Convolutional Neural Network&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#1034-data-augmentation&quot; id=&quot;markdown-toc-1034-data-augmentation&quot;&gt;10.3.4. Data Augmentation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#104-document-classification&quot; id=&quot;markdown-toc-104-document-classification&quot;&gt;10.4. Document Classification&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#105-recurrent-neural-networks&quot; id=&quot;markdown-toc-105-recurrent-neural-networks&quot;&gt;10.5. Recurrent Neural Networks&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1051-sequential-models-for-document-classification&quot; id=&quot;markdown-toc-1051-sequential-models-for-document-classification&quot;&gt;10.5.1. Sequential Models for Document Classification&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#1052-time-series-forecasting&quot; id=&quot;markdown-toc-1052-time-series-forecasting&quot;&gt;10.5.2. Time Series Forecasting&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#rnn-forecaster&quot; id=&quot;markdown-toc-rnn-forecaster&quot;&gt;RNN forecaster&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#autoregression&quot; id=&quot;markdown-toc-autoregression&quot;&gt;Autoregression&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#1053-summary-of-rnns&quot; id=&quot;markdown-toc-1053-summary-of-rnns&quot;&gt;10.5.3. Summary of RNNs&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#106-when-to-use-deep-learning&quot; id=&quot;markdown-toc-106-when-to-use-deep-learning&quot;&gt;10.6. When to Use Deep Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#107-fitting-a-neural-network&quot; id=&quot;markdown-toc-107-fitting-a-neural-network&quot;&gt;10.7. Fitting a Neural Network&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1071-backpropagation&quot; id=&quot;markdown-toc-1071-backpropagation&quot;&gt;10.7.1. Backpropagation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#1072-regularization-and-stochastic-gradient-descent&quot; id=&quot;markdown-toc-1072-regularization-and-stochastic-gradient-descent&quot;&gt;10.7.2. Regularization and Stochastic Gradient Descent&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#1073-dropout-learning&quot; id=&quot;markdown-toc-1073-dropout-learning&quot;&gt;10.7.3. Dropout Learning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#1074-network-tuning&quot; id=&quot;markdown-toc-1074-network-tuning&quot;&gt;10.7.4. Network Tuning&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#108-interpolation-and-double-descent&quot; id=&quot;markdown-toc-108-interpolation-and-double-descent&quot;&gt;10.8. Interpolation and Double Descent&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-10-deep-learning&quot;&gt;Chapter 10. Deep Learning&lt;/h2&gt;

&lt;h2 id=&quot;101-single-layer-neural-networks&quot;&gt;10.1. Single Layer Neural Networks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A neural network takes input vector of &lt;em&gt;p&lt;/em&gt; variables \(X=(X_1,X_2,\ldots,X_p)\) and builds a nonlinear function $f(X)$ to predict the response &lt;em&gt;Y&lt;/em&gt;. What distinguishes neural networks from these methods is the particular &lt;em&gt;structure&lt;/em&gt; of the model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;E.g. a simple &lt;em&gt;feed-forward neural network&lt;/em&gt;:&lt;br /&gt;
\(\begin{align*}
f(X)&amp;amp;= \beta_0 + \sum_{k=1}^K\beta_k h_k(X) \\
  &amp;amp;= \beta_0 + \sum_{k=1}^K\beta_k g(w_{k0}+\sum_{j=1}^p w_{kj}X_j).
\end{align*}\)&lt;br /&gt;
In the terminology, the features \(X_1,\ldots,X_4\) make up the units in the &lt;em&gt;input layer&lt;/em&gt;. Each of the inputs from the input lyaer feed into each of the &lt;em&gt;K hidden units&lt;/em&gt;. In two steps; First the &lt;em&gt;K activations&lt;/em&gt; $A_k$ in the hidden layer are computed as functions of the input features $X_1,\ldots,X_p$,&lt;br /&gt;
\(A_k = h_k(X) = g(w_{k0}+\sum_{j=1}^p w_{kj}X_j)\),&lt;br /&gt;
where \(g(z)\) is a nonlinear &lt;em&gt;activation function&lt;/em&gt; that is specified in advance. We can think of each \(A_k\) as a different transformation \(h_k(X)\) of the original features, like basis functions of Chapter 7. These &lt;em&gt;K&lt;/em&gt; activations then feed into the output layer, resulting in&lt;br /&gt;
\(f(X) = \beta_0 + \sum_{k=1}^K \beta_k A_k\),&lt;br /&gt;
a linear regression model. All the parameters $\beta_0,\ldots,\beta_K$ and $w_{10},\ldots,w_{Kp}$ need to be estimated from data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With deriving &lt;em&gt;K&lt;/em&gt; new features by computing &lt;em&gt;K&lt;/em&gt; different linear combinations of &lt;em&gt;X&lt;/em&gt;, the model squashes each through an activation function $g(\cdot)$ to transform it. The final model is linear in these derived variables.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Activation functions
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;sigmoid&lt;/em&gt; activation function:&lt;br /&gt;
  \(g(z)=\frac{e^z}{1+e^z}=\frac{1}{1+e^{-z}}\)&lt;br /&gt;
  which is the same function used in logistic regression to convert a linear function into probabilities between zero and one.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;ReLU&lt;/em&gt;&lt;/strong&gt;(&lt;em&gt;rectified linear unit&lt;/em&gt;) activation function:&lt;br /&gt;
  \(g(z) = (z)_{+} = \begin{cases}0, &amp;amp; \mbox{if }z&amp;lt;0 \\
                              z, &amp;amp; \mbox{othersize}.\end{cases}\)&lt;br /&gt;
  can be computed and stored more efficiently than a sigmoid function. Although it thresholds at zero, because we apply it to a linear function the constant term \(w_{k0}\) will shift this inflection point.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The nonlinearity in the activation function \(g(\cdot)\) is essential, since without it the model would collapse into a simple linear model. Moreoever, having a nonlinear activation function allows the model to capture complex nonlinearities and interaction effects.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Fitting a neural network requires estimating the unknown parameters. For a quantitative response, typically squared-error loss is used, so that the parameters are chosen to minimize \(\sum_{i=1}^n(y_i-f(x_i))^2\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;102-multilayer-neural-networks&quot;&gt;10.2. Multilayer Neural Networks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;With &lt;em&gt;H&lt;/em&gt; multiple hidden layers,&lt;br /&gt;
the first hidden layer is&lt;br /&gt;
\(\begin{align*}
A_k^{(1)} &amp;amp;= h_k^{(1)}(X) \\
        &amp;amp;= g(w_{k0}^{(1)}+\sum_{j=1}^p w_{kj}^{(1)}X_j)
\end{align*}\)&lt;br /&gt;
for \(k=1,\ldots,K_1\).&lt;br /&gt;
The &lt;em&gt;h&lt;/em&gt;th hidden layer treats the activations \(A_k^{(h-1)}\) as inputs and computes new activations&lt;br /&gt;
\(\begin{align*}
A_{k'}^{(h)} &amp;amp;= h_{k'}^{(h)}(X) \\
        &amp;amp;= g(w_{k'0}^{(h)}+\sum_{k'=1}^{K_1} w_{hk'}^{(h)}A_k^{(h-1)})
\end{align*}\)&lt;br /&gt;
for \(k'=1,\ldots,K_h\). Each \(A_{k'}^{(h)} = h_{k'}^{(h)}(X)\) is a function of input vector &lt;em&gt;X&lt;/em&gt;. Thus, through a chain of transformations, the network is able to build up fairly complex transformations of &lt;em&gt;X&lt;/em&gt; that ultimately feed into the output layer as features.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The notation \(\mathbf{W}_h\) represents the entire matrix of weights that feed from the hidden layer $L_{h-1}$ to $L_h$. Each element $A_k^{(h-1)}$ feeds to the hidden layer $L_h$ via the matrix of weights \(\mathbf{W}_h\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On the output layer in the classification case, compute &lt;em&gt;M&lt;/em&gt; different linear models similar to our single model to get separate quantitative responses for each classes. To represent class probabilities \(f_m(X)=Pr(Y=m|X)\), we use the &lt;em&gt;softmax&lt;/em&gt; activation function&lt;br /&gt;
\(f_m(X)=Pr(Y=m|X)=\frac{e^{Z_m}}{\sum_{l=0}^{M-1} e^{Z_l}}\).&lt;br /&gt;
The classifier assigns the image to the class with the highest probability.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To train this network, since the response is qualitative, we look for coefficient estimates that minimize the negative multinomial log-likelihood&lt;br /&gt;
\(-\sum_{i=1}^n\sum_{m=0}^{M-1}y_{im}\log(f_m(x_i))\),&lt;br /&gt;
known as the &lt;em&gt;cross-entropy&lt;/em&gt;, a generalization of the criterion for two-class logistic regression. If the response were quantitative, we would instead minimize squared-error loss as before.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;103-convolutional-neural-networks&quot;&gt;10.3. Convolutional Neural Networks&lt;/h2&gt;

&lt;h3 id=&quot;1031-convolution-layers&quot;&gt;10.3.1. Convolution Layers&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;em&gt;convolution layer&lt;/em&gt; is made up of a large number of &lt;em&gt;convolution filters&lt;/em&gt;, each of which is a template that determines whether a particular local feature is present in an image. A very simple operation, called a &lt;em&gt;convolution&lt;/em&gt; amounts to repeatedly multiplying matrix elements and then adding the results. Resulting &lt;em&gt;convolved image highlights regions of the original image that resemble the convolution filter&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In a convolution layer, we use a whole bank of filters to pick out a variety of differently-oriented edges and shapes in the image. Using predefined filters in this way is standard practice in image processing. By contrast, we can think of the filter weights as the parameters going from an input layer to a hidden layer. Thus, with CNNs the filters are learned for the specific classification task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1032-pooling-layers&quot;&gt;10.3.2. Pooling Layers&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A &lt;em&gt;pooling&lt;/em&gt; layer provides a way to condense a large image into a smaller summary image. The &lt;em&gt;max pooling&lt;/em&gt; operation summarizes each non-overlapping block of pixels in an image using the maximum value in the block. This reduces the size of the image and also provides some &lt;em&gt;location invariance&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1033-architecture-of-a-convolutional-neural-network&quot;&gt;10.3.3. Architecture of a Convolutional Neural Network&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Convolve-then-pool sequence:&lt;br /&gt;
Start with the three-dimensional feature map of a color image, consist of three $n\times n$ two-dimensional feature map of pixels.&lt;br /&gt;
Each convolve layer takes as input the three-dimensional feature map from the previous layer and treats it like a single multi-channel image. Each convolution filter learned has as many channels as this feature map.&lt;br /&gt;
Since the channel feature maps are reduced in size after each pool layer, we usually increase the number of filters in the next convolve layer to compensate.&lt;br /&gt;
Sometimes we repeat several convolve layers before a pool layer. This effectively increases the dimension of the filter.&lt;br /&gt;
These operations are repeated until the pooling has reduced each channel feature map down to just a few pixels in each dimension.&lt;br /&gt;
Then the three-dimensional feature maps are flattened; the pixels are treated as separate units, and fed into one or more fully-connected layers before reaching the output layer, which is a &lt;em&gt;softmax activation&lt;/em&gt; for the classification.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1034-data-augmentation&quot;&gt;10.3.4. Data Augmentation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Each training image is replicated many times, with each replicate randomly distorted in a natural way such that human recognition is unaffected. Typical distortions are zoom, horizontal and vertical shift, shear, small rotations, and horizontal flips. This is a way of increasing the training set considerably with somewhat different examples, and thus protects against overfitting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We can see this as a form of regularization: we build a cloud of images around each original image, all with the same label. This kind of fattening of the data is similar in spirit to ridge regularization.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;104-document-classification&quot;&gt;10.4. Document Classification&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For the two-class response of the sentiment of the text data, which will be positive or negative. Each document can be a different length, include slang or non-words, have spelling errors, etc. We need to find a way to featurize such a document.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The simplest featurization is BOW, &lt;em&gt;bog-of-words&lt;/em&gt; model. With dictionary containing &lt;em&gt;M&lt;/em&gt; words, we create a binary feature vector of length &lt;em&gt;M&lt;/em&gt; for each document, scoring 1 for presence, and 0 otherwise. The resulting training feature matrix &lt;strong&gt;X&lt;/strong&gt; is a &lt;em&gt;sparse matrix&lt;/em&gt;, most of the values are the same(to zero); it can be stored efficiently in &lt;em&gt;sparse matrix format&lt;/em&gt;. Then we can build a neural network model with the feature matrix as input layers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The bag-of-words model summarizes a document by the words present, and ignores their context. There are at least two popular ways to take the context into account:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;The &lt;em&gt;bag-of-n-grams&lt;/em&gt; model recording the consecutive co-occurrence of every distinct set of words.&lt;/li&gt;
      &lt;li&gt;Treat the document as a sequence, taking account of all the words in the context of those that preceded and those that follow.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;105-recurrent-neural-networks&quot;&gt;10.5. Recurrent Neural Networks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In a &lt;em&gt;recurrent neural network&lt;/em&gt; (RNN), the input object &lt;em&gt;X&lt;/em&gt; is a &lt;em&gt;sequence&lt;/em&gt;. Considering a corpus of documents, each document can be represented as a sequence of &lt;em&gt;L&lt;/em&gt; words, so \(X=\left\{ X_1,X_2,\ldots,X_L\right\}\). RNNs are designed to capture the sequential nature of such(text) input objects like CNNs for the spatial structure of image inputs. The output &lt;em&gt;Y&lt;/em&gt; can also be a sequence (such as in language translation), but often is a scalar, like the binary sentiment label of a movie review document.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The structure of a very basic RNN; with a sequence \(X=\left\{ X_1,X_2,\ldots,X_L\right\}\), a simple output &lt;em&gt;Y&lt;/em&gt;, and a hidden-layer sequence \(\left\{ A_l \right\}_1^L =\left\{ A_1,A_2,\ldots,A_L\right\}\). Each \(X_l\) is a vector representation for the $l$th word.&lt;br /&gt;
Suppose it has &lt;em&gt;p&lt;/em&gt; components \(X_l^T = (X_{l1},X_{l2},\ldots,X_{lp})\), and hidden layer consists of &lt;em&gt;K&lt;/em&gt; units \(A_l^T = (A_{l1},\ldots,A_{lK})\). We represent the collection of \(K \times (p+1)\) shared weights $w_{kj}$ for the input layer by a matrix &lt;strong&gt;W&lt;/strong&gt;, and similarly &lt;strong&gt;U&lt;/strong&gt; is a \(K \times K\) matrix of the weights $u_{ks}$ for the hidden-to hideen layers, and &lt;strong&gt;B&lt;/strong&gt; is a &lt;em&gt;K+1&lt;/em&gt; vector of weights $\beta_k$ for the output layer.&lt;br /&gt;
Then \(A_{lk}=g(w_{k0}+\sum_{j=1}^p w_{kj}X_{lj} + \sum_{s=1}^K u_{ks} A_{l-1,s})\), and the output \(O_l = \beta_0 + \sum_{k=1}^K\beta_k A_{lk}\) for a quantitative response, for example. Here $g(\cdot)$ is an activation function such as ReLU. Note that the same weights &lt;strong&gt;W&lt;/strong&gt;, &lt;strong&gt;U&lt;/strong&gt;, and &lt;strong&gt;B&lt;/strong&gt; are not functions of $l$. This is a form of &lt;em&gt;weights sharing&lt;/em&gt; used by RNNs, similar to the use of filters in CNNs.&lt;br /&gt;
Proceeded from beginning to end, the activations $A_l$ accumulate a history of what has been seen before, so that the learned context can be used for prediction.&lt;br /&gt;
&lt;img src=&quot;/assets/images/ch10_simpleRNN.png&quot; alt=&quot;png&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For regression problems the loss function is $(Y-O_L)^2$, which only references the final output \(O_L = \beta_0 + \sum_{k=1}^K\beta_k A_{Lk}\) and others before are not used. With &lt;em&gt;n&lt;/em&gt; input sequence/response pairs, the parameters are found by minimizing the sum of squares \(\sum_{i=1}^n(y_i-o_{iL})^2\).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1051-sequential-models-for-document-classification&quot;&gt;10.5.1. Sequential Models for Document Classification&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Beacuse of the dimensionality problem of bag-of-words model(one-hot-encoded vector), we use lower-dimensional &lt;em&gt;embedding&lt;/em&gt; space instead. Rather than representing each word by a binary vector with zeros and a single one in some position, we will represent it by a set of &lt;em&gt;m&lt;/em&gt; real numbers, none of which are typically zero.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;embedding layer&lt;/em&gt; &lt;strong&gt;E&lt;/strong&gt; comes from the optimization part of a neural network, or we can use a &lt;em&gt;weight freezing&lt;/em&gt;, inserting a precomputed matrix &lt;strong&gt;E&lt;/strong&gt; in the embedding layer. The idea is that the positions of words in the embedding space preserve semantic meaning: synonyms should appear near each other.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The next step is to limit each document to the last &lt;em&gt;L&lt;/em&gt; words. Documents that are shorter get padded with zeros upfront. So now each document is represented by a series consisting of &lt;em&gt;L&lt;/em&gt; vectors, and each $X_l$ in the sequence has &lt;em&gt;m&lt;/em&gt; components.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Then we run the process of simple RNN as presented before, with &lt;strong&gt;B&lt;/strong&gt; has &lt;em&gt;2(K+1)&lt;/em&gt; for two-class logistic regression. If the embedding layer &lt;strong&gt;E&lt;/strong&gt; is learned, that adds an additional $m \times D$ parameters, and is by far the biggest cost.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More elaborate versions use long term and short term memory (LSTM). However, LSTM models take a long time to train, which makes exploring many architectures and parameter optimization tedious.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1052-time-series-forecasting&quot;&gt;10.5.2. Time Series Forecasting&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In an example of Stock price predition, one feature of stock market data is that the day-to-day observations are not independent of each other. The series exhibit &lt;em&gt;auto-correlation&lt;/em&gt;; values nearby in time tend to be similar to each other.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Consider pairs of observations $(v_t,v_{t-l})$, a &lt;em&gt;lag&lt;/em&gt; of $l$ days apart. If we take all such pairs in the $v_t$ series and compute their correlation coefficient, this gives the autocorrelation at lag $l$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another interesting characteristic of this forecasting problem is that the response variable $v_t$; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log_volume&lt;/code&gt; is also a predictor. We will use the past values of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log_volume&lt;/code&gt; to predict values in the future.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;rnn-forecaster&quot;&gt;RNN forecaster&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We wish to predict a value $v_t$ from past values $v_{t-1},v_{t-2},\ldots$ and also to make use of past values of the other series $r_{t-1},r_{t-2},\ldots$ and $z_{t-1},z_{t-2},\ldots$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Idea: to extract many short mini-series of input sequences with a predefined length &lt;em&gt;L&lt;/em&gt;(lag), and a corresponding target &lt;em&gt;Y&lt;/em&gt;.&lt;br /&gt;
\(X_1= \begin{pmatrix} v_{t-L}\\ r_{t-L}\\ z_{t-L} \end{pmatrix}, X_2= \begin{pmatrix} v_{t-L+1}\\ r_{t-L+1}\\ z_{t-L+1} \end{pmatrix}, \cdots, X_L= \begin{pmatrix} v_{t-1}\\ r_{t-1}\\ z_{t-1} \end{pmatrix}\), and \(Y=v_t\).&lt;br /&gt;
Here the target &lt;em&gt;Y&lt;/em&gt; is the value of response at a single timepoint &lt;em&gt;t&lt;/em&gt;, and the input sequence &lt;em&gt;X&lt;/em&gt; is the series of 3-vectors \(\left\{X_l\right\}_1^L\) consisting of measurements from day &lt;em&gt;t-L, t-L+1,&lt;/em&gt; up to &lt;em&gt;t-1&lt;/em&gt;. Each value of &lt;em&gt;t&lt;/em&gt; makes a separate pair (&lt;em&gt;X,Y&lt;/em&gt;). Then we run RNN model for prediction.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;autoregression&quot;&gt;Autoregression&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The RNN we just fit has much in common with a traditional &lt;em&gt;autoregression&lt;/em&gt;(AR) linear model. Constructed a response vector &lt;strong&gt;y&lt;/strong&gt; and a matrix &lt;strong&gt;M&lt;/strong&gt; of predictors for least squares regression:&lt;br /&gt;
\(\mathbf{y}=\begin{bmatrix} v_{L+1}\\ v_{L+2}\\ \vdots\\ v_T \end{bmatrix}\), 
\(\mathbf{M}=\begin{bmatrix} 1 &amp;amp; v_L &amp;amp; v_{L-1} &amp;amp; \cdots &amp;amp; v_1 \\
                           1 &amp;amp; v_{L+1} &amp;amp; v_{L} &amp;amp; \cdots &amp;amp; v_2 \\
                           \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
                           1 &amp;amp; v_{T-1} &amp;amp; v_{T-2} &amp;amp; \cdots &amp;amp; v_{T-L}
           \end{bmatrix}\)&lt;br /&gt;
each have &lt;em&gt;T-L&lt;/em&gt; rows, one per observation. The predictors for any given reponse $v_t$ on day &lt;em&gt;t&lt;/em&gt; are the previous &lt;em&gt;L&lt;/em&gt; values of the same series. Fitting a regression of &lt;strong&gt;y&lt;/strong&gt; on &lt;strong&gt;M&lt;/strong&gt; amounts to fitting the model&lt;br /&gt;
\(\hat{y}_t = \hat\beta_0 + \hat\beta_1 v_{t-1} + \hat\beta_2 v_{t-2} + \cdots + 
              \hat\beta_L v_{t-L}\),&lt;br /&gt;
and is called an order-&lt;em&gt;L&lt;/em&gt; autoregressive model, or simplay AR(&lt;em&gt;L&lt;/em&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The RNN processes data sequence from left to right with the same weights &lt;strong&gt;W&lt;/strong&gt;, while the AR model simply treats all &lt;em&gt;L&lt;/em&gt; elements of the sequence equally as a vector of $L \times p$ predictors; a process called &lt;em&gt;flattening&lt;/em&gt; in the neural network. Of course the RNN also includes the hidden layer activations which transfer information along the sequence, and introduces additional nonlinearity with much more parameters.
    &lt;h3 id=&quot;1053-summary-of-rnns&quot;&gt;10.5.3. Summary of RNNs&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;There are many variations and enhancements of the simple RNN for sequence modeling. One approach uses a onedimensional convolutional neural network, treating the sequence of vectors as an image. One can also have additional hidden layers; multi-layer RNN. Alternative &lt;em&gt;bidirectional&lt;/em&gt; RNNs scan the sequences in both directions. In language translation the target is also a sequence of words, in a language different from that of the input sequence. Both the input sequence and the target sequence share the hidden units, so-called &lt;em&gt;Seq2Seq&lt;/em&gt; learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;106-when-to-use-deep-learning&quot;&gt;10.6. When to Use Deep Learning&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Should we discard all our older tools, and use deep learning on every problem with data? We follow Occam’s razor principle: when faced with several methods that give roughly equivalent performance, pick the simplest.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Typically we expect deep learning to be an attractive choice when the sample size of the training set is extremely large, and when interpretability of the model is not a high priority.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;107-fitting-a-neural-network&quot;&gt;10.7. Fitting a Neural Network&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In simple network, the parameters are \(\beta = (\beta_0,\beta_1,\ldots,\beta_K)\), each of the \(w_k=(w_{k0},w_{k1},\ldots,w_{kp})\) for &lt;em&gt;k&lt;/em&gt; in &lt;em&gt;K&lt;/em&gt;. Given observations ($x_i,y_i$) for &lt;em&gt;i&lt;/em&gt; in &lt;em&gt;n&lt;/em&gt;, we fit the model by solving a nonlinear least squares problem&lt;br /&gt;
\(\begin{align*}\text{min}_{\left\{ w_k \right\}_1^K, \beta }\frac{1}{2}\sum_{i=1}^n(y_i-f(x_i))^2,\end{align*}\)&lt;br /&gt;
where \(f(x_i)=\beta_0+\sum_{k=1}^K\beta_k g(w_{k0}+\sum_{j=1}^p w_{kj}x_{ij})\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The minimization objective is quite simple, but because of the nested arrangement of the parameters and the symmetry of the hidden units, it is not straightforward to minimize. The problem is nonconvex in the parameters, and hence there are multiple solutions. To overcome some of these issues and to protect from overfitting, two general strategies are employed when fitting neural networks: &lt;em&gt;Slow learning&lt;/em&gt; and &lt;em&gt;Regularization&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Suppose we represent all the parameters in one long vecter \(\theta\). Then we can rewrite the objective as&lt;br /&gt;
\(R(\theta)=\frac{1}{2}\sum_{i=1}^n(y_i-f_{\theta}(x_i))^2\), where we make explicit the dependence of &lt;em&gt;f&lt;/em&gt; on the parameters. The idea of gradient descent is&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;Start with a guess \(\theta^0\) for all the parameters in \(\theta\), and set \(t=0\).&lt;/li&gt;
      &lt;li&gt;Iterate until the objective fails to decrease:&lt;br /&gt;
  (a) Find a vector \(\delta\) reflects a small change in \(\theta\), such that \(\theta^{t+1} = \theta^t + \delta\) reduces the objective; i.e. such that \(R(\theta^{t+1})&amp;lt;R(\theta^t)\).&lt;br /&gt;
  (b) Set \(t \leftarrow t+1\).&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1071-backpropagation&quot;&gt;10.7.1. Backpropagation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;How do we find the directions to move \(\theta\) so as to decrease the objective? The &lt;em&gt;gradient&lt;/em&gt; of $R(\theta)$, evaluated at some current value \(\theta=\theta^m\), is the vector of partial derivatives at that point:&lt;br /&gt;
\(\begin{align*}\triangledown R(\theta^m) = \frac{\partial R(\theta)}{\partial\theta}|_{\theta=\theta^m}\end{align*}\).&lt;br /&gt;
The subscript \(\theta=\theta^m\) means that after computing the vector of derivatives, we evaluate it at the current guess, \(\theta^m\). This gives the direction in $\theta$-space in which $R(\theta)$ &lt;em&gt;increases&lt;/em&gt; most rapidly. The idea of gradient descent is to move $\theta$ a little in the &lt;em&gt;opposite&lt;/em&gt; direction(since we wish to go downhill):&lt;br /&gt;
\(\theta^{m+1}\leftarrow\theta^m - \rho\triangledown R(\theta^m)\).&lt;br /&gt;
For a small enough value of the &lt;em&gt;learning rate&lt;/em&gt; $\rho$, this step will decrease the objective $R(\theta)$; i.e. $R(\theta^{m+1})\le R(\theta^m)$. If the gradient vector is zero, then we may have arrived at a minimum of the objective.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Calculation: the &lt;em&gt;chain rule&lt;/em&gt; of differentiation&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1072-regularization-and-stochastic-gradient-descent&quot;&gt;10.7.2. Regularization and Stochastic Gradient Descent&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;stochastic gradient descent&lt;/em&gt;(SGD):&lt;br /&gt;
When n is large, instead of calculating over all &lt;em&gt;n&lt;/em&gt; observations, we can sample a small fraction or &lt;em&gt;minibatch&lt;/em&gt; of them each time we compute a gradient step.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Regularization:&lt;br /&gt;
E.g. Ridge regularization on the weights, augmenting the objective function with a penalty term on classification problem:&lt;br /&gt;
\(R(\theta;\lambda)=-\sum_{i=1}^n\sum_{m=0}^{M-1}y_{im}\log(f_m(x_i))
                  +\lambda\sum_j\theta_j^2\).&lt;br /&gt;
With $\lambda$ as preset at a small value or found using the validation-set approach. We can also use different values of $\lambda$ for the groups of weights from different layers. Lasso is also a popular alternative.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1073-dropout-learning&quot;&gt;10.7.3. Dropout Learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Efficient form of regularization, similar in some respects to ridge regularization. The idea is to randomly remove a fraction of the units in a layer when fitting the model, separately each time a training observation is processed. This prevents nodes from becoming over-specialized, and can be seen as a form of regularization.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1074-network-tuning&quot;&gt;10.7.4. Network Tuning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Choices that all have an effect on the performance:&lt;br /&gt;
  The number of hidden layers, and the number of units per layer.&lt;br /&gt;
  Regularization tuning parameters.&lt;br /&gt;
  Details of stochastic gradient descent.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;108-interpolation-and-double-descent&quot;&gt;10.8. Interpolation and Double Descent&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In certain specific settings it can be possible for a statistical learning method that interpolates the training data to perform well; or at least, better than a slightly less complex model that does not quite interpolate the data. The phenomenon is known as &lt;em&gt;double descent&lt;/em&gt;, where the test error has a U-shape before the interpolation threshold is reached, and then it descends again (for a while, at least) as an increasingly flexible model is fit. The double-descent phenomenon does not contradict the bias-variance trade-off.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It has been used by the machine learning community to explain the successful practice of using an overparametrized neural network (many layers, and many hidden units), and then fitting all the way to zero training error. However, zero error fit is not always optimal, we typically do not want to rely on this behavior.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        
        <pubDate>Wed, 27 May 2020 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch10</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch10</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 9. Support Vector Machines</title>
        
          <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#chapter-9-support-vector-machines&quot; id=&quot;markdown-toc-chapter-9-support-vector-machines&quot;&gt;Chapter 9. Support Vector Machines&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#91-maximal-margin-classifier&quot; id=&quot;markdown-toc-91-maximal-margin-classifier&quot;&gt;9.1. Maximal Margin Classifier&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#911-what-is-a-hyperplane&quot; id=&quot;markdown-toc-911-what-is-a-hyperplane&quot;&gt;9.1.1. What Is a Hyperplane?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#912-classification-using-a-separating-hyperplane&quot; id=&quot;markdown-toc-912-classification-using-a-separating-hyperplane&quot;&gt;9.1.2. Classification Using a Separating Hyperplane&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#913-the-maximal-margin-classifier&quot; id=&quot;markdown-toc-913-the-maximal-margin-classifier&quot;&gt;9.1.3. The Maximal Margin Classifier&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#914-construction-of-the-maximal-margin-classifier&quot; id=&quot;markdown-toc-914-construction-of-the-maximal-margin-classifier&quot;&gt;9.1.4. Construction of the Maximal Margin Classifier&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#915-the-non-separable-case&quot; id=&quot;markdown-toc-915-the-non-separable-case&quot;&gt;9.1.5. The Non-separable Case&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#92-support-vector-classifiers&quot; id=&quot;markdown-toc-92-support-vector-classifiers&quot;&gt;9.2. Support Vector Classifiers&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#921-overview-of-the-support-vector-classifier&quot; id=&quot;markdown-toc-921-overview-of-the-support-vector-classifier&quot;&gt;9.2.1. Overview of the Support Vector Classifier&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#922-details-of-the-support-vector-classifier&quot; id=&quot;markdown-toc-922-details-of-the-support-vector-classifier&quot;&gt;9.2.2. Details of the Support Vector Classifier&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#93-support-vector-machines&quot; id=&quot;markdown-toc-93-support-vector-machines&quot;&gt;9.3. Support Vector Machines&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#931-classification-with-non-linear-decision-boundaries&quot; id=&quot;markdown-toc-931-classification-with-non-linear-decision-boundaries&quot;&gt;9.3.1. Classification with Non-Linear Decision Boundaries&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#932-the-support-vector-machine&quot; id=&quot;markdown-toc-932-the-support-vector-machine&quot;&gt;9.3.2. The Support Vector Machine&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#94-svms-with-more-than-two-classes&quot; id=&quot;markdown-toc-94-svms-with-more-than-two-classes&quot;&gt;9.4. SVMs with More than Two Classes&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#941-one-versus-one-classification&quot; id=&quot;markdown-toc-941-one-versus-one-classification&quot;&gt;9.4.1. One-Versus-One Classification&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#95-relationship-to-logistic-regression&quot; id=&quot;markdown-toc-95-relationship-to-logistic-regression&quot;&gt;9.5. Relationship to Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-9-support-vector-machines&quot;&gt;Chapter 9. Support Vector Machines&lt;/h2&gt;

&lt;h2 id=&quot;91-maximal-margin-classifier&quot;&gt;9.1. Maximal Margin Classifier&lt;/h2&gt;

&lt;h3 id=&quot;911-what-is-a-hyperplane&quot;&gt;9.1.1. What Is a Hyperplane?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;a &lt;em&gt;hyperplane&lt;/em&gt; is a flat affine subspace of dimemsion &lt;em&gt;p-1&lt;/em&gt;, in a &lt;em&gt;p&lt;/em&gt;-dimemsional space.&lt;br /&gt;
The mathematical definition: \(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p = 0\)&lt;br /&gt;
 A hyperplane can be referred as &lt;em&gt;decision boundary&lt;/em&gt;, means that a point \(X=(X_1,X_2,\ldots,X_p)^T\) in &lt;em&gt;p&lt;/em&gt;-dimemsional space satisfies the equation, then &lt;em&gt;X&lt;/em&gt; lies on the hyperplane. Rather, any point $X&amp;gt;0$ or $X&amp;lt;0$ lies to the both side of the hyperplane.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;912-classification-using-a-separating-hyperplane&quot;&gt;9.1.2. Classification Using a Separating Hyperplane&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Suppose that $n\times p$ data matrix &lt;strong&gt;X&lt;/strong&gt; that consists of &lt;em&gt;n&lt;/em&gt; training observations in &lt;em&gt;p&lt;/em&gt;-dimemsional space, $x_i = (x_{i1},x_{i2},\ldots,x_{ip})^T$ falls into two classes, $y_i\in { -1,1 }$. A test observation is a &lt;em&gt;p&lt;/em&gt;-vector of observed features \(x^* = (x_1^* \cdots x_p^*)^T\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Separating Hyperplane&lt;br /&gt;
  \(f(x_i) = 
  \begin{cases}
  \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} &amp;gt; 0 &amp;amp; \mbox{if }y_i = 1,\\
  \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} &amp;lt; 0 &amp;amp; \mbox{if }y_i = -1.
  \end{cases}\)&lt;br /&gt;
  Equivalently, it has the property that&lt;br /&gt;
  \(y_i f(x_i) = y_i(\beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}) &amp;gt; 0\)&lt;br /&gt;
  Classify the test observation \(x*\) based on the sign of&lt;br /&gt;
  \(f(x^*) = \beta_0 + \beta_1 x_1^* + \beta_2 x_2^* + \cdots + \beta_p x_p^*\).&lt;br /&gt;
  if \(f(x^*)\) is positive, assign the test observation to class &lt;em&gt;1&lt;/em&gt; and if \(f(x^*)\) is negative, then assign it to class &lt;em&gt;-1&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Magnitude&lt;/em&gt; of \(f(x^*)\)&lt;br /&gt;
  if \(f(x^*)\) is far from zero, it means that \(x^*\) lies far from the hyperplane, and so we can be confident about our class assignment for it.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;913-the-maximal-margin-classifier&quot;&gt;9.1.3. The Maximal Margin Classifier&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Separating hyperplanes have no unique solution, there are infinite number of $\beta$. The &lt;em&gt;maximal margin hyperplane&lt;/em&gt; (or &lt;em&gt;optimal separating hyperplane&lt;/em&gt;) is the separating hyperplane that is farthest from the training observations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;margin&lt;/em&gt; is the smallest perpendicular distance from the observations to the hyperplane and the maximal margin hyperplane is the separating hyperplane for which the margin is largest. Then we classify a test observation based on which side of the maximal margin hyperplane it lies, is the &lt;em&gt;maximal margin classifier&lt;/em&gt;. The observations equidistant from the maximal margin hyperplane are the &lt;em&gt;support vectors&lt;/em&gt;.
    &lt;h3 id=&quot;914-construction-of-the-maximal-margin-classifier&quot;&gt;9.1.4. Construction of the Maximal Margin Classifier&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The maximal margin hyperplane is the solution to the optimization problem:&lt;br /&gt;
  \(\text{max}_{\beta_0,\beta_1,\ldots,\beta_p,M}M\) subject to \(\sum_{j=1}^p\beta_j^2=1,\)&lt;br /&gt;
  \(y_i(\beta_0+\beta_1 x_{i1}+\beta_2 x_{i2}+\cdots+\beta_p x_{ip})\ge M\) \(\forall i=1,\cdots,n.\)&lt;br /&gt;
  where the constraint $\beta^T\beta=1$ means that it is the unique solution of &lt;strong&gt;\(\beta\)&lt;/strong&gt;, the perpendicular distance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;E.g. in &lt;em&gt;2&lt;/em&gt;-dimemsion space:&lt;br /&gt;
  a hyperplane $\beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0$,&lt;br /&gt;
  the distance \(M = \frac{|\beta_0+\beta_1 X_1+\beta_2 X_2|}{\sqrt{\beta_1^2+\beta_2^2}}\)&lt;br /&gt;
  with the constraint $\beta^T\beta=1$,&lt;br /&gt;
  \(M = |\beta_0+\beta_1 X_1+\beta_2 X_2|\), that is,&lt;br /&gt;
  \(y_i(\beta_0+\beta_1 x_{i1}+\beta_2 x_{i2})&amp;gt;0\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;915-the-non-separable-case&quot;&gt;9.1.5. The Non-separable Case&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The maximal margin classifier is a very natural way to perform classification, &lt;em&gt;if a separating hyperplane exists&lt;/em&gt;. In cases no separating hyperplane exists, there is no maximal margin classifier and the optimization problem has no solution with $M&amp;gt;0$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;92-support-vector-classifiers&quot;&gt;9.2. Support Vector Classifiers&lt;/h2&gt;

&lt;h3 id=&quot;921-overview-of-the-support-vector-classifier&quot;&gt;9.2.1. Overview of the Support Vector Classifier&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A classifier based on a separating hyperplane will perfectly classify all of the training observations, can lead to overfit; sensitivity to individual observations. Thus, we consider a classifier based on a hyperplane that does not perfectly separate the two classes, in the interest of &lt;strong&gt;greater robustness and better classification&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;support vector classifier&lt;/em&gt;, or a &lt;em&gt;soft margin classifier&lt;/em&gt;:&lt;br /&gt;
Rather than seeking the largest possible margin with every observation is on the correct side of the hyperplane or margin, we allow some violations; observations to be on the incorrect side of the hyperplane or margin.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;922-details-of-the-support-vector-classifier&quot;&gt;9.2.2. Details of the Support Vector Classifier&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The support vector classifier is the solution to the optimization problem:&lt;br /&gt;
\(\begin{align*}
\text{max}_{\beta_0,\beta_1,\ldots,\beta_p,\epsilon_1,\ldots,\epsilon_n,M}M
\end{align*}\) 
subject to \(\sum_{j=1}^p\beta_j^2=1,\)&lt;br /&gt;
\(y_i(\beta_0+\beta_1 x_{i1}+\beta_2 x_{i2}+\cdots+\beta_p x_{ip})\ge M(1-\epsilon_i)\), \(\epsilon_i\ge 0\), \(\sum_{i=1}^n\epsilon_i\le C\),&lt;br /&gt;
where &lt;em&gt;C&lt;/em&gt; is a nonnegative tuning parameter and &lt;em&gt;M&lt;/em&gt; is the width of the maximizing margin. &lt;strong&gt;$\epsilon$&lt;/strong&gt; are the &lt;em&gt;slack variables&lt;/em&gt; that allow individual observations to be on the wrong side. Then we classify a test observation \(x^*\) based on the sign of \(f(x^*)=\beta_0 + \beta_1 x_1^* + \cdots + \beta_p x_p^*\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The slack variable $\epsilon_i$:&lt;br /&gt;
If $\epsilon_i=0$ then the &lt;em&gt;i&lt;/em&gt;th observation is on the correct side of the margin and 
if $\epsilon_i&amp;gt;0$ then he &lt;em&gt;i&lt;/em&gt;th observation is on the wrong side of the margin; we say that the &lt;em&gt;i&lt;/em&gt;th observation has violated the margin. If $\epsilon_i&amp;gt;1$ then it is on the wrong side of the hyperplane.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The tuning parameter &lt;em&gt;C&lt;/em&gt;:&lt;br /&gt;
It bounds the sum of the $\epsilon_i$’s, determines the number and severity of the violations to the margin(and hyperplane) that we will tolerate. It can be referred as a &lt;em&gt;budget&lt;/em&gt; for the amount that the margin can be violated by the &lt;em&gt;n&lt;/em&gt; observations. For $C&amp;gt;0$ no more than &lt;em&gt;C&lt;/em&gt; observations can be on the wrong side of the hyperplane. As &lt;em&gt;C&lt;/em&gt; increases, the model will become more robust and the margin will widen. In practice, &lt;em&gt;C&lt;/em&gt; is chosen via cross-validation and it controls the bias-variance trade-off.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;support vectors&lt;/em&gt;, only observations that either lie on the margin or that violate the margin will affect the hyperplane and the classifier. In other words, an observation that lies strictly on the correct side of the margin does not affect the support vector classifier. Changing the position of that (correct) observation would not change the classifier at all. The fact that only support vectors affect the classifier is in line with our previous assertion that C controls the bias-variance trade-off of the support vector classifier.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the support vector classifier’s decision rule is based only on a potentially small subset of the training observations, it is quite robust to the behavior of observations that are far away from the hyperplane.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;93-support-vector-machines&quot;&gt;9.3. Support Vector Machines&lt;/h2&gt;

&lt;h3 id=&quot;931-classification-with-non-linear-decision-boundaries&quot;&gt;9.3.1. Classification with Non-Linear Decision Boundaries&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The support vector classifier is for the setting that the decision boundary between the two classes is linear. In non-linear cases, we consider enlarging the feature space using functions of the predictors, such as quadratic and cubic terms.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;932-the-support-vector-machine&quot;&gt;9.3.2. The Support Vector Machine&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;support vector machine&lt;/em&gt; (SVM) is an extension of the support vector classifier that results from enlarging the feature space in a specific way, using &lt;em&gt;kernels&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The linear support vector classifier can be represented as:&lt;br /&gt;
\(f(x) = \beta_0+\sum_{i=1}^n\alpha_i\left\langle x,x_i \right\rangle\),&lt;br /&gt;
where there are &lt;em&gt;n&lt;/em&gt; parameters $\alpha_i$, one per training observation. To estimate the parameters $\alpha_i$ and $\beta_0$, all we need are the \({n choose 2}\) inner products \(\left\langle x,x_i \right\rangle\) between all pairs of training observations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In order to evaluate the function $f(x)$, we need to compute the inner product between the new point $x$ and each of the training points $x_i$. However, it turns out that $\alpha_i$ is nonzero only for the support vectors in the solution; that is, if a training observation is not a support vector, then its $\alpha_i$ equals zero. So if &lt;em&gt;S&lt;/em&gt; is the collection of indices of these support points, we can rewrite as:&lt;br /&gt;
\(f(x) = \beta_0+\sum_{i\in\mathcal{S}}\alpha_i\left\langle x,x_i \right\rangle\). To summarize, in representing the linear classifier $f(x)$, and in computing its coefficients, all we need are inner products.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now we replace the inner product with a &lt;em&gt;generalization&lt;/em&gt; of the form \(K(x_i,x_{i'})\), where &lt;em&gt;K&lt;/em&gt; is some function referred as a &lt;em&gt;kernel&lt;/em&gt;, quantifies the similarity of two observations. When the support vector classifier is combined with a non-linear kernel, with the function of polynomial or radial, the resulting classifier is known as a support vector machine. The function has the form:&lt;br /&gt;
\(f(x) = \beta_0+\sum_{i\in\mathcal{S}}\alpha_i K(x,x_i)\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The advantage of using a kernel&lt;br /&gt;
Rather than simply enlarging the feature space using functions of the original features, one advantage is computational, and it amounts to the fact that using kernels, one need only compute kernel function for all $n(n-1)/2$ distinct pairs $i,i’$. This can be done without explicitly working in the enlarged feature space, so large that computations are intractable. For some kernels, the feature space is &lt;em&gt;implicit&lt;/em&gt; and infinite-dimensional, the computations are impossible.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;94-svms-with-more-than-two-classes&quot;&gt;9.4. SVMs with More than Two Classes&lt;/h2&gt;

&lt;h3 id=&quot;941-one-versus-one-classification&quot;&gt;9.4.1. One-Versus-One Classification&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;An alternative procedure for applying SVMs. Fit &lt;em&gt;K&lt;/em&gt; SVMs, each time comparing one of the &lt;em&gt;K&lt;/em&gt; classes to the remaining &lt;em&gt;K−1&lt;/em&gt; classes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Let \(\beta_{0k},\beta_{1k},\ldots,\beta_{pk}\) denote the parameters that result from fitting an SVM comparing the &lt;em&gt;k&lt;/em&gt;th class(coded as +1) to the others (coded as −1)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For a test observation \(x^*\), assign it to the class for which \(\beta_{0k}+\beta_{1k}x_1^*+\ldots+\beta_{pk}x_p^*\) is largest, as this amounts to a high level of confidence that the test observation belongs to the &lt;em&gt;k&lt;/em&gt;th class rather than to any of the other classes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;95-relationship-to-logistic-regression&quot;&gt;9.5. Relationship to Logistic Regression&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Rewrite the criterion for fitting the support vector classifier as&lt;br /&gt;
\(\text{min}_{\beta_0,\beta_1,\ldots,\beta_p}\left\{\sum_{i=1}^n\text{max}\left[0, 1- y_i f(x_i) \right] + \lambda\sum_{j=1}^p\beta_j^2 \right\}\),&lt;br /&gt;
where $\lambda$ is a nonnegative tuning parameter. When $\lambda$ is large then the coefficients $\beta_j$’s are small, more violations to the margin are tolerated, and a low-variance but high-bias classifier will result. Thus, a small value of $\lambda$ amounts to a small value of &lt;em&gt;C&lt;/em&gt; in support vector classifier. Note that \(\lambda\sum_{j=1}^p\beta_j^2\) term is the ridge penalty, plays a similar role in controlling the bias-variance trade-off for the support vector classifier.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now taktes the “Loss + Penalty” form:&lt;br /&gt;
\(\text{min}_{\beta_0,\beta_1,\ldots,\beta_p}\left\{L(\mathbf{X},\mathbf{y},\beta)+\lambda P(\beta)\right\}\).&lt;br /&gt;
Left side term is some loss function quantifying the extent to which the model, parametrized by $\beta$, fits the data (&lt;strong&gt;X&lt;/strong&gt;,&lt;strong&gt;y&lt;/strong&gt;). Right side term is a penalty function on the parameter vector $\beta$ whose effect is controlled by a nonnegative tuning parameter $\lambda$. For support vector machine, the loss function takes the form:&lt;br /&gt;
\(L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^n\text{max}\left[0,1- y_i(\beta_0+\beta_1 x_{i1}+\cdots+\beta_p x_{ip})\right]\).&lt;br /&gt;
This is known as &lt;em&gt;hinge loss&lt;/em&gt;. It turns out that the hinge loss function is closely related to the loss function used in logistic regression.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The only support vectors play a role in the classifier obtained; observations on the correct side of the margin do not affect it. This is due to the fact that the hinge loss function is exactly zero for observations for which \(y_i(\beta_0+\beta_1 x_{i1}+\cdots+\beta_p x_{ip})\ge 1\), the observations that are on the correct side of the margin. In contrast, the loss function for logistic regression is not exactly zero anywhere. But it is very small for observations that are far from the decision boundary. Due to the similarities between two loss functions, they often give very similar results. When the classes are well separated, SVMs tend to behave better than logistic regression; in more overlapping regimes, logistic regression is often preferred.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The tuning parameter &lt;em&gt;C&lt;/em&gt; was first thought to be an unimportant “nuisance” parameter that could be set to some default value, like 1. However, the “Loss + Penalty” formulation indicates that this is not the case. Now we see that $\lambda$ or &lt;em&gt;C&lt;/em&gt;, determines the extent to which the model underfits or overfits the data.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        
        <pubDate>Wed, 20 May 2020 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch9</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch9</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 8. Tree-Based Methods</title>
        
          <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#chapter-8-tree-based-methods&quot; id=&quot;markdown-toc-chapter-8-tree-based-methods&quot;&gt;Chapter 8. Tree-Based Methods&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#81-the-basics-of-decision-trees&quot; id=&quot;markdown-toc-81-the-basics-of-decision-trees&quot;&gt;8.1. The Basics of Decision Trees&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#811-regression-trees&quot; id=&quot;markdown-toc-811-regression-trees&quot;&gt;8.1.1. Regression Trees&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#prediction-via-stratification-of-the-feature-space&quot; id=&quot;markdown-toc-prediction-via-stratification-of-the-feature-space&quot;&gt;Prediction via Stratification of the Feature Space&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#tree-pruning&quot; id=&quot;markdown-toc-tree-pruning&quot;&gt;Tree Pruning&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#812-classification-trees&quot; id=&quot;markdown-toc-812-classification-trees&quot;&gt;8.1.2. Classification Trees&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#813-trees-versus-linear-models&quot; id=&quot;markdown-toc-813-trees-versus-linear-models&quot;&gt;8.1.3. Trees Versus Linear Models&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#814-advantages-and-disadvantages-of-trees&quot; id=&quot;markdown-toc-814-advantages-and-disadvantages-of-trees&quot;&gt;8.1.4. Advantages and Disadvantages of Trees&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#82-bagging-random-forests-boosting-and-bayesian-additive-regression-trees&quot; id=&quot;markdown-toc-82-bagging-random-forests-boosting-and-bayesian-additive-regression-trees&quot;&gt;8.2. Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#821-bagging&quot; id=&quot;markdown-toc-821-bagging&quot;&gt;8.2.1. Bagging&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#out-of-bag-error-estimation&quot; id=&quot;markdown-toc-out-of-bag-error-estimation&quot;&gt;Out-of-Bag Error Estimation&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#variable-importance-measures&quot; id=&quot;markdown-toc-variable-importance-measures&quot;&gt;Variable Importance Measures&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#822-random-forests&quot; id=&quot;markdown-toc-822-random-forests&quot;&gt;8.2.2. Random Forests&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#permutation-importance&quot; id=&quot;markdown-toc-permutation-importance&quot;&gt;Permutation Importance&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#823-boosting&quot; id=&quot;markdown-toc-823-boosting&quot;&gt;8.2.3. Boosting&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#824-bayesian-additive-regression-trees&quot; id=&quot;markdown-toc-824-bayesian-additive-regression-trees&quot;&gt;8.2.4. Bayesian Additive Regression Trees&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#825-summary-of-tree-ensemble-methods&quot; id=&quot;markdown-toc-825-summary-of-tree-ensemble-methods&quot;&gt;8.2.5. Summary of Tree Ensemble Methods&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-8-tree-based-methods&quot;&gt;Chapter 8. Tree-Based Methods&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Background&lt;br /&gt;
  Tree by binary question, splitting variables with split points.&lt;br /&gt;
  Partitions of input space &amp;amp; fit a constant to each one.&lt;br /&gt;
  $X = (X_1,\ldots,X_p)$ to $R_1,\ldots,R_M$ regions, &lt;em&gt;p&lt;/em&gt;-dimension rectangles.&lt;br /&gt;
  where \(R_m\cap R_{m'}, m\ne m', \bigcup_{m=1}^M R_m = \mathbb{X}.\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model:&lt;br /&gt;
  \(f(X)=\sum_{m=1}^M C_m I(X\in R_m)\)&lt;br /&gt;
  where $C_m$ is a constant for each region &lt;em&gt;m&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;81-the-basics-of-decision-trees&quot;&gt;8.1. The Basics of Decision Trees&lt;/h2&gt;

&lt;h3 id=&quot;811-regression-trees&quot;&gt;8.1.1. Regression Trees&lt;/h3&gt;

&lt;h4 id=&quot;prediction-via-stratification-of-the-feature-space&quot;&gt;Prediction via Stratification of the Feature Space&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Two steps of building a regression tree:
    &lt;ol&gt;
      &lt;li&gt;Divide the predictor space; the set of possible values for $X_1, \ldots, X_p$ 
 into &lt;em&gt;J&lt;/em&gt; distinct and non-overlapping regions; $R_1, \ldots, R_J$.&lt;/li&gt;
      &lt;li&gt;For every observations that falls into the region $R_j$, we make the same 
 prediction, which is simply the mean of the response values for the training 
 observations in $R_j$.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To divide the predictor space into high-dimensional rectangles, or &lt;em&gt;boxes&lt;/em&gt;, our goal 
  is to find the boxes that minimize the RSS, given by:&lt;br /&gt;
  \(\sum_{j=1}^J\sum_{i\in R_j}(y_i-\hat{y}_{R_j})^2\)&lt;br /&gt;
  where $\hat{y}_{R_j}$ is the mean response for the training observations within 
  the &lt;em&gt;j&lt;/em&gt;th box.&lt;br /&gt;
  I.e., the Least Squares Criterion in given regions:&lt;br /&gt;
  \(\begin{align*}
  \text{min}_{C_m}\sum_{i=1}^N RSS &amp;amp;= \text{min}_{C_m}\sum_{i=1}^N(y_i-f(x_i))^2 \\
      &amp;amp;= \text{min}_{C_m}\sum_{i=1}^N\left[y_i - \sum_{m=1}^M C_m I(x_i\in R_m) \right]^2 \\
  \rightarrow \hat{C}_m &amp;amp;= \text{ave}(y_i|x_i\in R_m) = \bar{y}_m
  \end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;How to split into spaces?&lt;br /&gt;
  For its computational infeasibility, we take a &lt;em&gt;top-down&lt;/em&gt;, &lt;em&gt;greedy&lt;/em&gt; approach that 
  it known as &lt;em&gt;recursive binary splitting&lt;/em&gt;. To perform recursive binary splitting, 
  we first select the predictor $X_j$ and the cutpoint &lt;em&gt;s&lt;/em&gt; such that splitting the 
  predictor spaace in to the regions \(\{X|X_j&amp;lt;s\}\) and \(\{X|X_j\ge s\}\) leads to 
  the greatest possible reduction in RSS.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For splitting variable $X_j$ and split point &lt;em&gt;s&lt;/em&gt;,&lt;br /&gt;
  In two binary partitions \(R_1(j,s) = \{X|X_j&amp;lt;s\}\) and \(R_2(j,s) = \{X|X_j\ge s\}\),&lt;br /&gt;
  \(\text{min} \left[ \sum_{i:x_i\in R_1(j,s)}(y_i-\hat{y}_{R_1})^2 + \sum_{i:x_i\in R_2(j,s)}(y_i-\hat{y}_{R_2})^2 \right]\), or&lt;br /&gt;
  \(\text{min}_{C_1}\sum_{i:x_i\in R_1(j,s)}(y_i-C_1)^2 + \min_{C_2}\sum_{i:x_i\in R_2(j,s)}(y_i-C_2)^2\).&lt;br /&gt;
  \(\begin{align*}
  \rightarrow &amp;amp; \text{ave}(y_i|x_i\in R_1(j,s)) = \hat{C}_1 = \hat{y}_{R_1} \\
              &amp;amp; \text{ave}(y_i|x_i\in R_2(j,s)) = \hat{C}_2 = \hat{y}_{R_2}
  \end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;We repeat this process over the two previously identified regions, looking for the 
  best predictor and best cutpoint in order to split the data further so as the minimize 
  the RSS within each of the resulting regions. The process continues until a stopping 
  criterion is reached; for instance, until no region contains more than five observations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;tree-pruning&quot;&gt;Tree Pruning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tree size is based on the number of regions(&lt;em&gt;M&lt;/em&gt;) and the model complexity is on the
  number of parameters($C_m$). Since there is an extremely large number of possible 
  subtrees, estimating the CV error for every possible subtree would be too cumbersome. 
  Instead, to find the optimal &lt;em&gt;M&lt;/em&gt;, we use &lt;em&gt;Cost-Complexity Pruning&lt;/em&gt;, reducing from a 
  very large tree $T_0$ to a subtree that has the lowest test error rate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each value of a nonnegative tuning parameter $\alpha$ there corresponds a subtree 
  $T\in T_0$, such that minimizing&lt;br /&gt;
  \(\sum_{m=1}^{|T|}\sum_{i:x_i\in R_m}(y_i-\hat{y}_{R_m})^2 + \alpha|T|\).&lt;br /&gt;
  Here |&lt;em&gt;T&lt;/em&gt;| indicates the number of terminal nodes of the tree &lt;em&gt;T&lt;/em&gt;, $R_m$ is the 
  rectangle, or the subset of predictor space corresponding to the &lt;em&gt;m&lt;/em&gt;th terminal node, 
  and $\hat{y}_{R_m} = \hat{C}_m$ is the predicted response, the mean of the training 
  observations in $R_m$. When $\alpha=0$, then the subtree &lt;em&gt;T&lt;/em&gt; will simply equal $T_0$. 
  As $\alpha$ increases, there is a price to pay for having a tree with many terminal 
  nodes, and so the quantity will tend to be minimized for a smaller subtree.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Algorithm&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;Use recursive binary splitting to grow a large tree, stopping only when each 
 terminal node has fewer than some minimum number of observations, threshold.&lt;/li&gt;
      &lt;li&gt;Apply cost complexity pruning to obtain a sequence of best subtrees, as a function 
 of $\alpha$.&lt;/li&gt;
      &lt;li&gt;Use K-fold CV to choose $\alpha$:&lt;br /&gt;
 (a) Repeat Steps 1 and 2 on all but &lt;em&gt;k&lt;/em&gt;th fold of the training data.&lt;br /&gt;
 (b) Evaluate the mean squared prediction error on the data in the left-out &lt;em&gt;k&lt;/em&gt;th 
 fold, as a function of $\alpha$.&lt;br /&gt;
 Average the results for each value of $\alpha$, and pick a $\alpha$ to minimize 
 the average error.&lt;/li&gt;
      &lt;li&gt;Return the subtree from Step 2 that corresponds to the chosen value of $\alpha$.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;812-classification-trees&quot;&gt;8.1.2. Classification Trees&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of the mean response of the training observations that belong to the same 
  terminal node, a classification tree predict that each observation in the region 
  to the most commonly occurring class of training observations in the region to 
  which it belongs. In interpreting the result, we are often interested not only in 
  the class prediction, but also in the class proportions among the training observations 
  that fall into that region.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A natural alternative to RSS, the classification error rate; the fraction of the 
  training observations in that region that do not belong to the most common class:&lt;br /&gt;
  \(E=1-\text{max}_k(\hat{p}_{mk})\), where \(\hat{p}_{mk} = \frac{1}{N_m}\sum_{x_i\in R_m}I(y_i=k)\),&lt;br /&gt;
  represents the proportion of training observations in the &lt;em&gt;m&lt;/em&gt;th region that are from 
  the &lt;em&gt;k&lt;/em&gt;th class. By majority vote rule, \(k(m) = \text{argmax}_k \hat{p}_mk\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;However, the classification error is not sufficiently sensitive for tree-growing, 
  in practice we use two other measures preferable.
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Gini index&lt;/em&gt;:&lt;br /&gt;
  \(G=\sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})\),&lt;br /&gt;
  a measure of total variance across the &lt;em&gt;K&lt;/em&gt; classes. It is often referred to as 
  a measure of node &lt;em&gt;purity&lt;/em&gt;; a small value indicates that a node contains predominantly 
  observations from a single class.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Cross-entropy&lt;/em&gt; or deviance:&lt;br /&gt;
  \(D=-\sum_{k=1}^K\hat{p}_{mk}\log\hat{p}_{mk}\),&lt;br /&gt;
  a nonnegative value that take on a small value if the &lt;em&gt;m&lt;/em&gt;th node is pure.&lt;br /&gt;
  these measures are typically used to evaluate the quality of a particular split, 
  since they are more sensitive to node purity than is the classification error rate. 
  The classification error rate is preferable if prediction accuracy of the final 
  pruned tree is the goal.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Being performed to increase node purity, some of the splits yield two terminal nodes 
  that have the same predicted value. Even though such splits do not reduce the classification 
  error, it improves the Gini index and the entropy.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;813-trees-versus-linear-models&quot;&gt;8.1.3. Trees Versus Linear Models&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Linear regression model is:&lt;br /&gt;
  \(f(X) = \beta_0 + \sum_{j=1}^p X_j\beta_j\),&lt;br /&gt;
  whereas Regression tree model is:&lt;br /&gt;
  \(f(X) = \sum_{m=1}^M c_m\cdot 1_{(X\in R_m)}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If there is a highly non-linear and complex relationship between the features and 
  the response, then decision trees may outperform classical approaches. Also, trees 
  may be preffered for the sake of interpretability and visualization.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;814-advantages-and-disadvantages-of-trees&quot;&gt;8.1.4. Advantages and Disadvantages of Trees&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Pros
    &lt;ul&gt;
      &lt;li&gt;Good interpretability and easy to explain.&lt;/li&gt;
      &lt;li&gt;Closely mirror human decision-making than do some of the regression and classification 
  approaches.&lt;/li&gt;
      &lt;li&gt;Can be displayed graphically, and are easily interpreted even by a non-expert.&lt;/li&gt;
      &lt;li&gt;Can easily handle qualitative predictors without the need to create dummy variables.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cons
    &lt;ul&gt;
      &lt;li&gt;Do not have the same level of predictive accuracy as the regression and classification 
  approaches.&lt;/li&gt;
      &lt;li&gt;Non-robust, a small change in the data can cause a large change in the final 
  estimated tree(High variance).&lt;/li&gt;
      &lt;li&gt;Lack of smoothness, can be far from the true function.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By aggregating many decision trees, the predictive performance of trees can be 
	substantially improved.&lt;/p&gt;

&lt;h2 id=&quot;82-bagging-random-forests-boosting-and-bayesian-additive-regression-trees&quot;&gt;8.2. Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;An &lt;em&gt;ensemble&lt;/em&gt; method is an approach that combines many simple “building ensemble block” 
  models in order to obtain a single and potentially very powerful model. These simple 
  building block models are sometimes known as &lt;em&gt;weak learners&lt;/em&gt;, since they may lead 
  to mediocre predictions on their own.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;821-bagging&quot;&gt;8.2.1. Bagging&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Bootstrap aggregation&lt;/em&gt;, or &lt;em&gt;bagging&lt;/em&gt;, is a general-purpose procedure for reducing 
  the variance of a statistical learning method. Given a set of &lt;em&gt;n&lt;/em&gt; independent 
  observations $Z_1,\ldots,Z_n$, each with variance $\sigma^2$, the variance of the 
  mean $\bar{Z}$ of the observations is given by $\sigma^2/n$. In other words, 
  &lt;em&gt;averaging a set of observations reduces variance&lt;/em&gt;.&lt;br /&gt;
  Hence it is a natural way to reduce the variance and increase the test set accuracy 
  of a statistical learning method. We could calculate \(\hat{f}^1(x), \ldots, \hat{f}^B(x)\) 
  using &lt;em&gt;B&lt;/em&gt; seperate training sets, and average them in order to obtain a single 
  low-variance model, \(\hat{f}_{\text{avg}}(x) = \frac{1}{B}\sum_{b=1}^B\hat{f}^b(x)\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, we do not have access to multiple training sets. Instaed, we can bootstrap, 
  by taking repeated samples from the (single) training data set. The bagging model,&lt;br /&gt;
  \(\hat{f}_{\text{bag}}(x) = \frac{1}{B}\sum_{b=1}^B\hat{f}^{*b}(x)\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is particularly useful for decision trees. To apply begging to regression trees, 
  we simply construct &lt;em&gt;B&lt;/em&gt; regression trees using &lt;em&gt;B&lt;/em&gt; bootstrapped training sets, and 
  average the resulting predictions. These trees are grown deep but not pruned. Each 
  individual tree has high variance but low bias. Averaging these trees reduces the 
  variance. Bagging improves the accuracy by combining hundreds or even thousands of 
  trees into a single procedure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To a classification problem where &lt;em&gt;Y&lt;/em&gt; is qualitative, the simplest approach is the
  &lt;em&gt;majority vote&lt;/em&gt;; For a given test observation, record the class predicted by each 
  of the &lt;em&gt;B&lt;/em&gt; trees and the overall prediction is the most commonly occurring class 
  among those predictions.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;out-of-bag-error-estimation&quot;&gt;Out-of-Bag Error Estimation&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Without cross-validation or the validation set approach, there is a very straightforward 
  way to estimate the test error of a bagged model. The reamining observations not 
  used to fit a given bagged tree are referred to as the &lt;em&gt;out-of-bag&lt;/em&gt;(OOB) observations. 
  We can predict the response for the &lt;em&gt;i&lt;/em&gt;th observation using each of the trees in 
  which that observation was OOB. To obtain a single prediction for the &lt;em&gt;i&lt;/em&gt;th observation, 
  we average these predicted responses to a regression set, or take a majority vote 
  to a classification set. An OOB prediction can be obtained in this way for every 
  &lt;em&gt;n&lt;/em&gt; observations, from which the overall OOB MSE or classification error can be computed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sinse the response for each observation is predicted using only the trees that were 
  not fit using that observation, the resulting OOB error is a valid estimate of the 
  test error for the bagged model.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;variable-importance-measures&quot;&gt;Variable Importance Measures&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Bagging improves prediction accuracy at the expense of interpretability. One can obtain 
  an overall summary of the importance of each predictor using the RSS or the Gini 
  index. In the case of bagging regression trees, we can record the total amount that 
  the RSS is decreased due to splits over a given predictor, averaged over all &lt;em&gt;B&lt;/em&gt; 
  trees. A large value indicates an important predictor.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;822-random-forests&quot;&gt;8.2.2. Random Forests&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If there is one very strong predictor along with a number of other moderately strong 
  predictors, in bagging method, most trees will use this strong predictor in the top 
  split. Consequently, all of the bagged trees will look quite similar and the predictions 
  from the bagged trees will be highly correlated. Averaging highly correlated quantities 
  does not lead to a substantial reduction in variance over a single tree in this setting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Random forests overcome this problem by forcing each split to consider only a subset 
  of the predictors. As in bagging, we build a number of decision trees on bootstrapped 
  training samples. But when building these decision trees, each time a split in a 
  tree is considered, &lt;em&gt;a random sample of m predictors&lt;/em&gt; is chosen as split candidates 
  from the full set of &lt;em&gt;p&lt;/em&gt; predictors. Typically, the number of predictors considered 
  at each split &lt;em&gt;m&lt;/em&gt; is approximately equal to the square root of the total number of 
  predictors &lt;em&gt;p&lt;/em&gt;; $m\approx\sqrt{p}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On average $(p-m)/p$ of the splits will not even consider the strong predictor, and 
  so other predictors will have more of a chance. This process is &lt;em&gt;decorrelating&lt;/em&gt; 
  the trees, thereby making the average of the resulting trees less variable and more 
  reliable.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;permutation-importance&quot;&gt;Permutation Importance&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Calculate the estimated test error with the original OOB samples as validation data, 
  then randomly suffle; or &lt;em&gt;permute&lt;/em&gt; the order of the &lt;em&gt;j&lt;/em&gt;th variable on OOB sample 
  to break the relationship between $X_j$ and &lt;em&gt;Y&lt;/em&gt;. With this permuted OOB samples, 
  calculate the Permuation Measure. If the result of permuation measure is worse than 
  that of reference measure, we can consider $X_j$ is an important variable. For all 
  variables &lt;em&gt;X&lt;/em&gt;, we can make the rank of variable importance. However, permuation 
  importance can overestimates the importance of correlated predictors.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;823-boosting&quot;&gt;8.2.3. Boosting&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Bagging creates multiple copies of the original training data set using the bootstrap, 
  fits a separate decision tree to each copy and then combines all of the trees in 
  order to create a single predictive model. Each tree is built on a bootstrap data 
  set, independent of the other trees.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Boosting works in a similar way, except that the trees are grown &lt;em&gt;sequentially&lt;/em&gt;: each 
  each tree is grown using information from previously grown trees. This approach 
  does not involve bootstrap sampling; instead each tree is fit on a modified version 
  of the original data set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm
    &lt;ol&gt;
      &lt;li&gt;Set $\hat{f}(x)=0$ and $r_i=y_i$ for all &lt;em&gt;i&lt;/em&gt; in the training set.&lt;/li&gt;
      &lt;li&gt;For $b=1,2,\ldots,B$, repeat:&lt;br /&gt;
 (a) Fit a tree $\hat{f}^b$ with &lt;em&gt;d&lt;/em&gt; splits ($d+1$ terminal nodes) to the 
 training data (&lt;em&gt;X,r&lt;/em&gt;).&lt;br /&gt;
 (b) Update $\hat{f}$ by adding in a shrunken version of the new tree:&lt;br /&gt;
 \(\hat{f}(x)\leftarrow\hat{f}(x)+\lambda\hat{f}^b(x)\).&lt;br /&gt;
 (c) Update the residuals, removing the effect of &lt;em&gt;b&lt;/em&gt;th tree,&lt;br /&gt;
 Unexplained residuals \(r_i\leftarrow r_i - \lambda\hat{f}^b(x_i)\).&lt;/li&gt;
      &lt;li&gt;Output the boosted model,&lt;br /&gt;
 \(\hat{f}(x)=\sum_{i=1}^B\lambda\hat{f}^b(x)\).&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Boosting combines multiple weak learners(poor prediction models) and make better 
  prediction. In procedure, data(the weights of observations) are repeatedly modified 
  and the model &lt;em&gt;learns slowly&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Boosting tuning parameters
    &lt;ol&gt;
      &lt;li&gt;&lt;em&gt;B&lt;/em&gt; the number of trees. Unlike bagging and random forests, boosting can overfit 
 if &lt;em&gt;B&lt;/em&gt; is too large, although this overfitting tends to occur slowly if at all. 
 We use cross-validation to select &lt;em&gt;B&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;$\lambda$ the shrinkage parameter, a small positive number. This controls the 
 rate at which boosting learns. Typical values are 0.01 or 0.001. Very small 
 $\lambda$ can require using a very large value of &lt;em&gt;B&lt;/em&gt; to achieve good performance.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;d&lt;/em&gt; the number of splits in each tree, which controls the complexity of the boosted 
 ensemble. $d=1$ works well, in which case each tree is a &lt;em&gt;stump&lt;/em&gt;, consisting 
 of a single split. In this case, the boosted ensemble is fitting an additive 
 model, each term involves only a single variable. Generally &lt;em&gt;d&lt;/em&gt; is the 
 &lt;em&gt;interaction depth&lt;/em&gt;, and controls the interaction order of the boosted model.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;824-bayesian-additive-regression-trees&quot;&gt;8.2.4. Bayesian Additive Regression Trees&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Bagging and random forests make predictions from an average of independent trees, 
  while boosting uses a weighted sum of trees. BART is related to both approaches: 
  each tree is constructed in a random manner and each tree tries to caputre signal 
  not yet accounted for by the current model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;let &lt;em&gt;K&lt;/em&gt; denote the number of regression trees, and &lt;em&gt;B&lt;/em&gt; the number of iterations for 
  which the BART will be run. The notation $\hat{f}_k^b(x)$ represents the prediction 
  at &lt;em&gt;x&lt;/em&gt; for the &lt;em&gt;k&lt;/em&gt;th regression tree used in the &lt;em&gt;b&lt;/em&gt;th iteration. At the end of 
  each iteration, the &lt;em&gt;K&lt;/em&gt; trees from that iteration will be summed; 
  \(\hat{f}^b(x)=\sum_{k=1}^K\hat{f}_k^b(x)\) for $b=1,\ldots,B$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;First iteration, all trees are initialized to have a single root node, with 
 \(\hat{f}_k^1(x) = \frac{1}{nK}\sum_{i=1}^n y_i\), the mean of the response values 
 divided by the total number of trees. Thus, 
 \(\hat{f}^1(x)=\sum_{k=1}^K\hat{f}_k^1(x)=\frac{1}{n}\sum_{i=1}^n y_i = \bar{y}\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Next, updates each of the &lt;em&gt;K&lt;/em&gt; trees, one at a time.&lt;br /&gt;
 In the &lt;em&gt;b&lt;/em&gt;th iteration, we subtract from each response value the predictions from 
 all but the &lt;em&gt;k&lt;/em&gt;th tree to obtain a &lt;em&gt;partial residual&lt;/em&gt;&lt;br /&gt;
 \(r_i = y_i - \sum_{k'&amp;lt;k}\hat{f}_{k'}^b(x_i) - \sum_{k'&amp;gt;k}\hat{f}_{k'}^{b-1}(x_i)\)&lt;br /&gt;
 Rather than fitting a fresh tree to this partial residual, BART randomly chooses 
 a perturbation to the tree from the previous iteration from a set of possible 
 perturbations, favoring ones that improve the fit to the partial residual.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Two components to the perturbation:&lt;br /&gt;
  We may change the structure of the tree by adding or pruning branches.&lt;br /&gt;
  We may change the prediction in each terminal node of the tree.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Output of BART is a collection of prediction models,&lt;br /&gt;
 \(\hat{f}^b(x) = \sum_{k=1}^K\hat{f}_k^b(x)\)&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Typically, the first few of these prediction models in earlier iterations, known 
  as the &lt;em&gt;burn-in&lt;/em&gt; period, tend not to provide very good results. For the number of 
  burn-in iterations &lt;em&gt;L&lt;/em&gt;, for instance we might take &lt;em&gt;L&lt;/em&gt;=200, we simply remove and 
  take the average after &lt;em&gt;L&lt;/em&gt; iterations; \(\hat{f}(x)=\frac{1}{B-L}\sum_{b=L+1}^B\hat{f}^b(x)\). 
  Or we can compute quantities other than the average.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;825-summary-of-tree-ensemble-methods&quot;&gt;8.2.5. Summary of Tree Ensemble Methods&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Bagging: The trees are grown independently on random samples of the observations and 
  tend to be quite similar to each other. Thus, bagging can get caught in local optima 
  and fail to thoroughly explore the model space.&lt;/li&gt;
  &lt;li&gt;Random Forests: The trees are grown independently on random samples but each split 
  on each tree is performed using a random subset of the features, decorrelating the 
  trees and leading to a more thorough exploration of the model space.&lt;/li&gt;
  &lt;li&gt;Boosting: We only use the original data and do not draw any random samples. Trees 
  are grown successively, usiang slow learning approach: each new tree is fit to the 
  signal that is left over from the earlier trees, and shrunken down before it is 
  used.&lt;/li&gt;
  &lt;li&gt;BART: WE only use the original data and trees are grown successively. However, each 
  tree is perturbed in order to avoid local minima and achieve a more thorough exploration 
  of the model space.&lt;/li&gt;
&lt;/ul&gt;
</description>
        
        <pubDate>Wed, 13 May 2020 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch8</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch8</guid>
      </item>
      
    
      
      <item>
        <title>NLP - Text Analysis with ML algorithms</title>
        
          <description>
</description>
        
        <pubDate>Sun, 10 May 2020 07:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/NLP_eng_ml</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/NLP_eng_ml</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 7. Moving Beyond Linearity</title>
        
          <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#chapter-7-moving-beyond-linearity&quot; id=&quot;markdown-toc-chapter-7-moving-beyond-linearity&quot;&gt;Chapter 7. Moving Beyond Linearity&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#71-polynomial-regression&quot; id=&quot;markdown-toc-71-polynomial-regression&quot;&gt;7.1. Polynomial Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#72-step-functions&quot; id=&quot;markdown-toc-72-step-functions&quot;&gt;7.2. Step Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#73-basis-functions&quot; id=&quot;markdown-toc-73-basis-functions&quot;&gt;7.3. Basis Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#74-regression-splines&quot; id=&quot;markdown-toc-74-regression-splines&quot;&gt;7.4. Regression Splines&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#741-piecewise-polynomials&quot; id=&quot;markdown-toc-741-piecewise-polynomials&quot;&gt;7.4.1. Piecewise Polynomials&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#742-constraints-and-splines&quot; id=&quot;markdown-toc-742-constraints-and-splines&quot;&gt;7.4.2. Constraints and Splines&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#743-the-spline-basis-representation&quot; id=&quot;markdown-toc-743-the-spline-basis-representation&quot;&gt;7.4.3. The Spline Basis Representation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#744-choosing-the-number-and-locations-of-the-knots&quot; id=&quot;markdown-toc-744-choosing-the-number-and-locations-of-the-knots&quot;&gt;7.4.4. Choosing the Number and Locations of the Knots&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#745-comparison-to-polynomial-regression&quot; id=&quot;markdown-toc-745-comparison-to-polynomial-regression&quot;&gt;7.4.5. Comparison to Polynomial Regression&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#75-smoothing-splines&quot; id=&quot;markdown-toc-75-smoothing-splines&quot;&gt;7.5. Smoothing Splines&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#751-an-overview-of-smoothing-splines&quot; id=&quot;markdown-toc-751-an-overview-of-smoothing-splines&quot;&gt;7.5.1. An Overview of Smoothing Splines&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#752-choosing-the-smoothing-parameter-lambda&quot; id=&quot;markdown-toc-752-choosing-the-smoothing-parameter-lambda&quot;&gt;7.5.2. Choosing the Smoothing Parameter $\lambda$&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#76-local-regression&quot; id=&quot;markdown-toc-76-local-regression&quot;&gt;7.6. Local Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#77-generalized-additive-models&quot; id=&quot;markdown-toc-77-generalized-additive-models&quot;&gt;7.7. Generalized Additive Models&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#771-gams-for-regression-problems&quot; id=&quot;markdown-toc-771-gams-for-regression-problems&quot;&gt;7.7.1. GAMs for Regression Problems&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#772-gams-for-classification-problems&quot; id=&quot;markdown-toc-772-gams-for-classification-problems&quot;&gt;7.7.2. GAMs for Classification Problems&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#pros-and-cons-of-gams&quot; id=&quot;markdown-toc-pros-and-cons-of-gams&quot;&gt;Pros and Cons of GAMs&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-7-moving-beyond-linearity&quot;&gt;Chapter 7. Moving Beyond Linearity&lt;/h2&gt;

&lt;h2 id=&quot;71-polynomial-regression&quot;&gt;7.1. Polynomial Regression&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;standard linear model $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$&lt;br /&gt;
  to a polynomial function $y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \cdots + \beta_d x_i^d + \epsilon_i$&lt;br /&gt;
  For large enough degree &lt;em&gt;d&lt;/em&gt;, a polynomial regression produces an extremely non-
  linear curve. The coefficients $\beta_d$’s can be easily estimated using least 
  squares linear regression.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;72-step-functions&quot;&gt;7.2. Step Functions&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Break the range of &lt;em&gt;X&lt;/em&gt; into &lt;em&gt;bins&lt;/em&gt;, and fit a different constant in each bin; 
  converting a continuous variables into an &lt;em&gt;ordered categorical variable&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;By cutpoints $c_1, c_2, \ldots, c_K$ in the range of &lt;em&gt;X&lt;/em&gt;, construct &lt;em&gt;K+1&lt;/em&gt; new variables&lt;br /&gt;
  \(\begin{align*}
  C_0(X) &amp;amp;= I(X &amp;lt; c_1), \\
  C_1(X) &amp;amp;= I(c_1 \le X &amp;lt; c_2), \\
  C_2(X) &amp;amp;= I(c_2 \le X &amp;lt; c_3), \\
         &amp;amp;\vdots \\
  C_{K-1}(X) &amp;amp;= I(c_{K-1} \le X &amp;lt; c_K), \\
  C_{K}(X) &amp;amp;= I(c_K \le X), \\
  \end{align*}\)&lt;br /&gt;
  where I($\cdot$) is an &lt;em&gt;indicator function&lt;/em&gt; returning value of 1 or 0. These are 
  sometimes called &lt;em&gt;dummy variables&lt;/em&gt;. For any value of &lt;em&gt;X&lt;/em&gt;, $\sum_{i=0}^K C_i(X) = 1$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The least squares model: $y_i = \beta_0 + \beta_1 C_1(x_i) + \cdots + \beta_K C_K(x_i) + \epsilon_i$&lt;br /&gt;
  When $X&amp;lt;c_1$, all of the predictors $C_1(X), C_2(X), \ldots, C_K(X)$ are zero, so 
  $\beta_0$ is the mean value of &lt;em&gt;Y&lt;/em&gt; or $X&amp;lt;c_1$. By comparison, the model predicts 
  a response of $\beta_0 + \beta_j$ for $c_j \le X &amp;lt; c_{j+1}$, so $\beta_j$ represents the 
  average increase in the response for &lt;em&gt;X&lt;/em&gt; in such range.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unless there are natural breakpoints in the predictors, piecewise-constant functions 
  can miss the action.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;73-basis-functions&quot;&gt;7.3. Basis Functions&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Polynomial and piecewise-constant regression models are in fact special cases of a 
  &lt;em&gt;basis function&lt;/em&gt; approach; with a family of functions or transformations applied 
  to a variable &lt;em&gt;X&lt;/em&gt;: $b_1(X), b_2(X), \ldots, b_K(X)$, fit a linear model&lt;br /&gt;
  $y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \cdots + \beta_K b_K(x_i) + \epsilon_i$.&lt;br /&gt;
  as a standard linear model with predictors $b_i(x_i), b_2(x_i), \ldots, b_K(x_i)$. 
  Hence, we can use least squares and all of the inference tools, such as std.err 
  for coefficient estimates and F-statistics for the model’s overall significance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that the basis functions $b_k(\cdot)$ are fixed and known(or, we choose the 
  functions before). E.g., for polynomial regression, the basis functions are $b_j(x_i)=x_i^j$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;74-regression-splines&quot;&gt;7.4. Regression Splines&lt;/h2&gt;

&lt;h3 id=&quot;741-piecewise-polynomials&quot;&gt;7.4.1. Piecewise Polynomials&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of fitting a high-degree polynomial over the entire range of &lt;em&gt;X&lt;/em&gt;, piecewise 
  polynomial regression fits separate low-degree polynomials over different regions. 
  Divide dimension &lt;em&gt;X&lt;/em&gt; by &lt;em&gt;knots&lt;/em&gt; and apply different polynomial model to each 
  region. If we place &lt;em&gt;K&lt;/em&gt; knots throughout the range of &lt;em&gt;X&lt;/em&gt;, then we fit &lt;em&gt;K+1&lt;/em&gt; different 
  models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;a cubic regression model; $y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$&lt;br /&gt;
  is a piecewise cubic with no knots with &lt;em&gt;d&lt;/em&gt; = 3.&lt;br /&gt;
  On the other hand,&lt;br /&gt;
  \(y_i = \begin{cases}
          \beta_{01}+\beta_{11}x_i + \beta_{21}x_i^2 + \beta_{31}x_i^3 + \epsilon_i &amp;amp; \mbox{if }x_i &amp;lt; c \\
          \beta_{02}+\beta_{12}x_i + \beta_{22}x_i^2 + \beta_{32}x_i^3 + \epsilon_i &amp;amp; \mbox{if }x_i \ge c. \\
          \end{cases}\)&lt;br /&gt;
  is with a single knot at a point c.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For other degrees &lt;em&gt;d&lt;/em&gt;:&lt;br /&gt;
  a piecewise-constant functions are piecewise polynomials of degree &lt;em&gt;0&lt;/em&gt;, a piecewise 
  linear functions are of degree &lt;em&gt;1&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;742-constraints-and-splines&quot;&gt;7.4.2. Constraints and Splines&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;To fix the discontinuity and overcomplexity, we add some additional constraints:&lt;br /&gt;
  in a degree-&lt;em&gt;d&lt;/em&gt; spline, or a piecewise degree-&lt;em&gt;d&lt;/em&gt; polynomial, it requires the 
  continuity in derivatives up to degree &lt;em&gt;d-1&lt;/em&gt; at each knot.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;743-the-spline-basis-representation&quot;&gt;7.4.3. The Spline Basis Representation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \cdots + \beta_{K+3} b_{K+3}(x_i) + \epsilon_i$&lt;br /&gt;
  is a cubic spline model with &lt;em&gt;K&lt;/em&gt; knots.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;with a &lt;em&gt;truncated power basis&lt;/em&gt; function per knot $\xi$, which is defined as 	
  \(h(x,\xi) = (x-\xi)_{+}^3 = \begin{cases} (x-\xi)^3 &amp;amp; \mbox{if }x&amp;gt;\xi \\
                                                  0	 &amp;amp; \mbox{otherwise,}
                               \end{cases}\)&lt;br /&gt;
  so, in fitting a cubic spline with &lt;em&gt;K&lt;/em&gt; knots, we perform least squares regression 
  with an intercept and &lt;em&gt;3+K&lt;/em&gt; predictors, of the form $X, X^2, X^3, h(X,\xi_1), h(X,\xi_2), \ldots, h(X,\xi_K)$.&lt;br /&gt;
  This amounts to estimating a total of &lt;em&gt;K+4&lt;/em&gt; regression coefficients; for this 
  regression, fitting a cubic spline with K knots uses &lt;em&gt;K+4&lt;/em&gt; degrees of freedom.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, splines can have high variance at the outer range of the predictors; 
  $X&amp;lt;c_1$ or $X\ge c_K$, when &lt;em&gt;X&lt;/em&gt; takes on either a very small or very large value. 
  We see that the confidence bands in the boundary region appear fairly wild.&lt;br /&gt;
  A &lt;em&gt;natural spline&lt;/em&gt; with additional &lt;em&gt;boundary constraints&lt;/em&gt; that the function is 
  required to be linear at the boundary, generally produce more stable estimates. In 
  this case, the corresponding confidence intervals are narrower.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;744-choosing-the-number-and-locations-of-the-knots&quot;&gt;7.4.4. Choosing the Number and Locations of the Knots&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The regression spline is most flexible in regions that contain a lot of knots, because 
  in those regions the polynomial coefficients can change rapidly. Hence, one option 
  is to place more knots in places where the function might vary most rapidly, and 
  to place fewer knots where it seems more stable. In practice, it is common to place 
  knots in a uniform fashion.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To determine the number of knots, or equivalently the degrees of freedom of the spline 
  contain, we use cross-validation method.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;745-comparison-to-polynomial-regression&quot;&gt;7.4.5. Comparison to Polynomial Regression&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Regression splines often give superior results to polynomial regression. This is 
  because unlike polynomials, which must use a high degree to produce flexible fits, 
  splines introduce flexibility by increasing the number of knots but keeping the 
  degree fixed. Generally, this approach produces more stable estimates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Splines can produce a reasonable fit at the boundaries and also allow us to place 
  more knots, or flexibility, over regions where the function &lt;em&gt;f&lt;/em&gt; seems to be changing 
  rapidly, and fewer knots where &lt;em&gt;f&lt;/em&gt; appears more stable.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;75-smoothing-splines&quot;&gt;7.5. Smoothing Splines&lt;/h2&gt;

&lt;h3 id=&quot;751-an-overview-of-smoothing-splines&quot;&gt;7.5.1. An Overview of Smoothing Splines&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In fitting a smooth curve to a set of data, some function $g(x)$, that fits the 
  observed data well: Minimizing $RSS = \sum_{i=1}^n(y_i-g(x_i))^2$. But if we don’t 
  put any constraints on &lt;em&gt;g&lt;/em&gt;, then we can always make &lt;em&gt;RSS&lt;/em&gt; zero simply by choosing 
  &lt;em&gt;g&lt;/em&gt; such that it &lt;em&gt;interpolates&lt;/em&gt; all of the $y_i$. Such a function would woefully 
  overfit the data, would be too flexible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Smoothing spline is the function &lt;em&gt;g&lt;/em&gt; that minimizes&lt;br /&gt;
  \(\sum_{i=1}^n(y_i - g(x_i))^2 + \lambda\int g''(t)^2, dt\)&lt;br /&gt;
  with a nonnegative tuning parameter $\lambda$, takes the “Loss+Penalty” formulation 
  like the ridge and lasso. The term $\sum_{i=1}^n(y_i - g(x_i))^2$ is a &lt;em&gt;loss function&lt;/em&gt; 
  that makes &lt;em&gt;g&lt;/em&gt; to fit the data well, and the term \(\lambda\int g''(t)^2, dt\) is a 
  &lt;em&gt;penalty term&lt;/em&gt; that reduces the variability in &lt;em&gt;g&lt;/em&gt;. \(g''(t)\) indicates the second 
  derivative of the function &lt;em&gt;g&lt;/em&gt;, corresponds to the amount by which the slope(the first 
  derivative) is changing. Broadly speaking, the second derivative is a measure of 
  its &lt;em&gt;roughness&lt;/em&gt;: large in absolute value if &lt;em&gt;g(t)&lt;/em&gt; is very wiggly near &lt;em&gt;t&lt;/em&gt;, close 
  to zero otherwise. It is zero as the derivative of a straight line, the function is 
  perfectly smooth. The integral is a summation over the range of &lt;em&gt;t&lt;/em&gt;, so \(\int g''(t)^2, dt\) 
  is a measure of the total change in the function $g’(t)$ over its entire range. If 
  &lt;em&gt;g&lt;/em&gt; is very smooth, then $g’(t)$ will be close to constant and \(\int g''(t)^2, dt\) will 
  take on a small value. Therefore, the penalty term encourages &lt;em&gt;g&lt;/em&gt; to be smooth. 
  The larger the $\lambda$, the smoother the &lt;em&gt;g&lt;/em&gt; will be.&lt;br /&gt;
  When $\lambda=0$, then there’s no penalty and the function &lt;em&gt;g&lt;/em&gt; will be very jumpy 
  and have perfect fit. When $\lambda\rightarrow\infty$, &lt;em&gt;g&lt;/em&gt; will be perfectly smooth; 
  a linear least squares line $g(x)=ax+b$. The $\lambda$ controls the bias-variance 
  trade-off of the smoothing spline.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Smoothing spline &lt;em&gt;g(x)&lt;/em&gt; have some special properties: it is a piecewise polynomial 
  with knots at the unique values of $x_1,\ldots,x_n$, and continuous first and second 
  derivatives at each knot. Furthermore, it is linear in the region outside of the 
  extreme knots.&lt;br /&gt;
  In other words, the Smoothing spline function is a nautral spline with knots at 
  $x_1,\ldots,x_n$; it is a shrunken version of such a natural spline, where the value 
  of the tuning parameter $\lambda$ controls the level of shrinkage.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;752-choosing-the-smoothing-parameter-lambda&quot;&gt;7.5.2. Choosing the Smoothing Parameter $\lambda$&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Have seen that a smoothing spline is simply a natural spline with knots at every unique 
  value of $x_i$, it might seem that a smoothing spline will have far too many degrees 
  of freedom, since a knot at each data point allows a great deal of flexibility. 
  But $\lambda$ controls the roughness of the smoothing spline, and hence the &lt;em&gt;effective&lt;/em&gt; 
  &lt;em&gt;degrees of freedom&lt;/em&gt;. We can show that as $\lambda$ increase from &lt;em&gt;0&lt;/em&gt; to $\infty$, 
  the effective degrees of freedom $df_\lambda$ decrease from &lt;em&gt;n&lt;/em&gt; to 2.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Usually degrees of freedom refer to the number of free parameters, such as the number 
  of coefficients fit in a polynomial or cubic spline. Though a smoothing spline has 
  &lt;em&gt;n&lt;/em&gt; parameters and &lt;em&gt;n&lt;/em&gt; nominal degrees of freedom, these &lt;em&gt;n&lt;/em&gt; parameters are heavily 
  constrainted or shrunk down. Thus $df_\lambda$ is a measure of the flexibility of 
  the smoothing spline, the higher it is, the more flexible the smoothing spline.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The definition of effective degrees of freedom, the measure of model complexity:&lt;br /&gt;
  \(\hat{\mathbf{g}}_\lambda = \mathbf{S}_\lambda\mathbf{y}\),&lt;br /&gt;
  where \(\hat{\mathbf{g}}_\lambda\) is the solution to the smoothing spline function 
  &lt;em&gt;g&lt;/em&gt; for a particular choice of $\lambda$, it is an &lt;em&gt;n&lt;/em&gt;-vector containing the fitted 
  values of the model at the training points $x_1,\ldots,x_n$.&lt;br /&gt;
  Then the effective degrees of freedom is defined to be&lt;br /&gt;
  \(df_\lambda = \text{trace}(\mathbf{S}_\lambda) = \sum_{i=1}^n\{ \mathbf{S}_\lambda \}_{ii}\),&lt;br /&gt;
  the sum of the diagonal elements of the matrix $\mathbf{S}_\lambda$.&lt;br /&gt;
  e.g.) for a linear regression model:&lt;br /&gt;
  \(\mathbb{H} = \mathbb{X}(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\),&lt;br /&gt;
  \(\text{trace}(\mathbb{H}) = p+1\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In fitting a smoothing spline, we do not need to select the number or location of the 
  knots; there will be a knot at each training observation. Instead, we need to choose 
  the value of $\lambda$. For this case, LOOCV can be computed very efficiently with 
  essentially the same cost as computing a single fit, using the following formula:&lt;br /&gt;
  \(RSS_{cv}(\lambda) = \sum_{i=1}^n(y_i - \hat{g}_\lambda^{(-i)}(x_i))^2 
  = \sum_{i=1}^n\left[\frac{y_i-\hat{g}_\lambda(x_i)}{1-\{\mathbf{S}_\lambda\}_{ii}}\right]^2\)&lt;br /&gt;
  where \(\hat{g}_\lambda^{(-i)}(x_i)\) indicates the fitted value for this smoothing 
  spline evaluated at $x_i$, where the fit uses all the training observations except 
  for the &lt;em&gt;i&lt;/em&gt;th observation. In contrast, \(\hat{g}_\lambda(x_i)\) indicates the smoothing 
  spline evaluated at $x_i$, where the function is fit to the full data. Thus, we can compute 
  each of LOOCV fits, by one-time computing of the original fit to all of the data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;By effective d.f, we can directly compare the model complexities of models discussed so 
  far, such as linear regression, ridge regression, smoothing splines, cubic splines, etc.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;76-local-regression&quot;&gt;7.6. Local Regression&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A different approach for fitting flexible non-linear functions, with the idea of KNN 
  but closer observations have more weights. This is sometimes referred to as a &lt;em&gt;memory-based&lt;/em&gt; 
  procedure, because we need all the training data each time we compute a prediction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm: Local Regression At $X = x_0$
    &lt;ol&gt;
      &lt;li&gt;Gather the faction $s = k/n$ of training points whose $x_i$ are closest to $x_0$.&lt;/li&gt;
      &lt;li&gt;Assign a weight $K_{i0} = K(x_i,x_0)$ to each point in this neighborhood, so that 
  the point furthest from $x_0$ has weight zero, and the closest has the highest weight. 
  All other points get weight zero.&lt;/li&gt;
      &lt;li&gt;Fit a &lt;em&gt;weighted least squares regression&lt;/em&gt; using the aforementioned weights;&lt;br /&gt;
  Objective function: \(\text{min}_{\beta_0,\beta_1}
                 \left[\sum_{i=1}^n K_{i0}(y_i - \beta_0 - \beta_1 x_i)^2 \right]\)&lt;/li&gt;
      &lt;li&gt;The fitted value at $x_0$ is given by $\hat{f}(x_0) = \hat\beta_0 + \hat\beta_1 x_0$.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In performing local regression, there are choices to be made, such as how to define 
  the weighting function &lt;em&gt;K&lt;/em&gt;, and which regression model to fit. The most important 
  choice is the &lt;em&gt;span s&lt;/em&gt;, which is the proportion of points used to compute the local 
  regression at $x_0$; “How many neighbors?”. It plays a role of the tuning parameter 
  $\lambda$, controls the flexibility of the fit. The smaller &lt;em&gt;s&lt;/em&gt;, the more &lt;em&gt;local&lt;/em&gt; and 
  wiggly the fit. We can use CV methods to choose &lt;em&gt;s&lt;/em&gt;, or we can specify it directly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea of local regression can be generalized in many different ways. In multivariate 
  settings, one very useful generalization involves fitting a multiple linear regression 
  model that is global in some variables, but local in another, such as time. Such 
  &lt;em&gt;varying coefficient models&lt;/em&gt; are a useful way of adapting a model to the most recently 
  gathered data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Local regression can perform poorly if the dimension &lt;em&gt;p&lt;/em&gt; is much larger, suffers the 
  dimensionality problem. Also, it has a boundary problem because there are less data 
  points for neighbors.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;77-generalized-additive-models&quot;&gt;7.7. Generalized Additive Models&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Approaches presented in sections above can be seen as extensions of simple linear 
  regression, flexibly predicting a response &lt;em&gt;Y&lt;/em&gt; on the basis of a single predictor &lt;em&gt;X&lt;/em&gt;. 
  GAMs provide a general framework for extending a standard linear model by allowing 
  non-linear functions of each of the variables, while maintaining &lt;em&gt;additivity&lt;/em&gt;. They 
  can be applied with both quantitative and qualitative responses.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;771-gams-for-regression-problems&quot;&gt;7.7.1. GAMs for Regression Problems&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;standard multiple linear regression model is&lt;br /&gt;
  \(y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \epsilon_i\).&lt;br /&gt;
  for non-linear relationships, replace each linear component \(\beta_j x_{ij}\) with an 
  unspecified (smoothing) non-linear function \(f_j(x_{ij})\), now the model is&lt;br /&gt;
  \(\begin{align*}
  y_i &amp;amp;= \beta_0 + \sum_{j=1}^p f_j(x_{ij}) + \epsilon_i \\
      &amp;amp;= \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \cdots + f_p(x_{ip}) + \epsilon_i.
  \end{align*}\)&lt;br /&gt;
  It is called an additive model because we calculate a separate &lt;em&gt;f&lt;/em&gt; for each &lt;em&gt;X&lt;/em&gt;, 
  then add together all of their contributions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case of using a smoothing spline to fit a GAM, it is not quite as simple as a 
  natural spline case, since the least squares cannot be used. However, Standard 
  softwares have some functions for GAMs using smoothing splines, via an approach 
  known as &lt;em&gt;backfitting&lt;/em&gt;; a method to fit a multivariate model by repeatedly updating 
  the fit for each predictor in turn, holding the others fixed. Each time we update 
  a function, we simply apply the fitting method for that variable to a partial residual.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;772-gams-for-classification-problems&quot;&gt;7.7.2. GAMs for Classification Problems&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;For qualitative response &lt;em&gt;Y&lt;/em&gt;, standard logistic regression model is&lt;br /&gt;
  \(\log\left(\frac{p(X)}{1-p(X)}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p.\)&lt;br /&gt;
  An extension allowing non-linear relationships; a logistic regression GAM is&lt;br /&gt;
  \(\log\left(\frac{p(X)}{1-p(X)}\right) = \beta_0 + f_1(X_1) + f_2(X_2) + \cdots + f_p(X_p).\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;pros-and-cons-of-gams&quot;&gt;Pros and Cons of GAMs&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Pros
    &lt;ol&gt;
      &lt;li&gt;GAMs allow us to fit a non-linear function to each variable, so that we can 
 automatically model non-linear relationships. This means we do not need to 
 manually try out many different transformations on each variable individually.&lt;/li&gt;
      &lt;li&gt;Potentially make more accurate predictions with non-linear fits.&lt;/li&gt;
      &lt;li&gt;Because the model is additive, we can examine the effect of each variable on the 
 response individually while holding all of the other variables fixed.&lt;/li&gt;
      &lt;li&gt;The smoothness of the function for the variable can be summarized via degrees 
 of freedom.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Cons
    &lt;ol&gt;
      &lt;li&gt;The model is restricted to be additive; with many variables, important interactions 
 can be missed. However, we can manually add interaction terms to the model by 
 including additional predictors of the form like $X_j \times X_k$.&lt;/li&gt;
      &lt;li&gt;The solution of the optimization is not unique; $\beta_0$ is not identifiable 
 because each $f_j$ model has its intercept term; a GAM has &lt;em&gt;p+1&lt;/em&gt; total intercepts 
 and they are not distinguishable. For this problem, we make a restriction that 
 every &lt;em&gt;j&lt;/em&gt;th &lt;em&gt;X&lt;/em&gt; variable to be centered; \(\sum_{i=1}^n f_j(x_{ij}) = 0\) and 
 $\hat\beta_0 = \bar{y}$.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        
        <pubDate>Wed, 06 May 2020 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch7</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch7</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 6. Linear Model Selection and Regularization</title>
        
          <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#chapter-6-linear-model-selection-and-regularization&quot; id=&quot;markdown-toc-chapter-6-linear-model-selection-and-regularization&quot;&gt;Chapter 6. Linear Model Selection and Regularization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#61-subset-selection&quot; id=&quot;markdown-toc-61-subset-selection&quot;&gt;6.1. Subset Selection&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#611-best-subset-selection&quot; id=&quot;markdown-toc-611-best-subset-selection&quot;&gt;6.1.1. Best Subset Selection&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#612-stepwise-selection&quot; id=&quot;markdown-toc-612-stepwise-selection&quot;&gt;6.1.2. Stepwise Selection&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#forward-stepwise-selection&quot; id=&quot;markdown-toc-forward-stepwise-selection&quot;&gt;Forward Stepwise Selection&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#backward-stepwise-selection&quot; id=&quot;markdown-toc-backward-stepwise-selection&quot;&gt;Backward Stepwise Selection&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#hybrid-approaches&quot; id=&quot;markdown-toc-hybrid-approaches&quot;&gt;Hybrid Approaches&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#613-choosing-the-optimal-model&quot; id=&quot;markdown-toc-613-choosing-the-optimal-model&quot;&gt;6.1.3. Choosing the Optimal Model&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#validation-and-cross-validation&quot; id=&quot;markdown-toc-validation-and-cross-validation&quot;&gt;Validation and Cross-Validation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#62-shrinkage-methods&quot; id=&quot;markdown-toc-62-shrinkage-methods&quot;&gt;6.2. Shrinkage Methods&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#621-ridge-regression&quot; id=&quot;markdown-toc-621-ridge-regression&quot;&gt;6.2.1. Ridge Regression&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#in-singular-value-decomposition&quot; id=&quot;markdown-toc-in-singular-value-decomposition&quot;&gt;in Singular Value Decomposition&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#622-the-lasso&quot; id=&quot;markdown-toc-622-the-lasso&quot;&gt;6.2.2. The Lasso&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#another-formulation-for-ridge-regression-and-the-lasso&quot; id=&quot;markdown-toc-another-formulation-for-ridge-regression-and-the-lasso&quot;&gt;Another Formulation for Ridge Regression and the Lasso&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#a-simple-special-case&quot; id=&quot;markdown-toc-a-simple-special-case&quot;&gt;A Simple Special Case&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#bayesian-interpretation&quot; id=&quot;markdown-toc-bayesian-interpretation&quot;&gt;Bayesian Interpretation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#63-dimension-reduction-methods&quot; id=&quot;markdown-toc-63-dimension-reduction-methods&quot;&gt;6.3. Dimension Reduction Methods&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#631--principal-components-regression&quot; id=&quot;markdown-toc-631--principal-components-regression&quot;&gt;6.3.1.  Principal Components Regression&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#principal-components-analysis&quot; id=&quot;markdown-toc-principal-components-analysis&quot;&gt;Principal Components Analysis&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-principal-components-regression-approach&quot; id=&quot;markdown-toc-the-principal-components-regression-approach&quot;&gt;The Principal Components Regression Approach&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#632-partial-least-squares&quot; id=&quot;markdown-toc-632-partial-least-squares&quot;&gt;6.3.2. Partial Least Squares&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#64-considerations-in-high-dimensions&quot; id=&quot;markdown-toc-64-considerations-in-high-dimensions&quot;&gt;6.4. Considerations in High Dimensions&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#641-high-dimensional-data&quot; id=&quot;markdown-toc-641-high-dimensional-data&quot;&gt;6.4.1. High-Dimensional Data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#642-what-goes-wrong-in-high-dimensions&quot; id=&quot;markdown-toc-642-what-goes-wrong-in-high-dimensions&quot;&gt;6.4.2. What Goes Wrong in High Dimensions?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#643-regression-in-high-dimensions&quot; id=&quot;markdown-toc-643-regression-in-high-dimensions&quot;&gt;6.4.3. Regression in High Dimensions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#644-interpreting-results-in-high-dimensions&quot; id=&quot;markdown-toc-644-interpreting-results-in-high-dimensions&quot;&gt;6.4.4. Interpreting Results in High Dimensions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-6-linear-model-selection-and-regularization&quot;&gt;Chapter 6. Linear Model Selection and Regularization&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Limitations of LSE
    &lt;ol&gt;
      &lt;li&gt;Prediction Accuracy:
        &lt;ul&gt;
          &lt;li&gt;if &lt;em&gt;n&lt;/em&gt; is not much larger than &lt;em&gt;p&lt;/em&gt;, the least squares fit can have a lot 
 of variability, results in overfitting and poor predictions to test data.&lt;/li&gt;
          &lt;li&gt;if &lt;em&gt;p&lt;/em&gt; &amp;gt; &lt;em&gt;n&lt;/em&gt;, there is no unique solution for the least squares coefficient 
 estimate; as $ Var(\hat\beta)=\infty$.&lt;/li&gt;
          &lt;li&gt;if &lt;em&gt;p&lt;/em&gt; is large, there can be correlations between &lt;em&gt;X&lt;/em&gt; variables. A model 
 having multicollinearity can have high variance.&lt;br /&gt;
&lt;em&gt;Constraining&lt;/em&gt; or &lt;em&gt;Shrinking&lt;/em&gt; the estimated coefficients can reduce the variance 
with negligible increase in bias, and improve in the accuracy to the test data.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Model Interpretability:
        &lt;ul&gt;
          &lt;li&gt;There are irrelevant variables $X_j$. Removing by setting coefficient estimates 
 $\beta_j = 0$, we can have more interpretability.&lt;br /&gt;
&lt;em&gt;Feature selection&lt;/em&gt; or &lt;em&gt;Variable selection&lt;/em&gt; can exclude irrelevant variables from a 
multiple regression model.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;61-subset-selection&quot;&gt;6.1. Subset Selection&lt;/h2&gt;

&lt;h3 id=&quot;611-best-subset-selection&quot;&gt;6.1.1. Best Subset Selection&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;fit a separate least squares regression for all $2^p$ possible models with combinations 
of the &lt;em&gt;p&lt;/em&gt; predictors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Algorithm
    &lt;ol&gt;
      &lt;li&gt;$\mathcal{M}_0$ as &lt;em&gt;null model&lt;/em&gt; (i.e., $ Y = \beta_0 + \epsilon $)&lt;/li&gt;
      &lt;li&gt;For $ k = 1, 2, \ldots, p $:&lt;br /&gt;
  (a) Fit all \({p \choose k}\) models with &lt;em&gt;k&lt;/em&gt; predictors&lt;br /&gt;
  (b) Pick the smallest RSS, (or largest $R^2$) = $ \mathcal{M}_k $&lt;/li&gt;
      &lt;li&gt;Select best model among $\mathcal{M}_0, \ldots,\mathcal{M}_p$ using cross-validated 
  prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Guarantees the best selection, while it suffers from computational limitations. Also, it 
only works for least squares linear regression.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;in the case of logistic regression, we use &lt;em&gt;deviance&lt;/em&gt;, $-2\log$MLE, instead of RSS in 
the 2nd step of algorithm upon.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;612-stepwise-selection&quot;&gt;6.1.2. Stepwise Selection&lt;/h3&gt;

&lt;h4 id=&quot;forward-stepwise-selection&quot;&gt;Forward Stepwise Selection&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Algorithm
    &lt;ol&gt;
      &lt;li&gt;$\mathcal{M}_0$ as &lt;em&gt;null model&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;For $ k = 1, 2, \ldots, p $:&lt;br /&gt;
  (a) Fit all &lt;em&gt;p - k&lt;/em&gt; models in \(\mathcal{M}_k\) with one additional predictor&lt;br /&gt;
  (b) Pick the smallest RSS among &lt;em&gt;p - k&lt;/em&gt; models, $\mathcal{M}_{k+1}$&lt;/li&gt;
      &lt;li&gt;Select best model among $\mathcal{M}_0, \ldots,\mathcal{M}_p$ with CV scores&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Total $\frac{p(p+1)}{2}+1$ possible models. No guarantee but available for the case of 
high dimensional data($n&amp;lt;p$).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;backward-stepwise-selection&quot;&gt;Backward Stepwise Selection&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Algorithm
    &lt;ol&gt;
      &lt;li&gt;$\mathcal{M}_p$ as &lt;em&gt;full model&lt;/em&gt;, contains all &lt;em&gt;p&lt;/em&gt; predictors&lt;/li&gt;
      &lt;li&gt;For $ k = p, p-1, \ldots, 1 $:&lt;br /&gt;
  (a) Fit all &lt;em&gt;k - 1&lt;/em&gt; models contain all but one of the predictors in \(\mathcal{M}_k\)&lt;br /&gt;
  (b) Pick the smallest RSS among &lt;em&gt;k - 1&lt;/em&gt; models, $\mathcal{M}_{k-1}$&lt;/li&gt;
      &lt;li&gt;Select best model among $\mathcal{M}_0, ldots,\mathcal{M}_p$ with CV scores&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Total $\frac{p(p+1)}{2}+1$ possible models. No guarantee and not for &lt;em&gt;n &amp;lt; p&lt;/em&gt; case.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;hybrid-approaches&quot;&gt;Hybrid Approaches&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;add then remove one predictors in each step.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;613-choosing-the-optimal-model&quot;&gt;6.1.3. Choosing the Optimal Model&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A model containing all of the predictors will always have the smallest RSS and the largest 
$R^2$, since these quantities are related to the training error. Instead, we need a model with a 
low test error.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;$C_p = \frac{1}{n}(RSS + 2 d \hat\sigma^2)$&lt;br /&gt;
 For a fitted least squares model, with &lt;em&gt;d&lt;/em&gt; as the number of predictors and $\hat\sigma^2$ as 
 an estimate of the variance of the error. Typically $\hat\sigma^2$ is estimated using the full 
 model containing all predictors. Adding a penalty to the training RSS is to adjust its 
 underestimation to the test error. As the number of predictors increase, the penalty increase. 
 If there is a proof of $\hat\sigma^2$ is an unbiased estimate of $\sigma^2$, $C_p$ is an unbiased 
 estimate of test MSE. Then, a model with the lowest $C_p$ is the best model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AIC $= \frac{1}{n}(RSS + 2 d \hat\sigma^2)$&lt;br /&gt;
 For a models fit by maximum likelihood(MLE), given by omitted irrelevant constants. $C_p$ and 
 AIC are proportional to each other.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BIC $= \frac{1}{n}(RSS + \log(n)d\hat\sigma^2)$&lt;br /&gt;
 From a Bayesian point of view, for a fitted least squares model. Also given by omitted 
 irrelevant constants. BIC has heavier penalty then $C_p$ or AIC, results in selecting smaller 
 models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adjusted $R^2 = 1 - \frac{RSS/(n-d-1)}{TSS/(n-1)}$&lt;br /&gt;
 Since the usual $R^2$ is defined as $1 - RSS/TSS$, it always increases as more variables added. 
 Adjusted $R^2$ gives penalty of &lt;em&gt;d&lt;/em&gt;, the number of predictors in the denominator. Unlike other 
 statistics, a large value of adjusted $R^2$ indicates a small test error.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;validation-and-cross-validation&quot;&gt;Validation and Cross-Validation&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;one-standard-error rule&lt;/em&gt;&lt;br /&gt;
First calculate the standard error of the estimated test MSE for each model size, then select the 
smallest model for which the estimated test error is within one standard error of the lowest point 
on the curve.&lt;br /&gt;
If a set of models appear to be more or less equally good, then we might as well choose the simplest 
model; the model with the smallest number of predictors.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;62-shrinkage-methods&quot;&gt;6.2. Shrinkage Methods&lt;/h2&gt;

&lt;h3 id=&quot;621-ridge-regression&quot;&gt;6.2.1. Ridge Regression&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ridge regression coefficient estimates&lt;br /&gt;
\(\begin{align*}
\hat\beta^R &amp;amp;= \text{min}_{\beta}\left[
                  \underbrace{\sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p \beta_j x_{ij})}_{RSS}
                  + \lambda\sum_{j=1}^p \beta_j^2 \right] \\
            &amp;amp;= (X^TX + \lambda I)^{-1} X^T\underline{y}	
\end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\lambda \ge 0 $ is a &lt;em&gt;tuning parameter&lt;/em&gt;, $\lambda\sum_{j=1}^p \beta_j^2$ is a &lt;em&gt;shrinkage penalty&lt;/em&gt;. 
The penalty is small when the coefficients are close to zero, and so it has the effect of &lt;em&gt;shrinking&lt;/em&gt; 
the estimates of $\beta_j$ towards zero. Ridge regression will produce a different set of coefficient 
estimates $\beta_{\lambda}^R$, for each value of $\lambda$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We do not want to shrink the intercept $\beta_0$, which is simply a measure of the mean value of 
the response when $x_{i1}=x_{i2}=\ldots=x_{ip}=0$. If the variables, the columns of the data matrix
&lt;strong&gt;$X$&lt;/strong&gt;, have been centered to have mean zero before ridge regression is performed, then the estiamted 
intercept will take the form $\hat\beta_0 = \bar{y} = \sum_{i=1}^n y_i/n$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The standard least squares coefficient estimates are &lt;em&gt;scale equivariant&lt;/em&gt;; multiplying $X_j$ by a constant 
&lt;em&gt;c&lt;/em&gt; leads to a scaling of the least squares coefficient estimates by a factor of 1/&lt;em&gt;c&lt;/em&gt;. I.e., regardless 
of how the &lt;em&gt;j&lt;/em&gt;th predictor is scaled, $X_j\hat\beta_j$ will remain the same.&lt;br /&gt;
In contrast, the ridge regression coefficient estimates can change substantially when multiplying a 
given predictor by a constant. The value of $X_j\hat\beta_{j,\lambda}^R$ may depend on the scaling of 
the other predictors. Thus, before applying ridge regression, the variables need to be standardized to 
have a standard deviation of one.&lt;br /&gt;
The formula: \(\tilde{x}_{ij}=\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2}}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ridge regression overperforms the standard least squares when the number of variables &lt;em&gt;p&lt;/em&gt; is almost 
as large as the number of observations &lt;em&gt;n&lt;/em&gt;, or even when $p &amp;gt; n$. Also it has computational advantages 
over best subset selection, which requires searching through $2^p$ models. Ridge regression only fits a 
single model for any fixed value of $\lambda$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;in-singular-value-decomposition&quot;&gt;in Singular Value Decomposition&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;where $ X = \mathbb{UDV}^T$,&lt;br /&gt;
\(\begin{align*} X\hat\beta^{\text{LSE}} &amp;amp;= X(X^TX)^{-1}X^T\underline{y} \\
                                         &amp;amp;= \mathbb{UU}^T\underline{y} \\
                            X\hat\beta^R &amp;amp;= UD(D^2 + \lambda I)^{-1}DU^T\underline{y} \\
                                         &amp;amp;= \sum_{j=1}^p\underline{u}_j\frac{d_{ij}^2}{d_{ij}^2+\lambda}\underline{u}_j^T\underline{y}
\end{align*}\)&lt;br /&gt;
\(\begin{align*}
\rightarrow \partial f(\lambda) &amp;amp;= tr[X(X^TX + \lambda I)^{-1} X^T] \\
                                &amp;amp;= tr(\mathbb{H}_{\lambda})  \\
                                &amp;amp;= \sum_{j=1}^p\frac{d_{ij}^2}{d_{ij}^2+\lambda}
\end{align*}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;622-the-lasso&quot;&gt;6.2.2. The Lasso&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ridge regression estimates shrink towards zero but will not set nay of them exactly to zero(unless 
$\lambda = \infty$). This may not be a problem for prediction accuracy, but it can be a challenge in 
model interpretation when &lt;em&gt;p&lt;/em&gt; is quite large.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;lasso&lt;/em&gt;&lt;br /&gt;
\(\hat\beta^L_{\lambda} = \text{min}_{\beta}\left[RSS+\lambda\sum_{j=1}^p|\beta_j|\right]\)&lt;br /&gt;
Instead of $\mathcal{l}_2$ penalty in Ridge, the lasso uses an $\mathcal{l}_1$ penalty. 
The $\mathcal{l}_1$ norm of a coefficient vector $\beta$ is given by $\lVert \beta \rVert_1 = 
\sum |\beta_j|$. This penalty has the effect of forcing some of the coefficient estimates to be 
exactly equal to zero when the tuning parameter is sufficiently large. Hence, the lasso performs 
&lt;em&gt;variable selection&lt;/em&gt;, these &lt;em&gt;sparse&lt;/em&gt; models with the lasso are much easier to interpret than those 
with ridge.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;another-formulation-for-ridge-regression-and-the-lasso&quot;&gt;Another Formulation for Ridge Regression and the Lasso&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ridge:
\(\text{min}_{\beta}\left\{ \sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p\beta_j x_{ij})^2 
                      \right\}\) subject to $\sum_{j=1}^p\beta_j^2 \le s $&lt;br /&gt;
Lasso:
\(\text{min}_{\beta}\left\{ \sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p\beta_j x_{ij})^2 
                      \right\}\) subject to $\sum_{j=1}^p|\beta_j| \le s $&lt;br /&gt;
where the &lt;em&gt;budget s&lt;/em&gt; as the regularization parameter ($\lambda\uparrow \equiv s\downarrow$).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;when $p = 2$, then the ridge regression estimates have the smallest RSS out of all points that lie 
within the circle defined by $\beta_1^2 + \beta_2^2 \le s$, while the lasso estimates have within 
the diamond defined by $|\beta_1|+|\beta_2| \le s$. when $p = 3$, he constraint region for ridge 
becomes a sphere, for lasso becomes a polyhedron. For larger &lt;em&gt;p&lt;/em&gt;, it becomes a hypersphere and a 
polytope each. The lasso leads to feature selection due to the sharp corners of its constraint region.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;the number of predictors that is related to the response is never known a &lt;em&gt;priori&lt;/em&gt; for real data sets. 
A technique such as cross-validation can be used in order to determine which approach is better on a 
particular data set.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;a-simple-special-case&quot;&gt;A Simple Special Case&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;An analytical method(solution) for the case when $n = p$, and &lt;strong&gt;&lt;em&gt;X&lt;/em&gt;&lt;/strong&gt; a diagonal matrix with 1’s on 
the diagonal and 0’s in all off-diagonal elements. I.e., the columns of &lt;strong&gt;&lt;em&gt;X&lt;/em&gt;&lt;/strong&gt; are orthogonal. Also, 
assume that we are performing regression without an intercept(or standardized).&lt;br /&gt;
(c.f. in real world cases, we need to use numerical methods.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The usual least squares, $\hat\beta$ is that minimizes; $\sum_{j=1}^p(y_j-\beta_j)^2$.&lt;br /&gt;
and for the ridge, minimizing $\sum_{j=1}^p(y_j-\beta_j)^2+\lambda\sum_{j=1}^p\beta_j^2$.&lt;br /&gt;
and for the lasso, minimizing $\sum_{j=1}^p(y_j-\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j|$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The ridge regression estiamtes $\hat\beta_j^R = y_j/(1+\lambda)$ and&lt;br /&gt;
\(\text{the lasso estimates} \begin{align*}
\hat\beta_j^L &amp;amp;= \text{sign}(\hat\beta_j)(|\hat\beta_j|-\lambda)_{+}, \\
    \text{or} &amp;amp;= \begin{cases}
                  y_j - \lambda/2, &amp;amp; \mbox{if }y_j &amp;gt; \lambda/2; \\
                  y_j + \lambda/2, &amp;amp; \mbox{if }y_j &amp;lt; -\lambda/2; \\
                  0				 &amp;amp; \mbox{if }|y_j| \le \lambda/2.
                  \end{cases}
\end{align*}\)&lt;br /&gt;
&lt;img src=&quot;/assets/images/ch6_ridge_lasso_effect_0.png&quot; alt=&quot;png&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;&lt;br /&gt;
Ridge shrinks all coefficients towards zero by the same &lt;em&gt;“proportion”&lt;/em&gt;,&lt;br /&gt;
Lasso shrinks all coefficients towards zero by the same &lt;em&gt;“amount”&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;bayesian-interpretation&quot;&gt;Bayesian Interpretation&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;$p(\beta|X,Y)\propto f(Y|X,\beta)p(\beta|X) = f(Y|X,\beta)p(\beta)$&lt;br /&gt;
with assumption of $p(\beta)=\prod_{j=1}^p g(\beta_j)$ for some density function &lt;em&gt;g&lt;/em&gt;.&lt;br /&gt;
Two special cases of &lt;em&gt;g&lt;/em&gt;:
    &lt;ul&gt;
      &lt;li&gt;If &lt;em&gt;g&lt;/em&gt; is a Gaussian distribution with mean zero and standard deviation a function of $\lambda$, 
it follows that the &lt;em&gt;posterior mode&lt;/em&gt; for $\beta$, is given by the ridge regression solution. Also, 
the solution is equal to posterior mean.&lt;/li&gt;
      &lt;li&gt;If &lt;em&gt;g&lt;/em&gt; is a double-exponential(Laplace) distribution with mean zero and scale parameter a function 
of $\lambda$, it follows that the posterior mode for $\beta$ is the lasso soultion(which is not the 
posteriror mean in this case).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hence, the lasso expects a priori that many of the coefficients are (exactly) zero, while ridge 
assumes the coefficients are randomly distributed about zero.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;63-dimension-reduction-methods&quot;&gt;6.3. Dimension Reduction Methods&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;p&lt;/em&gt; predictors to &lt;em&gt;M&lt;/em&gt; new transformed variables.&lt;br /&gt;
Let $Z_m = \sum_{j=1}^p\phi_{jm}X_j$ represent &lt;em&gt;M &amp;lt; p linear combinations&lt;/em&gt; of original &lt;em&gt;p&lt;/em&gt; predictors. 
Then fit the linear regression model $y_i = \theta_0 + \sum_{m=1}^M\theta_m z_{im} + \epsilon_i, \quad i = 1, \ldots, n$, 
using least squares. If the constants $\phi_{1m}, \ldots, \phi_{pm}$ are chosen wisely, dimension 
reduction approaches can outperform least squares regression. I.e., using least squares, fitting 
reduced model can lead to better results than fitting the standard linear model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(\sum_{m=1}^M\theta_m z_{im} = \sum_{m=1}^M\theta_m\sum_{j=1}^p\phi_{jm}x_{ij} = 
  \sum_{j=1}^p\sum_{m=1}^M\theta_m\phi_{jm}x_{ij} = \sum_{j=1}^p\beta_j x_{ij},\)&lt;br /&gt;
  where \(\beta_j = \sum_{m=1}^M\theta_m\phi_{jm}\).&lt;br /&gt;
  Hence, this model can be a special case of the standard linear regression model. In situations where 
  &lt;em&gt;p&lt;/em&gt; is large relative to &lt;em&gt;n&lt;/em&gt;, demension reduction methods can significantly reduce the variance of the 
  fitted coefficients. If $M = p$, and all the $Z_m$ are linearly independent, then there are no constraints 
  and the model is equivalent to the standard linear model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All dimension reduction methods work in two steps. First, the transformed predictors $Z_m$ are obtained. 
  Second, the model is fit using these &lt;em&gt;M&lt;/em&gt; predictors. The choice of $Z_m$, which is, the selection of the 
  $\phi_{jm}$’s can be achieved in different ways.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;631--principal-components-regression&quot;&gt;6.3.1.  Principal Components Regression&lt;/h3&gt;

&lt;h4 id=&quot;principal-components-analysis&quot;&gt;Principal Components Analysis&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Goal of PCA:&lt;br /&gt;
  PCA is a technique for reducing the dimension of an &lt;em&gt;n by p&lt;/em&gt; data matrix &lt;strong&gt;&lt;em&gt;X&lt;/em&gt;&lt;/strong&gt;, finding small number 
  of dimensions &lt;em&gt;M&lt;/em&gt;, which have simillar amount of information to original &lt;em&gt;p&lt;/em&gt; predictors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;principal component&lt;/em&gt; direction of the data is that along which the observations &lt;em&gt;vary the most&lt;/em&gt;; 
  with the largest variance of the observations projected onto. The principal component vector $Z_m$ 
  defines the line that is &lt;em&gt;as close as possible&lt;/em&gt; to the data, minimizing the sum of the squared 
  perpendicular distances between each point and the line. In other word, the principal component appears 
  to capture most of the information contained in two variables.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;e.g. in the first principal component,&lt;br /&gt;
  &lt;img src=&quot;/assets/images/ch6_pca_0.png&quot; alt=&quot;png&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;&lt;br /&gt;
  total variance keeped: $Var(X_1)+Var(X_2) = Var(PC_1)+Var(PC_2)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;where $X_s$ is $n \times p$ standardized matrix,&lt;em&gt;j&lt;/em&gt;th Principal Component Vector of $X_s$: $z_j = X_s v_j$, 
  $\quad j=1,\ldots,p$ is that satisfying \(\text{max}_{\alpha}Var(X_s\alpha)\) subject to \(\lVert\alpha\rVert=1\). 
  Here, the values of $z_{1j}, \ldots, z_{nj}$ are known as the &lt;em&gt;principal component scores&lt;/em&gt;.&lt;br /&gt;
  $v_j$ is $p \times 1$ size eigenvector of $X_s^T X_s$ corresponding to the &lt;em&gt;j&lt;/em&gt;th largest eigenvalue, 
  and $\alpha$ is $v_j$’s orthogonality to $v_1,\ldots,v_{j-1}$ ($\alpha^T S v_k = 0$, where S is the 
  sample covariance matrix of $X_s$, or $X_s^T X_s$, and $k = 1, \cdots, j-1$).&lt;br /&gt;
  Then $z_1 = X_s v_1$, $z_2\bot z_1$, $z_3\bot z_1,z_2$, $\cdots$, $z_p\bot z_1,\ldots,z_{p-1}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;derivation&lt;/em&gt;&lt;br /&gt;
  Since $X_s$ is standardized matrix,&lt;br /&gt;
  \(Var(X_s\alpha) = \alpha^T X_s^T X_s\alpha\)&lt;br /&gt;
  &lt;em&gt;by Lagrangian form&lt;/em&gt;,&lt;br /&gt;
  \(\begin{align*}
  \text{max}_{\alpha}Q(X_s,\lambda) &amp;amp;= \text{max}_{alpha}\left[\alpha^T X_s^T X_s\alpha
                                                              -\lambda\alpha^T\alpha \right] \\
  \rightarrow \frac{\partial Q}{\partial\alpha} &amp;amp;= 2X_s^T X\alpha - 2\lambda\alpha \\
  \text{for } \hat\alpha, X_s^T X\alpha &amp;amp;= \lambda\alpha
  \end{align*}\)&lt;br /&gt;
  &lt;em&gt;note that&lt;/em&gt; $\mathbb{A}_v = ev$, the combination of eigenvalue and eigenvector of $\mathbb{A}$.&lt;br /&gt;
  Thus, $\alpha = v_j$, the &lt;em&gt;j&lt;/em&gt;th eigenvector of $X_s^T X_s$, that is, the constraint of orthogonality 
  is satisfied.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since PCA has no single solution &lt;em&gt;M&lt;/em&gt;;&lt;br /&gt;
  the proportion of variance explained by &lt;em&gt;m&lt;/em&gt;th PC($Z_m$) used:&lt;br /&gt;
  \(PVE_m = \frac{Var(Z_m)}{\sum_{j=1}^p(Var(Z_j))}\)&lt;br /&gt;
  (\(\sum_{j=1}^p(Var(Z_j)) = \sum Var(X_j) =\) total variance)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;in &lt;em&gt;SVD&lt;/em&gt; of covariance matrix $X^T X$,&lt;br /&gt;
  \(\begin{align*}
  X^T X &amp;amp;= \mathbb{VDU}^T\mathbb{UDV}^T \\
        &amp;amp;= \mathbb{VD^2 V}^T
  \end{align*}\)&lt;br /&gt;
  in this eigen decomposition,&lt;br /&gt;
  \(\mathbb{V} = (v_1,\ldots,v_p)\) the eigen vectors of $X^T X$&lt;br /&gt;
  \(\mathbb{D}^2 = \begin{bmatrix}
                      d_1^2 &amp;amp; \cdots &amp;amp; 0 \\
                      \vdots &amp;amp; \ddots &amp;amp; \vdots \\
                      0 &amp;amp; \cdots &amp;amp; d_p^2
                      \end{bmatrix}\)
                      $d_j^2 = e_j$, &lt;em&gt;j&lt;/em&gt;th eigenvalue of $X^T X$&lt;br /&gt;
  thus,&lt;br /&gt;
  \(\begin{align*}
  Var(Z_m) &amp;amp;= \frac{1}{n}(Z_m^T Z_m) \\
           &amp;amp;= \frac{1}{n}(v_m^T X_s^T X_s v_m) \\
           &amp;amp;= \frac{1}{n}(v_m^T\mathbb{VD}^2\mathbb{V}^T v_m) \\
           &amp;amp;= \frac{1}{n}d_m^2 = \frac{1}{n}e_m
  \end{align*}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Therefore,&lt;br /&gt;
  \(PVE_m = \frac{Var(Z_m)}{\sum_{j=1}^p(Var(Z_j))} = \frac{e_m}{\sum_{j=1}^p e_j}\)&lt;br /&gt;
  we can draw a &lt;em&gt;scree plot&lt;/em&gt; on the value of $PVE_m$ over the value of &lt;em&gt;m&lt;/em&gt; to find optimal “M”.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;the-principal-components-regression-approach&quot;&gt;The Principal Components Regression Approach&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The key idea is that a small number of principal components can explain most of the variability in the 
  data, as well as the relationship with the response. Under this assumption, fitting a least squares model 
  to $Z_1, \ldots, Z_M$ will lead to better results than fitting a least squares model to $X_1, \ldots, X_p$, 
  since most or all of the information in the data is contained in $Z_m$ and there are smaller number of 
  coefficients, we can mitigate overfitting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that PCR is not a feature selection method; is a linear combination of all &lt;em&gt;p&lt;/em&gt; of the original features. 
  In this sense, PCR is more closely related to ridge regression than to the lasso.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deciding “M”:&lt;br /&gt;
  full model is \(\hat{Y} = \hat{\theta}_0 + \hat{\theta}_1 Z_1 + \cdots + \hat{\theta}_p Z_p\)&lt;br /&gt;
  when $Z_1,\ldots,Z_m$ is from standardized $X_s$ and \(\hat{y}_0 = \bar{y}\),&lt;br /&gt;
  as $Z_j$’s are orthogonal, adding variable $Z_{j+1}$ does not affect the coefficients. Thus, $\theta_j$’s are 
  not changed by feature selection; that is,&lt;br /&gt;
  \(\hat{Y} = \hat{\theta}_0 + \hat{\theta}_1 Z_1 \\
  \hat{Y} = \hat{\theta}_0 + \hat{\theta}_1 Z_1 + \hat{\theta}_2 Z_2 \\
  \vdots \\
  \hat{Y} = \hat{\theta}_0 + \hat{\theta}_1 Z_1 +\cdots + \hat{\theta}_p Z_p\) the value of $\theta_k$ is the same.&lt;br /&gt;
  Then we can use CV methods over these models to get optimal &lt;em&gt;M&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;632-partial-least-squares&quot;&gt;6.3.2. Partial Least Squares&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The PCR approach identifies linear combinations, or &lt;em&gt;directions&lt;/em&gt;, that best represent the predictors. 
  These directions are identified in an &lt;em&gt;unsupervised&lt;/em&gt; way, since the response &lt;em&gt;Y&lt;/em&gt; is not used to help 
  determine the principal component directions. There, PCR suffers from a drawback: there is no guarantee 
  that the directions that best explain the predictors will also be the best directions to use for 
  predicting the response.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;PLS is a &lt;em&gt;supervised&lt;/em&gt; alternative to PCR; finding PLS directions $Z_1,\ldots,Z_m$ that 
  $Cov(Y,Z_1)\ge Cov(Y,Z_2)\ge\cdots\ge Cov(Y,Z_M)$ instead of $Var(Z_1)\ge\cdots\ge Var(Z_M)$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;First PLS direction is computed, after standardizing predictors, by setting each $\phi_{j1}$ equal to 
  the coefficient from the simple linear regression $Y$ onto $X_j$. As $Z_1 = \sum_{j=1}^p\phi_{j1}X_j$, 
  PLS places the highest weight on the variables that are most strongly related to the response.&lt;br /&gt;
  To find second PLS direction, we adjust each of the variables for $Z_1$, by regressing each variable 
  on $Z_1$ and taking &lt;em&gt;residuals&lt;/em&gt;. The residuals can be interpreted as the remaining information that has 
  not been explained by the first PLS direction. Then we compute $Z_2$ using this orthogonalized data by 
  the same way of computing $Z_1$. This predecure repeated &lt;em&gt;M&lt;/em&gt; times.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;in Simple Regression case,&lt;br /&gt;
  $\hat X_j^s$ is a projection of original data $X_j^s$ to a vector $Z_1$; $X_j^s = \alpha Z_1$.&lt;br /&gt;
  the residual vector $r_j = \hat X_j^s - X_j^s$ and $r_j\bot Z_1$.&lt;br /&gt;
  Then, $r_j = X_j^{(2)}$ is the orthogonalized data for computing the next $Z_2$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;m&lt;/em&gt;th PLS direction:&lt;br /&gt;
  \(\text{max}_{\phi} Cov(y,X_s\phi)\) subject to $\lVert\phi\rVert = 1$, $\phi^T S v_l = 0$&lt;br /&gt;
  for $\phi$ as orthogonal directions, sample covariance matrix &lt;em&gt;S&lt;/em&gt;, and $v_l$ as &lt;em&gt;l&lt;/em&gt; th PLS direction.&lt;br /&gt;
  \(\text{max}_{\phi}[E(\phi^T X_s^T y)-E(y)E(\phi^T X_s)]\), as standardized, $E(X_s) = 0$,&lt;br /&gt;
  \(\equiv \text{max}_{\phi}\phi^T \dot X_s^T y\) is maximization of dot product of 2 vectors.&lt;br /&gt;
  note that, when two vectors are in the same direction, dot product is maximized.&lt;br /&gt;
  $\therefore \phi=X_s^T y$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ch6_pls_algorithm_0.png&quot; alt=&quot;png&quot; width=&quot;80%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;64-considerations-in-high-dimensions&quot;&gt;6.4. Considerations in High Dimensions&lt;/h2&gt;

&lt;h3 id=&quot;641-high-dimensional-data&quot;&gt;6.4.1. High-Dimensional Data&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Data sets that containing more features than observations, $p &amp;gt; n$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;642-what-goes-wrong-in-high-dimensions&quot;&gt;6.4.2. What Goes Wrong in High Dimensions?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Standard least squares cannot be performed. Regardless of the true relationship between features and response, 
  least squares will result in a perfect fit to the data, lead to overfitting of the data and poor 
  predictions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;643-regression-in-high-dimensions&quot;&gt;6.4.3. Regression in High Dimensions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;new technologies that allow for the collection of measurements for thousands or millions of features 
  are a double-edged sword: they can lead to improved predictive models if these features are in fact 
  relevant to the problem at hand, but will lead to worse results if the features are not relevant. 
  Even if they are relevant, the variance incurred in fitting their coefficients may outweigh the 
  reduction in bias that they bring.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;644-interpreting-results-in-high-dimensions&quot;&gt;6.4.4. Interpreting Results in High Dimensions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In high-dimensional setting, the multicollinearity problem is extreme:&lt;br /&gt;
  any variable in the model is a linear combination of all of the other variables in the model. This means 
  we can never know exactly which variables truly are predictive of the outcome, and we can never identify 
  the best coefficients for use in the regression.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When $p &amp;gt; n$, it is easy to obtain a a useless model that has zero residuals. Therefore, we should never 
  use sum of squared errors, p-values, $R^2$ statistics, or other traditional measures of model fit on the 
  training data as evidence of a good model fit. Instead we report results on an independent test set, or 
  cross-validation errors.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        
        <pubDate>Wed, 29 Apr 2020 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch6</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch6</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 5. Resampling Methods</title>
        
          <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#chapter-5-resampling-methods&quot; id=&quot;markdown-toc-chapter-5-resampling-methods&quot;&gt;Chapter 5. Resampling Methods&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#51--cross-validation&quot; id=&quot;markdown-toc-51--cross-validation&quot;&gt;5.1.  Cross-Validation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#511-the-validation-set-approach&quot; id=&quot;markdown-toc-511-the-validation-set-approach&quot;&gt;5.1.1. The Validation Set Approach&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#512-leave-one-out-cross-validation&quot; id=&quot;markdown-toc-512-leave-one-out-cross-validation&quot;&gt;5.1.2. Leave-One-Out Cross-Validation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#513-k-fold-cross-validation&quot; id=&quot;markdown-toc-513-k-fold-cross-validation&quot;&gt;5.1.3. k-Fold Cross-Validation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#514-bias-variance-trade-off-for-k-fold-cross-validation&quot; id=&quot;markdown-toc-514-bias-variance-trade-off-for-k-fold-cross-validation&quot;&gt;5.1.4. Bias-Variance Trade-Off for k-Fold Cross-Validation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#515-cross-validation-on-classification-problems&quot; id=&quot;markdown-toc-515-cross-validation-on-classification-problems&quot;&gt;5.1.5. Cross-Validation on Classification Problems&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#52-the-bootstrap&quot; id=&quot;markdown-toc-52-the-bootstrap&quot;&gt;5.2. The Bootstrap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chapter-5-resampling-methods&quot;&gt;Chapter 5. Resampling Methods&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;repeatedly drawing samples from a training set and refitting a model of interest on 
each sample in order to obtain additional information about the fitted model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To optain information that would not be available from fitting the model only once 
using the original training sample.&lt;br /&gt;
e.g. to estimate the variability of a model fit, draw different samples and fit it to 
each new sample, then examine the extent to which the resulting fits differ.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;51--cross-validation&quot;&gt;5.1.  Cross-Validation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;In the absence of a very large designated test set that can be used to directly estimate 
the test error rate, a class of methods that estimate the test error rate by &lt;em&gt;holding out&lt;/em&gt; 
a subset of the training observations from the fitting process, then applying the statistical 
learning method to those held out observations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;511-the-validation-set-approach&quot;&gt;5.1.1. The Validation Set Approach&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;randomly dividing the available set of observations into two parts;&lt;br /&gt;
a &lt;em&gt;training set&lt;/em&gt; and a &lt;em&gt;validation set&lt;/em&gt; (or hold-out set)&lt;br /&gt;
model is fit on the training set, and the fitted model is used to predict the responses for the 
observations in the validation set. The validation set error rate estimates the test error rate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Repeating this predecure, we have different estimate for the test MSE over random splits of the 
observations and there are two issues:&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;The validation estimate of the test error rate can be highly variable, depending on which observations 
 are included in the training set or the validation test.&lt;/li&gt;
      &lt;li&gt;Only a subset of the observations are used to fit the model. Trained on fewer observations, the 
 validation set error rate may &lt;em&gt;overestimate&lt;/em&gt; the test error rate for the model fit on the entire 
 data set.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;512-leave-one-out-cross-validation&quot;&gt;5.1.2. Leave-One-Out Cross-Validation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;each single observation is used for the validation set, and the remaining observations are for the 
training set. The statistical learning method is fit on the &lt;em&gt;n-1&lt;/em&gt; training obs. The prediction is made 
for the excluded observation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LOOCV estiamte for the test MSE:&lt;br /&gt;
\(CV_{(n)} = \frac{1}{n}\sum_{i=1}^n MSE_i\)&lt;br /&gt;
No overestimation on the test error, No variance of test MSE, but Expansive.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;a shortcut of LOOCV on Least Squares(regression):&lt;br /&gt;
\(\begin{align*}
CV_{(n)} = \frac{1}{n} \sum_{i=1}^n \left(\frac{y_i - \hat y_i}{1-h_i}\right)^2
\end{align*}\)&lt;br /&gt;
where $\hat y_i$ is the fitted value from the original least squares fit, one-time build of a full 
model and set a leverage $h_i = \frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum_{i^\prime=1}^n(x_{i^\prime}-\bar{x})^2}$. 
The levearge lies between &lt;em&gt;1/n&lt;/em&gt; and &lt;em&gt;1&lt;/em&gt;, reflects the amount that an observation influences its own fit.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;513-k-fold-cross-validation&quot;&gt;5.1.3. k-Fold Cross-Validation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;random division into &lt;em&gt;k&lt;/em&gt; groups, or &lt;em&gt;folds&lt;/em&gt;, of approximately equal size. A fold is used for the 
validation set, and the method is fit on the remaining &lt;em&gt;k-1&lt;/em&gt; folds. The MSE is computed on the 
observations in the held-out fold and the procedure is repeated &lt;em&gt;k&lt;/em&gt; times.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;k-Fold CV estimate for the test MSE:&lt;br /&gt;
\(CV_{(k)} = \frac{1}{n}\sum_{i=1}^k MSE_i\)&lt;br /&gt;
when &lt;em&gt;k=n&lt;/em&gt;, LOOCV is a special case of k-Fold. Using smaller &lt;em&gt;k&lt;/em&gt;, k-fold CV has a computational 
advantage to LOOCV.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We perform CV to:&lt;br /&gt;
To determine how well a given model can be expected to perform on independent data.&lt;br /&gt;
To identify a model results in the lowest test error, over different models or different levels of 
flexibility.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;514-bias-variance-trade-off-for-k-fold-cross-validation&quot;&gt;5.1.4. Bias-Variance Trade-Off for k-Fold Cross-Validation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Besides the computational advantage, k-fold CV often gives more accurate estimates of the test error 
rate than does LOOCV.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LOOCV will give approximately unbiased estiamtes of the test error, containing &lt;em&gt;n-1&lt;/em&gt;, almost as many as 
the number of observations in the full data set. By contrast, k-fold CV will lead to an intermediate 
level of bias, containing &lt;em&gt;(k-1)n/k&lt;/em&gt; observations. Clearly, LOOCV is to be preferred in the perspective 
of bias reduction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;But, in LOOCV, averaging the outputs of &lt;em&gt;n&lt;/em&gt; fitted models, which are trained on an almost identical set 
of observations, these outputs are highly correlated with each other. This high correlation results in 
higher variance of test error estimate from LOOCV than from k-fold CV.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;515-cross-validation-on-classification-problems&quot;&gt;5.1.5. Cross-Validation on Classification Problems&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;LOOCV on the classification:&lt;br /&gt;
\(CV_{(n)} = \frac{1}{n}\sum_{i=1}^n Err_i\),  where \(Err_i = I(y_i \ne \hat{y}_i)\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;k-fold CV on the classification:&lt;br /&gt;
\(\frac{1}{n}\sum_{i=1}^k MCR_i\).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;52-the-bootstrap&quot;&gt;5.2. The Bootstrap&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Sampling with &lt;em&gt;replacement&lt;/em&gt; on:&lt;br /&gt;
Dataset $Z = (z_1, \ldots, z_n)$, $ z_i = (x_i,y_i)$&lt;br /&gt;
Sample $Z^{*b}$, where $ b = 1, \ldots, B$ samples&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;for Any statistic term $S(Z)$ computed from full dataset &lt;em&gt;Z&lt;/em&gt;,&lt;br /&gt;
and $S(Z^{*b})$ from bootstrap samples,&lt;br /&gt;
\(\begin{align*}
Var(\hat{S(Z)}) = \frac{1}{B-1}\sum_{b=1}^B(S(Z^{*b})-\bar{S}^*)^2
\end{align*}\)&lt;br /&gt;
$\cdots \bar{S}^{*} = \frac{1}{B}\sum_{b=1}^B S(Z^{*b})$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        
        <pubDate>Wed, 22 Apr 2020 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch5</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch5</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 4. Classification</title>
        
          <description>
</description>
        
        <pubDate>Wed, 15 Apr 2020 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch4</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch4</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 3. Linear Regression</title>
        
          <description>
</description>
        
        <pubDate>Thu, 02 Apr 2020 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch3</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch3</guid>
      </item>
      
    
      
      <item>
        <title>ISLR - Chapter 2. Statistical Learning</title>
        
          <description>
</description>
        
        <pubDate>Sun, 15 Mar 2020 15:00:00 +0000</pubDate>
        <link>
        http://0.0.0.0:4000/islr_ch2</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/islr_ch2</guid>
      </item>
      
    
  </channel>
</rss>
