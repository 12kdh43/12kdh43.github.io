<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
	
	<!-- On Post front-matter YAML, set "use_math: true" to use LaTex -->
	
	  
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
        equationNumbers: {
        autoNumber: "AMS"
        }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$', '$$'], ["\\[","\\]"]  ],
    processEscapes: true,
    }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>ISLR - Chapter 7. Moving Beyond Linearity</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
	
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="" />
    <link rel="shortcut icon" href="http://0.0.0.0:4000/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="http://0.0.0.0:4000/islr_ch7" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Darron's Devlog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="ISLR - Chapter 7. Moving Beyond Linearity" />
    <meta property="og:description" content="Chapter 7. Moving Beyond Linearity 7.1. Polynomial Regression 7.2. Step Functions 7.3. Basis Functions 7.4. Regression Splines 7.4.1. Piecewise Polynomials 7.4.2. Constraints and Splines 7.4.3. The Spline Basis Representation 7.4.4. Choosing the Number and Locations of the Knots 7.4.5. Comparison to Polynomial Regression 7.5. Smoothing Splines 7.5.1. An Overview of" />
    <meta property="og:url" content="http://0.0.0.0:4000/islr_ch7" />
    <meta property="og:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2020-05-07T00:00:00+09:00" />
    <meta property="article:modified_time" content="2020-05-07T00:00:00+09:00" />
    <meta property="article:tag" content="ISLR" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="ISLR - Chapter 7. Moving Beyond Linearity" />
    <meta name="twitter:description" content="Chapter 7. Moving Beyond Linearity 7.1. Polynomial Regression 7.2. Step Functions 7.3. Basis Functions 7.4. Regression Splines 7.4.1. Piecewise Polynomials 7.4.2. Constraints and Splines 7.4.3. The Spline Basis Representation 7.4.4. Choosing the Number and Locations of the Knots 7.4.5. Comparison to Polynomial Regression 7.5. Smoothing Splines 7.5.1. An Overview of" />
    <meta name="twitter:url" content="http://0.0.0.0:4000/" />
    <meta name="twitter:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Darron's Devlog" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="ISLR" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Darron's Devlog",
        "logo": "http://0.0.0.0:4000/"
    },
    "url": "http://0.0.0.0:4000/islr_ch7",
    "image": {
        "@type": "ImageObject",
        "url": "http://0.0.0.0:4000/assets/built/images/blog-cover1.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://0.0.0.0:4000/islr_ch7"
    },
    "description": "Chapter 7. Moving Beyond Linearity 7.1. Polynomial Regression 7.2. Step Functions 7.3. Basis Functions 7.4. Regression Splines 7.4.1. Piecewise Polynomials 7.4.2. Constraints and Splines 7.4.3. The Spline Basis Representation 7.4.4. Choosing the Number and Locations of the Knots 7.4.5. Comparison to Polynomial Regression 7.5. Smoothing Splines 7.5.1. An Overview of"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="ISLR - Chapter 7. Moving Beyond Linearity" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="/">Darron's Devlog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-projects" role="menuitem"><a href="/tag/projects/">Projects</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/studies/">Studies</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  tag-islr  no-image">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime=" 7 May 2020"> 7 May 2020</time>
                    
                        <span class="date-divider">/</span>
                        
                            
                               <a href='/tag/islr/'>ISLR</a>
                            
                        
                    
                </section>
                <h1 class="post-full-title">ISLR - Chapter 7. Moving Beyond Linearity</h1>
            </header>
	<!--
            
	-->
            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <ul id="markdown-toc">
  <li><a href="#chapter-7-moving-beyond-linearity" id="markdown-toc-chapter-7-moving-beyond-linearity">Chapter 7. Moving Beyond Linearity</a></li>
  <li><a href="#71-polynomial-regression" id="markdown-toc-71-polynomial-regression">7.1. Polynomial Regression</a></li>
  <li><a href="#72-step-functions" id="markdown-toc-72-step-functions">7.2. Step Functions</a></li>
  <li><a href="#73-basis-functions" id="markdown-toc-73-basis-functions">7.3. Basis Functions</a></li>
  <li><a href="#74-regression-splines" id="markdown-toc-74-regression-splines">7.4. Regression Splines</a>    <ul>
      <li><a href="#741-piecewise-polynomials" id="markdown-toc-741-piecewise-polynomials">7.4.1. Piecewise Polynomials</a></li>
      <li><a href="#742-constraints-and-splines" id="markdown-toc-742-constraints-and-splines">7.4.2. Constraints and Splines</a></li>
      <li><a href="#743-the-spline-basis-representation" id="markdown-toc-743-the-spline-basis-representation">7.4.3. The Spline Basis Representation</a></li>
      <li><a href="#744-choosing-the-number-and-locations-of-the-knots" id="markdown-toc-744-choosing-the-number-and-locations-of-the-knots">7.4.4. Choosing the Number and Locations of the Knots</a></li>
      <li><a href="#745-comparison-to-polynomial-regression" id="markdown-toc-745-comparison-to-polynomial-regression">7.4.5. Comparison to Polynomial Regression</a></li>
    </ul>
  </li>
  <li><a href="#75-smoothing-splines" id="markdown-toc-75-smoothing-splines">7.5. Smoothing Splines</a>    <ul>
      <li><a href="#751-an-overview-of-smoothing-splines" id="markdown-toc-751-an-overview-of-smoothing-splines">7.5.1. An Overview of Smoothing Splines</a></li>
      <li><a href="#752-choosing-the-smoothing-parameter-lambda" id="markdown-toc-752-choosing-the-smoothing-parameter-lambda">7.5.2. Choosing the Smoothing Parameter $\lambda$</a></li>
    </ul>
  </li>
  <li><a href="#76-local-regression" id="markdown-toc-76-local-regression">7.6. Local Regression</a></li>
  <li><a href="#77-generalized-additive-models" id="markdown-toc-77-generalized-additive-models">7.7. Generalized Additive Models</a>    <ul>
      <li><a href="#771-gams-for-regression-problems" id="markdown-toc-771-gams-for-regression-problems">7.7.1. GAMs for Regression Problems</a></li>
      <li><a href="#772-gams-for-classification-problems" id="markdown-toc-772-gams-for-classification-problems">7.7.2. GAMs for Classification Problems</a>        <ul>
          <li><a href="#pros-and-cons-of-gams" id="markdown-toc-pros-and-cons-of-gams">Pros and Cons of GAMs</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="chapter-7-moving-beyond-linearity">Chapter 7. Moving Beyond Linearity</h2>

<h2 id="71-polynomial-regression">7.1. Polynomial Regression</h2>
<ul>
  <li>standard linear model $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$<br />
  to a polynomial function $y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \cdots + \beta_d x_i^d + \epsilon_i$<br />
  For large enough degree <em>d</em>, a polynomial regression produces an extremely non-
  linear curve. The coefficients $\beta_d$’s can be easily estimated using least 
  squares linear regression.</li>
</ul>

<h2 id="72-step-functions">7.2. Step Functions</h2>
<ul>
  <li>
    <p>Break the range of <em>X</em> into <em>bins</em>, and fit a different constant in each bin; 
  converting a continuous variables into an <em>ordered categorical variable</em>.</p>
  </li>
  <li>
    <p>By cutpoints $c_1, c_2, \ldots, c_K$ in the range of <em>X</em>, construct <em>K+1</em> new variables<br />
  \(\begin{align*}
  C_0(X) &amp;= I(X &lt; c_1), \\
  C_1(X) &amp;= I(c_1 \le X &lt; c_2), \\
  C_2(X) &amp;= I(c_2 \le X &lt; c_3), \\
         &amp;\vdots \\
  C_{K-1}(X) &amp;= I(c_{K-1} \le X &lt; c_K), \\
  C_{K}(X) &amp;= I(c_K \le X), \\
  \end{align*}\)<br />
  where I($\cdot$) is an <em>indicator function</em> returning value of 1 or 0. These are 
  sometimes called <em>dummy variables</em>. For any value of <em>X</em>, $\sum_{i=0}^K C_i(X) = 1$.</p>
  </li>
  <li>
    <p>The least squares model: $y_i = \beta_0 + \beta_1 C_1(x_i) + \cdots + \beta_K C_K(x_i) + \epsilon_i$<br />
  When $X&lt;c_1$, all of the predictors $C_1(X), C_2(X), \ldots, C_K(X)$ are zero, so 
  $\beta_0$ is the mean value of <em>Y</em> or $X&lt;c_1$. By comparison, the model predicts 
  a response of $\beta_0 + \beta_j$ for $c_j \le X &lt; c_{j+1}$, so $\beta_j$ represents the 
  average increase in the response for <em>X</em> in such range.</p>
  </li>
  <li>
    <p>Unless there are natural breakpoints in the predictors, piecewise-constant functions 
  can miss the action.</p>
  </li>
</ul>

<h2 id="73-basis-functions">7.3. Basis Functions</h2>
<ul>
  <li>
    <p>Polynomial and piecewise-constant regression models are in fact special cases of a 
  <em>basis function</em> approach; with a family of functions or transformations applied 
  to a variable <em>X</em>: $b_1(X), b_2(X), \ldots, b_K(X)$, fit a linear model<br />
  $y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \cdots + \beta_K b_K(x_i) + \epsilon_i$.<br />
  as a standard linear model with predictors $b_i(x_i), b_2(x_i), \ldots, b_K(x_i)$. 
  Hence, we can use least squares and all of the inference tools, such as std.err 
  for coefficient estimates and F-statistics for the model’s overall significance.</p>
  </li>
  <li>
    <p>Note that the basis functions $b_k(\cdot)$ are fixed and known(or, we choose the 
  functions before). E.g., for polynomial regression, the basis functions are $b_j(x_i)=x_i^j$.</p>
  </li>
</ul>

<h2 id="74-regression-splines">7.4. Regression Splines</h2>

<h3 id="741-piecewise-polynomials">7.4.1. Piecewise Polynomials</h3>
<ul>
  <li>
    <p>Instead of fitting a high-degree polynomial over the entire range of <em>X</em>, piecewise 
  polynomial regression fits separate low-degree polynomials over different regions. 
  Divide dimension <em>X</em> by <em>knots</em> and apply different polynomial model to each 
  region. If we place <em>K</em> knots throughout the range of <em>X</em>, then we fit <em>K+1</em> different 
  models.</p>
  </li>
  <li>
    <p>a cubic regression model; $y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$<br />
  is a piecewise cubic with no knots with <em>d</em> = 3.<br />
  On the other hand,<br />
  \(y_i = \begin{cases}
          \beta_{01}+\beta_{11}x_i + \beta_{21}x_i^2 + \beta_{31}x_i^3 + \epsilon_i &amp; \mbox{if }x_i &lt; c \\
          \beta_{02}+\beta_{12}x_i + \beta_{22}x_i^2 + \beta_{32}x_i^3 + \epsilon_i &amp; \mbox{if }x_i \ge c. \\
          \end{cases}\)<br />
  is with a single knot at a point c.</p>
  </li>
  <li>
    <p>For other degrees <em>d</em>:<br />
  a piecewise-constant functions are piecewise polynomials of degree <em>0</em>, a piecewise 
  linear functions are of degree <em>1</em>.</p>
  </li>
</ul>

<h3 id="742-constraints-and-splines">7.4.2. Constraints and Splines</h3>
<ul>
  <li>To fix the discontinuity and overcomplexity, we add some additional constraints:<br />
  in a degree-<em>d</em> spline, or a piecewise degree-<em>d</em> polynomial, it requires the 
  continuity in derivatives up to degree <em>d-1</em> at each knot.</li>
</ul>

<h3 id="743-the-spline-basis-representation">7.4.3. The Spline Basis Representation</h3>
<ul>
  <li>
    <p>$y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \cdots + \beta_{K+3} b_{K+3}(x_i) + \epsilon_i$<br />
  is a cubic spline model with <em>K</em> knots.</p>
  </li>
  <li>
    <p>with a <em>truncated power basis</em> function per knot $\xi$, which is defined as 	
  \(h(x,\xi) = (x-\xi)_{+}^3 = \begin{cases} (x-\xi)^3 &amp; \mbox{if }x&gt;\xi \\
                                                  0	 &amp; \mbox{otherwise,}
                               \end{cases}\)<br />
  so, in fitting a cubic spline with <em>K</em> knots, we perform least squares regression 
  with an intercept and <em>3+K</em> predictors, of the form $X, X^2, X^3, h(X,\xi_1), h(X,\xi_2), \ldots, h(X,\xi_K)$.<br />
  This amounts to estimating a total of <em>K+4</em> regression coefficients; for this 
  regression, fitting a cubic spline with K knots uses <em>K+4</em> degrees of freedom.</p>
  </li>
  <li>
    <p>However, splines can have high variance at the outer range of the predictors; 
  $X&lt;c_1$ or $X\ge c_K$, when <em>X</em> takes on either a very small or very large value. 
  We see that the confidence bands in the boundary region appear fairly wild.<br />
  A <em>natural spline</em> with additional <em>boundary constraints</em> that the function is 
  required to be linear at the boundary, generally produce more stable estimates. In 
  this case, the corresponding confidence intervals are narrower.</p>
  </li>
</ul>

<h3 id="744-choosing-the-number-and-locations-of-the-knots">7.4.4. Choosing the Number and Locations of the Knots</h3>
<ul>
  <li>
    <p>The regression spline is most flexible in regions that contain a lot of knots, because 
  in those regions the polynomial coefficients can change rapidly. Hence, one option 
  is to place more knots in places where the function might vary most rapidly, and 
  to place fewer knots where it seems more stable. In practice, it is common to place 
  knots in a uniform fashion.</p>
  </li>
  <li>
    <p>To determine the number of knots, or equivalently the degrees of freedom of the spline 
  contain, we use cross-validation method.</p>
  </li>
</ul>

<h3 id="745-comparison-to-polynomial-regression">7.4.5. Comparison to Polynomial Regression</h3>
<ul>
  <li>
    <p>Regression splines often give superior results to polynomial regression. This is 
  because unlike polynomials, which must use a high degree to produce flexible fits, 
  splines introduce flexibility by increasing the number of knots but keeping the 
  degree fixed. Generally, this approach produces more stable estimates.</p>
  </li>
  <li>
    <p>Splines can produce a reasonable fit at the boundaries and also allow us to place 
  more knots, or flexibility, over regions where the function <em>f</em> seems to be changing 
  rapidly, and fewer knots where <em>f</em> appears more stable.</p>
  </li>
</ul>

<h2 id="75-smoothing-splines">7.5. Smoothing Splines</h2>

<h3 id="751-an-overview-of-smoothing-splines">7.5.1. An Overview of Smoothing Splines</h3>
<ul>
  <li>
    <p>In fitting a smooth curve to a set of data, some function $g(x)$, that fits the 
  observed data well: Minimizing $RSS = \sum_{i=1}^n(y_i-g(x_i))^2$. But if we don’t 
  put any constraints on <em>g</em>, then we can always make <em>RSS</em> zero simply by choosing 
  <em>g</em> such that it <em>interpolates</em> all of the $y_i$. Such a function would woefully 
  overfit the data, would be too flexible.</p>
  </li>
  <li>
    <p>Smoothing spline is the function <em>g</em> that minimizes<br />
  \(\sum_{i=1}^n(y_i - g(x_i))^2 + \lambda\int g''(t)^2, dt\)<br />
  with a nonnegative tuning parameter $\lambda$, takes the “Loss+Penalty” formulation 
  like the ridge and lasso. The term $\sum_{i=1}^n(y_i - g(x_i))^2$ is a <em>loss function</em> 
  that makes <em>g</em> to fit the data well, and the term \(\lambda\int g''(t)^2, dt\) is a 
  <em>penalty term</em> that reduces the variability in <em>g</em>. \(g''(t)\) indicates the second 
  derivative of the function <em>g</em>, corresponds to the amount by which the slope(the first 
  derivative) is changing. Broadly speaking, the second derivative is a measure of 
  its <em>roughness</em>: large in absolute value if <em>g(t)</em> is very wiggly near <em>t</em>, close 
  to zero otherwise. It is zero as the derivative of a straight line, the function is 
  perfectly smooth. The integral is a summation over the range of <em>t</em>, so \(\int g''(t)^2, dt\) 
  is a measure of the total change in the function $g’(t)$ over its entire range. If 
  <em>g</em> is very smooth, then $g’(t)$ will be close to constant and \(\int g''(t)^2, dt\) will 
  take on a small value. Therefore, the penalty term encourages <em>g</em> to be smooth. 
  The larger the $\lambda$, the smoother the <em>g</em> will be.<br />
  When $\lambda=0$, then there’s no penalty and the function <em>g</em> will be very jumpy 
  and have perfect fit. When $\lambda\rightarrow\infty$, <em>g</em> will be perfectly smooth; 
  a linear least squares line $g(x)=ax+b$. The $\lambda$ controls the bias-variance 
  trade-off of the smoothing spline.</p>
  </li>
  <li>
    <p>Smoothing spline <em>g(x)</em> have some special properties: it is a piecewise polynomial 
  with knots at the unique values of $x_1,\ldots,x_n$, and continuous first and second 
  derivatives at each knot. Furthermore, it is linear in the region outside of the 
  extreme knots.<br />
  In other words, the Smoothing spline function is a nautral spline with knots at 
  $x_1,\ldots,x_n$; it is a shrunken version of such a natural spline, where the value 
  of the tuning parameter $\lambda$ controls the level of shrinkage.</p>
  </li>
</ul>

<h3 id="752-choosing-the-smoothing-parameter-lambda">7.5.2. Choosing the Smoothing Parameter $\lambda$</h3>
<ul>
  <li>
    <p>Have seen that a smoothing spline is simply a natural spline with knots at every unique 
  value of $x_i$, it might seem that a smoothing spline will have far too many degrees 
  of freedom, since a knot at each data point allows a great deal of flexibility. 
  But $\lambda$ controls the roughness of the smoothing spline, and hence the <em>effective</em> 
  <em>degrees of freedom</em>. We can show that as $\lambda$ increase from <em>0</em> to $\infty$, 
  the effective degrees of freedom $df_\lambda$ decrease from <em>n</em> to 2.</p>
  </li>
  <li>
    <p>Usually degrees of freedom refer to the number of free parameters, such as the number 
  of coefficients fit in a polynomial or cubic spline. Though a smoothing spline has 
  <em>n</em> parameters and <em>n</em> nominal degrees of freedom, these <em>n</em> parameters are heavily 
  constrainted or shrunk down. Thus $df_\lambda$ is a measure of the flexibility of 
  the smoothing spline, the higher it is, the more flexible the smoothing spline.</p>
  </li>
  <li>
    <p>The definition of effective degrees of freedom, the measure of model complexity:<br />
  \(\hat{\mathbf{g}}_\lambda = \mathbf{S}_\lambda\mathbf{y}\),<br />
  where \(\hat{\mathbf{g}}_\lambda\) is the solution to the smoothing spline function 
  <em>g</em> for a particular choice of $\lambda$, it is an <em>n</em>-vector containing the fitted 
  values of the model at the training points $x_1,\ldots,x_n$.<br />
  Then the effective degrees of freedom is defined to be<br />
  \(df_\lambda = \text{trace}(\mathbf{S}_\lambda) = \sum_{i=1}^n\{ \mathbf{S}_\lambda \}_{ii}\),<br />
  the sum of the diagonal elements of the matrix $\mathbf{S}_\lambda$.<br />
  e.g.) for a linear regression model:<br />
  \(\mathbb{H} = \mathbb{X}(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\),<br />
  \(\text{trace}(\mathbb{H}) = p+1\)</p>
  </li>
  <li>
    <p>In fitting a smoothing spline, we do not need to select the number or location of the 
  knots; there will be a knot at each training observation. Instead, we need to choose 
  the value of $\lambda$. For this case, LOOCV can be computed very efficiently with 
  essentially the same cost as computing a single fit, using the following formula:<br />
  \(RSS_{cv}(\lambda) = \sum_{i=1}^n(y_i - \hat{g}_\lambda^{(-i)}(x_i))^2 
  = \sum_{i=1}^n\left[\frac{y_i-\hat{g}_\lambda(x_i)}{1-\{\mathbf{S}_\lambda\}_{ii}}\right]^2\)<br />
  where \(\hat{g}_\lambda^{(-i)}(x_i)\) indicates the fitted value for this smoothing 
  spline evaluated at $x_i$, where the fit uses all the training observations except 
  for the <em>i</em>th observation. In contrast, \(\hat{g}_\lambda(x_i)\) indicates the smoothing 
  spline evaluated at $x_i$, where the function is fit to the full data. Thus, we can compute 
  each of LOOCV fits, by one-time computing of the original fit to all of the data.</p>
  </li>
  <li>
    <p>By effective d.f, we can directly compare the model complexities of models discussed so 
  far, such as linear regression, ridge regression, smoothing splines, cubic splines, etc.</p>
  </li>
</ul>

<h2 id="76-local-regression">7.6. Local Regression</h2>
<ul>
  <li>
    <p>A different approach for fitting flexible non-linear functions, with the idea of KNN 
  but closer observations have more weights. This is sometimes referred to as a <em>memory-based</em> 
  procedure, because we need all the training data each time we compute a prediction.</p>
  </li>
  <li>Algorithm: Local Regression At $X = x_0$
    <ol>
      <li>Gather the faction $s = k/n$ of training points whose $x_i$ are closest to $x_0$.</li>
      <li>Assign a weight $K_{i0} = K(x_i,x_0)$ to each point in this neighborhood, so that 
  the point furthest from $x_0$ has weight zero, and the closest has the highest weight. 
  All other points get weight zero.</li>
      <li>Fit a <em>weighted least squares regression</em> using the aforementioned weights;<br />
  Objective function: \(\text{min}_{\beta_0,\beta_1}
                 \left[\sum_{i=1}^n K_{i0}(y_i - \beta_0 - \beta_1 x_i)^2 \right]\)</li>
      <li>The fitted value at $x_0$ is given by $\hat{f}(x_0) = \hat\beta_0 + \hat\beta_1 x_0$.</li>
    </ol>
  </li>
  <li>
    <p>In performing local regression, there are choices to be made, such as how to define 
  the weighting function <em>K</em>, and which regression model to fit. The most important 
  choice is the <em>span s</em>, which is the proportion of points used to compute the local 
  regression at $x_0$; “How many neighbors?”. It plays a role of the tuning parameter 
  $\lambda$, controls the flexibility of the fit. The smaller <em>s</em>, the more <em>local</em> and 
  wiggly the fit. We can use CV methods to choose <em>s</em>, or we can specify it directly.</p>
  </li>
  <li>
    <p>The idea of local regression can be generalized in many different ways. In multivariate 
  settings, one very useful generalization involves fitting a multiple linear regression 
  model that is global in some variables, but local in another, such as time. Such 
  <em>varying coefficient models</em> are a useful way of adapting a model to the most recently 
  gathered data.</p>
  </li>
  <li>Local regression can perform poorly if the dimension <em>p</em> is much larger, suffers the 
  dimensionality problem. Also, it has a boundary problem because there are less data 
  points for neighbors.</li>
</ul>

<h2 id="77-generalized-additive-models">7.7. Generalized Additive Models</h2>
<ul>
  <li>Approaches presented in sections above can be seen as extensions of simple linear 
  regression, flexibly predicting a response <em>Y</em> on the basis of a single predictor <em>X</em>. 
  GAMs provide a general framework for extending a standard linear model by allowing 
  non-linear functions of each of the variables, while maintaining <em>additivity</em>. They 
  can be applied with both quantitative and qualitative responses.</li>
</ul>

<h3 id="771-gams-for-regression-problems">7.7.1. GAMs for Regression Problems</h3>
<ul>
  <li>
    <p>standard multiple linear regression model is<br />
  \(y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \epsilon_i\).<br />
  for non-linear relationships, replace each linear component \(\beta_j x_{ij}\) with an 
  unspecified (smoothing) non-linear function \(f_j(x_{ij})\), now the model is<br />
  \(\begin{align*}
  y_i &amp;= \beta_0 + \sum_{j=1}^p f_j(x_{ij}) + \epsilon_i \\
      &amp;= \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \cdots + f_p(x_{ip}) + \epsilon_i.
  \end{align*}\)<br />
  It is called an additive model because we calculate a separate <em>f</em> for each <em>X</em>, 
  then add together all of their contributions.</p>
  </li>
  <li>
    <p>In case of using a smoothing spline to fit a GAM, it is not quite as simple as a 
  natural spline case, since the least squares cannot be used. However, Standard 
  softwares have some functions for GAMs using smoothing splines, via an approach 
  known as <em>backfitting</em>; a method to fit a multivariate model by repeatedly updating 
  the fit for each predictor in turn, holding the others fixed. Each time we update 
  a function, we simply apply the fitting method for that variable to a partial residual.</p>
  </li>
</ul>

<h3 id="772-gams-for-classification-problems">7.7.2. GAMs for Classification Problems</h3>
<ul>
  <li>For qualitative response <em>Y</em>, standard logistic regression model is<br />
  \(\log\left(\frac{p(X)}{1-p(X)}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p.\)<br />
  An extension allowing non-linear relationships; a logistic regression GAM is<br />
  \(\log\left(\frac{p(X)}{1-p(X)}\right) = \beta_0 + f_1(X_1) + f_2(X_2) + \cdots + f_p(X_p).\)</li>
</ul>

<h4 id="pros-and-cons-of-gams">Pros and Cons of GAMs</h4>
<ul>
  <li>Pros
    <ol>
      <li>GAMs allow us to fit a non-linear function to each variable, so that we can 
 automatically model non-linear relationships. This means we do not need to 
 manually try out many different transformations on each variable individually.</li>
      <li>Potentially make more accurate predictions with non-linear fits.</li>
      <li>Because the model is additive, we can examine the effect of each variable on the 
 response individually while holding all of the other variables fixed.</li>
      <li>The smoothness of the function for the variable can be summarized via degrees 
 of freedom.</li>
    </ol>
  </li>
  <li>Cons
    <ol>
      <li>The model is restricted to be additive; with many variables, important interactions 
 can be missed. However, we can manually add interaction terms to the model by 
 including additional predictors of the form like $X_j \times X_k$.</li>
      <li>The solution of the optimization is not unique; $\beta_0$ is not identifiable 
 because each $f_j$ model has its intercept term; a GAM has <em>p+1</em> total intercepts 
 and they are not distinguishable. For this problem, we make a restriction that 
 every <em>j</em>th <em>X</em> variable to be centered; \(\sum_{i=1}^n f_j(x_{ij}) = 0\) and 
 $\hat\beta_0 = \bar{y}$.</li>
    </ol>
  </li>
</ul>


                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
	<!--
            
                <section class="subscribe-form">
                    <h3 class="subscribe-form-title">Subscribe to Darron's Devlog</h3>
                    <p>Get the latest posts delivered right to your inbox</p>
                    <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
                </section>
            
	-->
            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            var this_page_url = 'http://0.0.0.0:4000/islr_ch7';
                            var this_page_identifier = '/islr_ch7';
                            var this_page_title = 'ISLR - Chapter 7. Moving Beyond Linearity';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            


        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/built/images/blog-cover1.png)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; Darron's Devlog &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/islr/">Islr</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/islr_ch10">ISLR - Chapter 10. Deep Learning</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/islr_ch9">ISLR - Chapter 9. Support Vector Machines</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/islr_ch8">ISLR - Chapter 8. Tree-Based Methods</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/islr/">
                                
                                    See all 8 posts  →
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                    <article class="post-card post-template no-image">
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/NLP_eng_ml">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Projects</span>
                            
                        
                    

                    <h2 class="post-card-title">NLP - Text Analysis with ML algorithms</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>
</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
            </footer>
        </div>
    </article>

            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                    <article class="post-card post-template no-image">
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/islr_ch6">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Islr</span>
                            
                        
                    

                    <h2 class="post-card-title">ISLR - Chapter 6. Linear Model Selection and Regularization</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>Chapter 6. Linear Model Selection and Regularization 6.1. Subset Selection 6.1.1. Best Subset Selection 6.1.2. Stepwise Selection Forward Stepwise Selection Backward Stepwise Selection Hybrid Approaches 6.1.3. Choosing the Optimal Model Validation and Cross-Validation</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="/">
            
            <span>Darron's Devlog</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">ISLR - Chapter 7. Moving Beyond Linearity</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=ISLR+-+Chapter+7.+Moving+Beyond+Linearity&amp;url=https://12kdh43.github.io/islr_ch7"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://12kdh43.github.io/islr_ch7"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Darron's Devlog</a> &copy; 2021</section>
                <!-- 
				<section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                -->
				<nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search Darron's Devlog</h1>
                <p class="subscribe-overlay-description">
				</p>
                <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

 </script>

	
    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
