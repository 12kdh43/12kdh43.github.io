<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
	
	<!-- On Post front-matter YAML, set "use_math: true" to use LaTex -->
	
	  
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
        equationNumbers: {
        autoNumber: "AMS"
        }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$', '$$'], ["\\[","\\]"]  ],
    processEscapes: true,
    }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Assignment 1 - Q4, Two-Layer Neural Network</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
	
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="" />
    <link rel="shortcut icon" href="http://0.0.0.0:4000/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="http://0.0.0.0:4000/cs231n_a1_q4" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Darron's Devlog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Assignment 1 - Q4, Two-Layer Neural Network" />
    <meta property="og:description" content="My solution to the assignments of the CS231n course from Stanford University(Spring 2021) You can find official courseworks and assignments here. Assignment 1 - Q4: Two-Layer Neural Network layers.py def affine_forward(x, w, b): """ Computes the forward pass for an affine (fully-connected) layer. The input x has shape (N, d_1," />
    <meta property="og:url" content="http://0.0.0.0:4000/cs231n_a1_q4" />
    <meta property="og:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2022-01-09T03:00:00+09:00" />
    <meta property="article:modified_time" content="2022-01-09T03:00:00+09:00" />
    <meta property="article:tag" content="cs231n" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Assignment 1 - Q4, Two-Layer Neural Network" />
    <meta name="twitter:description" content="My solution to the assignments of the CS231n course from Stanford University(Spring 2021) You can find official courseworks and assignments here. Assignment 1 - Q4: Two-Layer Neural Network layers.py def affine_forward(x, w, b): """ Computes the forward pass for an affine (fully-connected) layer. The input x has shape (N, d_1," />
    <meta name="twitter:url" content="http://0.0.0.0:4000/" />
    <meta name="twitter:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Darron's Devlog" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="cs231n" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Darron's Devlog",
        "logo": "http://0.0.0.0:4000/"
    },
    "url": "http://0.0.0.0:4000/cs231n_a1_q4",
    "image": {
        "@type": "ImageObject",
        "url": "http://0.0.0.0:4000/assets/built/images/blog-cover1.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://0.0.0.0:4000/cs231n_a1_q4"
    },
    "description": "My solution to the assignments of the CS231n course from Stanford University(Spring 2021) You can find official courseworks and assignments here. Assignment 1 - Q4: Two-Layer Neural Network layers.py def affine_forward(x, w, b): """ Computes the forward pass for an affine (fully-connected) layer. The input x has shape (N, d_1,"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Assignment 1 - Q4, Two-Layer Neural Network" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="/">Darron's Devlog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-projects" role="menuitem"><a href="/tag/projects/">Projects</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/studies/">Studies</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  tag-cs231n tag-assignment  no-image">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime=" 9 January 2022"> 9 January 2022</time>
                    
                        <span class="date-divider">/</span>
                        
							
                            
                               <a href='/tag/cs231n/'>CS231N</a>,
                            
                        
							
								<a href='/tag/cs231n/assignment/'>ASSIGNMENT</a>
								
                    
                </section>
                <h1 class="post-full-title">Assignment 1 - Q4, Two-Layer Neural Network</h1>
            </header>
	<!--
            
	-->
            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <p>My solution to the assignments of the CS231n course from Stanford University(Spring 2021)</p>

<p>You can find official courseworks and assignments <a href="https://cs231n.github.io/assignments2021/assignment1/" target="_blank">here</a>.</p>

<h2 id="assignment-1---q4-two-layer-neural-network">Assignment 1 - Q4: Two-Layer Neural Network</h2>

<h3 id="layerspy">layers.py</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">affine_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="s">"""
    Computes the forward pass for an affine (fully-connected) layer.

    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N
    examples, where each example x[i] has shape (d_1, ..., d_k). We will
    reshape each input into a vector of dimension D = d_1 * ... * d_k, and
    then transform it to an output vector of dimension M.

    Inputs:
    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)
    - w: A numpy array of weights, of shape (D, M)
    - b: A numpy array of biases, of shape (M,)

    Returns a tuple of:
    - out: output, of shape (N, M)
    - cache: (x, w, b)
    """</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="c1">###########################################################################
</span>    <span class="c1"># TODO: Implement the affine forward pass. Store the result in out. You   #
</span>    <span class="c1"># will need to reshape the input into rows.                               #
</span>    <span class="c1">###########################################################################
</span>    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">w</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>    <span class="c1">###########################################################################
</span>    <span class="c1">#                             END OF YOUR CODE                            #
</span>    <span class="c1">###########################################################################
</span>    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">affine_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
    <span class="s">"""
    Computes the backward pass for an affine layer.

    Inputs:
    - dout: Upstream derivative, of shape (N, M)
    - cache: Tuple of:
      - x: Input data, of shape (N, d_1, ... d_k)
      - w: Weights, of shape (D, M)
      - b: Biases, of shape (M,)

    Returns a tuple of:
    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)
    - dw: Gradient with respect to w, of shape (D, M)
    - db: Gradient with respect to b, of shape (M,)
    """</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">cache</span>
    <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
    <span class="c1">###########################################################################
</span>    <span class="c1"># TODO: Implement the affine backward pass.                               #
</span>    <span class="c1">###########################################################################
</span>    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">w</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">T</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D</span><span class="p">).</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span>

    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>    <span class="c1">###########################################################################
</span>    <span class="c1">#                             END OF YOUR CODE                            #
</span>    <span class="c1">###########################################################################
</span>    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">relu_forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="s">"""
    Computes the forward pass for a layer of rectified linear units (ReLUs).

    Input:
    - x: Inputs, of any shape

    Returns a tuple of:
    - out: Output, of the same shape as x
    - cache: x
    """</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="c1">###########################################################################
</span>    <span class="c1"># TODO: Implement the ReLU forward pass.                                  #
</span>    <span class="c1">###########################################################################
</span>    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>    <span class="c1">###########################################################################
</span>    <span class="c1">#                             END OF YOUR CODE                            #
</span>    <span class="c1">###########################################################################
</span>    <span class="n">cache</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">relu_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
    <span class="s">"""
    Computes the backward pass for a layer of rectified linear units (ReLUs).

    Input:
    - dout: Upstream derivatives, of any shape
    - cache: Input x, of same shape as dout

    Returns:
    - dx: Gradient with respect to x
    """</span>
    <span class="n">dx</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">cache</span>
    <span class="c1">###########################################################################
</span>    <span class="c1"># TODO: Implement the ReLU backward pass.                                 #
</span>    <span class="c1">###########################################################################
</span>    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>
    <span class="n">dout</span><span class="p">[</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span>

    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>    <span class="c1">###########################################################################
</span>    <span class="c1">#                             END OF YOUR CODE                            #
</span>    <span class="c1">###########################################################################
</span>    <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">svm_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="s">"""
    Computes the loss and gradient using for multiclass SVM classification.

    Inputs:
    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth
      class for the ith input.
    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and
      0 &lt;= y[i] &lt; C

    Returns a tuple of:
    - loss: Scalar giving the loss
    - dx: Gradient of the loss with respect to x
    """</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
    <span class="c1">###########################################################################
</span>    <span class="c1"># TODO: Implement loss and gradient for multiclass SVM classification.    #
</span>    <span class="c1"># This will be similar to the svm loss vectorized implementation in       #
</span>    <span class="c1"># cs231n/classifiers/linear_svm.py.                                       #
</span>    <span class="c1">###########################################################################
</span>    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>
    <span class="n">num_train</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">correct_class_score</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="n">num_train</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">margin</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span> <span class="o">-</span> <span class="n">correct_class_score</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">margin</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">margin</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_train</span>

    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">margin</span><span class="p">)</span>
    <span class="n">dx</span><span class="p">[</span><span class="n">margin</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">num_safe</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dx</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">-=</span> <span class="n">num_safe</span>
    <span class="n">dx</span> <span class="o">/=</span> <span class="n">num_train</span>

    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>    <span class="c1">###########################################################################
</span>    <span class="c1">#                             END OF YOUR CODE                            #
</span>    <span class="c1">###########################################################################
</span>    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dx</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">softmax_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="s">"""
    Computes the loss and gradient for softmax classification.

    Inputs:
    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth
      class for the ith input.
    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and
      0 &lt;= y[i] &lt; C

    Returns a tuple of:
    - loss: Scalar giving the loss
    - dx: Gradient of the loss with respect to x
    """</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
    <span class="c1">###########################################################################
</span>    <span class="c1"># TODO: Implement the loss and gradient for softmax classification. This  #
</span>    <span class="c1"># will be similar to the softmax loss vectorized implementation in        #
</span>    <span class="c1"># cs231n/classifiers/softmax.py.                                          #
</span>    <span class="c1">###########################################################################
</span>    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>
    <span class="n">num_train</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">U</span> <span class="o">/</span> <span class="n">c</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">]))</span> <span class="o">/</span> <span class="n">num_train</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">dx</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="n">dx</span> <span class="o">/=</span> <span class="n">num_train</span>

    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>    <span class="c1">###########################################################################
</span>    <span class="c1">#                             END OF YOUR CODE                            #
</span>    <span class="c1">###########################################################################
</span>    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dx</span>
</code></pre></div></div>

<h3 id="fc_netpy">fc_net.py</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TwoLayerNet</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="s">"""
    A two-layer fully-connected neural network with ReLU nonlinearity and
    softmax loss that uses a modular layer design. We assume an input dimension
    of D, a hidden dimension of H, and perform classification over C classes.

    The architecure should be affine - relu - affine - softmax.

    Note that this class does not implement gradient descent; instead, it
    will interact with a separate Solver object that is responsible for running
    optimization.

    The learnable parameters of the model are stored in the dictionary
    self.params that maps parameter names to numpy arrays.
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="mi">3</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">weight_scale</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">reg</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="s">"""
        Initialize a new network.

        Inputs:
        - input_dim: An integer giving the size of the input
        - hidden_dim: An integer giving the size of the hidden layer
        - num_classes: An integer giving the number of classes to classify
        - weight_scale: Scalar giving the standard deviation for random
          initialization of the weights.
        - reg: Scalar giving L2 regularization strength.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reg</span> <span class="o">=</span> <span class="n">reg</span>

        <span class="c1">############################################################################
</span>        <span class="c1"># TODO: Initialize the weights and biases of the two-layer net. Weights    #
</span>        <span class="c1"># should be initialized from a Gaussian centered at 0.0 with               #
</span>        <span class="c1"># standard deviation equal to weight_scale, and biases should be           #
</span>        <span class="c1"># initialized to zero. All weights and biases should be stored in the      #
</span>        <span class="c1"># dictionary self.params, with first layer weights                         #
</span>        <span class="c1"># and biases using the keys 'W1' and 'b1' and second layer                 #
</span>        <span class="c1"># weights and biases using the keys 'W2' and 'b2'.                         #
</span>        <span class="c1">############################################################################
</span>        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span> <span class="o">*</span> <span class="n">weight_scale</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span> <span class="o">*</span> <span class="n">weight_scale</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span>

        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>        <span class="c1">############################################################################
</span>        <span class="c1">#                             END OF YOUR CODE                             #
</span>        <span class="c1">############################################################################
</span>
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="s">"""
        Compute loss and gradient for a minibatch of data.

        Inputs:
        - X: Array of input data of shape (N, d_1, ..., d_k)
        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].

        Returns:
        If y is None, then run a test-time forward pass of the model and return:
        - scores: Array of shape (N, C) giving classification scores, where
          scores[i, c] is the classification score for X[i] and class c.

        If y is not None, then run a training-time forward and backward pass and
        return a tuple of:
        - loss: Scalar value giving the loss
        - grads: Dictionary with the same keys as self.params, mapping parameter
          names to gradients of the loss with respect to those parameters.
        """</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="c1">############################################################################
</span>        <span class="c1"># TODO: Implement the forward pass for the two-layer net, computing the    #
</span>        <span class="c1"># class scores for X and storing them in the scores variable.              #
</span>        <span class="c1">############################################################################
</span>        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>        <span class="c1"># affine_relu - affine - softmax
</span>        <span class="n">z_out</span><span class="p">,</span> <span class="n">z_cache</span> <span class="o">=</span> <span class="n">affine_relu_forward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">])</span>
        <span class="n">scores</span><span class="p">,</span> <span class="n">s_cache</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">z_out</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">])</span>

        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>        <span class="c1">############################################################################
</span>        <span class="c1">#                             END OF YOUR CODE                             #
</span>        <span class="c1">############################################################################
</span>
        <span class="c1"># If y is None then we are in test mode so just return scores
</span>        <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span>

        <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="p">{}</span>
        <span class="c1">############################################################################
</span>        <span class="c1"># TODO: Implement the backward pass for the two-layer net. Store the loss  #
</span>        <span class="c1"># in the loss variable and gradients in the grads dictionary. Compute data #
</span>        <span class="c1"># loss using softmax, and make sure that grads[k] holds the gradients for  #
</span>        <span class="c1"># self.params[k]. Don't forget to add L2 regularization!                   #
</span>        <span class="c1">#                                                                          #
</span>        <span class="c1"># NOTE: To ensure that your implementation matches ours and you pass the   #
</span>        <span class="c1"># automated tests, make sure that your L2 regularization includes a factor #
</span>        <span class="c1"># of 0.5 to simplify the expression for the gradient.                      #
</span>        <span class="c1">############################################################################
</span>        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">dout</span> <span class="o">=</span> <span class="n">softmax_loss</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">reg</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">],</span> <span class="mi">2</span><span class="p">)))</span>
        <span class="n">dz</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">affine_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">s_cache</span><span class="p">)</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">reg</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span>
        <span class="n">dx</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">affine_relu_backward</span><span class="p">(</span><span class="n">dz</span><span class="p">,</span> <span class="n">z_cache</span><span class="p">)</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">reg</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span>

        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>        <span class="c1">############################################################################
</span>        <span class="c1">#                             END OF YOUR CODE                             #
</span>        <span class="c1">############################################################################
</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span>
</code></pre></div></div>
<h3 id="inline-question-1"><strong>Inline Question 1</strong></h3>
<p>Weâ€™ve only asked you to implement ReLU, but there are a number of different activation functions that one could use in neural networks, each with its pros and cons. In particular, an issue commonly seen with activation functions is getting zero (or close to zero) gradient flow during backpropagation. Which of the following activation functions have this problem? If you consider these functions in the one dimensional case, what types of input would lead to this behaviour?</p>
<ol>
  <li>Sigmoid</li>
  <li>ReLU</li>
  <li>Leaky ReLU</li>
</ol>

<p>$\color{blue}{\textit Your Answer:}$ Hence the sigmoid function is $s = \frac{1}{1+e^{-z}}$, an output from the sigmoid function is in a range of <code class="language-plaintext highlighter-rouge">(0,1]</code>. The derivative of the sigmoid function is $s(1-s)$ and the output of the derivative of the loss function is always between <em>0</em> and <em>1/4</em>. In a backpropagation, the derivatives are multiplied through the network to compute the gradients of initial layers. For any input <em>z</em> with a large absolute value, say <em>10</em>, its derivative will be close to <em>0</em> and multiplied to the gradients. In this case, we have a vanishing gradient problem.</p>

<h3 id="hyperparameter-tuning">Hyperparameter Tuning</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_model</span> <span class="o">=</span> <span class="bp">None</span>

<span class="c1">#################################################################################
# TODO: Tune hyperparameters using the validation set. Store your best trained  #
# model in best_model.                                                          #
#                                                                               #
# To help debug your network, it may help to use visualizations similar to the  #
# ones we used above; these visualizations will have significant qualitative    #
# differences from the ones we saw above for the poorly tuned network.          #
#                                                                               #
# Tweaking hyperparameters by hand can be fun, but you might find it useful to  #
# write code to sweep through possible combinations of hyperparameters          #
# automatically like we did on thexs previous exercises.                          #
#################################################################################
# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>

<span class="n">best_accuracy</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="n">input_size</span> <span class="o">=</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"learning_rates"</span> <span class="p">:</span> <span class="p">[</span><span class="mf">2.5e-3</span><span class="p">,</span> <span class="mf">2e-3</span><span class="p">,</span> <span class="mf">1.5e-3</span><span class="p">],</span>
    <span class="s">"lr_decay"</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="p">}</span>
<span class="n">key</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">.</span><span class="n">items</span><span class="p">())</span>
<span class="n">param_prods</span><span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="n">values</span><span class="p">))</span>

<span class="k">for</span> <span class="n">lr</span><span class="p">,</span> <span class="n">dec</span> <span class="ow">in</span> <span class="n">param_prods</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"lr={}, dec={}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">dec</span><span class="p">))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">TwoLayerNet</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    <span class="n">solver</span> <span class="o">=</span> <span class="n">Solver</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span>
                    <span class="n">update_rule</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span>
                    <span class="n">optim_config</span><span class="o">=</span><span class="p">{</span>
                        <span class="s">'learning_rate'</span><span class="p">:</span> <span class="n">lr</span><span class="p">,</span>
                    <span class="p">},</span>
                    <span class="n">lr_decay</span><span class="o">=</span><span class="n">dec</span><span class="p">,</span>
                    <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                    <span class="n">print_every</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">solver</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">solver</span><span class="p">.</span><span class="n">val_acc_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">best_accuracy</span><span class="p">:</span>
        <span class="n">best_params</span> <span class="o">=</span> <span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">dec</span><span class="p">)</span>
        <span class="n">best_model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="n">best_accuracy</span> <span class="o">=</span> <span class="n">solver</span><span class="p">.</span><span class="n">val_acc_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'best accuracy:'</span><span class="p">,</span> <span class="n">best_accuracy</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'best params:'</span><span class="p">,</span> <span class="n">best_params</span><span class="p">)</span>

<span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
################################################################################
#                              END OF YOUR CODE                                #
################################################################################
</span></code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">...</span>
<span class="n">lr</span><span class="o">=</span><span class="mf">0.0025</span><span class="p">,</span> <span class="n">dec</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">2.306032</span>
<span class="p">(</span><span class="n">Epoch</span> <span class="mi">0</span> <span class="o">/</span> <span class="mi">5</span><span class="p">)</span> <span class="n">train</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.158000</span><span class="p">;</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.155000</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">101</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.769848</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">201</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.644557</span>
<span class="p">(</span><span class="n">Epoch</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">5</span><span class="p">)</span> <span class="n">train</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.451000</span><span class="p">;</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.421000</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">301</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.427050</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">401</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.509737</span>
<span class="p">(</span><span class="n">Epoch</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">5</span><span class="p">)</span> <span class="n">train</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.505000</span><span class="p">;</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.463000</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">501</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.284518</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">601</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.325806</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">701</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.372223</span>
<span class="p">(</span><span class="n">Epoch</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">5</span><span class="p">)</span> <span class="n">train</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.514000</span><span class="p">;</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.485000</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">801</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.385698</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">901</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.349968</span>
<span class="p">(</span><span class="n">Epoch</span> <span class="mi">4</span> <span class="o">/</span> <span class="mi">5</span><span class="p">)</span> <span class="n">train</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.520000</span><span class="p">;</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.483000</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">1001</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.329035</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">1101</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.464375</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">1201</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.361397</span>
<span class="p">(</span><span class="n">Epoch</span> <span class="mi">5</span> <span class="o">/</span> <span class="mi">5</span><span class="p">)</span> <span class="n">train</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.538000</span><span class="p">;</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.485000</span>
<span class="n">lr</span><span class="o">=</span><span class="mf">0.002</span><span class="p">,</span> <span class="n">dec</span><span class="o">=</span><span class="mf">0.25</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">2.303175</span>
<span class="p">(</span><span class="n">Epoch</span> <span class="mi">0</span> <span class="o">/</span> <span class="mi">5</span><span class="p">)</span> <span class="n">train</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.133000</span><span class="p">;</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.120000</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">101</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.708662</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">201</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.590761</span>
<span class="p">(</span><span class="n">Epoch</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">5</span><span class="p">)</span> <span class="n">train</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.439000</span><span class="p">;</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.413000</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">301</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.370455</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">401</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.483202</span>
<span class="p">(</span><span class="n">Epoch</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">5</span><span class="p">)</span> <span class="n">train</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.443000</span><span class="p">;</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.481000</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">501</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.555753</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">601</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.415455</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">701</span> <span class="o">/</span> <span class="mi">1225</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.348051</span>
<span class="p">...</span>
<span class="n">best</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.485</span>
<span class="n">best</span> <span class="n">params</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.0025</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># the accuracy is still going up, train longer
</span><span class="n">model</span> <span class="o">=</span> <span class="n">best_model</span>
<span class="n">lr</span><span class="p">,</span> <span class="n">dec</span> <span class="o">=</span> <span class="n">best_params</span>
<span class="n">solver</span> <span class="o">=</span> <span class="n">Solver</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span>
                <span class="n">update_rule</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span>
                <span class="n">optim_config</span><span class="o">=</span><span class="p">{</span>
                    <span class="s">'learning_rate'</span><span class="p">:</span> <span class="n">lr</span><span class="p">,</span>
                <span class="p">},</span>
                <span class="n">lr_decay</span> <span class="o">=</span> <span class="n">dec</span><span class="p">,</span>
                <span class="n">num_epochs</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                <span class="n">print_every</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">solver</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">...</span>
<span class="p">(</span><span class="n">Epoch</span> <span class="mi">13</span> <span class="o">/</span> <span class="mi">15</span><span class="p">)</span> <span class="n">train</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.592000</span><span class="p">;</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.524000</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">3201</span> <span class="o">/</span> <span class="mi">3675</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.242333</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">3301</span> <span class="o">/</span> <span class="mi">3675</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.094588</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">3401</span> <span class="o">/</span> <span class="mi">3675</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.314383</span>
<span class="p">(</span><span class="n">Epoch</span> <span class="mi">14</span> <span class="o">/</span> <span class="mi">15</span><span class="p">)</span> <span class="n">train</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.591000</span><span class="p">;</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.524000</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">3501</span> <span class="o">/</span> <span class="mi">3675</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.141455</span>
<span class="p">(</span><span class="n">Iteration</span> <span class="mi">3601</span> <span class="o">/</span> <span class="mi">3675</span><span class="p">)</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.220166</span>
<span class="p">(</span><span class="n">Epoch</span> <span class="mi">15</span> <span class="o">/</span> <span class="mi">15</span><span class="p">)</span> <span class="n">train</span> <span class="n">acc</span><span class="p">:</span> <span class="mf">0.570000</span><span class="p">;</span> <span class="n">val_acc</span><span class="p">:</span> <span class="mf">0.524000</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_model</span> <span class="o">=</span> <span class="n">model</span>
<span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">best_model</span><span class="p">.</span><span class="n">loss</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'X_val'</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Validation set accuracy: '</span><span class="p">,</span> <span class="p">(</span><span class="n">y_val_pred</span> <span class="o">==</span> <span class="n">data</span><span class="p">[</span><span class="s">'y_val'</span><span class="p">]).</span><span class="n">mean</span><span class="p">())</span>
<span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">best_model</span><span class="p">.</span><span class="n">loss</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'X_test'</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test set accuracy: '</span><span class="p">,</span> <span class="p">(</span><span class="n">y_test_pred</span> <span class="o">==</span> <span class="n">data</span><span class="p">[</span><span class="s">'y_test'</span><span class="p">]).</span><span class="n">mean</span><span class="p">())</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Validation</span> <span class="nb">set</span> <span class="n">accuracy</span><span class="p">:</span>  <span class="mf">0.524</span>
<span class="n">Test</span> <span class="nb">set</span> <span class="n">accuracy</span><span class="p">:</span>  <span class="mf">0.518</span>
</code></pre></div></div>
<h3 id="inline-question-2"><strong>Inline Question 2</strong></h3>
<p><img src="/assets/images/cs231n_a1_3.png" alt="png" width="80%&quot;, height=&quot;80%" /><br />
Now that you have trained a Neural Network classifier, you may find that your testing accuracy is much lower than the training accuracy. In what ways can we decrease this gap? Select all that apply.</p>

<ol>
  <li>Train on a larger dataset.</li>
  <li>Add more hidden units.</li>
  <li>Increase the regularization strength.</li>
  <li>None of the above.</li>
</ol>

<p>$\color{blue}{\textit Your Answer:}$ 1, 3</p>

<p>$\color{blue}{\textit Your Explanation:}$ It happens due to the overfitting problem. By getting more data, we can generalize the model to consider much broader features from a larger dataset. Otherwise, we can increase the regularization strength to reduce the effect of weights to avoid overfitting.</p>

                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
	<!--
            
                <section class="subscribe-form">
                    <h3 class="subscribe-form-title">Subscribe to Darron's Devlog</h3>
                    <p>Get the latest posts delivered right to your inbox</p>
                    <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
                </section>
            
	-->
            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            var this_page_url = 'http://0.0.0.0:4000/cs231n_a1_q4';
                            var this_page_identifier = '/cs231n_a1_q4';
                            var this_page_title = 'Assignment 1 - Q4, Two-Layer Neural Network';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            


        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/built/images/blog-cover1.png)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; Darron's Devlog &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/cs231n/">Cs231n</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs231n_lec12">cs231n - Lecture 12. Generative Models</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs231n_a1_q5">Assignment 1 - Q5, Image Features</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/cs231n_a1_q3">Assignment 1 - Q3, Softmax</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/cs231n/">
                                
                                    See all 15 posts  â†’
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                    <article class="post-card post-template no-image">
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/cs231n_a1_q5">
                <header class="post-card-header">
                    
                        
                            
                               <span class="post-card-tags">Cs231n</span>
                            
                        
                            
                                <span class="post-card-tags">Assignment</span>
                            
                        
                    

                    <h2 class="post-card-title">Assignment 1 - Q5, Image Features</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>My solution to the assignments of the CS231n course from Stanford University(Spring 2021) You can find official courseworks and assignments here. Assignment 1 - Q5: Higher Level Representations: Image Features Train SVM on</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
            </footer>
        </div>
    </article>

            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                    <article class="post-card post-template no-image">
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/cs231n_a1_q3">
                <header class="post-card-header">
                    
                        
                            
                               <span class="post-card-tags">Cs231n</span>
                            
                        
                            
                                <span class="post-card-tags">Assignment</span>
                            
                        
                    

                    <h2 class="post-card-title">Assignment 1 - Q3, Softmax</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>My solution to the assignments of the CS231n course from Stanford University(Spring 2021) You can find official courseworks and assignments here. Assignment 1 - Q3: Softmax softmax.py: softmax_loss_naive def softmax_loss_naive(W, X, y, reg):</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="/">
            
            <span>Darron's Devlog</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">Assignment 1 - Q4, Two-Layer Neural Network</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=Assignment+1+-+Q4%2C+Two-Layer+Neural+Network&amp;url=https://12kdh43.github.io/cs231n_a1_q4"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://12kdh43.github.io/cs231n_a1_q4"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Darron's Devlog</a> &copy; 2022</section>
                <!-- 
				<section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                -->
				<nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search Darron's Devlog</h1>
                <p class="subscribe-overlay-description">
				</p>
                <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

 </script>

	
    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
