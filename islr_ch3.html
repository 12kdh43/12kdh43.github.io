<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
	
	<!-- On Post front-matter YAML, set "use_math: true" to use LaTex -->
	
	  
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
        equationNumbers: {
        autoNumber: "AMS"
        }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$', '$$'], ["\\[","\\]"]  ],
    processEscapes: true,
    }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>ISLR - Chapter 3. Linear Regression</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
	
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="" />
    <link rel="shortcut icon" href="http://0.0.0.0:4000/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="http://0.0.0.0:4000/islr_ch3" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Darron's Devlog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="ISLR - Chapter 3. Linear Regression" />
    <meta property="og:description" content="Chapter 3. Linear Regression 3.1. Simple Linear Regression 3.1.1. Estimating the Coefficients 3.1.2. Assessing the Accuracy of the Coefficient Estimates 3.1.3. Assessing the Accuracy of the Model Residual Standard Error (RSE) $R^2$ Statistics 3.2. Multiple Linear Regression 3.2.1. Estimating the Regression Coefficients Ordinary Least Squares Estimator Gauss-Markov Theorem Properties of" />
    <meta property="og:url" content="http://0.0.0.0:4000/islr_ch3" />
    <meta property="og:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2020-04-02T15:00:00+00:00" />
    <meta property="article:modified_time" content="2020-04-02T15:00:00+00:00" />
    <meta property="article:tag" content="ISLR" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="ISLR - Chapter 3. Linear Regression" />
    <meta name="twitter:description" content="Chapter 3. Linear Regression 3.1. Simple Linear Regression 3.1.1. Estimating the Coefficients 3.1.2. Assessing the Accuracy of the Coefficient Estimates 3.1.3. Assessing the Accuracy of the Model Residual Standard Error (RSE) $R^2$ Statistics 3.2. Multiple Linear Regression 3.2.1. Estimating the Regression Coefficients Ordinary Least Squares Estimator Gauss-Markov Theorem Properties of" />
    <meta name="twitter:url" content="http://0.0.0.0:4000/" />
    <meta name="twitter:image" content="http://0.0.0.0:4000/assets/built/images/blog-cover1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Darron's Devlog" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="ISLR" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Darron's Devlog",
        "logo": "http://0.0.0.0:4000/"
    },
    "url": "http://0.0.0.0:4000/islr_ch3",
    "image": {
        "@type": "ImageObject",
        "url": "http://0.0.0.0:4000/assets/built/images/blog-cover1.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://0.0.0.0:4000/islr_ch3"
    },
    "description": "Chapter 3. Linear Regression 3.1. Simple Linear Regression 3.1.1. Estimating the Coefficients 3.1.2. Assessing the Accuracy of the Coefficient Estimates 3.1.3. Assessing the Accuracy of the Model Residual Standard Error (RSE) $R^2$ Statistics 3.2. Multiple Linear Regression 3.2.1. Estimating the Regression Coefficients Ordinary Least Squares Estimator Gauss-Markov Theorem Properties of"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="ISLR - Chapter 3. Linear Regression" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="/">Darron's Devlog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-projects" role="menuitem"><a href="/tag/projects/">Projects</a></li>
    <li class="nav-studies" role="menuitem"><a href="/tag/studies/">Studies</a></li>
	<li class="nav-blog" role="menuitem"><a href="/tag/blog/">Blog</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  tag-islr  no-image">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime=" 2 April 2020"> 2 April 2020</time>
                    
                        <span class="date-divider">/</span>
                        
							
                            
                               <a href='/tag/islr/'>ISLR</a>
                            
                        
                    
                </section>
                <h1 class="post-full-title">ISLR - Chapter 3. Linear Regression</h1>
            </header>
	<!--
            
	-->
            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <ul id="markdown-toc">
  <li><a href="#chapter-3-linear-regression" id="markdown-toc-chapter-3-linear-regression">Chapter 3. Linear Regression</a></li>
  <li><a href="#31-simple-linear-regression" id="markdown-toc-31-simple-linear-regression">3.1. Simple Linear Regression</a>    <ul>
      <li><a href="#311-estimating-the-coefficients" id="markdown-toc-311-estimating-the-coefficients">3.1.1. Estimating the Coefficients</a></li>
      <li><a href="#312--assessing-the-accuracy-of-the-coefficient-estimates" id="markdown-toc-312--assessing-the-accuracy-of-the-coefficient-estimates">3.1.2.  Assessing the Accuracy of the Coefficient Estimates</a></li>
      <li><a href="#313-assessing-the-accuracy-of-the-model" id="markdown-toc-313-assessing-the-accuracy-of-the-model">3.1.3. Assessing the Accuracy of the Model</a>        <ul>
          <li><a href="#residual-standard-error-rse" id="markdown-toc-residual-standard-error-rse">Residual Standard Error (RSE)</a></li>
          <li><a href="#r2-statistics" id="markdown-toc-r2-statistics">$R^2$ Statistics</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#32-multiple-linear-regression" id="markdown-toc-32-multiple-linear-regression">3.2. Multiple Linear Regression</a>    <ul>
      <li><a href="#321-estimating-the-regression-coefficients" id="markdown-toc-321-estimating-the-regression-coefficients">3.2.1. Estimating the Regression Coefficients</a>        <ul>
          <li><a href="#ordinary-least-squares-estimator" id="markdown-toc-ordinary-least-squares-estimator">Ordinary Least Squares Estimator</a></li>
          <li><a href="#gauss-markov-theorem" id="markdown-toc-gauss-markov-theorem">Gauss-Markov Theorem</a></li>
          <li><a href="#properties-of-good-estimators" id="markdown-toc-properties-of-good-estimators"><em>Properties of Good estimators</em></a></li>
        </ul>
      </li>
      <li><a href="#322-important-questions" id="markdown-toc-322-important-questions">3.2.2. Important Questions</a>        <ul>
          <li><a href="#3221-hypothesis-test" id="markdown-toc-3221-hypothesis-test">3.2.2.1. Hypothesis Test</a></li>
          <li><a href="#3222-variable-selection" id="markdown-toc-3222-variable-selection">3.2.2.2. Variable Selection</a></li>
          <li><a href="#3223-model-fit" id="markdown-toc-3223-model-fit">3.2.2.3. Model fit</a></li>
          <li><a href="#3224-prediction-errors" id="markdown-toc-3224-prediction-errors">3.2.2.4. Prediction Errors</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#33-other-considerations-in-regression-model" id="markdown-toc-33-other-considerations-in-regression-model">3.3. Other Considerations in Regression Model</a>    <ul>
      <li><a href="#331-qualitative-predictors" id="markdown-toc-331-qualitative-predictors">3.3.1. Qualitative Predictors</a></li>
      <li><a href="#332-extensions-of-the-linear-model" id="markdown-toc-332-extensions-of-the-linear-model">3.3.2. Extensions of the Linear Model</a></li>
      <li><a href="#333-potential-problems" id="markdown-toc-333-potential-problems">3.3.3. Potential Problems</a></li>
    </ul>
  </li>
  <li><a href="#35-comparison-with-k-nearest-neighbors" id="markdown-toc-35-comparison-with-k-nearest-neighbors">3.5. Comparison with <em>K</em>-Nearest Neighbors</a></li>
</ul>

<h2 id="chapter-3-linear-regression">Chapter 3. Linear Regression</h2>
<ul>
  <li>input variable <em>X</em> can be:<br />
Quantitative or Transformations of quantitative inputs<br />
Basis expansions leading to polynomial representation<br />
Dummy coding of qualitative variables: for <em>G groups</em>, <em>G-1 dummy variables</em> required<br />
Interactions between inputs ( $X_3 = X_1 \cdot X_2$ )</li>
</ul>

<h2 id="31-simple-linear-regression">3.1. Simple Linear Regression</h2>
<ul>
  <li>
    <p>$Y \approx \beta_0 + \beta_1 X$<br />
saying <em>regressing</em> <em>Y</em> on <em>X</em>,<br />
for predicting a quantitative response <em>Y</em><br />
on the basis of a single predictor variable <em>X</em><br />
assumes a linear relationship between.</p>
  </li>
  <li>
    <p>$\beta$: model <em>coefficients</em> or <em>parameters</em><br />
this case, two unknown <em>constants</em><br />
$\beta_0$ = <em>intercept</em><br />
$\beta_1$ = <em>slope</em></p>
  </li>
</ul>

<h3 id="311-estimating-the-coefficients">3.1.1. Estimating the Coefficients</h3>
<ul>
  <li>
    <p>for given data of <em>n</em> observastions: {$(x_1,y_1,), (x_2,y_2), \ldots, (x_n,y_n)$}<br />
\(\hat{y}_i = \hat\beta_0 + \hat\beta_1 x_i\) for prediction of <em>Y</em> on <em>i</em>th value of <em>X</em>,<br />
\(e_i = y_i - \hat{y}_i\) for representation of <em>i</em>th residual,<br />
\(\bar{y} \equiv \frac{1}{n}\sum_{i=1}^n y_i\) and \(\bar{x} \equiv \frac{1}{n}\sum_{i=1}^n x_i\) are sample means</p>
  </li>
  <li>
    <p>the <em>least squares coefficient estimates</em><br />
\(\begin{align*}
RSS &amp;= e_1^2 + e_2^2 + \cdots + e_n^2 \\
    &amp;= \sum_{i=1}^n(y_i-\beta_0-\beta_1 x_i)
\end{align*}\) is a square function of each coefficients $\beta_0, \beta_1$,<br />
then \(\frac{\partial RSS}{\partial\beta_0} = -2\sum(y_i - \beta_0 - \beta_1 x_i)\)<br />
in least squares,<br />
\(\sum y_i - n\hat\beta_0 - \sum\hat\beta_1 x_i = 0\)<br />
\(\begin{align*}
\Leftrightarrow \hat\beta_0 &amp;= \frac{1}{n}\sum_{i=1}^n y_i - \frac{\hat\beta_1}{n}\sum_{i=1}^n x_i \\
                            &amp;= \bar{y} - \hat\beta_1\bar{x} \\
      \therefore\hat\beta_0 &amp;= \bar{y} - \hat\beta_1 \bar{x}  
\end{align*}\)<br />
on the other partial derivaties \(\frac{\partial RSS}{\partial\beta_1} = -2\sum x_i(y_i - \beta_0 - \beta_1 x_i)\),<br />
substituting $ \bar{y} - \hat\beta_1 \bar{x} $ for $ \hat\beta_0$:<br />
\(\sum x_i(y_i - \hat\beta_0 - \hat\beta_1 x_i) = 0 \\
  \sum x_i(y_i - \bar{y} - \hat\beta_1(x_i - \bar{x}) = 0 \\
  \sum x_i(y_i - \bar{y}) - \hat\beta_1 \sum x_i(x_i - \bar{x}) = 0 \\
  \Leftrightarrow \sum x_i(y_i - \bar{y}) = \hat\beta_1 \sum x_i(x_i - \bar{x})\)<br />
\(\begin{align*}
\therefore \hat\beta_1 &amp;= \frac{\sum x_i(y_i - \bar{y})}
                               {\sum x_i(x_i - \bar{x})} \\
 						 &amp;= \frac{\sum(x_i-\bar{x})(y_i - \bar{y})}
 								 {\sum(x_i - \bar{x})^2}
\end{align*}\)</p>
  </li>
</ul>

<h3 id="312--assessing-the-accuracy-of-the-coefficient-estimates">3.1.2.  Assessing the Accuracy of the Coefficient Estimates</h3>
<ul>
  <li>
    <p>when the <em>true</em> function of $Y = f(X) + \epsilon$,<br />
linear function $Y = \beta_0 + \beta_1 X + \epsilon$</p>
  </li>
  <li>
    <p>an <em>unbiased</em> estimation?<br />
if we use the sample mean $\hat\mu$ to estimate $\mu$<br />
in the sense that unbiased on average, we expact both are equal.<br />
for one particular set of obs., its $\hat\mu$ might not correct,<br />
but if we could average a huge number of estimates from a number of sets of obs.,<br />
this average would <em>exactly</em> equal $\mu$.<br />
by the same way, $\hat\beta_0$ and $\hat\beta_1$ are <em>unbiased</em> estimators.</p>
  </li>
  <li>
    <p>how accurate is the sample mean $\hat\mu$ as an estimate of $\mu$?<br />
for a single estimate $\hat\mu$, by computing the <em>standard error</em> $SE(\hat\mu)$:<br />
\(Var(\hat\mu) = SE(\hat\mu)^2 = \frac{\sigma^2}{n}\)</p>
  </li>
  <li>
    <p>Assume that $\epsilon_i$ have common variance $\sigma^2$ and are uncorrelated,<br />
standard errors of <em>unbiased estimators</em> in linear regression<br />
\(SE(\hat\beta_0)^2 = \sigma^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\right]\),<br />
\(SE(\hat\beta_1)^2 = \frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\)</p>
  </li>
  <li>
    <p>from data, estimation of $\sigma$ <em>residual standard error</em><br />
RSE = \(\sqrt{RSS/(n-2)}\)</p>
  </li>
  <li>
    <p>standard error and <em>confidence interval</em><br />
confidence interval is defined as a range of values such that with given probability,<br />
the range will contain the true unknown value of the parameter<br />
defined in terms of lower and upper limits computed from the sample of data<br />
95% confidence interval for $\beta_1$: \(\hat\beta_1 \pm 2 \cdot SE(\hat\beta_1)\)<br />
95% confidence interval for $\beta_0$: \(\hat\beta_0 \pm 2 \cdot SE(\hat\beta_0)\)</p>
  </li>
  <li>
    <p>Hypothesis Test<br />
<em>null hyphothesis</em> $H_0$: no relationship between <em>X</em> and <em>Y</em>; $\beta_1 = 0$<br />
<em>alternative hypothesis</em> $H_a$: some relationship between <em>X</em> and <em>Y</em>; $\beta_1 \ne 0$<br />
How far is $\hat\beta_1$ far enough from 0, we can be confident that true $\beta_1$ is non-zero?</p>
  </li>
  <li>
    <p><em>t-statistics</em><br />
\(t = \frac{\hat\beta_1 - 0}{SE(\hat\beta_1)}\)<br />
measures the number of standard deviations that $\hat\beta_1$ is away from 0</p>
  </li>
  <li>
    <p><em>p</em>-value<br />
prob. of observing any number equal to $|t|$ or larger, assuming $\beta_1$ = 0<br />
with small <em>p</em>-value, infer that there is an association between the predictor and the response,<br />
we can <em>reject the null hypothesis</em></p>
  </li>
</ul>

<h3 id="313-assessing-the-accuracy-of-the-model">3.1.3. Assessing the Accuracy of the Model</h3>

<h4 id="residual-standard-error-rse">Residual Standard Error (RSE)</h4>
<ul>
  <li>RSE = \(\sqrt{\frac{1}{n-2}RSS} = \sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i - \hat{y}_i)^2}\)<br />
an estimate of the standard deviation of \(\epsilon\)<br />
a measure of the <em>lack of fit</em> of the model to the data</li>
</ul>

<h4 id="r2-statistics">$R^2$ Statistics</h4>
<ul>
  <li>
    <p>\(R^2 = 1 - \frac{RSS}{TSS}\)<br />
TSS; the total variance in the response <em>Y</em>, amount of variability inherent<br />
RSS; amount of variability that is left unexplained after performing the regression<br />
TSS-RSS; amount of variability in the response that is explained<br />
$R^2$; <em>proportion of variability in Y that can be explained using X</em></p>
  </li>
  <li>
    <p>\(Cor(X,Y) = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i - \bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}\)<br />
is also a measure of the linear relationship between <em>X</em> and <em>Y</em><br />
we can use $r = Cor(X,Y) $ instead of $R^2$ to assess the fit of the linear model<br />
in simple linear regression, $R^2 = r^2$,<br />
squared correlation and the squared R statistic are identical.</p>
  </li>
</ul>

<h2 id="32-multiple-linear-regression">3.2. Multiple Linear Regression</h2>
<ul>
  <li>Model: <strong>$ f(X) = \beta_0 + \sum_{j=1}^p \beta_j X_j + \epsilon$</strong><br />
$\Rightarrow$ Prediction of <em>Y</em> from <em>X</em> (<em>p</em> input variables)<br />
$\quad$ <em>f</em>: Regression function $E(Y|X)$</li>
</ul>

<h3 id="321-estimating-the-regression-coefficients">3.2.1. Estimating the Regression Coefficients</h3>
<ul>
  <li>
    <p>prediction: $\hat{y} = \hat\beta_0 + \sum_{j=1}^p \hat\beta_j X_j$</p>
  </li>
  <li>
    <p>How to estimate coefficients $\beta = (\beta_0, \beta_1, \ldots , \beta_p)^T$?<br />
Least Squares $\hat\beta$: $\beta$ minimizing RSS<br />
when <em>Y</em> is <em>(n by 1)</em> / <em>X</em> is <em>(n by (p+1))</em> matrix,<br />
RSS = $ ( y - X\hat\beta)^T ( y - X\hat\beta) $,<br />
Least square estimator (LSE): $ \hat\beta = (X^T X)^{-1} X^T Y $</p>
  </li>
  <li>
    <p><em>derivation</em><br />
\(\begin{align*}
RSS &amp;= y^T y - \hat\beta^T X^T y - y^T X \hat\beta \hat\beta^T X^T X \hat\beta \\
    &amp;= y^T y - 2\hat\beta^T X^T y + \hat\beta^T X^T X \hat\beta
\end{align*}\)<br />
take the derivative with respect to $\hat\beta$ minimizing RSS,<br />
<br />
\(\frac{\partial e^T e}{\partial \hat\beta} = -2X^T y + 2X^T X\hat\beta = 0\)<br />
we get ‘normal equations’ $(X^T X)\hat\beta = X^T y$<br />
<br />
$\therefore \hat\beta = (X^T X)^{-1} X^T y $</p>
  </li>
</ul>

<h4 id="ordinary-least-squares-estimator">Ordinary Least Squares Estimator</h4>
<ul>
  <li>
    <p>With Assumption:<br />
1) $Y_i$ responses are uncorrelated and Var($Y_i$) = $\sigma^2$<br />
$\quad$($\equiv$ $\epsilon_i$ are independent and Var($\epsilon_i$) = $\sigma^2$)<br />
2) $X = (X_1, \ldots, X_p)^T$ is fixed (not random)</p>
  </li>
  <li>
    <p>By (1) &amp; (2), for OLS estimator $\hat\beta$;<br />
\(\begin{align*}
\hat\beta &amp;= (X^TX)^{-1}X^TY &amp; \cdots Y=X\beta + \epsilon \\
          &amp;= \beta + (X^TX)^{-1}X^T\epsilon
\end{align*}\)<br />
<strong>$\hat\beta$ is a linear estimator of $\beta$</strong><br />
<br />
\(\begin{align*}
E(\hat\beta) &amp;= E(\beta) + (X^TX)^{-1}X^T E(\epsilon) \\
             &amp; \quad \scriptstyle\text{ where } E[\epsilon] = 0 \\
             &amp;= \beta
\end{align*}\)<br />
so that <strong>$\hat\beta$ is an unbiased estimator of $\beta$</strong>   <br />
<br />
\(\begin{align*}
Var(\hat\beta) &amp;= E[\hat\beta - E(\hat\beta)]^2 \\
               &amp;= E[ (X^TX)^{-1}X^T\epsilon ]^2 \\
               &amp;= (X^TX)^{-1}X^TE(\epsilon^2)X(X^TX)^{-1}\\
               &amp;= \sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}\\
               &amp;=\sigma^2(X^T X)^{-1}
\end{align*}\)<br />
<br />
while $\hat\sigma^2$ as Estimator of Variance of $\epsilon$ (= $\sigma^2$):<br />
\(\begin{align*}
\hat\sigma^2 &amp;= \frac 1 {n-p-1} \sum_{i=1}^n (y_i - \hat y_i)^2 \\
             &amp;= \frac1{n-p-1}\sum_{i=1}^n \epsilon_i^2 \\
\rightarrow E(\hat\sigma^2) &amp;= \sigma^2
\end{align*}\)<br />
$\therefore \text{ unbiased estimator; }\frac{RSS}{n-p-1}$</p>
  </li>
  <li>
    <p>With Assumption:<br />
3) $\epsilon \sim ^{\text{iid}} N(0,\sigma^2)$ : normal distribution assumption of  error</p>
    <ul>
      <li>$\hat\beta\sim MVN(\beta,\sigma^2(X^TX)^{-1}), \quad (Y\sim MVN(X\beta,\sigma^2 I)$ <br />
$\frac{(n-p-1)\hat\sigma^2}{\sigma^2}\sim\chi^2_{n-p-1}$<br />
$\hat\beta$ and $\hat\sigma^2$ are independent</li>
    </ul>
  </li>
</ul>

<h4 id="gauss-markov-theorem">Gauss-Markov Theorem</h4>
<ul>
  <li>Assumption: $E(\epsilon_i) = 0, Var(\epsilon_i) = \sigma^2 &lt; \infty\text{,  } \epsilon_i$’s are independent</li>
  <li>among all linear unbiased estimator $\tilde{\beta} = Cy$ &amp; $E(\tilde{\beta}) = \beta)$<br />
Then, $Var(\hat\beta) \le Var(\tilde{\beta})$ when  LSE $\hat\beta$ = <em>BLUE</em></li>
  <li>G-M theorem says that LSE  $\hat\beta$ is <em>the best linear unbiased estimator (BLUE)</em></li>
  <li>Conversely, there may exist biased estimators with smaller MSE than LSE.</li>
</ul>

<h4 id="properties-of-good-estimators"><em>Properties of Good estimators</em></h4>
<ul>
  <li>unbiaseness</li>
  <li>efficiency(small variance)</li>
  <li>consistancy(as <em>n</em> goes infinity, estimator goes true parameter)</li>
  <li>sufficiency</li>
</ul>

<h3 id="322-important-questions">3.2.2. Important Questions</h3>

<h4 id="3221-hypothesis-test">3.2.2.1. Hypothesis Test</h4>

<ul>
  <li>
    <p>1) Test for all coefficients<br />
$H_0$: all $\beta$ coefficients are zero<br />
$H_a$: at least one $\beta_j$ is non-zero</p>
  </li>
  <li>
    <p>Test statistic: $ F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}$<br />
where TSS = $\sum_{i=1}^n(y_i-\bar y)^2$ and RSS = $\sum_{i=1}^n(y_i-\hat{y}_i)^2$</p>
  </li>
  <li>from the linear model assumptions,<br />
\(E\left[RSS/(n-p-1)\right] = \sigma^2\)
    <ul>
      <li>if $H_0$ is true, \(E\left[TSS-RSS/p\right] = \sigma^2\)<br />
F-statistic value is close to 1</li>
      <li>if $H_a$ is true, \(E\left[TSS-RSS/p\right] &gt; \sigma^2\)<br />
F-statistic value is greater than 1</li>
    </ul>
  </li>
  <li>
    <p>2) Test for a particular subset of coefficients<br />
$H_0: \beta_{p-q+1} = \beta_{p-q+2} = \cdots = \beta_{p} = 0$<br />
$H_1$: At least one of those is not zero</p>
  </li>
  <li>
    <p>Full model: $Y=\beta_0 + \beta_1X_1 + \cdots + \beta_pX_p + \epsilon$<br />
Reduced model under $H_0: Y= \beta_0  + \beta_1X_1 + \cdots + \beta_{p-q}X_{p-q} + \epsilon$</p>
  </li>
  <li>
    <p>Test statistic: F = \(\frac{(RSS_0 - RSS)/q}{RSS/(n-p-1)}\)<br />
where <em>RSS</em> is from full model and $RSS_0$ is from reduced model</p>
  </li>
  <li>
    <p><strong>Check F-test first before t-tests for each $\beta_j$</strong><br />
F-test : test for all coef, T-test: test for indiv. coeff<br />
when F-test is significant ($H_1$ for all coeff is true)<br />
$\Rightarrow$ perform t-test but still need to control $\alpha$<br />
but if F-test is not sig. $\Rightarrow$ can’t trust t-test result : all coeffs are ZERO</p>
  </li>
  <li>if $p &gt; n$, more coefficients $\beta_j$ to estimate than observastions,<br />
we cannot fit the <em>MLR</em> model using least squares<br />
and F-statistic cannot be used</li>
</ul>

<!--
##### Test for a specific coefficient  
- $H_0: \beta_j = 0$ vs . $H_1: B_j \not= 0$
- Test for partial effect of individual $\beta_j$
- $\hat\beta_j \sim N(\beta_j,\sigma^2 v_j)$, where $v_j$ is the *j* th diagonal element of $(X^TX)^{-1}$  
$\Rightarrow \frac{\hat\beta_j - \beta_j}{\sigma\sqrt{v_j}}\sim N(0,1)$  ($\leftarrow z=\frac{x-\mu}{\sigma}$)
- under $H_0$, $t_j = \frac{\hat\beta_j}{\hat\sigma\sqrt{v_j}}\sim t_{n-p-1}$
- if $ \left\vert t_j \right\vert > t_{(1-\alpha/2), n-p-1}$ , we reject $H_0$ with level of significance : $\alpha$
-->

<h4 id="3222-variable-selection">3.2.2.2. Variable Selection</h4>
<ul>
  <li>often, the response is only associated with a subset of the predictors<br />
But we can’t consider all $2^p$ models that contain subsets of <em>p</em> variables
    <ul>
      <li>subset selection, e.g. <em>Forward</em>, <em>Backward</em>, <em>Mixed selection</em></li>
    </ul>
  </li>
  <li>Criterions to judge the quality of a model
    <ul>
      <li>statistics, e.g. <em>Mallow’s $C_p$</em>, <em>Akaike information criterion</em>(AIC),<br />
<em>Bayesian information criterion</em>(BIC), <em>adjusted $R^2$</em></li>
      <li>model outputs, residuals, patterns.</li>
    </ul>
  </li>
</ul>

<h4 id="3223-model-fit">3.2.2.3. Model fit</h4>
<ul>
  <li>most common measures of model fit: RSE and $R^2$<br />
<em>SLR</em> model; $R^2$, square of the correlation of the response and the variable<br />
<em>MLR</em> model; $R^2$ = $Cor(Y,\hat{Y})^2$, correlation between response and fitted model<br />
<em>fitted</em>; model that maximizes this correlation among all possible linear model</li>
</ul>

<h4 id="3224-prediction-errors">3.2.2.4. Prediction Errors</h4>
<ol>
  <li>
    <p>Uncertainty between $\hat Y$ and $f(X)$<br />
 <em>the least squares plane</em> $\hat{Y} = X^T \hat\beta$<br />
 <em>the true population regression plane</em> $f(X) = X^T\beta$<br />
 Variation due to $\hat\beta$ (Model variance)<br />
 in ideal situation, we can drive number of training datasets from population and have several $f(x)$ and $\hat\beta$<br />
 $\scriptstyle\text{Confidence interval}$ C.I. for Y:<br />
 $E(\hat Y) = x^T\beta = f(x)$, $Var(\hat Y) = \sigma^2 x^T( X^TX)^{-1}x$<br />
 $\therefore (1-\alpha)100$% C.I. for Y = $\hat Y \pm t_{(1-\alpha/2, n-p-1)} \hat\sigma\sqrt{x^T(X^TX)^{-1}x}$</p>
  </li>
  <li>
    <p>Model bias:<br />
 caused by assuming a linear model for f(x).<br />
 $\rightarrow$ (1)(2) are reducible error</p>
  </li>
  <li>
    <p>Uncertainty between Y and $\hat Y$<br />
 random error $\epsilon$ is in the model, <em>irreducible error</em><br />
 $\scriptstyle\text{Prediction interval}$ P.I. = reducible + irreducible error<br />
 for new, unseen test obs. $x_0 = (1, x_{01}, \ldots, x_{op})^T$, $\hat Y_0 = x_0^T \beta$<br />
 $Var(\hat Y_0) =  \sigma^2 + \sigma^2 x_0^T( X^TX)^{-1}x_0$ $\scriptstyle\text{(irreducible + reducible)}$<br />
 $\therefore (1-\alpha)100$% P.I. for $Y_0$ =  $\hat Y_0 \pm t_{(1-\alpha/2, n-p-1)} \hat\sigma\sqrt{1+x^T(X^TX)^{-1}x}$</p>
  </li>
</ol>

<h2 id="33-other-considerations-in-regression-model">3.3. Other Considerations in Regression Model</h2>

<h3 id="331-qualitative-predictors">3.3.1. Qualitative Predictors</h3>
<ul>
  <li>
    <p>for <em>p</em> qualitative predictors, create <em>p-1</em> dummy variables<br />
with a baseline of the last predictor that is not included in the dummies</p>
  </li>
  <li>
    <p>various  approaches lead to equivalent model fits with different coefficients and interpretations,<br />
are designed to measure particular <em>contrasts</em></p>
  </li>
</ul>

<h3 id="332-extensions-of-the-linear-model">3.3.2. Extensions of the Linear Model</h3>
<ul>
  <li>
    <p>standard linear regression model has highly restrictive assumptions,<br />
that are often violated in practice.</p>
  </li>
  <li>
    <p>additivity: relationship between predictor $X_j$ and <em>Y</em> is independent<br />
to the values of the other predictors</p>
  </li>
  <li>
    <p>linearity: model has a <em>constant</em> slope of $X_j$</p>
  </li>
  <li>
    <p>Removing the Additive Assumption<br />
in real-world problems and it’s potential variables are not independent;<br />
it has <em>synergy</em> or <em>interaction</em> effect.<br />
by introducing <em>interaction term</em>, we can relax the additive assumption</p>
  </li>
  <li>
    <p>e.g.<br />
\(\begin{align*}
Y &amp;= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon \\
  &amp;= \beta_0 + (\beta_1 + \beta_3 X_2)X_1 + \beta_2 X_2 + \epsilon \\
  &amp;= \beta_0 + \tilde{\beta_1}X_1 + \beta_2 X_2 + \epsilon
\end{align*}\)</p>
  </li>
  <li>
    <p><em>hierarchical principle</em><br />
if we include an interaction in a model, we should also include the main effects,<br />
even if the <em>p</em>-values associated with their coefficients are not significant.</p>
  </li>
  <li>
    <p>Non-linear Relationships; e.g. <em>polynomial regression</em></p>
  </li>
</ul>

<h3 id="333-potential-problems">3.3.3. Potential Problems</h3>
<ol>
  <li>Non-linearity of the response-predictor relationships
    <ul>
      <li><em>Residual plots</em> for identifying non-linearity<br />
residuals $e_i = y_i - \hat{y}_i$ versus a predictor $x_i$, or fitted values $\hat{y}_i$</li>
    </ul>
  </li>
  <li>
    <p>Correlation of error terms<br />
 we assumed the error terms, $\epsilon_1, \epsilon_2, \ldots, \epsilon_n$ are uncorrelated<br />
 But if error terms are correlated, confidence of our model cannot be guaranteed.<br />
 correlations frequently occur in context of <em>time series</em> data,<br />
 we may see <em>tracking</em> in the residuals.</p>
  </li>
  <li>Non-constant variance of error terms<br />
 standard errors, confidence intervals and hyphothesis test in linear model<br />
 relpy upon the assumption of $Var(\epsilon_i) = \sigma^2$,<br />
 that the error terms have a constant variance.
    <ul>
      <li>one solution for this <em>heteroscedasticity</em><br />
transform using a concave function such as $log Y$ or $\sqrt{Y}$</li>
    </ul>
  </li>
  <li>
    <p>Outliers<br />
 to identify outliers, plot the <em>studentized residuals</em><br />
 computed by dividing each residual $e_i$ by its estimated standard error</p>
  </li>
  <li>High-leverage points: unusual value for $x_i$<br />
 the least squares line can be heavily affected by just a couple of observations<br />
 but there is no simple way to plot all dimensions of the data simultaneously
    <ul>
      <li><em>leverage statistic</em> $h_i$ to quantify an observation’s leverage</li>
    </ul>
  </li>
  <li>Collinearity<br />
 when two or more predictor variables are closely related to one another, it makes difficult 
 to separate out the individual effects of collinear variables. Reducing the accuracy of the 
 estimates of the regression coefficients, standard error for $\hat\beta_j$ increases and the 
 <em>t</em>-statistic decreases.<br />
 we may fail to reject $H_0: \beta_j = 0$,<br />
 the <em>power</em> of hypothesis test is reduced by collinearity.
    <ul>
      <li>Correlation matrix of the predictors</li>
      <li><em>variance inflation factor</em>(VIF) to assess multicollinearity<br />
where \(R^2_{X_j|X_{-j}}\) is from a regression of \(X_j\) onto all of the other predictors,<br />
\(X_j = \sum_{i\ne j}\beta_i X_i + e\), and \(VIF(\hat\beta_j) = \frac{1}{1-R^2_{X_j|X_{-j}}}\).<br />
having the smallest possible value of 1 if there’s no collinearity among the predictors and 
bigger values if there are more collinearity. Practically, a VIF value that exceeds 5 or 10 
indicates a problematic amount of collinearity.</li>
      <li>solution: Drop or Combine</li>
    </ul>
  </li>
</ol>

<h2 id="35-comparison-with-k-nearest-neighbors">3.5. Comparison with <em>K</em>-Nearest Neighbors</h2>
<ul>
  <li>KNN regression<br />
a <em>non-parameric</em> method: No assumption of $f(x)$<br />
\(\hat f(x_0) = \frac{1}{K}\sum_{x_i\in\mathcal{N}_0} y_i\)
    <ul>
      <li>($x_i,y_i$): training data</li>
      <li>$\mathcal N_0$ : <em>K</em> neighbors of prediction point $x_0$</li>
    </ul>
  </li>
  <li>Preferable situations to linear regression:
    <ul>
      <li>True $f(x)$ is nonlinear</li>
      <li>when Goal is Prediction rather than Inference</li>
      <li>large number of observations per predictor(or low dimensions)</li>
    </ul>
  </li>
  <li>Curse of Dimensionality<br />
to capture 10% data as neighbor space for variable X…
    <ul>
      <li>p=1 : <em>X</em> range (0,1), edge rank $e_1(0.1) = 0.1$</li>
      <li>p=2 : $X_1, X_2$ range (0,1),<br />
edge rank $e_2(0.1) = \sqrt{0.1} = 0.1^{1/2} \approx 0.316 $</li>
      <li>p=3 : $e_3(0.1) = 0.1^{1/3} \approx 0.464 $</li>
      <li>p=10 : $e_{10}(0.1) = 0.1^{1/10} \approx 0.794 $</li>
    </ul>
  </li>
  <li>
    <p>Reduction of <em>r</em> : Average using fewer obs.<br />
small <em>K</em> : higher variance of the fit, poor prediction</p>
  </li>
  <li>For the same density, when <em>p</em> = 1, <em>n</em> = 100 -&gt; when <em>p</em> = 10, <em>n</em> = $100^{10}$</li>
</ul>

                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
	<!--
            
                <section class="subscribe-form">
                    <h3 class="subscribe-form-title">Subscribe to Darron's Devlog</h3>
                    <p>Get the latest posts delivered right to your inbox</p>
                    <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
                </section>
            
	-->
            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            var this_page_url = 'http://0.0.0.0:4000/islr_ch3';
                            var this_page_identifier = '/islr_ch3';
                            var this_page_title = 'ISLR - Chapter 3. Linear Regression';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            


        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/built/images/blog-cover1.png)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; Darron's Devlog &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/islr/">Islr</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/islr_ch10">ISLR - Chapter 10. Deep Learning</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/islr_ch9">ISLR - Chapter 9. Support Vector Machines</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/islr_ch8">ISLR - Chapter 8. Tree-Based Methods</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/islr/">
                                
                                    See all 8 posts  →
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                    <article class="post-card post-template no-image">
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/islr_ch4">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Islr</span>
                            
                        
                    

                    <h2 class="post-card-title">ISLR - Chapter 4. Classification</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>
</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
            </footer>
        </div>
    </article>

            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                    <article class="post-card post-template no-image">
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/islr_ch2">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Islr</span>
                            
                        
                    

                    <h2 class="post-card-title">ISLR - Chapter 2. Statistical Learning</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>
</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="/">
            
            <span>Darron's Devlog</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">ISLR - Chapter 3. Linear Regression</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=ISLR+-+Chapter+3.+Linear+Regression&amp;url=https://12kdh43.github.io/islr_ch3"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://12kdh43.github.io/islr_ch3"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Darron's Devlog</a> &copy; 2022</section>
                <!-- 
				<section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                -->
				<nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search Darron's Devlog</h1>
                <p class="subscribe-overlay-description">
				</p>
                <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

 </script>

	
    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
